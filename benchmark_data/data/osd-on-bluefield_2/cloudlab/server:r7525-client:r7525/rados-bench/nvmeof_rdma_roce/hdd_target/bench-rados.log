[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:23:44,403775200-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 mkdir --parents /tmp/bench-rados
[1] 11:23:44 [SUCCESS] ljishen@10.10.2.1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:23:44,591615322-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 mkdir --parents /tmp/bench-rados
[1] 11:23:44 [SUCCESS] ljishen@10.10.2.2
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:23:44,791340890-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
[1] 11:23:44 [SUCCESS] ljishen@10.10.2.2


[1;7;39;49m[2021-10-28T11:23:44,980738477-07:00][RUNNING][ROUND 1/1/21] object_size=4KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:23:44,983789656-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:23:44,988890879-07:00] INFO: > Get OSD hostname[0m
## ./benchmarks/bench-rados:178 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 uname --nodename
## ./benchmarks/bench-rados:178 - launch_ceph_cluster() > tail -n1
# ./benchmarks/bench-rados:178 - launch_ceph_cluster() > OSD_HOSTNAME=node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:23:45,188096607-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:23:45,601481014-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/2044e81c-381b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 2044e81c-381b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:23:45,611795694-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:23:45,615310552-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '2044e81c-381b-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 2044e81c-381b-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:23:45,623991541-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 2044e81c-381b-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 11:23:51 [SUCCESS] 10.10.2.2\n[2] 11:23:51 [SUCCESS] 10.10.2.1\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:23:51,107152271-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:23:51,118417097-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:23:51,122150676-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:23:51,271742910-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:23:51,276723193-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:23:51,427026163-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:23:51,715645611-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:23:51,720395099-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--c5c5bf70--ade6--41d7--9fe5--941e47ebee60-osd--block--50487d4c--c0c5--4b9d--a133--87fdc44e921b (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-c5c5bf70-ade6-41d7-9fe5-941e47ebee60" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-50487d4c-c0c5-4b9d-a133-87fdc44e921b"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-c5c5bf70-ade6-41d7-9fe5-941e47ebee60" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-50487d4c-c0c5-4b9d-a133-87fdc44e921b" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-c5c5bf70-ade6-41d7-9fe5-941e47ebee60"\n'
10.10.2.1: b'  Volume group "ceph-c5c5bf70-ade6-41d7-9fe5-941e47ebee60" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:23:52,065707083-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:23:52,075622583-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:23:52,079085813-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 33ebc1b4-381c-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\n'
10.10.2.1: b'Waiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 33ebc1b4-381c-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:24:49,807404584-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:25:09,813777646-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:25:09,823842026-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:25:09,827290789-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 33ebc1b4-381c-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/33ebc1b4-381c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:25:18,488390356-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:25:18,498319430-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:25:18,501538362-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 33ebc1b4-381c-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/33ebc1b4-381c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:25:27,618351081-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:25:27,623596572-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:25:27,834071538-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:25:27,837425523-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid 33ebc1b4-381c-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/33ebc1b4-381c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:25:36,903495991-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:25:56,908628472-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:25:56,916479581-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:25:56,926420038-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:25:56,929794702-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 33ebc1b4-381c-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/33ebc1b4-381c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:26:21,171319335-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:26:41,176987915-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:26:41,186377496-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:26:41,189760406-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 33ebc1b4-381c-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/33ebc1b4-381c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     33ebc1b4-381c-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.kkuaiv(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 11:26:49 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:23:45,601481014-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/2044e81c-381b-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 2044e81c-381b-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:23:45,611795694-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:23:45,615310552-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '2044e81c-381b-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 2044e81c-381b-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:23:45,623991541-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 2044e81c-381b-11ec-b51d-53e6e728d2d3'
[1] 11:23:51 [SUCCESS] 10.10.2.2
[2] 11:23:51 [SUCCESS] 10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:23:51,107152271-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:23:51,118417097-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:23:51,122150676-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:23:51,271742910-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:23:51,276723193-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:23:51,427026163-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:23:51,715645611-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:23:51,720395099-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--c5c5bf70--ade6--41d7--9fe5--941e47ebee60-osd--block--50487d4c--c0c5--4b9d--a133--87fdc44e921b (253:0)
  Archiving volume group "ceph-c5c5bf70-ade6-41d7-9fe5-941e47ebee60" metadata (seqno 5).
  Releasing logical volume "osd-block-50487d4c-c0c5-4b9d-a133-87fdc44e921b"
  Creating volume group backup "/etc/lvm/backup/ceph-c5c5bf70-ade6-41d7-9fe5-941e47ebee60" (seqno 6).
  Logical volume "osd-block-50487d4c-c0c5-4b9d-a133-87fdc44e921b" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-c5c5bf70-ade6-41d7-9fe5-941e47ebee60"
  Volume group "ceph-c5c5bf70-ade6-41d7-9fe5-941e47ebee60" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:23:52,065707083-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:23:52,075622583-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:23:52,079085813-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 33ebc1b4-381c-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 33ebc1b4-381c-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:24:49,807404584-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:25:09,813777646-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:25:09,823842026-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:25:09,827290789-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 33ebc1b4-381c-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/33ebc1b4-381c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:25:18,488390356-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:25:18,498319430-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:25:18,501538362-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 33ebc1b4-381c-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/33ebc1b4-381c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:25:27,618351081-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:25:27,623596572-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:25:27,834071538-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:25:27,837425523-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 33ebc1b4-381c-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/33ebc1b4-381c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:25:36,903495991-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:25:56,908628472-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:25:56,916479581-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:25:56,926420038-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:25:56,929794702-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 33ebc1b4-381c-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/33ebc1b4-381c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:26:21,171319335-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:26:41,176987915-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:26:41,186377496-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:26:41,189760406-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 33ebc1b4-381c-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/33ebc1b4-381c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     33ebc1b4-381c-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.kkuaiv(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:26:49,726253465-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:26:49,733634905-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 11:26:49 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:26:50,207726281-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:26:50,210220120-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:26:50,232904907-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:26:50,235970745-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '33ebc1b4-381c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 33ebc1b4-381c-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:26:54,162063071-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:26:54,164974868-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '33ebc1b4-381c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 33ebc1b4-381c-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:26:58,095421109-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:26:58,098366489-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '33ebc1b4-381c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 33ebc1b4-381c-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:27:02,069340039-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:27:02,072635980-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '33ebc1b4-381c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 33ebc1b4-381c-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:27:10,010492722-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:27:10,013368922-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '33ebc1b4-381c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 33ebc1b4-381c-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:27:14,376783659-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:27:14,379948413-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '33ebc1b4-381c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 33ebc1b4-381c-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:27:18,943678209-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:27:18,946555491-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '33ebc1b4-381c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 33ebc1b4-381c-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:27:23,967352132-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:27:23,970445371-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '33ebc1b4-381c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 33ebc1b4-381c-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:27:28,991717008-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:27:28,994889136-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '33ebc1b4-381c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 33ebc1b4-381c-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:27:33,374790921-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:27:33,378053679-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '33ebc1b4-381c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 33ebc1b4-381c-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:27:38,424832161-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:27:38,427878321-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '33ebc1b4-381c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 33ebc1b4-381c-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/17 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 19 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:27:42,224375111-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:27:42,227259096-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '33ebc1b4-381c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 33ebc1b4-381c-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.39059         -  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default   
-3         0.39059         -  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host node-1
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0  
                       TOTAL  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:27:46,284575879-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:28:10,149032976-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:28:19,059364527-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:28:27,966898927-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:28:36,890585805-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:28:36,896594728-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:28:45,756561964-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:28:45,762957686-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:28:54,640131366-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:28:54,646518181-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:29:03,564115303-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:29:03,569952122-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:29:12,529436245-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:29:12,535451781-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:29:12,539910242-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:29:12,542593398-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:29:12,547339472-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:29:12,551390926-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=582748
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:29:12,556847288-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:29:12,565196191-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'260986\n'
[1] 11:29:12 [SUCCESS] ljishen@10.10.2.2
260986

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:29:12,756953925-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4KB_write.log.1
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:29:12,776126773-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:29:12,778941728-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '33ebc1b4-381c-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 33ebc1b4-381c-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T18:29:15.676759+0000 Maintaining 128 concurrent writes of 4096 bytes to objects of size 4096 for up to 60 seconds or 0 objects
2021-10-28T18:29:15.676770+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T18:29:15.677286+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T18:29:15.677286+0000     0       0         0         0         0         0           -           0
2021-10-28T18:29:16.677417+0000     1     128      1798      1670   6.52295   6.52344    0.116877   0.0752512
2021-10-28T18:29:17.677519+0000     2     128      3846      3718   7.26108         8   0.0756369    0.068017
2021-10-28T18:29:18.677605+0000     3     128      5772      5644   7.34831   7.52344   0.0506068   0.0678992
2021-10-28T18:29:19.677681+0000     4     128      7821      7693   7.51206   8.00391   0.0589404   0.0664176
2021-10-28T18:29:20.677774+0000     5     128      9758      9630   7.52279   7.56641   0.0751304   0.0653931
2021-10-28T18:29:21.677864+0000     6     128     11403     11275   7.33986   6.42578    0.241873   0.0675822
2021-10-28T18:29:22.677950+0000     7     128     13214     13086   7.30182   7.07422   0.0588012   0.0681241
2021-10-28T18:29:23.678064+0000     8     128     15134     15006   7.32649       7.5   0.0746633   0.0677399
2021-10-28T18:29:24.678152+0000     9     128     16798     16670   7.23459       6.5   0.0667494   0.0688395
2021-10-28T18:29:25.678245+0000    10     128     18699     18571   7.25364   7.42578    0.126094   0.0685943
2021-10-28T18:29:26.678331+0000    11     128     20619     20491   7.27598       7.5   0.0901421   0.0685181
2021-10-28T18:29:27.678445+0000    12     128     22283     22155   7.21125       6.5     0.05075   0.0691768
2021-10-28T18:29:28.678529+0000    13     128     23966     23838   7.16221   6.57422   0.0586308   0.0696202
2021-10-28T18:29:29.678608+0000    14     128     25739     25611   7.14528   6.92578   0.0860065   0.0698851
2021-10-28T18:29:30.678706+0000    15     128     27787     27659   7.20221         8   0.0887042   0.0693439
2021-10-28T18:29:31.678818+0000    16     128     29598     29470   7.19416   7.07422   0.0749953   0.0693589
2021-10-28T18:29:32.678903+0000    17     128     30878     30750   7.06507         5   0.0582202   0.0705971
2021-10-28T18:29:33.678993+0000    18     128     32926     32798   7.11697         8    0.058719   0.0701253
2021-10-28T18:29:34.679088+0000    19     128     34718     34590   7.11078         7   0.0585183   0.0701358
2021-10-28T18:29:35.679210+0000 min lat: 0.0331874 max lat: 0.2668 avg lat: 0.0700432
2021-10-28T18:29:35.679210+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T18:29:35.679210+0000    20     128     36638     36510   7.13019       7.5   0.0666132   0.0700432
2021-10-28T18:29:36.679308+0000    21     128     38539     38411   7.14423   7.42578    0.093467    0.069847
2021-10-28T18:29:37.679402+0000    22     128     40094     39966   7.09557   6.07422   0.0668307   0.0702934
2021-10-28T18:29:38.679488+0000    23     128     41886     41758   7.09139         7   0.0668776   0.0703982
2021-10-28T18:29:39.679599+0000    24     128     43531     43403   7.06363   6.42578     0.10917   0.0707205
2021-10-28T18:29:40.679678+0000    25     128     45486     45358   7.08653   7.63672   0.0616463   0.0703175
2021-10-28T18:29:41.679778+0000    26     128     47795     47667   7.16084   9.01953   0.0333512   0.0697972
2021-10-28T18:29:42.679860+0000    27     128     49716     49588   7.17352   7.50391    0.057084    0.069612
2021-10-28T18:29:43.679945+0000    28     127     51316     51189   7.14066   6.25391   0.0579995   0.0699321
2021-10-28T18:29:44.680035+0000    29     128     53172     53044   7.14427   7.24609   0.0670127   0.0699272
2021-10-28T18:29:45.680171+0000    30     128     55115     54987   7.15909   7.58984    0.133546   0.0697169
2021-10-28T18:29:46.680271+0000    31     128     56907     56779   7.15394         7    0.062865   0.0697174
2021-10-28T18:29:47.680385+0000    32     128     58955     58827   7.18035         8   0.0584831   0.0695771
2021-10-28T18:29:48.680473+0000    33     128     60363     60235   7.12941       5.5   0.0603478    0.070024
2021-10-28T18:29:49.680556+0000    34     128     62283     62155   7.14029       7.5    0.141551    0.069814
2021-10-28T18:29:50.680649+0000    35     128     64308     64180   7.16227   7.91016   0.0752363   0.0697063
2021-10-28T18:29:51.680768+0000    36     128     66251     66123   7.17412   7.58984   0.0585667   0.0696194
2021-10-28T18:29:52.680850+0000    37     128     68020     67892   7.16697   6.91016   0.0586458   0.0697357
2021-10-28T18:29:53.680938+0000    38     128     69556     69428   7.13625         6   0.0667456   0.0700435
2021-10-28T18:29:54.681022+0000    39     128     71092     70964    7.1071         6   0.0672055   0.0701977
2021-10-28T18:29:55.681138+0000 min lat: 0.0329617 max lat: 0.275706 avg lat: 0.070013
2021-10-28T18:29:55.681138+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T18:29:55.681138+0000    40     128     73163     73035   7.13165   8.08984   0.0664308    0.070013
2021-10-28T18:29:56.681227+0000    41     128     75083     74955   7.14062       7.5   0.0615823   0.0699452
2021-10-28T18:29:57.681318+0000    42     128     77003     76875   7.14916       7.5   0.0668011   0.0698654
2021-10-28T18:29:58.681404+0000    43     128     78923     78795    7.1573       7.5    0.108499   0.0697817
2021-10-28T18:29:59.681481+0000    44     128     80814     80686    7.1625   7.38672     0.05915   0.0697747
2021-10-28T18:30:00.681574+0000    45     128     82734     82606   7.16999       7.5    0.058951   0.0697049
2021-10-28T18:30:01.681666+0000    46     127     84585     84458   7.17137   7.23438     0.17597   0.0695347
2021-10-28T18:30:02.681765+0000    47     128     86475     86347   7.17577   7.37891   0.0669405   0.0695291
2021-10-28T18:30:03.681877+0000    48     128     88523     88395   7.19292         8    0.065543    0.069445
2021-10-28T18:30:04.681967+0000    49     128     90414     90286   7.19687   7.38672   0.0872927   0.0694116
2021-10-28T18:30:05.682067+0000    50     128     92078     91950   7.18292       6.5   0.0504799   0.0695914
2021-10-28T18:30:06.682151+0000    51     128     93870     93742   7.17932         7   0.0583624   0.0696036
2021-10-28T18:30:07.682264+0000    52     128     95819     95691   7.18765   7.61328   0.0660438   0.0695109
2021-10-28T18:30:08.682346+0000    53     128     97867     97739   7.20296         8    0.125589   0.0693165
2021-10-28T18:30:09.682421+0000    54     128     99886     99758   7.21561   7.88672   0.0501413   0.0692564
2021-10-28T18:30:10.682509+0000    55     128    101451    101323   7.19556   6.11328   0.0592874   0.0694464
2021-10-28T18:30:11.682625+0000    56     128    103627    103499   7.21884       8.5    0.050122   0.0692234
2021-10-28T18:30:12.682706+0000    57     128    105547    105419   7.22376       7.5   0.0696143    0.069132
2021-10-28T18:30:13.682795+0000    58     128    106699    106571   7.17679       4.5    0.158617   0.0694749
2021-10-28T18:30:14.682876+0000    59     128    107467    107339   7.10599         3    0.158706   0.0702419
2021-10-28T18:30:15.683005+0000 min lat: 0.0329617 max lat: 0.299053 avg lat: 0.0709489
2021-10-28T18:30:15.683005+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T18:30:15.683005+0000    60     128    108206    108078   7.03566   2.88672    0.283278   0.0709489
2021-10-28T18:30:16.683129+0000 Total time run:         60.1899
Total writes made:      108206
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     7.02244
Stddev Bandwidth:       1.11247
Max bandwidth (MB/sec): 9.01953
Min bandwidth (MB/sec): 2.88672
Average IOPS:           1797
Stddev IOPS:            284.793
Max IOPS:               2309
Min IOPS:               739
Average Latency(s):     0.0711582
Stddev Latency(s):      0.0304648
Max latency(s):         0.303107
Min latency(s):         0.0329617

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:30:17,420423604-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:30:17,424750177-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 582748

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:30:17,429142143-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 260986
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:30:17,436354473-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 260986
[1] 11:30:17 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:30:17,620791142-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4KB_write.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4KB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:30:17,768313249-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:30:41,642504801-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 108.21k objects, 423 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:30:41,648453090-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:30:50,692675622-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 108.21k objects, 423 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:30:50,698714792-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:30:59,424816927-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 108.21k objects, 423 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:30:59,431615177-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:31:08,283851135-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 108.21k objects, 423 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:31:08,290208365-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:31:17,232033823-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 108.21k objects, 423 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:31:17,238024923-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:31:17,242684483-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:31:17,245518193-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:31:17,250499399-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:31:17,254872250-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=585099
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:31:17,261004395-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:31:17,269439920-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'261759\n'
[1] 11:31:17 [SUCCESS] ljishen@10.10.2.2
261759

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:31:17,457335056-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4KB_seq.log.1
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:31:17,476941672-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:31:17,479726760-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '33ebc1b4-381c-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 33ebc1b4-381c-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T18:31:20.382007+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T18:31:20.382007+0000     0       0         0         0         0         0           -           0
2021-10-28T18:31:21.382122+0000     1     128      4994      4866   19.0043   19.0078   0.0382876   0.0255111
2021-10-28T18:31:22.382236+0000     2     128      9629      9501   18.5539   18.1055   0.0205224   0.0267433
2021-10-28T18:31:23.382349+0000     3     128     14739     14611   19.0221   19.9609   0.0355536   0.0261196
2021-10-28T18:31:24.382461+0000     4     128     19427     19299   18.8442   18.3125   0.0228138   0.0263397
2021-10-28T18:31:25.382537+0000     5     128     24184     24056   18.7915    18.582   0.0336517   0.0264138
2021-10-28T18:31:26.382649+0000     6     128     28621     28493   18.5479    17.332  0.00723405   0.0269133
2021-10-28T18:31:27.382763+0000     7     128     33847     33719   18.8142   20.4141   0.0732979   0.0264419
2021-10-28T18:31:28.382882+0000     8     128     38970     38842   18.9636   20.0117   0.0185877   0.0263166
2021-10-28T18:31:29.382958+0000     9     128     43071     42943   18.6363   16.0195   0.0521227   0.0267431
2021-10-28T18:31:30.383068+0000    10     128     48915     48787   19.0553   22.8281   0.0156871   0.0260664
2021-10-28T18:31:31.383182+0000    11     128     53533     53405   18.9627   18.0391  0.00442342   0.0263321
2021-10-28T18:31:32.383294+0000    12     128     58035     57907   18.8478   17.5859    0.110356   0.0264724
2021-10-28T18:31:33.383366+0000    13     128     62192     62064    18.647   16.2383   0.0351316   0.0267567
2021-10-28T18:31:34.383474+0000    14     128     66850     66722   18.6146   18.1953  0.00605929   0.0268424
2021-10-28T18:31:35.383582+0000    15     128     71791     71663   18.6602   19.3008   0.0249811   0.0267613
2021-10-28T18:31:36.383693+0000    16     128     77401     77273   18.8634   21.9141    0.084615   0.0264355
2021-10-28T18:31:37.383772+0000    17     128     82640     82512   18.9575   20.4648   0.0165063    0.026319
2021-10-28T18:31:38.383883+0000    18     128     87481     87353   18.9548   18.9102     0.05385   0.0263123
2021-10-28T18:31:39.383994+0000    19     128     92841     92713    19.059   20.9375   0.0286263   0.0260974
2021-10-28T18:31:40.384108+0000 min lat: 0.000218141 max lat: 0.299354 avg lat: 0.0263
2021-10-28T18:31:40.384108+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T18:31:40.384108+0000    20     128     97287     97159   18.9743   17.3672 0.000771123      0.0263
2021-10-28T18:31:41.384197+0000    21     128    101735    101607   18.8981    17.375   0.0410353   0.0263847
2021-10-28T18:31:42.384311+0000    22     128    106812    106684   18.9404    19.832   0.0195655   0.0263758
2021-10-28T18:31:43.384439+0000 Total time run:       22.2236
Total reads made:     108206
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   19.0194
Average IOPS:         4868
Stddev IOPS:          444.557
Max IOPS:             5844
Min IOPS:             4101
Average Latency(s):   0.0262779
Max latency(s):       0.299354
Min latency(s):       0.000218141

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:31:43,926538359-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:31:43,931396032-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 585099

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:31:43,936352161-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 261759
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:31:43,943664150-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 261759
[1] 11:31:44 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:31:44,128873073-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4KB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4KB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:31:44,281729382-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:32:08,140066490-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 108.21k objects, 423 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:32:08,146731138-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:32:17,097392357-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 108.21k objects, 423 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:32:17,104529035-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:32:25,922700025-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 108.21k objects, 423 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:32:25,929599145-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:32:34,799286339-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 108.21k objects, 423 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:32:34,806060364-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:32:43,706095163-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 108.21k objects, 423 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:32:43,712858568-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:32:43,718124180-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T11:32:43,720317894-07:00][RUNNING][ROUND 2/1/21] object_size=4KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:32:43,723407426-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:32:43,732664099-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:32:44,201148151-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/33ebc1b4-381c-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 33ebc1b4-381c-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:32:44,212726215-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:32:44,216834619-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '33ebc1b4-381c-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 33ebc1b4-381c-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:32:44,225751712-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 33ebc1b4-381c-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 11:32:49 [SUCCESS] 10.10.2.2\n[2] 11:32:49 [SUCCESS] 10.10.2.1\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:32:49,736577162-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:32:49,747003542-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:32:49,751698498-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:32:49,899344733-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:32:49,903896801-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:32:50,055402041-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:32:50,344010396-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:32:50,349077632-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--6b488dda--83e8--4b3f--9266--02a7b4d8c144-osd--block--f95475ee--fbb4--4a6d--a329--0019ee2519d0 (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-6b488dda-83e8-4b3f-9266-02a7b4d8c144" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-f95475ee-fbb4-4a6d-a329-0019ee2519d0"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-6b488dda-83e8-4b3f-9266-02a7b4d8c144" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-f95475ee-fbb4-4a6d-a329-0019ee2519d0" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-6b488dda-83e8-4b3f-9266-02a7b4d8c144"\n'
10.10.2.1: b'  Volume group "ceph-6b488dda-83e8-4b3f-9266-02a7b4d8c144" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:32:50,674091346-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:32:50,683794827-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:32:50,687364417-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\nVerifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\nHost looks OK\n'
10.10.2.1: b'Cluster fsid: 74f416ce-381d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\nAdding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 74f416ce-381d-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\nPlease consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:33:49,584559074-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:34:09,591571221-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:34:09,601850325-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:34:09,605760024-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 74f416ce-381d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/74f416ce-381d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:34:18,244761287-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:34:18,254441234-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:34:18,257866002-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 74f416ce-381d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/74f416ce-381d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:34:27,258302756-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:34:27,264371725-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:34:27,473667002-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:34:27,477145472-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid 74f416ce-381d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/74f416ce-381d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:34:36,540713393-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:34:56,545320772-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:34:56,552055483-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:34:56,561544821-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:34:56,565221473-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 74f416ce-381d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/74f416ce-381d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:35:21,575365153-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:35:41,580634394-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:35:41,590522293-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:35:41,594221076-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 74f416ce-381d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/74f416ce-381d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     74f416ce-381d-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.sylgkz(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 11:35:49 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:32:44,201148151-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/33ebc1b4-381c-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 33ebc1b4-381c-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:32:44,212726215-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:32:44,216834619-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '33ebc1b4-381c-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 33ebc1b4-381c-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:32:44,225751712-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 33ebc1b4-381c-11ec-b51d-53e6e728d2d3'
[1] 11:32:49 [SUCCESS] 10.10.2.2
[2] 11:32:49 [SUCCESS] 10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:32:49,736577162-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:32:49,747003542-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:32:49,751698498-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:32:49,899344733-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:32:49,903896801-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:32:50,055402041-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:32:50,344010396-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:32:50,349077632-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--6b488dda--83e8--4b3f--9266--02a7b4d8c144-osd--block--f95475ee--fbb4--4a6d--a329--0019ee2519d0 (253:0)
  Archiving volume group "ceph-6b488dda-83e8-4b3f-9266-02a7b4d8c144" metadata (seqno 5).
  Releasing logical volume "osd-block-f95475ee-fbb4-4a6d-a329-0019ee2519d0"
  Creating volume group backup "/etc/lvm/backup/ceph-6b488dda-83e8-4b3f-9266-02a7b4d8c144" (seqno 6).
  Logical volume "osd-block-f95475ee-fbb4-4a6d-a329-0019ee2519d0" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-6b488dda-83e8-4b3f-9266-02a7b4d8c144"
  Volume group "ceph-6b488dda-83e8-4b3f-9266-02a7b4d8c144" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:32:50,674091346-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:32:50,683794827-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:32:50,687364417-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 74f416ce-381d-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 74f416ce-381d-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:33:49,584559074-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:34:09,591571221-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:34:09,601850325-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:34:09,605760024-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 74f416ce-381d-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/74f416ce-381d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:34:18,244761287-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:34:18,254441234-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:34:18,257866002-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 74f416ce-381d-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/74f416ce-381d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:34:27,258302756-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:34:27,264371725-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:34:27,473667002-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:34:27,477145472-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 74f416ce-381d-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/74f416ce-381d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:34:36,540713393-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:34:56,545320772-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:34:56,552055483-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:34:56,561544821-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:34:56,565221473-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 74f416ce-381d-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/74f416ce-381d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:35:21,575365153-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:35:41,580634394-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:35:41,590522293-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:35:41,594221076-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 74f416ce-381d-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/74f416ce-381d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     74f416ce-381d-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.sylgkz(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:35:49,936743173-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:35:49,943929956-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 11:35:50 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:35:50,412653488-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:35:50,415616762-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:35:50,437270665-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:35:50,440214863-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '74f416ce-381d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 74f416ce-381d-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:35:54,436548903-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:35:54,439472612-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '74f416ce-381d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 74f416ce-381d-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:35:58,430576587-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:35:58,433520895-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '74f416ce-381d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 74f416ce-381d-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:36:02,314093787-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:36:02,317434863-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '74f416ce-381d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 74f416ce-381d-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:36:09,983594157-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:36:09,986633234-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '74f416ce-381d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 74f416ce-381d-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:36:14,842363971-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:36:14,845314140-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '74f416ce-381d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 74f416ce-381d-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:36:19,236618543-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:36:19,239728012-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '74f416ce-381d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 74f416ce-381d-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:36:23,935331199-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:36:23,938416813-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '74f416ce-381d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 74f416ce-381d-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:36:28,197527113-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:36:28,200710070-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '74f416ce-381d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 74f416ce-381d-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:36:32,500172789-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:36:32,503349946-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '74f416ce-381d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 74f416ce-381d-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:36:36,791091657-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:36:36,794347743-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '74f416ce-381d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 74f416ce-381d-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 20 lfor 0/0/17 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 19 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:36:40,734293376-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:36:40,737143547-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '74f416ce-381d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 74f416ce-381d-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default   
-3         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host node-1
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0  
                       TOTAL  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:36:44,529405478-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:37:08,511099430-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:37:17,523188032-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:37:26,505536603-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:37:35,369336474-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:37:35,376189056-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:37:44,267695887-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:37:44,274281547-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:37:53,166711498-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:37:53,173649041-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:38:02,182368494-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:38:02,188863123-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:38:11,080536978-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:38:11,087301526-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:38:11,092536811-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:38:11,095865093-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:38:11,101314591-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:38:11,106018826-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=593078
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:38:11,112090167-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:38:11,120584804-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'267558\n'
[1] 11:38:11 [SUCCESS] ljishen@10.10.2.2
267558

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:38:11,309986708-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4KB_write.log.2
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:38:11,329657215-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:38:11,332731888-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '74f416ce-381d-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 74f416ce-381d-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T18:38:14.312124+0000 Maintaining 128 concurrent writes of 4096 bytes to objects of size 4096 for up to 60 seconds or 0 objects
2021-10-28T18:38:14.312134+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T18:38:14.312672+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T18:38:14.312672+0000     0       0         0         0         0         0           -           0
2021-10-28T18:38:15.312775+0000     1     128      1939      1811   7.07381   7.07422   0.0500673   0.0670905
2021-10-28T18:38:16.312852+0000     2     128      3603      3475   6.78665       6.5   0.0666463   0.0724061
2021-10-28T18:38:17.312926+0000     3     128      5651      5523    7.1909         8   0.0583398    0.068741
2021-10-28T18:38:18.313001+0000     4     128      7557      7429   7.25437   7.44531   0.0953074   0.0686494
2021-10-28T18:38:19.313081+0000     5     128      9477      9349   7.30337       7.5   0.0669207   0.0682455
2021-10-28T18:38:20.313165+0000     6     128     11411     11283   7.34515   7.55469   0.0670227   0.0675906
2021-10-28T18:38:21.313247+0000     7     128     12947     12819   7.15292         6    0.108829   0.0686418
2021-10-28T18:38:22.313326+0000     8     128     14981     14853   7.25189   7.94531   0.0588671    0.068816
2021-10-28T18:38:23.313401+0000     9     128     16659     16531   7.17437   6.55469   0.0665015   0.0693032
2021-10-28T18:38:24.313477+0000    10     127     18691     18564   7.25101   7.94141   0.0669596   0.0689042
2021-10-28T18:38:25.313549+0000    11     128     20630     20502   7.27999   7.57031    0.119828   0.0683259
2021-10-28T18:38:26.313618+0000    12     128     22564     22436   7.30284   7.55469     0.06561   0.0684004
2021-10-28T18:38:27.313688+0000    13     128     24492     24364   7.32037   7.53125   0.0999588   0.0681138
2021-10-28T18:38:28.313753+0000    14     128     26447     26319   7.34293   7.63672   0.0665808   0.0679687
2021-10-28T18:38:29.313830+0000    15     128     28239     28111   7.32003         7    0.116851   0.0680355
2021-10-28T18:38:30.313911+0000    16     128     30159     30031   7.33124       7.5   0.0833036   0.0679414
2021-10-28T18:38:31.313987+0000    17     128     32172     32044    7.3625   7.86328   0.0582094    0.067838
2021-10-28T18:38:32.314060+0000    18     128     33359     33231   7.21105   4.63672    0.056884   0.0691704
2021-10-28T18:38:33.314133+0000    19     128     35279     35151   7.22623       7.5   0.0589248   0.0690355
2021-10-28T18:38:34.314213+0000 min lat: 0.0329767 max lat: 0.402375 avg lat: 0.0690619
2021-10-28T18:38:34.314213+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T18:38:34.314213+0000    20     128     37164     37036   7.23305   7.36328   0.0667202   0.0690619
2021-10-28T18:38:35.314306+0000    21     128     39119     38991   7.25224   7.63672   0.0585142   0.0688622
2021-10-28T18:38:36.314375+0000    22     128     40876     40748   7.23454   6.86328   0.0667511   0.0690557
2021-10-28T18:38:37.314447+0000    23     128     42447     42319   7.18679   6.13672    0.161709    0.069264
2021-10-28T18:38:38.314520+0000    24     128     44204     44076   7.17329   6.86328    0.067608   0.0696157
2021-10-28T18:38:39.314598+0000    25     128     46031     45903    7.1718   7.13672    0.116409   0.0694339
2021-10-28T18:38:40.314682+0000    26     128     48207     48079   7.22286       8.5   0.0582435   0.0691707
2021-10-28T18:38:41.314761+0000    27     127     50249     50122    7.2509   7.98047   0.0508995   0.0689019
2021-10-28T18:38:42.314838+0000    28     127     52210     52083   7.26549   7.66016    0.050266   0.0687437
2021-10-28T18:38:43.314913+0000    29     128     53839     53711   7.23423   6.35938   0.0841756   0.0689091
2021-10-28T18:38:44.314987+0000    30     128     55852     55724   7.25518   7.86328   0.0589544   0.0688789
2021-10-28T18:38:45.315065+0000    31     128     57807     57679   7.26747   7.63672   0.0587964   0.0687261
2021-10-28T18:38:46.315148+0000    32     128     59727     59599   7.27472       7.5   0.0664793   0.0686136
2021-10-28T18:38:47.315228+0000    33     128     61647     61519   7.28152       7.5     0.16714   0.0684278
2021-10-28T18:38:48.315306+0000    34     128     63439     63311   7.27323         7   0.0669676   0.0686673
2021-10-28T18:38:49.315384+0000    35     128     65487     65359   7.29398         8   0.0752745   0.0684358
2021-10-28T18:38:50.315470+0000    36     128     67535     67407   7.31357         8   0.0586532   0.0682821
2021-10-28T18:38:51.315565+0000    37     128     69455     69327   7.31859       7.5    0.157619   0.0681827
2021-10-28T18:38:52.315645+0000    38     128     71375     71247   7.32335       7.5   0.0669057   0.0682276
2021-10-28T18:38:53.315723+0000    39     128     73423     73295   7.34068         8   0.0843655   0.0679582
2021-10-28T18:38:54.315798+0000 min lat: 0.0329767 max lat: 0.402375 avg lat: 0.0679517
2021-10-28T18:38:54.315798+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T18:38:54.315798+0000    40     128     75087     74959   7.31965       6.5    0.118124   0.0679517
2021-10-28T18:38:55.315880+0000    41     128     77135     77007   7.33623         8   0.0583692   0.0681006
2021-10-28T18:38:56.315958+0000    42     128     79055     78927   7.34011       7.5   0.0585649    0.068055
2021-10-28T18:38:57.316034+0000    43     128     80940     80812   7.34064   7.36328    0.159663   0.0680375
2021-10-28T18:38:58.316103+0000    44     128     83023     82895   7.35872   8.13672   0.0580297   0.0679168
2021-10-28T18:38:59.316172+0000    45     128     84908     84780   7.35881   7.36328   0.0585293    0.067923
2021-10-28T18:39:00.316258+0000    46     128     86444     86316   7.32926         6    0.192886     0.06817
2021-10-28T18:39:01.316366+0000    47     128     88399     88271   7.33578   7.63672   0.0669917   0.0680636
2021-10-28T18:39:02.316445+0000    48     128     90156     90028   7.32593   6.86328    0.125323   0.0682013
2021-10-28T18:39:03.316527+0000    49     128     91983     91855   7.32206   7.13672   0.0585702   0.0681997
2021-10-28T18:39:04.316610+0000    50     128     93903     93775    7.3256       7.5   0.0584837   0.0682162
2021-10-28T18:39:05.316682+0000    51     128     95916     95788   7.33613   7.86328   0.0588569   0.0681347
2021-10-28T18:39:06.316755+0000    52     128     97580     97452   7.32004       6.5   0.0589566   0.0682769
2021-10-28T18:39:07.316830+0000    53     127     99466     99339     7.321   7.37109    0.143264   0.0682173
2021-10-28T18:39:08.316900+0000    54     128    101455    101327   7.32922   7.76562   0.0502266   0.0681875
2021-10-28T18:39:09.316966+0000    55     128    103375    103247   7.33232       7.5   0.0673214   0.0681278
2021-10-28T18:39:10.317052+0000    56     128    105388    105260   7.34179   7.86328   0.0583611   0.0680655
2021-10-28T18:39:11.317172+0000    57     128    106447    106319   7.28555   4.13672    0.188489   0.0684416
2021-10-28T18:39:12.317248+0000    58     128    107308    107180   7.21792   3.36328    0.158026   0.0691852
2021-10-28T18:39:13.317328+0000    59     128    108076    107948   7.14642         3    0.141619   0.0698545
2021-10-28T18:39:14.317406+0000 min lat: 0.0329767 max lat: 0.402375 avg lat: 0.0705953
2021-10-28T18:39:14.317406+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T18:39:14.317406+0000    60     128    108751    108623   7.07126   2.63672    0.150463   0.0705953
2021-10-28T18:39:15.317520+0000 Total time run:         60.0938
Total writes made:      108751
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     7.06909
Stddev Bandwidth:       1.21644
Max bandwidth (MB/sec): 8.5
Min bandwidth (MB/sec): 2.63672
Average IOPS:           1809
Stddev IOPS:            311.408
Max IOPS:               2176
Min IOPS:               675
Average Latency(s):     0.0706779
Stddev Latency(s):      0.0302021
Max latency(s):         0.402375
Min latency(s):         0.0329767

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:39:15,919500802-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:39:15,924814325-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 593078

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:39:15,929799529-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 267558
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:39:15,937130223-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 267558
[1] 11:39:16 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:39:16,120995003-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4KB_write.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4KB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:39:16,269067908-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:39:40,048204077-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 108.75k objects, 425 MiB
    usage:   2.4 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:39:40,054848558-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:39:48,914369466-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 108.75k objects, 425 MiB
    usage:   2.4 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:39:48,921677988-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:39:57,912186796-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 108.75k objects, 425 MiB
    usage:   2.4 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:39:57,918957194-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:40:06,937353954-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 108.75k objects, 425 MiB
    usage:   2.4 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:40:06,944132908-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:40:15,850668730-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 108.75k objects, 425 MiB
    usage:   2.4 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:40:15,857645557-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:40:15,862635079-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:40:15,865868232-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:40:15,871803195-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:40:15,876621194-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=595450
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:40:15,882789578-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:40:15,891213000-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'268330\n'
[1] 11:40:16 [SUCCESS] ljishen@10.10.2.2
268330

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:40:16,082646113-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4KB_seq.log.2
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:40:16,101976599-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:40:16,104859572-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '74f416ce-381d-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 74f416ce-381d-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T18:40:19.149536+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T18:40:19.149536+0000     0       0         0         0         0         0           -           0
2021-10-28T18:40:20.149691+0000     1     128      3894      3766   14.7077   14.7109   0.0293708   0.0323167
2021-10-28T18:40:21.149780+0000     2     127      9420      9293   18.1476   21.5898   0.0344089   0.0273305
2021-10-28T18:40:22.149891+0000     3     128     14685     14557   18.9518   20.5625   0.0058817   0.0262291
2021-10-28T18:40:23.150001+0000     4     128     19373     19245   18.7914   18.3125   0.0242505   0.0262503
2021-10-28T18:40:24.150114+0000     5     128     24043     23915   18.6812   18.2422  0.00328487   0.0266774
2021-10-28T18:40:25.150191+0000     6     128     28376     28248   18.3884   16.9258    0.202894   0.0267377
2021-10-28T18:40:26.150307+0000     7     128     33853     33725   18.8175   21.3945   0.0516175   0.0264097
2021-10-28T18:40:27.150423+0000     8     128     39308     39180   19.1286   21.3086 0.000738331   0.0259862
2021-10-28T18:40:28.150533+0000     9     128     44102     43974   19.0837   18.7266   0.0326258   0.0261092
2021-10-28T18:40:29.150606+0000    10     127     49308     49181   19.2091   20.3398   0.0195639   0.0259646
2021-10-28T18:40:30.150717+0000    11     128     53973     53845   19.1189   18.2188   0.0160969   0.0260704
2021-10-28T18:40:31.150835+0000    12     128     58090     57962   18.8657    16.082   0.0238243   0.0261455
2021-10-28T18:40:32.150945+0000    13     128     64196     64068    19.249   23.8516    0.090307   0.0259097
2021-10-28T18:40:33.151016+0000    14     128     68946     68818   19.1993   18.5547   0.0110772   0.0260061
2021-10-28T18:40:34.151100+0000    15     128     73838     73710   19.1932   19.1094 0.000321275   0.0260052
2021-10-28T18:40:35.151215+0000    16     128     79097     78969   19.2774    20.543    0.027991   0.0259103
2021-10-28T18:40:36.151326+0000    17     128     83923     83795   19.2523   18.8516  0.00145957   0.0257962
2021-10-28T18:40:37.151401+0000    18     128     88956     88828   19.2748   19.6602   0.0443469   0.0259061
2021-10-28T18:40:38.151510+0000    19     128     94505     94377   19.4011   21.6758   0.0188296    0.025722
2021-10-28T18:40:39.151622+0000 min lat: 0.000220235 max lat: 0.267068 avg lat: 0.0257442
2021-10-28T18:40:39.151622+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T18:40:39.151622+0000    20     128     99437     99309   19.3942   19.2656   0.0161977   0.0257442
2021-10-28T18:40:40.151748+0000    21     128    104196    104068   19.3558   18.5898   0.0345511   0.0258111
2021-10-28T18:40:41.151833+0000    22     128    108607    108479   19.2591   17.2305   0.0193999   0.0259408
2021-10-28T18:40:42.151967+0000 Total time run:       22.0864
Total reads made:     108751
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   19.234
Average IOPS:         4923
Stddev IOPS:          531.64
Max IOPS:             6106
Min IOPS:             3766
Average Latency(s):   0.0259525
Max latency(s):       0.267068
Min latency(s):       0.000220235

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:40:42,728576493-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:40:42,733772765-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 595450

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:40:42,738730397-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 268330
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:40:42,746078734-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 268330
[1] 11:40:42 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:40:42,933107627-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4KB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4KB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:40:43,085606462-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:41:06,948028163-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 108.75k objects, 425 MiB
    usage:   2.4 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:41:06,955706321-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:41:15,828623753-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 108.75k objects, 425 MiB
    usage:   2.4 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:41:15,835879737-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:41:24,737424634-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 108.75k objects, 425 MiB
    usage:   2.4 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:41:24,744545853-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:41:33,694349819-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 108.75k objects, 425 MiB
    usage:   2.4 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:41:33,701557841-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:41:42,623395709-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 108.75k objects, 425 MiB
    usage:   2.4 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:41:42,630304147-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:41:42,635444603-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T11:41:42,637512410-07:00][RUNNING][ROUND 3/1/21] object_size=4KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:41:42,641163980-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:41:42,651317473-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:41:43,100381055-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/74f416ce-381d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 74f416ce-381d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:41:43,112035654-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:41:43,115569768-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '74f416ce-381d-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 74f416ce-381d-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:41:43,123825088-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 74f416ce-381d-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 11:41:48 [SUCCESS] 10.10.2.2\n[2] 11:41:48 [SUCCESS] 10.10.2.1\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:41:48,701211511-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:41:48,712910303-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:41:48,717768015-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:41:48,867206166-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:41:48,871981183-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:41:49,026512528-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:41:49,319637829-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:41:49,324707260-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--c44d7da3--f515--4749--956b--50a2dbc29bf0-osd--block--f19afa3d--e10c--4e0e--9c18--4030de0a366f (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-c44d7da3-f515-4749-956b-50a2dbc29bf0" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-f19afa3d-e10c-4e0e-9c18-4030de0a366f"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-c44d7da3-f515-4749-956b-50a2dbc29bf0" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-f19afa3d-e10c-4e0e-9c18-4030de0a366f" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-c44d7da3-f515-4749-956b-50a2dbc29bf0"\n'
10.10.2.1: b'  Volume group "ceph-c44d7da3-f515-4749-956b-50a2dbc29bf0" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:41:49,677491594-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:41:49,687428224-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:41:49,691021159-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\npodman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: b639bba6-381e-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\nExtracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\nAdding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid b639bba6-381e-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\nPlease consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:42:48,878495847-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:43:08,885516960-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:43:08,895082944-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:43:08,898545073-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid b639bba6-381e-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/b639bba6-381e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:43:17,756103676-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:43:17,765770749-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:43:17,769514547-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid b639bba6-381e-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/b639bba6-381e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:43:27,224203632-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:43:27,230496853-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:43:27,447100790-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:43:27,450570492-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid b639bba6-381e-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/b639bba6-381e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:43:36,354542235-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:43:56,359426373-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:43:56,365685469-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:43:56,376395072-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:43:56,379976685-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid b639bba6-381e-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/b639bba6-381e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:44:20,807822576-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:44:40,812854675-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:44:40,822814007-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:44:40,826621034-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid b639bba6-381e-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/b639bba6-381e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     b639bba6-381e-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.yyygow(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 11:44:49 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:41:43,100381055-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/74f416ce-381d-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 74f416ce-381d-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:41:43,112035654-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:41:43,115569768-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '74f416ce-381d-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 74f416ce-381d-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:41:43,123825088-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 74f416ce-381d-11ec-b51d-53e6e728d2d3'
[1] 11:41:48 [SUCCESS] 10.10.2.2
[2] 11:41:48 [SUCCESS] 10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:41:48,701211511-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:41:48,712910303-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:41:48,717768015-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:41:48,867206166-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:41:48,871981183-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:41:49,026512528-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:41:49,319637829-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:41:49,324707260-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--c44d7da3--f515--4749--956b--50a2dbc29bf0-osd--block--f19afa3d--e10c--4e0e--9c18--4030de0a366f (253:0)
  Archiving volume group "ceph-c44d7da3-f515-4749-956b-50a2dbc29bf0" metadata (seqno 5).
  Releasing logical volume "osd-block-f19afa3d-e10c-4e0e-9c18-4030de0a366f"
  Creating volume group backup "/etc/lvm/backup/ceph-c44d7da3-f515-4749-956b-50a2dbc29bf0" (seqno 6).
  Logical volume "osd-block-f19afa3d-e10c-4e0e-9c18-4030de0a366f" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-c44d7da3-f515-4749-956b-50a2dbc29bf0"
  Volume group "ceph-c44d7da3-f515-4749-956b-50a2dbc29bf0" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:41:49,677491594-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:41:49,687428224-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:41:49,691021159-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: b639bba6-381e-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid b639bba6-381e-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:42:48,878495847-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:43:08,885516960-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:43:08,895082944-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:43:08,898545073-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid b639bba6-381e-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/b639bba6-381e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:43:17,756103676-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:43:17,765770749-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:43:17,769514547-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid b639bba6-381e-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/b639bba6-381e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:43:27,224203632-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:43:27,230496853-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:43:27,447100790-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:43:27,450570492-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid b639bba6-381e-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/b639bba6-381e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:43:36,354542235-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:43:56,359426373-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:43:56,365685469-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:43:56,376395072-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:43:56,379976685-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid b639bba6-381e-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/b639bba6-381e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:44:20,807822576-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:44:40,812854675-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:44:40,822814007-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:44:40,826621034-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid b639bba6-381e-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/b639bba6-381e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     b639bba6-381e-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.yyygow(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:44:49,157208812-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:44:49,164704367-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 11:44:49 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:44:49,640664976-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:44:49,643765118-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:44:49,665210720-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:44:49,668098722-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b639bba6-381e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b639bba6-381e-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:44:53,581283629-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:44:53,584265057-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b639bba6-381e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b639bba6-381e-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:44:57,582933867-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:44:57,585971260-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b639bba6-381e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b639bba6-381e-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:45:01,599138551-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:45:01,602147141-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b639bba6-381e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b639bba6-381e-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:45:09,316668248-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:45:09,319762088-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b639bba6-381e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b639bba6-381e-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:45:13,583817927-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:45:13,586827709-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b639bba6-381e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b639bba6-381e-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:45:18,583221189-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:45:18,586169314-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b639bba6-381e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b639bba6-381e-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:45:22,776808670-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:45:22,779758459-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b639bba6-381e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b639bba6-381e-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:45:27,473626126-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:45:27,476682656-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b639bba6-381e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b639bba6-381e-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:45:31,940354567-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:45:31,943259962-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b639bba6-381e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b639bba6-381e-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:45:36,638298345-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:45:36,641278992-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b639bba6-381e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b639bba6-381e-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/17 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 19 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:45:40,640459697-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:45:40,643348330-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b639bba6-381e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b639bba6-381e-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.39059         -  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default   
-3         0.39059         -  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host node-1
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0  
                       TOTAL  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:45:44,466119743-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:46:08,505412208-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:46:17,502555420-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:46:26,594113655-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:46:35,457891216-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:46:35,464805135-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:46:44,499316011-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:46:44,506161921-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:46:53,547159121-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:46:53,553872221-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:47:02,470870930-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:47:02,477540759-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:47:11,503096390-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:47:11,509655971-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:47:11,514599086-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:47:11,517967784-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:47:11,523933756-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:47:11,528871380-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=604652
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:47:11,535127258-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:47:11,543590096-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'274228\n'
[1] 11:47:11 [SUCCESS] ljishen@10.10.2.2
274228

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:47:11,734496175-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4KB_write.log.3
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:47:11,754439025-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:47:11,757286320-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b639bba6-381e-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b639bba6-381e-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T18:47:14.606934+0000 Maintaining 128 concurrent writes of 4096 bytes to objects of size 4096 for up to 60 seconds or 0 objects
2021-10-28T18:47:14.606948+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T18:47:14.607496+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T18:47:14.607496+0000     0       0         0         0         0         0           -           0
2021-10-28T18:47:15.607639+0000     1     128      1886      1758   6.86651   6.86719   0.0585835   0.0706697
2021-10-28T18:47:16.607763+0000     2     128      3591      3463   6.76292   6.66016    0.125302   0.0709306
2021-10-28T18:47:17.607856+0000     3     128      5598      5470   7.12165   7.83984    0.075041   0.0698415
2021-10-28T18:47:18.607973+0000     4     128      7390      7262   7.09103         7   0.0669413   0.0699423
2021-10-28T18:47:19.608091+0000     5     128      9223      9095   7.10469   7.16016   0.0748755   0.0700281
2021-10-28T18:47:20.608205+0000     6     128     10590     10462   6.81044   5.33984    0.059553   0.0729544
2021-10-28T18:47:21.608332+0000     7     128     12382     12254    6.8374         7   0.0724391   0.0726594
2021-10-28T18:47:22.608453+0000     8     127     14406     14279   6.97137   7.91016   0.0593602   0.0715155
2021-10-28T18:47:23.608574+0000     9     128     16094     15966   6.92889   6.58984   0.0587903   0.0719246
2021-10-28T18:47:24.608697+0000    10     128     17927     17799   6.95193   7.16016   0.0585378   0.0716065
2021-10-28T18:47:25.608770+0000    11     128     19550     19422   6.89625   6.33984   0.0749655   0.0722379
2021-10-28T18:47:26.608891+0000    12     128     21255     21127    6.8765   6.66016   0.0583616   0.0723459
2021-10-28T18:47:27.609013+0000    13     128     23006     22878   6.87362   6.83984   0.0671192   0.0726335
2021-10-28T18:47:28.609135+0000    14     128     24798     24670   6.88259         7   0.0668903   0.0725057
2021-10-28T18:47:29.609236+0000    15     128     26462     26334   6.85704       6.5   0.0671112   0.0727881
2021-10-28T18:47:30.609323+0000    16     128     27870     27742   6.77219       5.5   0.0662981   0.0730057
2021-10-28T18:47:31.609472+0000    17     128     29575     29447   6.76555   6.66016   0.0836053   0.0737511
2021-10-28T18:47:32.609589+0000    18     128     31239     31111   6.75075       6.5   0.0671096   0.0739695
2021-10-28T18:47:33.609681+0000    19     128     32990     32862   6.75541   6.83984   0.0669939   0.0738529
2021-10-28T18:47:34.609797+0000 min lat: 0.0327498 max lat: 0.317345 avg lat: 0.0737616
2021-10-28T18:47:34.609797+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T18:47:34.609797+0000    20     128     34782     34654    6.7676         7   0.0669181   0.0737616
2021-10-28T18:47:35.609884+0000    21     128     36487     36359   6.76245   6.66016   0.0849696   0.0736474
2021-10-28T18:47:36.610011+0000    22     128     37895     37767   6.70504       5.5    0.066683   0.0744159
2021-10-28T18:47:37.610106+0000    23     128     39687     39559   6.71783         7   0.0752091   0.0743102
2021-10-28T18:47:38.610225+0000    24     128     41438     41310   6.72288   6.83984    0.067434   0.0743132
2021-10-28T18:47:39.610343+0000    25     128     43143     43015   6.72034   6.66016   0.0665103   0.0743293
2021-10-28T18:47:40.610447+0000    26     128     44935     44807   6.73107         7   0.0502327   0.0741445
2021-10-28T18:47:41.610560+0000    27     128     46599     46471   6.72248       6.5   0.0669787   0.0742773
2021-10-28T18:47:42.610678+0000    28     128     48263     48135   6.71451       6.5   0.0582793   0.0743411
2021-10-28T18:47:43.610794+0000    29     128     50183     50055   6.74157       7.5   0.0670175   0.0740787
2021-10-28T18:47:44.610914+0000    30     128     51847     51719   6.73349       6.5   0.0672692   0.0741304
2021-10-28T18:47:45.610985+0000    31     128     53767     53639    6.7582       7.5    0.058227   0.0739199
2021-10-28T18:47:46.611091+0000    32     128     55559     55431   6.76573         7   0.0661123   0.0737842
2021-10-28T18:47:47.611219+0000    33     128     57054     56926   6.73765   5.83984   0.0664252   0.0741112
2021-10-28T18:47:48.611343+0000    34     128     58503     58375   6.70594   5.66016   0.0835223    0.074469
2021-10-28T18:47:49.611420+0000    35     128     60423     60295   6.72861       7.5   0.0673081   0.0742417
2021-10-28T18:47:50.611497+0000    36     128     62174     62046   6.73168   6.83984   0.0665653   0.0742123
2021-10-28T18:47:51.611598+0000    37     127     63970     63843   6.73944   7.01953   0.0653628   0.0741093
2021-10-28T18:47:52.611715+0000    38     128     65246     65118   6.69314   4.98047   0.0671193   0.0746365
2021-10-28T18:47:53.611804+0000    39     128     67079     66951    6.7051   7.16016   0.0667253    0.074486
2021-10-28T18:47:54.611920+0000 min lat: 0.0327498 max lat: 0.317345 avg lat: 0.0742835
2021-10-28T18:47:54.611920+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T18:47:54.611920+0000    40     128     68999     68871   6.72495       7.5   0.0581168   0.0742835
2021-10-28T18:47:55.612007+0000    41     128     70795     70667   6.73202   7.01562   0.0751323   0.0741637
2021-10-28T18:47:56.612095+0000    42     128     72542     72414    6.7342   6.82422     0.06716   0.0741719
2021-10-28T18:47:57.612185+0000    43     128     74247     74119   6.73247   6.66016   0.0670029     0.07416
2021-10-28T18:47:58.612283+0000    44     128     75870     75742   6.72353   6.33984   0.0675351   0.0743133
2021-10-28T18:47:59.612396+0000    45     128     77790     77662   6.74077       7.5   0.0588675   0.0741136
2021-10-28T18:48:00.612515+0000    46     128     79623     79495   6.74986   7.16016   0.0581861   0.0740218
2021-10-28T18:48:01.612623+0000    47     128     81415     81287   6.75517         7   0.0669686   0.0739587
2021-10-28T18:48:02.612781+0000    48     128     83079     82951   6.74983       6.5   0.0665902   0.0740338
2021-10-28T18:48:03.612863+0000    49     128     84615     84487   6.73452         6   0.0584752    0.074214
2021-10-28T18:48:04.612950+0000    50     128     86366     86238   6.73661   6.83984   0.0756165   0.0741551
2021-10-28T18:48:05.613021+0000    51     128     88158     88030   6.74177         7   0.0586266   0.0741024
2021-10-28T18:48:06.613098+0000    52     128     89991     89863    6.7498   7.16016   0.0585394    0.074045
2021-10-28T18:48:07.613169+0000    53     128     91742     91614   6.75149   6.83984   0.0757235   0.0740113
2021-10-28T18:48:08.613247+0000    54     128     93319     93191   6.74053   6.16016   0.0669315   0.0741423
2021-10-28T18:48:09.613327+0000    55     128     94942     94814   6.73324   6.33984    0.058808   0.0742112
2021-10-28T18:48:10.613411+0000    56     128     96903     96775   6.74978   7.66016   0.0672298   0.0740059
2021-10-28T18:48:11.613491+0000    57     128     98782     98654   6.76012   7.33984   0.0669642   0.0739227
2021-10-28T18:48:12.613564+0000    58     128    100359    100231   6.74977   6.16016   0.0751766   0.0740453
2021-10-28T18:48:13.613635+0000    59     128    102366    102238   6.76824   7.83984   0.0673411   0.0738411
2021-10-28T18:48:14.613709+0000 min lat: 0.0327498 max lat: 0.317345 avg lat: 0.0737919
2021-10-28T18:48:14.613709+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T18:48:14.613709+0000    60     128    104158    104030   6.77209         7    0.067954   0.0737919
2021-10-28T18:48:15.613876+0000 Total time run:         60.0359
Total writes made:      104158
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     6.77706
Stddev Bandwidth:       0.608265
Max bandwidth (MB/sec): 7.91016
Min bandwidth (MB/sec): 4.98047
Average IOPS:           1734
Stddev IOPS:            155.716
Max IOPS:               2025
Min IOPS:               1275
Average Latency(s):     0.0737687
Stddev Latency(s):      0.025428
Max latency(s):         0.317345
Min latency(s):         0.0327498

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:48:16,259117163-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:48:16,264172569-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 604652

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:48:16,269311904-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 274228
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:48:16,276882060-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 274228
[1] 11:48:16 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:48:16,461850450-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4KB_write.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4KB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:48:16,608979526-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:48:40,445543875-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 104.16k objects, 407 MiB
    usage:   1.4 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:48:40,452139564-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:48:49,434987394-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 104.16k objects, 407 MiB
    usage:   1.4 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:48:49,442300845-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:48:58,318297729-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 104.16k objects, 407 MiB
    usage:   1.4 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:48:58,325131407-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:49:07,335193308-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 104.16k objects, 407 MiB
    usage:   1.4 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:49:07,342061761-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:49:16,303903152-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 104.16k objects, 407 MiB
    usage:   1.4 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:49:16,311222585-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:49:16,316847114-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:49:16,320429084-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:49:16,326391460-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:49:16,331376354-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=607114
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:49:16,338104773-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:49:16,346926618-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'274996\n'
[1] 11:49:16 [SUCCESS] ljishen@10.10.2.2
274996

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:49:16,533781456-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4KB_seq.log.3
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:49:16,553648494-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:49:16,556602922-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b639bba6-381e-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b639bba6-381e-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T18:49:19.536482+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T18:49:19.536482+0000     0       0         0         0         0         0           -           0
2021-10-28T18:49:20.536615+0000     1     128      5333      5205   20.3279    20.332     0.00253    0.023712
2021-10-28T18:49:21.536708+0000     2     128     10699     10571   20.6434   20.9609  0.00246312   0.0238609
2021-10-28T18:49:22.536821+0000     3     128     15334     15206   19.7968   18.1055   0.0174239   0.0249895
2021-10-28T18:49:23.536891+0000     4     128     21231     21103   20.6059   23.0352  0.00852308   0.0240567
2021-10-28T18:49:24.537009+0000     5     128     26925     26797   20.9326   22.2422   0.0117905    0.023789
2021-10-28T18:49:25.537088+0000     6     128     31308     31180   20.2972   17.1211   0.0265688   0.0245541
2021-10-28T18:49:26.537205+0000     7     128     35498     35370   19.7355   16.3672    0.038929   0.0252487
2021-10-28T18:49:27.537319+0000     8     128     41589     41461   20.2423    23.793   0.0160754   0.0246365
2021-10-28T18:49:28.537448+0000     9     128     46745     46617   20.2307   20.1406   0.0710563   0.0246341
2021-10-28T18:49:29.537547+0000    10     128     52668     52540   20.5211   23.1367  0.00605599   0.0243187
2021-10-28T18:49:30.537665+0000    11     128     56687     56559   20.0826   15.6992   0.0208101     0.02485
2021-10-28T18:49:31.537783+0000    12     128     62922     62794   20.4384   24.3555   0.0456001   0.0244248
2021-10-28T18:49:32.537897+0000    13     128     67627     67499   20.2798   18.3789   0.0566342   0.0245774
2021-10-28T18:49:33.537976+0000    14     128     72304     72176   20.1361   18.2695   0.0306403   0.0248032
2021-10-28T18:49:34.538092+0000    15     128     77477     77349   20.1407    20.207   0.0065313   0.0247783
2021-10-28T18:49:35.538211+0000    16     128     82879     82751   20.2006   21.1016  0.00418776   0.0247262
2021-10-28T18:49:36.538327+0000    17     128     87162     87034   19.9964   16.7305  0.00646511    0.024945
2021-10-28T18:49:37.538408+0000    18     128     92812     92684   20.1115   22.0703   0.0488619   0.0248245
2021-10-28T18:49:38.538520+0000    19     128     97635     97507   20.0444   18.8398   0.0134011   0.0249075
2021-10-28T18:49:39.538632+0000 min lat: 0.000193956 max lat: 0.301308 avg lat: 0.0249628
2021-10-28T18:49:39.538632+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T18:49:39.538632+0000    20     128    102600    102472   20.0118   19.3945   0.0293559   0.0249628
2021-10-28T18:49:40.538794+0000 Total time run:       20.2897
Total reads made:     104158
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   20.0529
Average IOPS:         5133
Stddev IOPS:          658.262
Max IOPS:             6235
Min IOPS:             4019
Average Latency(s):   0.024909
Max latency(s):       0.301308
Min latency(s):       0.000193956

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:49:41,186336606-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:49:41,191380762-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 607114

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:49:41,196414768-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 274996
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:49:41,204095161-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 274996
[1] 11:49:41 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:49:41,388996158-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4KB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4KB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:49:41,541472532-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:50:05,434582620-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 104.16k objects, 407 MiB
    usage:   1.4 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:50:05,441902754-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:50:14,406564945-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 104.16k objects, 407 MiB
    usage:   1.4 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:50:14,413651509-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:50:23,298756538-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 104.16k objects, 407 MiB
    usage:   1.4 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:50:23,306109093-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:50:32,232011682-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 104.16k objects, 407 MiB
    usage:   1.4 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:50:32,239117052-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:50:41,169060354-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 104.16k objects, 407 MiB
    usage:   1.4 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:50:41,176047251-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:50:41,181531996-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T11:50:41,184715526-07:00][RUNNING][ROUND 1/2/21] object_size=16KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:50:41,187810057-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:50:41,197003081-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:50:41,535755227-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/b639bba6-381e-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid b639bba6-381e-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:50:41,547446856-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:50:41,551213827-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'b639bba6-381e-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid b639bba6-381e-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:50:41,560288617-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid b639bba6-381e-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 11:50:46 [SUCCESS] 10.10.2.2\n[2] 11:50:47 [SUCCESS] 10.10.2.1\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:50:47,179132912-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:50:47,190892769-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:50:47,195486545-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:50:47,347558776-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:50:47,352212004-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:50:47,506750712-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:50:47,799858768-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:50:47,804812119-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--8da36fdf--d503--4a5b--8094--9692ae2aaa11-osd--block--44421fef--ad45--47ad--9843--2ed282dd7ab5 (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-8da36fdf-d503-4a5b-8094-9692ae2aaa11" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-44421fef-ad45-47ad-9843-2ed282dd7ab5"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-8da36fdf-d503-4a5b-8094-9692ae2aaa11" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-44421fef-ad45-47ad-9843-2ed282dd7ab5" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-8da36fdf-d503-4a5b-8094-9692ae2aaa11"\n'
10.10.2.1: b'  Volume group "ceph-8da36fdf-d503-4a5b-8094-9692ae2aaa11" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:50:48,149532411-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:50:48,159232707-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:50:48,162858523-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\nRepeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\nlvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\nHost looks OK\n'
10.10.2.1: b'Cluster fsid: f72e07c4-381f-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\nWaiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\nAdding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid f72e07c4-381f-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:51:48,400697783-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:52:08,407180250-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:52:08,416853636-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:52:08,420803832-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid f72e07c4-381f-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/f72e07c4-381f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:52:17,639697666-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:52:17,649556329-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:52:17,653217522-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid f72e07c4-381f-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/f72e07c4-381f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:52:26,322137556-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:52:26,328866306-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:52:26,546458749-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:52:26,549631924-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid f72e07c4-381f-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/f72e07c4-381f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:52:35,476845694-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:52:55,481350860-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:52:55,487749579-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:52:55,497154089-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:52:55,500861869-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid f72e07c4-381f-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/f72e07c4-381f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:53:19,436442054-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:53:39,441797436-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:53:39,451592981-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:53:39,455074696-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid f72e07c4-381f-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/f72e07c4-381f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     f72e07c4-381f-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.wuldun(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 39s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 11:53:47 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:50:41,535755227-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/b639bba6-381e-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid b639bba6-381e-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:50:41,547446856-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:50:41,551213827-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'b639bba6-381e-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid b639bba6-381e-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:50:41,560288617-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid b639bba6-381e-11ec-b51d-53e6e728d2d3'
[1] 11:50:46 [SUCCESS] 10.10.2.2
[2] 11:50:47 [SUCCESS] 10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:50:47,179132912-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:50:47,190892769-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:50:47,195486545-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:50:47,347558776-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:50:47,352212004-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:50:47,506750712-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:50:47,799858768-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:50:47,804812119-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--8da36fdf--d503--4a5b--8094--9692ae2aaa11-osd--block--44421fef--ad45--47ad--9843--2ed282dd7ab5 (253:0)
  Archiving volume group "ceph-8da36fdf-d503-4a5b-8094-9692ae2aaa11" metadata (seqno 5).
  Releasing logical volume "osd-block-44421fef-ad45-47ad-9843-2ed282dd7ab5"
  Creating volume group backup "/etc/lvm/backup/ceph-8da36fdf-d503-4a5b-8094-9692ae2aaa11" (seqno 6).
  Logical volume "osd-block-44421fef-ad45-47ad-9843-2ed282dd7ab5" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-8da36fdf-d503-4a5b-8094-9692ae2aaa11"
  Volume group "ceph-8da36fdf-d503-4a5b-8094-9692ae2aaa11" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:50:48,149532411-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:50:48,159232707-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:50:48,162858523-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: f72e07c4-381f-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid f72e07c4-381f-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:51:48,400697783-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:52:08,407180250-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:52:08,416853636-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:52:08,420803832-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid f72e07c4-381f-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/f72e07c4-381f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:52:17,639697666-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:52:17,649556329-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:52:17,653217522-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid f72e07c4-381f-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/f72e07c4-381f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:52:26,322137556-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:52:26,328866306-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:52:26,546458749-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:52:26,549631924-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid f72e07c4-381f-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/f72e07c4-381f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:52:35,476845694-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:52:55,481350860-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:52:55,487749579-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:52:55,497154089-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:52:55,500861869-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid f72e07c4-381f-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/f72e07c4-381f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:53:19,436442054-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:53:39,441797436-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:53:39,451592981-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:53:39,455074696-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid f72e07c4-381f-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/f72e07c4-381f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     f72e07c4-381f-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.wuldun(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 39s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:53:47,744098648-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:53:47,751712085-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 11:53:47 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:53:48,228909509-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:53:48,232101073-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:53:48,253780176-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:53:48,256580182-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f72e07c4-381f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f72e07c4-381f-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:53:52,247595960-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:53:52,250527814-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f72e07c4-381f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f72e07c4-381f-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:53:56,334306698-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:53:56,337370722-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f72e07c4-381f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f72e07c4-381f-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:54:00,398459819-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:54:00,401510107-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f72e07c4-381f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f72e07c4-381f-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:54:08,435318300-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:54:08,438336889-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f72e07c4-381f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f72e07c4-381f-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:54:12,840094649-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:54:12,843000063-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f72e07c4-381f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f72e07c4-381f-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:54:17,274434766-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:54:17,277495133-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f72e07c4-381f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f72e07c4-381f-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:54:21,800589472-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:54:21,803574257-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f72e07c4-381f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f72e07c4-381f-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:54:26,792229411-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:54:26,795305177-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f72e07c4-381f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f72e07c4-381f-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:54:31,106134513-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:54:31,109172518-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f72e07c4-381f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f72e07c4-381f-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:54:35,831379244-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:54:35,834381221-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f72e07c4-381f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f72e07c4-381f-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/17 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 19 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:54:40,006835276-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:54:40,010106701-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f72e07c4-381f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f72e07c4-381f-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.39059         -  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default   
-3         0.39059         -  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host node-1
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0  
                       TOTAL  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:54:44,006776421-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:55:07,979862320-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:55:16,986165271-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:55:25,884968414-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:55:34,766559932-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:55:34,773635916-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:55:43,815807116-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:55:43,822769897-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:55:52,715950743-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:55:52,723136755-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:56:01,586821966-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:56:01,593743580-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:56:10,426749527-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:56:10,433680438-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:56:10,438630687-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:56:10,441743062-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:56:10,447746896-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:56:10,452532213-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=615228
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:56:10,459128073-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:56:10,468153220-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'280846\n'
[1] 11:56:10 [SUCCESS] ljishen@10.10.2.2
280846

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:56:10,657964269-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16KB_write.log.1
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:56:10,677861905-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:56:10,680898126-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f72e07c4-381f-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f72e07c4-381f-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T18:56:13.585087+0000 Maintaining 128 concurrent writes of 16384 bytes to objects of size 16384 for up to 60 seconds or 0 objects
2021-10-28T18:56:13.585097+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T18:56:13.586099+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T18:56:13.586099+0000     0       0         0         0         0         0           -           0
2021-10-28T18:56:14.586211+0000     1     128      1410      1282   20.0302   20.0312   0.0920166   0.0953501
2021-10-28T18:56:15.586289+0000     2     128      2562      2434   19.0144        18   0.0996884     0.10281
2021-10-28T18:56:16.586363+0000     3     128      3586      3458   18.0092        16    0.108036    0.108837
2021-10-28T18:56:17.586438+0000     4     128      4610      4482   17.5066        16    0.116971    0.112047
2021-10-28T18:56:18.586516+0000     5     128      5599      5471   17.0957   15.4531   0.0918004    0.115718
2021-10-28T18:56:19.586600+0000     6     128      6751      6623   17.2461        18    0.108292    0.115037
2021-10-28T18:56:20.586678+0000     7     128      7810      7682   17.1461   16.5469   0.0915355      0.1156
2021-10-28T18:56:21.586766+0000     8     128      8799      8671   16.9343   15.4531    0.117523     0.11736
2021-10-28T18:56:22.586884+0000     9     128     10079      9951   17.2747        20    0.091343    0.115475
2021-10-28T18:56:23.586995+0000    10     128     11103     10975    17.147        16    0.100382    0.116364
2021-10-28T18:56:24.587105+0000    11     128     12127     11999   17.0426        16    0.141639    0.116736
2021-10-28T18:56:25.587185+0000    12     128     13186     13058   17.0012   16.5469    0.100058    0.116941
2021-10-28T18:56:26.587294+0000    13     128     14210     14082    16.924        16    0.108213    0.117773
2021-10-28T18:56:27.587404+0000    14     128     15234     15106   16.8579        16     0.10837    0.117912
2021-10-28T18:56:28.587513+0000    15     128     16351     16223   16.8974   17.4531    0.108497    0.117941
2021-10-28T18:56:29.587596+0000    16     128     17410     17282   16.8754   16.5469    0.117864     0.11735
2021-10-28T18:56:30.587707+0000    17     128     18143     18015   16.5564   11.4531    0.108301    0.120618
2021-10-28T18:56:31.587821+0000    18     128     19202     19074   16.5558   16.5469    0.108342    0.120218
2021-10-28T18:56:32.587934+0000    19     128     20191     20063   16.4976   15.4531    0.116855    0.120845
2021-10-28T18:56:33.588015+0000 min lat: 0.0401323 max lat: 0.407628 avg lat: 0.121164
2021-10-28T18:56:33.588015+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T18:56:33.588015+0000    20     128     21215     21087   16.4727        16    0.100199    0.121164
2021-10-28T18:56:34.588139+0000    21     128     22367     22239   16.5453        18    0.108225    0.120548
2021-10-28T18:56:35.588247+0000    22     128     23135     23007   16.3386        12    0.108028    0.122101
2021-10-28T18:56:36.588357+0000    23     128     24322     24194   16.4346   18.5469     0.10009     0.12136
2021-10-28T18:56:37.588436+0000    24     128     25346     25218   16.4164        16    0.141726    0.121285
2021-10-28T18:56:38.588551+0000    25     128     26591     26463   16.5378   19.4531    0.100688    0.120753
2021-10-28T18:56:39.588666+0000    26     128     27522     27394   16.4612   14.5469    0.117001    0.121187
2021-10-28T18:56:40.588782+0000    27     128     28639     28511   16.4978   17.4531    0.107863    0.121125
2021-10-28T18:56:41.588867+0000    28     128     29570     29442   16.4281   14.5469      0.1086     0.12152
2021-10-28T18:56:42.588978+0000    29     128     30722     30594   16.4822        18    0.125262    0.121105
2021-10-28T18:56:43.589090+0000    30     128     31327     31199   16.2479   9.45312    0.359307    0.122677
2021-10-28T18:56:44.589169+0000    31     128     31711     31583   15.9173         6    0.345204    0.125072
2021-10-28T18:56:45.589253+0000    32     127     32009     31882   15.5659   4.67188    0.481155    0.126942
2021-10-28T18:56:46.589367+0000    33     128     32386     32258   15.2722     5.875    0.260385    0.130124
2021-10-28T18:56:47.589438+0000    34     128     32770     32642   14.9995         6    0.318066    0.132784
2021-10-28T18:56:48.589549+0000    35     128     33119     32991   14.7267   5.45312    0.348324    0.135192
2021-10-28T18:56:49.589629+0000    36     128     33503     33375   14.4843         6    0.273624    0.137744
2021-10-28T18:56:50.589745+0000    37     128     33794     33666   14.2157   4.54688    0.483888    0.139927
2021-10-28T18:56:51.589859+0000    38     128     34178     34050   13.9995         6    0.373082    0.142226
2021-10-28T18:56:52.589971+0000    39     128     34434     34306    13.743         4    0.508927    0.144784
2021-10-28T18:56:53.590054+0000 min lat: 0.0401323 max lat: 0.516406 avg lat: 0.1472
2021-10-28T18:56:53.590054+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T18:56:53.590054+0000    40     128     34818     34690   13.5495         6    0.322683      0.1472
2021-10-28T18:56:54.590170+0000    41     128     35202     35074   13.3653         6    0.362985    0.149259
2021-10-28T18:56:55.590278+0000    42     128     35458     35330   13.1423         4     0.45564    0.150859
2021-10-28T18:56:56.590389+0000    43     128     35935     35807     13.01   7.45312    0.370417    0.153402
2021-10-28T18:56:57.590470+0000    44     128     36191     36063   12.8052         4    0.560269    0.155595
2021-10-28T18:56:58.590579+0000    45     128     36575     36447    12.654         6    0.351731    0.157764
2021-10-28T18:56:59.590688+0000    46     128     36866     36738   12.4777   4.54688     0.53314    0.159731
2021-10-28T18:57:00.590800+0000    47     128     37215     37087   12.3282   5.45312    0.278625    0.161469
2021-10-28T18:57:01.590882+0000    48     128     37634     37506   12.2078   6.54688     0.32199    0.163431
2021-10-28T18:57:02.590992+0000    49     128     38111     37983   12.1107   7.45312    0.148764    0.165036
2021-10-28T18:57:03.591102+0000    50     128     39042     38914   12.1594   14.5469    0.091801    0.164324
2021-10-28T18:57:04.591216+0000    51     128     40066     39938   12.2347        16    0.108388    0.163325
2021-10-28T18:57:05.591301+0000    52     128     40962     40834   12.2686        14   0.0999615    0.162544
2021-10-28T18:57:06.591416+0000    53     128     42114     41986   12.3767        18   0.0996466    0.161381
2021-10-28T18:57:07.591530+0000    54     128     43138     43010   12.4438        16    0.108836    0.160472
2021-10-28T18:57:08.591640+0000    55     128     44383     44255   12.5712   19.4531    0.116691    0.158952
2021-10-28T18:57:09.591724+0000    56     128     45407     45279   12.6324        16    0.116487    0.158257
2021-10-28T18:57:10.591839+0000    57     128     46338     46210    12.666   14.5469    0.137917    0.157736
2021-10-28T18:57:11.591951+0000    58     128     47455     47327   12.7485   17.4531    0.125115     0.15673
2021-10-28T18:57:12.592067+0000    59     128     48514     48386   12.8128   16.5469    0.100147    0.155937
2021-10-28T18:57:13.592149+0000 min lat: 0.0401323 max lat: 0.560897 avg lat: 0.15532
2021-10-28T18:57:13.592149+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T18:57:13.592149+0000    60     128     49538     49410   12.8659        16    0.108482     0.15532
2021-10-28T18:57:14.592313+0000 Total time run:         60.0845
Total writes made:      49538
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     12.8824
Stddev Bandwidth:       5.33226
Max bandwidth (MB/sec): 20.0312
Min bandwidth (MB/sec): 4
Average IOPS:           824
Stddev IOPS:            341.265
Max IOPS:               1282
Min IOPS:               256
Average Latency(s):     0.155201
Stddev Latency(s):      0.0955356
Max latency(s):         0.560897
Min latency(s):         0.0401323

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:57:15,158377441-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:57:15,163668271-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 615228

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:57:15,168918766-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 280846
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:57:15,176395235-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 280846
[1] 11:57:15 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:57:15,361441194-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16KB_write.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16KB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:57:15,508956719-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:57:39,306216005-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 49.54k objects, 774 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:57:39,313295766-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:57:48,205189275-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 49.54k objects, 774 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:57:48,212304002-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:57:56,995690545-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 49.54k objects, 774 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:57:57,002819790-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:58:05,927372281-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 49.54k objects, 774 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:58:05,934872425-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:58:14,668873813-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 49.54k objects, 774 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:58:14,675973532-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:58:14,681388376-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:58:14,684965567-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:58:14,691083245-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:58:14,695798280-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=617217
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:58:14,702177772-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:58:14,710929384-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'281626\n'
[1] 11:58:14 [SUCCESS] ljishen@10.10.2.2
281626

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:58:14,902480011-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16KB_seq.log.1
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:58:14,921879708-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:58:14,924768522-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f72e07c4-381f-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f72e07c4-381f-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T18:58:17.834325+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T18:58:17.834325+0000     0       0         0         0         0         0           -           0
2021-10-28T18:58:18.834480+0000     1     128      4011      3883   60.6585   60.6719    0.014871   0.0300208
2021-10-28T18:58:19.834599+0000     2     128      6975      6847   53.4831   46.3125   0.0452667   0.0369957
2021-10-28T18:58:20.834711+0000     3     128     10264     10136   52.7837   51.3906     0.05849    0.037489
2021-10-28T18:58:21.834817+0000     4     128     12569     12441   48.5909   36.0156   0.0301175   0.0408655
2021-10-28T18:58:22.834936+0000     5     128     15936     15808   49.3933   52.6094   0.0151517   0.0403586
2021-10-28T18:58:23.835052+0000     6     128     18676     18548   48.2957   42.8125   0.0589642   0.0412125
2021-10-28T18:58:24.835171+0000     7     128     21908     21780   48.6097      50.5   0.0880049    0.040863
2021-10-28T18:58:25.835288+0000     8     128     24555     24427   47.7028   41.3594   0.0260834   0.0417527
2021-10-28T18:58:26.835407+0000     9     128     27334     27206   47.2266   43.4219   0.0302055   0.0422746
2021-10-28T18:58:27.835529+0000    10     128     30118     29990   46.8534      43.5  0.00981552   0.0425261
2021-10-28T18:58:28.835645+0000    11     128     33938     33810   48.0195   59.6875   0.0152256   0.0416026
2021-10-28T18:58:29.835749+0000    12     128     37765     37637   49.0004   59.7969   0.0253625     0.04077
2021-10-28T18:58:30.835864+0000    13     128     41375     41247   49.5696   56.4062    0.143255   0.0401569
2021-10-28T18:58:31.835977+0000    14     128     43028     42900   47.8736   25.8281    0.011509     0.04171
2021-10-28T18:58:32.836091+0000    15     128     46217     46089   48.0035   49.8281   0.0408737   0.0415754
2021-10-28T18:58:33.836195+0000    16     128     48950     48822    47.672   42.7031   0.0491649   0.0418767
2021-10-28T18:58:34.836341+0000 Total time run:       16.2303
Total reads made:     49538
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   47.6904
Average IOPS:         3052
Stddev IOPS:          601.74
Max IOPS:             3883
Min IOPS:             1653
Average Latency(s):   0.0418559
Max latency(s):       0.533141
Min latency(s):       0.000304283

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:58:35,444871587-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:58:35,450208394-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 617217

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:58:35,455729699-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 281626
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:58:35,463206939-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 281626
[1] 11:58:35 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:58:35,648928561-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16KB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16KB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:58:35,797256046-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:58:59,706392289-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 49.54k objects, 774 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:58:59,713575465-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:59:08,755019126-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 49.54k objects, 774 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:59:08,761966056-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:59:17,820200147-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 49.54k objects, 774 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:59:17,827409863-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:59:26,810873633-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 49.54k objects, 774 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:59:26,818484916-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:59:35,686028760-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 49.54k objects, 774 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:59:35,693591211-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:59:35,699115741-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T11:59:35,701143883-07:00][RUNNING][ROUND 2/2/21] object_size=16KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:59:35,704654298-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:59:35,713740741-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:59:36,138644274-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/f72e07c4-381f-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid f72e07c4-381f-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:59:36,149532523-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:59:36,152814243-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'f72e07c4-381f-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid f72e07c4-381f-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:59:36,161296920-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid f72e07c4-381f-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 11:59:41 [SUCCESS] 10.10.2.2\n[2] 11:59:41 [SUCCESS] 10.10.2.1\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:59:41,756981961-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:59:41,768473875-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:59:41,773095513-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:59:41,922317643-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:59:41,927037095-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:59:42,079578986-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:59:42,371708484-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:59:42,376553953-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--70f9db6e--90c5--492e--af22--4773f854e18e-osd--block--dbbe9d3c--7afa--45a2--989a--92a9b0f881b7 (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-70f9db6e-90c5-492e-af22-4773f854e18e" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-dbbe9d3c-7afa-45a2-989a-92a9b0f881b7"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-70f9db6e-90c5-492e-af22-4773f854e18e" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-dbbe9d3c-7afa-45a2-989a-92a9b0f881b7" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-70f9db6e-90c5-492e-af22-4773f854e18e"\n'
10.10.2.1: b'  Volume group "ceph-70f9db6e-90c5-492e-af22-4773f854e18e" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:59:42,757523646-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:59:42,767730334-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T11:59:42,771051498-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\npodman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 35d4fb62-3821-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\nWaiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\nCreating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\n'
10.10.2.1: b'Waiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 35d4fb62-3821-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\nPlease consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:00:41,386573611-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:01:01,393464355-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:01:01,404132010-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:01:01,407899913-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 35d4fb62-3821-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/35d4fb62-3821-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:01:09,990686357-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:01:10,000671809-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:01:10,004612177-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 35d4fb62-3821-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/35d4fb62-3821-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:01:18,655501165-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:01:18,661589671-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:01:18,873927744-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:01:18,877730824-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid 35d4fb62-3821-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/35d4fb62-3821-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:01:28,054851709-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:01:48,059578590-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:01:48,066321186-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:01:48,076570474-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:01:48,080789245-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 35d4fb62-3821-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/35d4fb62-3821-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:02:13,460198513-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:02:33,465700517-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:02:33,476211948-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:02:33,480128992-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 35d4fb62-3821-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/35d4fb62-3821-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     35d4fb62-3821-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.rxynbl(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 41s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 12:02:42 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:59:36,138644274-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/f72e07c4-381f-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid f72e07c4-381f-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:59:36,149532523-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:59:36,152814243-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'f72e07c4-381f-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid f72e07c4-381f-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:59:36,161296920-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid f72e07c4-381f-11ec-b51d-53e6e728d2d3'
[1] 11:59:41 [SUCCESS] 10.10.2.2
[2] 11:59:41 [SUCCESS] 10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:59:41,756981961-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:59:41,768473875-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:59:41,773095513-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:59:41,922317643-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:59:41,927037095-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:59:42,079578986-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:59:42,371708484-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:59:42,376553953-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--70f9db6e--90c5--492e--af22--4773f854e18e-osd--block--dbbe9d3c--7afa--45a2--989a--92a9b0f881b7 (253:0)
  Archiving volume group "ceph-70f9db6e-90c5-492e-af22-4773f854e18e" metadata (seqno 5).
  Releasing logical volume "osd-block-dbbe9d3c-7afa-45a2-989a-92a9b0f881b7"
  Creating volume group backup "/etc/lvm/backup/ceph-70f9db6e-90c5-492e-af22-4773f854e18e" (seqno 6).
  Logical volume "osd-block-dbbe9d3c-7afa-45a2-989a-92a9b0f881b7" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-70f9db6e-90c5-492e-af22-4773f854e18e"
  Volume group "ceph-70f9db6e-90c5-492e-af22-4773f854e18e" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:59:42,757523646-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:59:42,767730334-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T11:59:42,771051498-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 35d4fb62-3821-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 35d4fb62-3821-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:00:41,386573611-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:01:01,393464355-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:01:01,404132010-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:01:01,407899913-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 35d4fb62-3821-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/35d4fb62-3821-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:01:09,990686357-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:01:10,000671809-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:01:10,004612177-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 35d4fb62-3821-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/35d4fb62-3821-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:01:18,655501165-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:01:18,661589671-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:01:18,873927744-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:01:18,877730824-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 35d4fb62-3821-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/35d4fb62-3821-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:01:28,054851709-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:01:48,059578590-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:01:48,066321186-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:01:48,076570474-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:01:48,080789245-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 35d4fb62-3821-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/35d4fb62-3821-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:02:13,460198513-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:02:33,465700517-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:02:33,476211948-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:02:33,480128992-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 35d4fb62-3821-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/35d4fb62-3821-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     35d4fb62-3821-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.rxynbl(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 41s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:02:42,261450079-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:02:42,269178473-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 12:02:42 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:02:42,744604045-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:02:42,747904384-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:02:42,769404159-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:02:42,772230946-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '35d4fb62-3821-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 35d4fb62-3821-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:02:46,556207418-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:02:46,559341394-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '35d4fb62-3821-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 35d4fb62-3821-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:02:50,565314936-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:02:50,568473358-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '35d4fb62-3821-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 35d4fb62-3821-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:02:54,491253187-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:02:54,494026804-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '35d4fb62-3821-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 35d4fb62-3821-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:03:02,208561232-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:03:02,211689276-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '35d4fb62-3821-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 35d4fb62-3821-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:03:06,806661221-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:03:06,810058353-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '35d4fb62-3821-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 35d4fb62-3821-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:03:11,222573549-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:03:11,225907492-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '35d4fb62-3821-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 35d4fb62-3821-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:03:16,005756907-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:03:16,008875965-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '35d4fb62-3821-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 35d4fb62-3821-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:03:21,015121139-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:03:21,018193519-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '35d4fb62-3821-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 35d4fb62-3821-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:03:25,342717630-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:03:25,345662660-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '35d4fb62-3821-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 35d4fb62-3821-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:03:29,398734725-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:03:29,401980522-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '35d4fb62-3821-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 35d4fb62-3821-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 20 lfor 0/0/17 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 19 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:03:33,265735180-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:03:33,268684387-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '35d4fb62-3821-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 35d4fb62-3821-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default   
-3         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host node-1
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0  
                       TOTAL  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:03:37,281165171-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:04:01,271160980-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:04:10,161745560-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:04:19,018556755-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:04:27,918069578-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:04:27,925661344-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:04:36,944375885-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:04:36,951565213-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:04:45,851505580-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:04:45,858534075-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:04:54,651655818-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:04:54,658338852-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:05:03,725574256-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:05:03,733148809-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:05:03,738555438-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:05:03,742028052-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:05:03,748131684-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:05:03,753192521-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=625117
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:05:03,759412843-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:05:03,768289750-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'287459\n'
[1] 12:05:03 [SUCCESS] ljishen@10.10.2.2
287459

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:05:03,958386056-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16KB_write.log.2
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:05:03,978141854-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:05:03,981162486-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '35d4fb62-3821-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 35d4fb62-3821-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T19:05:06.931991+0000 Maintaining 128 concurrent writes of 16384 bytes to objects of size 16384 for up to 60 seconds or 0 objects
2021-10-28T19:05:06.932003+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T19:05:06.933057+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T19:05:06.933057+0000     0       0         0         0         0         0           -           0
2021-10-28T19:05:07.933195+0000     1     128      1281      1153   18.0139   18.0156    0.100133     0.10825
2021-10-28T19:05:08.933310+0000     2     128      2457      2329   18.1934    18.375   0.0998765    0.105465
2021-10-28T19:05:09.933406+0000     3     128      3609      3481   18.1284        18    0.153274    0.101564
2021-10-28T19:05:10.933519+0000     4     128      4865      4737    18.502    19.625    0.150822    0.105553
2021-10-28T19:05:11.933587+0000     5     128      5913      5785   18.0764    16.375   0.0998582    0.108841
2021-10-28T19:05:12.933702+0000     6     128      7065      6937   18.0633        18    0.108165    0.109359
2021-10-28T19:05:13.933800+0000     7     128      8321      8193   18.2861    19.625   0.0918089    0.108843
2021-10-28T19:05:14.933922+0000     8     128      9473      9345   18.2501        18    0.108303    0.107849
2021-10-28T19:05:15.934033+0000     9     128     10777     10649   18.4859    20.375    0.083035    0.107587
2021-10-28T19:05:16.934099+0000    10     127     11807     11680   18.2482   16.1094   0.0995656    0.108671
2021-10-28T19:05:17.934197+0000    11     128     13057     12929   18.3632   19.5156   0.0999745    0.108494
2021-10-28T19:05:18.934312+0000    12     128     14233     14105    18.364    18.375    0.175949    0.107875
2021-10-28T19:05:19.934425+0000    13     128     15385     15257   18.3359        18    0.100307    0.107954
2021-10-28T19:05:20.934541+0000    14     128     16641     16513   18.4278    19.625    0.100205    0.108335
2021-10-28T19:05:21.934607+0000    15     128     17561     17433   18.1576    14.375   0.0916074    0.109717
2021-10-28T19:05:22.934725+0000    16     128     18689     18561   18.1241    17.625   0.0924568    0.110177
2021-10-28T19:05:23.934838+0000    17     128     19737     19609   18.0211    16.375    0.258556    0.110505
2021-10-28T19:05:24.934949+0000    18     127     20993     20866    18.111   19.6406    0.148425    0.109778
2021-10-28T19:05:25.935042+0000    19     128     22169     22041    18.124   18.3594    0.091597    0.109875
2021-10-28T19:05:26.935112+0000 min lat: 0.0663791 max lat: 0.317042 avg lat: 0.109766
2021-10-28T19:05:26.935112+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T19:05:26.935112+0000    20     128     23425     23297    18.199    19.625   0.0999465    0.109766
2021-10-28T19:05:27.935200+0000    21     128     24449     24321   18.0942        16     0.16557    0.109671
2021-10-28T19:05:28.935318+0000    22     128     25729     25601   18.1807        20   0.0998556    0.109931
2021-10-28T19:05:29.935411+0000    23     127     26801     26674   18.1191   16.7656     0.11289    0.109987
2021-10-28T19:05:30.935525+0000    24     128     28033     27905   18.1655   19.2344    0.108128     0.10988
2021-10-28T19:05:31.935596+0000    25     128     29081     28953   18.0938    16.375    0.100487     0.11015
2021-10-28T19:05:32.935720+0000    26     128     30209     30081   18.0757    17.625    0.167319     0.11054
2021-10-28T19:05:33.935823+0000    27     128     31129     31001   17.9386    14.375    0.180563    0.111244
2021-10-28T19:05:34.935940+0000    28     128     31513     31385   17.5122         6    0.295825    0.113126
2021-10-28T19:05:35.936055+0000    29     128     32001     31873   17.1712     7.625    0.249742    0.116341
2021-10-28T19:05:36.936121+0000    30     128     32281     32153   16.7447     4.375    0.374859    0.118098
2021-10-28T19:05:37.936217+0000    31     128     32769     32641   16.4505     7.625    0.258823    0.120825
2021-10-28T19:05:38.936330+0000    32     128     33153     33025   16.1239         6    0.335067    0.123472
2021-10-28T19:05:39.936447+0000    33     128     33433     33305   15.7678     4.375    0.275843    0.126088
2021-10-28T19:05:40.936568+0000    34     128     33921     33793   15.5283     7.625    0.280738    0.128574
2021-10-28T19:05:41.936639+0000    35     128     34329     34201   15.2668     6.375    0.235417      0.1307
2021-10-28T19:05:42.936751+0000    36     128     34689     34561   14.9989     5.625    0.352866    0.132443
2021-10-28T19:05:43.936863+0000    37     128     35201     35073   14.8097         8    0.224602      0.1348
2021-10-28T19:05:44.936978+0000    38     128     35457     35329   14.5252         4    0.519564    0.136529
2021-10-28T19:05:45.937074+0000    39     128     35865     35737   14.3162     6.375    0.233299    0.139228
2021-10-28T19:05:46.937142+0000 min lat: 0.0663791 max lat: 0.709365 avg lat: 0.140681
2021-10-28T19:05:46.937142+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T19:05:46.937142+0000    40     128     36249     36121   14.1083         6    0.341845    0.140681
2021-10-28T19:05:47.937259+0000    41     128     36761     36633   13.9593         8    0.288793    0.142463
2021-10-28T19:05:48.937371+0000    42     128     37145     37017   13.7698         6    0.313845    0.144376
2021-10-28T19:05:49.937465+0000    43     128     37529     37401   13.5891         6    0.241132    0.145896
2021-10-28T19:05:50.937580+0000    44     128     37913     37785   13.4166         6    0.299924    0.148354
2021-10-28T19:05:51.937645+0000    45     128     38401     38273   13.2879     7.625    0.250366    0.150252
2021-10-28T19:05:52.937758+0000    46     128     38785     38657   13.1294         6    0.375972    0.152048
2021-10-28T19:05:53.937856+0000    47     128     39193     39065   12.9857     6.375     0.26623    0.153426
2021-10-28T19:05:54.937968+0000    48     128     40089     39961   13.0068        14     0.21994    0.153342
2021-10-28T19:05:55.938080+0000    49     128     40961     40833   13.0194    13.625     0.10822    0.153476
2021-10-28T19:05:56.938148+0000    50     128     42137     42009   13.1265    18.375    0.108628    0.152143
2021-10-28T19:05:57.938251+0000    51     128     43417     43289   13.2612        20   0.0837205    0.150619
2021-10-28T19:05:58.938372+0000    52     128     44673     44545   13.3836    19.625     0.16573    0.149262
2021-10-28T19:05:59.938484+0000    53     128     45849     45721   13.4777    18.375    0.108474    0.148242
2021-10-28T19:06:00.938601+0000    54     128     46745     46617   13.4873        14   0.0917563    0.148063
2021-10-28T19:06:01.938668+0000    55     128     47897     47769   13.5694        18    0.100781    0.147244
2021-10-28T19:06:02.938786+0000    56     128     49049     48921   13.6484        18    0.175901    0.146203
2021-10-28T19:06:03.938896+0000    57     128     50305     50177   13.7533    19.625    0.100236    0.145364
2021-10-28T19:06:04.938974+0000    58     128     51481     51353   13.8329    18.375    0.100426     0.14441
2021-10-28T19:06:05.939073+0000    59     128     52481     52353   13.8633    15.625    0.225099    0.144202
2021-10-28T19:06:06.939142+0000 min lat: 0.0663791 max lat: 0.709365 avg lat: 0.143371
2021-10-28T19:06:06.939142+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T19:06:06.939142+0000    60     128     53633     53505   13.9322        18    0.114544    0.143371
2021-10-28T19:06:07.939319+0000 Total time run:         60.1295
Total writes made:      53633
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     13.9369
Stddev Bandwidth:       5.68891
Max bandwidth (MB/sec): 20.375
Min bandwidth (MB/sec): 4
Average IOPS:           891
Stddev IOPS:            364.09
Max IOPS:               1304
Min IOPS:               256
Average Latency(s):     0.143466
Stddev Latency(s):      0.086577
Max latency(s):         0.709365
Min latency(s):         0.0663791

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:06:08,607230960-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:06:08,612856932-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 625117

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:06:08,618338992-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 287459
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:06:08,625807907-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 287459
[1] 12:06:08 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:06:08,809580214-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16KB_write.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16KB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:06:08,956874482-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:06:32,819886510-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 53.63k objects, 838 MiB
    usage:   2.5 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:06:32,826929774-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:06:41,664350805-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 53.63k objects, 838 MiB
    usage:   2.5 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:06:41,671744218-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:06:50,648762570-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 53.63k objects, 838 MiB
    usage:   2.5 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:06:50,656221927-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:06:59,552544326-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 53.63k objects, 838 MiB
    usage:   2.5 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:06:59,559577300-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:07:08,503671475-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 53.63k objects, 838 MiB
    usage:   2.5 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:07:08,511368120-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:07:08,516766744-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:07:08,519920267-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:07:08,526008121-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:07:08,531125475-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=627012
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:07:08,537382046-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:07:08,546062655-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'288240\n'
[1] 12:07:08 [SUCCESS] ljishen@10.10.2.2
288240

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:07:08,738115936-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16KB_seq.log.2
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:07:08,757679205-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:07:08,760499620-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '35d4fb62-3821-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 35d4fb62-3821-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T19:07:11.710517+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T19:07:11.710517+0000     0       0         0         0         0         0           -           0
2021-10-28T19:07:12.710644+0000     1     128      4010      3882   60.6438   60.6562  0.00566658   0.0315734
2021-10-28T19:07:13.710717+0000     2     128      6811      6683   52.2037   43.7656 0.000512466   0.0370909
2021-10-28T19:07:14.710787+0000     3     128     10264     10136   52.7855   53.9531   0.0608867    0.036743
2021-10-28T19:07:15.710856+0000     4     128     14254     14126   55.1739   62.3438    0.011245   0.0358978
2021-10-28T19:07:16.710927+0000     5     128     17160     17032   53.2198   45.4062   0.0172847   0.0373778
2021-10-28T19:07:17.710998+0000     6     128     20209     20081   52.2894   47.6406   0.0587402    0.038086
2021-10-28T19:07:18.711065+0000     7     128     23597     23469   52.3815   52.9375     0.09019   0.0377469
2021-10-28T19:07:19.711136+0000     8     128     27758     27630   53.9601   65.0156    0.177014   0.0367768
2021-10-28T19:07:20.711212+0000     9     128     30979     30851   53.5562   50.3281   0.0549301   0.0372615
2021-10-28T19:07:21.711294+0000    10     128     33826     33698   52.6486   44.4844   0.0289328   0.0379236
2021-10-28T19:07:22.711362+0000    11     128     36975     36847   52.3351   49.2031   0.0425576   0.0381194
2021-10-28T19:07:23.711431+0000    12     128     40409     40281   52.4449   53.6562   0.0197979   0.0380625
2021-10-28T19:07:24.711499+0000    13     128     43628     43500   52.2794   50.2969   0.0267434   0.0381957
2021-10-28T19:07:25.711568+0000    14     128     46464     46336   51.7101   44.3125    0.115729   0.0385844
2021-10-28T19:07:26.711639+0000    15     128     49407     49279   51.3282   45.9844   0.0411916      0.0389
2021-10-28T19:07:27.711711+0000    16     128     52368     52240   51.0116   46.2656   0.0569582   0.0387797
2021-10-28T19:07:28.711817+0000 Total time run:       16.4474
Total reads made:     53633
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   50.9514
Average IOPS:         3260
Stddev IOPS:          428.281
Max IOPS:             4161
Min IOPS:             2801
Average Latency(s):   0.0391888
Max latency(s):       0.352655
Min latency(s):       0.00025993

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:07:29,305923359-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:07:29,311389451-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 627012

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:07:29,316846956-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 288240
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:07:29,324853485-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 288240
[1] 12:07:29 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:07:29,508992881-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16KB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16KB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:07:29,656931755-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:07:53,652596077-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 53.63k objects, 838 MiB
    usage:   2.5 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:07:53,659836993-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:08:02,618866634-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 53.63k objects, 838 MiB
    usage:   2.5 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:08:02,626175468-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:08:11,710746209-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 53.63k objects, 838 MiB
    usage:   2.5 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:08:11,717707898-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:08:20,543512249-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 53.63k objects, 838 MiB
    usage:   2.5 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:08:20,550520787-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:08:29,425392946-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 53.63k objects, 838 MiB
    usage:   2.5 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:08:29,432080058-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:08:29,437531582-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T12:08:29,439493880-07:00][RUNNING][ROUND 3/2/21] object_size=16KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:08:29,442727785-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:08:29,452585743-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:08:29,884051066-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/35d4fb62-3821-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 35d4fb62-3821-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:08:29,895283613-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:08:29,898808910-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '35d4fb62-3821-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 35d4fb62-3821-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:08:29,907735962-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 35d4fb62-3821-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 12:08:35 [SUCCESS] 10.10.2.1\n[2] 12:08:35 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:08:35,469649069-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:08:35,480573567-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:08:35,485073646-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:08:35,634793690-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:08:35,639293780-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:08:35,795334897-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:08:36,087358081-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:08:36,091906511-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--5adebe68--4779--4880--b059--e570cf7979c5-osd--block--63057b5a--caff--43c4--896d--247e76d9671a (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-5adebe68-4779-4880-b059-e570cf7979c5" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-63057b5a-caff-43c4-896d-247e76d9671a"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-5adebe68-4779-4880-b059-e570cf7979c5" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-63057b5a-caff-43c4-896d-247e76d9671a" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-5adebe68-4779-4880-b059-e570cf7979c5"\n'
10.10.2.1: b'  Volume group "ceph-5adebe68-4779-4880-b059-e570cf7979c5" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:08:36,421944541-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:08:36,432206473-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:08:36,435483854-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\nsystemctl is present\nlvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 73eb7dd0-3822-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\n'
10.10.2.1: b'Waiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 73eb7dd0-3822-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:09:36,676741319-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:09:56,683644432-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:09:56,693574851-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:09:56,697067877-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 73eb7dd0-3822-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/73eb7dd0-3822-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:10:05,606372299-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:10:05,615873351-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:10:05,619215804-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 73eb7dd0-3822-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/73eb7dd0-3822-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:10:14,444272374-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:10:14,450411035-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:10:14,665338644-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:10:14,668915558-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid 73eb7dd0-3822-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/73eb7dd0-3822-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:10:23,982892042-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:10:43,987735623-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:10:43,994427953-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:10:44,004475022-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:10:44,008254997-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 73eb7dd0-3822-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/73eb7dd0-3822-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:11:08,849795951-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:11:28,855402066-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:11:28,865120946-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:11:28,868743617-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 73eb7dd0-3822-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/73eb7dd0-3822-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     73eb7dd0-3822-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.yvycgc(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 41s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 12:11:37 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:08:29,884051066-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/35d4fb62-3821-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 35d4fb62-3821-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:08:29,895283613-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:08:29,898808910-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '35d4fb62-3821-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 35d4fb62-3821-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:08:29,907735962-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 35d4fb62-3821-11ec-b51d-53e6e728d2d3'
[1] 12:08:35 [SUCCESS] 10.10.2.1
[2] 12:08:35 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:08:35,469649069-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:08:35,480573567-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:08:35,485073646-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:08:35,634793690-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:08:35,639293780-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:08:35,795334897-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:08:36,087358081-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:08:36,091906511-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--5adebe68--4779--4880--b059--e570cf7979c5-osd--block--63057b5a--caff--43c4--896d--247e76d9671a (253:0)
  Archiving volume group "ceph-5adebe68-4779-4880-b059-e570cf7979c5" metadata (seqno 5).
  Releasing logical volume "osd-block-63057b5a-caff-43c4-896d-247e76d9671a"
  Creating volume group backup "/etc/lvm/backup/ceph-5adebe68-4779-4880-b059-e570cf7979c5" (seqno 6).
  Logical volume "osd-block-63057b5a-caff-43c4-896d-247e76d9671a" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-5adebe68-4779-4880-b059-e570cf7979c5"
  Volume group "ceph-5adebe68-4779-4880-b059-e570cf7979c5" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:08:36,421944541-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:08:36,432206473-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:08:36,435483854-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 73eb7dd0-3822-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 73eb7dd0-3822-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:09:36,676741319-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:09:56,683644432-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:09:56,693574851-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:09:56,697067877-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 73eb7dd0-3822-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/73eb7dd0-3822-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:10:05,606372299-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:10:05,615873351-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:10:05,619215804-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 73eb7dd0-3822-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/73eb7dd0-3822-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:10:14,444272374-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:10:14,450411035-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:10:14,665338644-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:10:14,668915558-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 73eb7dd0-3822-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/73eb7dd0-3822-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:10:23,982892042-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:10:43,987735623-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:10:43,994427953-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:10:44,004475022-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:10:44,008254997-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 73eb7dd0-3822-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/73eb7dd0-3822-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:11:08,849795951-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:11:28,855402066-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:11:28,865120946-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:11:28,868743617-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 73eb7dd0-3822-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/73eb7dd0-3822-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     73eb7dd0-3822-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.yvycgc(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 41s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:11:37,822274929-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:11:37,830051854-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 12:11:38 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:11:38,304734981-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:11:38,307925614-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:11:38,329692104-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:11:38,332537186-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '73eb7dd0-3822-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 73eb7dd0-3822-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:11:42,366326846-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:11:42,369604754-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '73eb7dd0-3822-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 73eb7dd0-3822-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:11:46,348610570-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:11:46,351705072-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '73eb7dd0-3822-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 73eb7dd0-3822-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:11:50,245206237-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:11:50,248272285-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '73eb7dd0-3822-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 73eb7dd0-3822-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:11:57,944920985-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:11:57,948280857-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '73eb7dd0-3822-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 73eb7dd0-3822-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:12:02,828653323-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:12:02,831809151-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '73eb7dd0-3822-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 73eb7dd0-3822-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:12:07,055769378-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:12:07,058691114-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '73eb7dd0-3822-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 73eb7dd0-3822-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:12:11,846299156-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:12:11,849360976-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '73eb7dd0-3822-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 73eb7dd0-3822-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:12:16,655035326-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:12:16,658242039-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '73eb7dd0-3822-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 73eb7dd0-3822-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:12:20,782740220-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:12:20,785833028-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '73eb7dd0-3822-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 73eb7dd0-3822-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:12:24,698080767-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:12:24,701146705-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '73eb7dd0-3822-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 73eb7dd0-3822-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:12:28,616640191-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:12:28,619862283-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '73eb7dd0-3822-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 73eb7dd0-3822-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default   
-3         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host node-1
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0  
                       TOTAL  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:12:32,643512897-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:12:56,681736348-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:13:05,554752126-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:13:14,455076235-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:13:23,211029563-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:13:23,218564133-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:13:32,150798763-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:13:32,157801470-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:13:41,006727486-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:13:41,014281272-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:13:50,014347779-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:13:50,021739109-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:13:58,895424910-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:13:58,902739906-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:13:58,908090360-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:13:58,911581849-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:13:58,917746607-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:13:58,923034062-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=634643
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:13:58,929741001-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:13:58,938337993-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'293775\n'
[1] 12:13:59 [SUCCESS] ljishen@10.10.2.2
293775

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:13:59,130620683-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16KB_write.log.3
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:13:59,150854165-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:13:59,153664080-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '73eb7dd0-3822-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 73eb7dd0-3822-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T19:14:02.146052+0000 Maintaining 128 concurrent writes of 16384 bytes to objects of size 16384 for up to 60 seconds or 0 objects
2021-10-28T19:14:02.146063+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T19:14:02.147082+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T19:14:02.147082+0000     0       0         0         0         0         0           -           0
2021-10-28T19:14:03.147225+0000     1     128      1154      1026   16.0297   16.0312   0.0917207    0.113275
2021-10-28T19:14:04.147343+0000     2     128      2287      2159   16.8654   17.7031    0.108625    0.116762
2021-10-28T19:14:05.147415+0000     3     128      3458      3330   17.3421   18.2969    0.132606    0.113112
2021-10-28T19:14:06.147528+0000     4     128      4610      4482   17.5061        18   0.0916365    0.110658
2021-10-28T19:14:07.147647+0000     5     128      5890      5762   18.0044        20   0.0919622    0.109797
2021-10-28T19:14:08.147763+0000     6     128      6658      6530   17.0034        12    0.116667    0.115689
2021-10-28T19:14:09.147832+0000     7     128      7791      7663   17.1032   17.7031    0.249554    0.115877
2021-10-28T19:14:10.147948+0000     8     128      9071      8943    17.465        20    0.108927    0.114156
2021-10-28T19:14:11.148045+0000     9     128     10242     10114   17.5572   18.2969    0.100276    0.112859
2021-10-28T19:14:12.148171+0000    10     128     11266     11138   17.4013        16    0.105671    0.113615
2021-10-28T19:14:13.148238+0000    11     128     12527     12399   17.6104   19.7031    0.174926    0.113199
2021-10-28T19:14:14.148352+0000    12     128     13570     13442   17.5008   16.2969    0.142086    0.112443
2021-10-28T19:14:15.148416+0000    13     128     14594     14466   17.3853        16   0.0914769    0.114593
2021-10-28T19:14:16.148520+0000    14     128     15618     15490   17.2862        16    0.100174    0.115074
2021-10-28T19:14:17.148589+0000    15     128     16879     16751   17.4473   19.7031    0.158518    0.114476
2021-10-28T19:14:18.148704+0000    16     128     18159     18031   17.6067        20    0.108607    0.113416
2021-10-28T19:14:19.148821+0000    17     128     19311     19183   17.6297        18    0.100183    0.112952
2021-10-28T19:14:20.148932+0000    18     128     19981     19853   17.2318   10.4688    0.100031    0.115514
2021-10-28T19:14:21.148999+0000    19     128     21250     21122   17.3684   19.8281     0.10061    0.114846
2021-10-28T19:14:22.149114+0000 min lat: 0.0413014 max lat: 0.442291 avg lat: 0.114804
2021-10-28T19:14:22.149114+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T19:14:22.149114+0000    20     128     22383     22255    17.385   17.7031    0.108308    0.114804
2021-10-28T19:14:23.149238+0000    21     128     23554     23426   17.4283   18.2969    0.133578    0.114378
2021-10-28T19:14:24.149349+0000    22     128     24815     24687   17.5316   19.7031   0.0998936     0.11366
2021-10-28T19:14:25.149412+0000    23     128     25839     25711    17.465        16   0.0999886    0.114237
2021-10-28T19:14:26.149515+0000    24     128     26991     26863   17.4872        18    0.108201     0.11414
2021-10-28T19:14:27.149632+0000    25     128     28143     28015   17.5076        18    0.158395    0.114066
2021-10-28T19:14:28.149754+0000    26     128     29314     29186   17.5379   18.2969   0.0999603     0.11368
2021-10-28T19:14:29.149832+0000    27     128     30466     30338    17.555        18    0.108268    0.113726
2021-10-28T19:14:30.149954+0000    28     128     31234     31106   17.3565        12    0.286704    0.114802
2021-10-28T19:14:31.150045+0000    29     128     31490     31362   16.8959         4    0.418685     0.11759
2021-10-28T19:14:32.150164+0000    30     128     31874     31746   16.5327         6    0.277479    0.119719
2021-10-28T19:14:33.150235+0000    31     128     32258     32130   16.1929         6    0.368449     0.12288
2021-10-28T19:14:34.150347+0000    32     128     32642     32514   15.8744         6    0.300592    0.125453
2021-10-28T19:14:35.150410+0000    33     128     33026     32898   15.5752         6    0.301925    0.128051
2021-10-28T19:14:36.150515+0000    34     128     33282     33154   15.2347         4    0.509642    0.130118
2021-10-28T19:14:37.150584+0000    35     128     33666     33538   14.9708         6      0.4419     0.13286
2021-10-28T19:14:38.150704+0000    36     128     34050     33922   14.7216         6    0.283607    0.135003
2021-10-28T19:14:39.150824+0000    37     128     34434     34306   14.4859         6    0.337688    0.137658
2021-10-28T19:14:40.150939+0000    38     128     34818     34690   14.2626         6    0.297086    0.139795
2021-10-28T19:14:41.151022+0000    39     128     35183     35055   14.0431   5.70312    0.353332    0.141716
2021-10-28T19:14:42.151185+0000 min lat: 0.0413014 max lat: 0.540223 avg lat: 0.143973
2021-10-28T19:14:42.151185+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T19:14:42.151185+0000    40     128     35586     35458   13.8494   6.29688    0.275372    0.143973
2021-10-28T19:14:43.151316+0000    41     128     35842     35714   13.6091         4    0.305844    0.146058
2021-10-28T19:14:44.151430+0000    42     128     36207     36079   13.4209   5.70312    0.352661    0.148537
2021-10-28T19:14:45.151497+0000    43     128     36610     36482   13.2552   6.29688    0.316909    0.150471
2021-10-28T19:14:46.151600+0000    44     128     36866     36738   13.0448         4    0.472998    0.152447
2021-10-28T19:14:47.151710+0000    45     128     37359     37231   12.9261   7.70312    0.305536    0.154399
2021-10-28T19:14:48.151821+0000    46     128     37634     37506   12.7385   4.29688    0.369817    0.156589
2021-10-28T19:14:49.151887+0000    47     128     37999     37871   12.5888   5.70312     0.33885    0.158275
2021-10-28T19:14:50.151999+0000    48     128     38658     38530    12.541   10.2969     0.14451    0.159061
2021-10-28T19:14:51.152097+0000    49     128     39810     39682   12.6524        18    0.100275    0.157824
2021-10-28T19:14:52.152227+0000    50     127     40878     40751   12.7334   16.7031   0.0993755    0.156884
2021-10-28T19:14:53.152300+0000    51     128     42095     41967   12.8562        19    0.158266    0.155514
2021-10-28T19:14:54.152413+0000    52     128     43138     43010   12.9224   16.2969     0.16829    0.154241
2021-10-28T19:14:55.152478+0000    53     128     44143     44015   12.9748   15.7031    0.100182    0.154016
2021-10-28T19:14:56.152586+0000    54     128     45295     45167   13.0678        18      0.1006    0.152948
2021-10-28T19:14:57.152658+0000    55     128     46466     46338   13.1629   18.2969    0.166603    0.151569
2021-10-28T19:14:58.152775+0000    56     128     47746     47618   13.2849        20    0.100213     0.15036
2021-10-28T19:14:59.152892+0000    57     128     48879     48751   13.3624   17.7031    0.100001    0.149411
2021-10-28T19:15:00.153008+0000    58     128     49647     49519   13.3389        12   0.0999366    0.149852
2021-10-28T19:15:01.153081+0000    59     128     50927     50799   13.4518        20    0.158338    0.148623
2021-10-28T19:15:02.153204+0000 min lat: 0.0413014 max lat: 0.56645 avg lat: 0.14793
2021-10-28T19:15:02.153204+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T19:15:02.153204+0000    60     128     51970     51842   13.4992   16.2969   0.0998715     0.14793
2021-10-28T19:15:03.153359+0000 Total time run:         60.1234
Total writes made:      51970
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     13.5061
Stddev Bandwidth:       5.8745
Max bandwidth (MB/sec): 20
Min bandwidth (MB/sec): 4
Average IOPS:           864
Stddev IOPS:            375.968
Max IOPS:               1280
Min IOPS:               256
Average Latency(s):     0.14801
Stddev Latency(s):      0.0962148
Max latency(s):         0.56645
Min latency(s):         0.0413014

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:15:03,950688217-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:15:03,956271920-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 634643

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:15:03,961651538-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 293775
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:15:03,969293810-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 293775
[1] 12:15:04 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:15:04,157578956-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16KB_write.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16KB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:15:04,305210518-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:15:28,188147656-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 51.97k objects, 812 MiB
    usage:   2.5 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:15:28,195245733-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:15:37,145147664-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 51.97k objects, 812 MiB
    usage:   2.5 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:15:37,152894503-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:15:46,069076564-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 51.97k objects, 812 MiB
    usage:   2.5 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:15:46,076354249-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:15:55,009701095-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 51.97k objects, 812 MiB
    usage:   2.5 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:15:55,017038994-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:16:03,864965877-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 51.97k objects, 812 MiB
    usage:   2.5 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:16:03,872423642-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:16:03,877904231-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:16:03,881325128-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:16:03,887633957-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:16:03,892710133-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=636594
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:16:03,899422042-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:16:03,908324439-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'294422\n'
[1] 12:16:04 [SUCCESS] ljishen@10.10.2.2
294422

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:16:04,099005730-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16KB_seq.log.3
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:16:04,118540455-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:16:04,121455829-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '73eb7dd0-3822-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 73eb7dd0-3822-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T19:16:07.146545+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T19:16:07.146545+0000     0       0         0         0         0         0           -           0
2021-10-28T19:16:08.146680+0000     1     128      4107      3979   62.1591   62.1719   0.0339303   0.0313179
2021-10-28T19:16:09.146797+0000     2     128      7296      7168    55.991   49.8281  0.00360117   0.0342183
2021-10-28T19:16:10.146904+0000     3     128     10486     10358   53.9402   49.8438    0.114896   0.0368454
2021-10-28T19:16:11.147017+0000     4     128     14128     14000   54.6801   56.9062    0.120843   0.0359556
2021-10-28T19:16:12.147133+0000     5     128     17914     17786   55.5739   59.1562  0.00389125   0.0358408
2021-10-28T19:16:13.147242+0000     6     128     21429     21301   55.4643   54.9219   0.0672561   0.0356643
2021-10-28T19:16:14.147352+0000     7     128     25171     25043   55.8926   58.4688    0.116384   0.0353559
2021-10-28T19:16:15.147471+0000     8     128     27667     27539   53.7804        39   0.0660553   0.0369221
2021-10-28T19:16:16.147580+0000     9     128     30728     30600   53.1185   47.8281  0.00279641   0.0375476
2021-10-28T19:16:17.147690+0000    10     128     34364     34236   53.4873   56.8125     0.12381   0.0371257
2021-10-28T19:16:18.147801+0000    11     128     38385     38257   54.3358   62.8281   0.0415478    0.036727
2021-10-28T19:16:19.147912+0000    12     128     42005     41877   54.5208   56.5625   0.0201983   0.0366435
2021-10-28T19:16:20.148021+0000    13     128     44930     44802   53.8422   45.7031   0.0334703    0.037025
2021-10-28T19:16:21.148132+0000    14     128     47845     47717   53.2493   45.5469   0.0882582   0.0374606
2021-10-28T19:16:22.148238+0000    15     128     51334     51206   53.3333   54.5156   0.0386788   0.0374361
2021-10-28T19:16:23.148383+0000 Total time run:       15.1736
Total reads made:     51970
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   53.5161
Average IOPS:         3425
Stddev IOPS:          435.023
Max IOPS:             4021
Min IOPS:             2496
Average Latency(s):   0.0373283
Max latency(s):       0.432238
Min latency(s):       0.000297841

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:16:23,776508021-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:16:23,781978501-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 636594

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:16:23,787476963-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 294422
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:16:23,795097935-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 294422
[1] 12:16:23 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:16:23,981821518-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16KB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16KB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:16:24,133430315-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:16:47,982096913-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 51.97k objects, 812 MiB
    usage:   2.5 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:16:47,989519291-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:16:56,796716128-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 51.97k objects, 812 MiB
    usage:   2.5 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:16:56,803870351-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:17:05,618440260-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 51.97k objects, 812 MiB
    usage:   2.5 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:17:05,626053597-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:17:14,550619678-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 51.97k objects, 812 MiB
    usage:   2.5 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:17:14,557819657-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:17:23,577769391-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 51.97k objects, 812 MiB
    usage:   2.5 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:17:23,584778811-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:17:23,590072647-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T12:17:23,593385961-07:00][RUNNING][ROUND 1/3/21] object_size=64KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:17:23,596666974-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:17:23,605877453-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:17:24,049677419-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/73eb7dd0-3822-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 73eb7dd0-3822-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:17:24,060724798-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:17:24,064458286-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '73eb7dd0-3822-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 73eb7dd0-3822-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:17:24,072880940-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 73eb7dd0-3822-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 12:17:29 [SUCCESS] 10.10.2.1\n[2] 12:17:29 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:17:29,696183369-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:17:29,706897801-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:17:29,711408271-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:17:29,858693114-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:17:29,863418728-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:17:30,020730512-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:17:30,311784451-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:17:30,316850174-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--d458bf64--8e0a--4ecd--9b83--f428c28993a8-osd--block--f3079130--1c46--445c--8ca7--25885f1438b3 (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-d458bf64-8e0a-4ecd-9b83-f428c28993a8" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-f3079130-1c46-445c-8ca7-25885f1438b3"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-d458bf64-8e0a-4ecd-9b83-f428c28993a8" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-f3079130-1c46-445c-8ca7-25885f1438b3" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-d458bf64-8e0a-4ecd-9b83-f428c28993a8"\n'
10.10.2.1: b'  Volume group "ceph-d458bf64-8e0a-4ecd-9b83-f428c28993a8" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:17:30,685355225-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:17:30,694659366-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:17:30,698130571-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\nlvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: b25dffd8-3823-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid b25dffd8-3823-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:18:29,104876489-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:18:49,111765937-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:18:49,121832191-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:18:49,125355505-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid b25dffd8-3823-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/b25dffd8-3823-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:18:58,213942933-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:18:58,223452731-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:18:58,227202941-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid b25dffd8-3823-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/b25dffd8-3823-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:19:07,227727866-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:19:07,233668484-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:19:07,446305465-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:19:07,449940448-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid b25dffd8-3823-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/b25dffd8-3823-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:19:16,837115975-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:19:36,842026164-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:19:36,848416467-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:19:36,858101584-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:19:36,861849430-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid b25dffd8-3823-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/b25dffd8-3823-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:20:01,695023024-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:20:21,700444035-07:00] STAGE: Show Cluster Status\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:20:21,710295465-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:20:21,713673446-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid b25dffd8-3823-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/b25dffd8-3823-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     b25dffd8-3823-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.esviwy(active, since 2m)\n    osd: 1 osds: 1 up (since 19s), 1 in (since 41s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 12:20:30 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:17:24,049677419-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/73eb7dd0-3822-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 73eb7dd0-3822-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:17:24,060724798-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:17:24,064458286-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '73eb7dd0-3822-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 73eb7dd0-3822-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:17:24,072880940-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 73eb7dd0-3822-11ec-b51d-53e6e728d2d3'
[1] 12:17:29 [SUCCESS] 10.10.2.1
[2] 12:17:29 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:17:29,696183369-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:17:29,706897801-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:17:29,711408271-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:17:29,858693114-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:17:29,863418728-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:17:30,020730512-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:17:30,311784451-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:17:30,316850174-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--d458bf64--8e0a--4ecd--9b83--f428c28993a8-osd--block--f3079130--1c46--445c--8ca7--25885f1438b3 (253:0)
  Archiving volume group "ceph-d458bf64-8e0a-4ecd-9b83-f428c28993a8" metadata (seqno 5).
  Releasing logical volume "osd-block-f3079130-1c46-445c-8ca7-25885f1438b3"
  Creating volume group backup "/etc/lvm/backup/ceph-d458bf64-8e0a-4ecd-9b83-f428c28993a8" (seqno 6).
  Logical volume "osd-block-f3079130-1c46-445c-8ca7-25885f1438b3" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-d458bf64-8e0a-4ecd-9b83-f428c28993a8"
  Volume group "ceph-d458bf64-8e0a-4ecd-9b83-f428c28993a8" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:17:30,685355225-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:17:30,694659366-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:17:30,698130571-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: b25dffd8-3823-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid b25dffd8-3823-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:18:29,104876489-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:18:49,111765937-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:18:49,121832191-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:18:49,125355505-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid b25dffd8-3823-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/b25dffd8-3823-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:18:58,213942933-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:18:58,223452731-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:18:58,227202941-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid b25dffd8-3823-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/b25dffd8-3823-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:19:07,227727866-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:19:07,233668484-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:19:07,446305465-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:19:07,449940448-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid b25dffd8-3823-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/b25dffd8-3823-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:19:16,837115975-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:19:36,842026164-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:19:36,848416467-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:19:36,858101584-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:19:36,861849430-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid b25dffd8-3823-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/b25dffd8-3823-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:20:01,695023024-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:20:21,700444035-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:20:21,710295465-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:20:21,713673446-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid b25dffd8-3823-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/b25dffd8-3823-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     b25dffd8-3823-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.esviwy(active, since 2m)
    osd: 1 osds: 1 up (since 19s), 1 in (since 41s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:20:30,551435129-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:20:30,559056001-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 12:20:30 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:20:31,041294987-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:20:31,044537507-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:20:31,066702048-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:20:31,069543593-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b25dffd8-3823-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b25dffd8-3823-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:20:35,078344648-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:20:35,081404525-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b25dffd8-3823-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b25dffd8-3823-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:20:39,029966505-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:20:39,032996545-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b25dffd8-3823-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b25dffd8-3823-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:20:42,945266792-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:20:42,948456403-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b25dffd8-3823-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b25dffd8-3823-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:20:50,747180822-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:20:50,750486682-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b25dffd8-3823-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b25dffd8-3823-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:20:55,147355007-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:20:55,150531714-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b25dffd8-3823-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b25dffd8-3823-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:20:59,546494774-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:20:59,549674646-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b25dffd8-3823-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b25dffd8-3823-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:21:03,829849054-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:21:03,832857915-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b25dffd8-3823-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b25dffd8-3823-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:21:08,846551689-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:21:08,849648906-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b25dffd8-3823-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b25dffd8-3823-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:21:13,078408409-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:21:13,081247149-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b25dffd8-3823-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b25dffd8-3823-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:21:17,561063842-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:21:17,564292176-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b25dffd8-3823-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b25dffd8-3823-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 20 lfor 0/0/17 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 19 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:21:21,459859428-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:21:21,462665828-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b25dffd8-3823-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b25dffd8-3823-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default   
-3         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host node-1
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0  
                       TOTAL  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:21:25,463602574-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:21:49,293034468-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:21:58,240972541-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:22:07,114833770-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:22:16,055884297-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:22:16,063219150-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:22:25,052113078-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:22:25,058980760-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:22:33,818788952-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:22:33,826315306-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:22:42,624451486-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:22:42,631735373-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:22:51,564875852-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:22:51,572082073-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:22:51,577548034-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:22:51,580944504-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:22:51,587243615-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:22:51,592589019-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=644868
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:22:51,599016311-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:22:51,607900234-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'300006\n'
[1] 12:22:51 [SUCCESS] ljishen@10.10.2.2
300006

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:22:51,798492535-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_64KB_write.log.1
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:22:51,818311945-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:22:51,821215086-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b25dffd8-3823-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b25dffd8-3823-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T19:22:54.800318+0000 Maintaining 128 concurrent writes of 65536 bytes to objects of size 65536 for up to 60 seconds or 0 objects
2021-10-28T19:22:54.800330+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T19:22:54.803112+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T19:22:54.803112+0000     0       0         0         0         0         0           -           0
2021-10-28T19:22:55.803216+0000     1     128       513       385   24.0614   24.0625    0.131481    0.316972
2021-10-28T19:22:56.803288+0000     2     128      1026       898   28.0609   32.0625    0.229715    0.255967
2021-10-28T19:22:57.803357+0000     3     128      1622      1494   31.1231     37.25    0.233487    0.251445
2021-10-28T19:22:58.803430+0000     4     128      2050      1922   30.0293     26.75    0.225459    0.253481
2021-10-28T19:22:59.803500+0000     5     128      2518      2390    29.873     29.25    0.241484    0.261432
2021-10-28T19:23:00.803577+0000     6     128      3030      2902   30.2271        32    0.333598    0.258318
2021-10-28T19:23:01.803677+0000     7     128      3542      3414   30.4799        32    0.225087    0.256437
2021-10-28T19:23:02.803796+0000     8     128      4054      3926   30.6695        32    0.200129    0.255319
2021-10-28T19:23:03.803912+0000     9     128      4566      4438   30.8169        32    0.191552    0.255265
2021-10-28T19:23:04.804023+0000    10     128      5122      4994   31.2098     34.75    0.262371     0.25158
2021-10-28T19:23:05.804120+0000    11     128      5634      5506   31.2814        32    0.208051    0.251249
2021-10-28T19:23:06.804235+0000    12     128      6146      6018    31.341        32    0.216667    0.253147
2021-10-28T19:23:07.804345+0000    13     128      6658      6530   31.3914        32    0.199886    0.253011
2021-10-28T19:23:08.804456+0000    14     128      7170      7042   31.4346        32    0.225476    0.252286
2021-10-28T19:23:09.804551+0000    15     128      7682      7554   31.4721        32    0.241825    0.252741
2021-10-28T19:23:10.804663+0000    16     128      8194      8066   31.5049        32    0.305169    0.251851
2021-10-28T19:23:11.804781+0000    17     128      8322      8194   30.1221         8    0.961708    0.261235
2021-10-28T19:23:12.804898+0000    18     128      8450      8322   28.8931         8    0.958931    0.271753
2021-10-28T19:23:13.804992+0000    19     128      8578      8450   27.7934         8     1.05521     0.28214
2021-10-28T19:23:14.805103+0000 min lat: 0.0828392 max lat: 1.05554 avg lat: 0.292175
2021-10-28T19:23:14.805103+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T19:23:14.805103+0000    20     128      8706      8578   26.8037         8    0.894688    0.292175
2021-10-28T19:23:15.805222+0000    21     128      8834      8706   25.9082         8    0.826067    0.300439
2021-10-28T19:23:16.805335+0000    22     128      8962      8834   25.0941         8    0.975993     0.31138
2021-10-28T19:23:17.805440+0000    23     128      9090      8962   24.3509         8      1.0128    0.320215
2021-10-28T19:23:18.805561+0000    24     128      9218      9090   23.6695         8    0.916756    0.329237
2021-10-28T19:23:19.805676+0000    25     128      9346      9218   23.0427         8    0.944739    0.338064
2021-10-28T19:23:20.805740+0000    26     128      9558      9430    22.666     13.25     0.80131     0.35019
2021-10-28T19:23:21.805810+0000    27     128      9686      9558   22.1228         8    0.846173    0.356935
2021-10-28T19:23:22.805882+0000    28     128      9814      9686   21.6184         8    0.987709    0.364888
2021-10-28T19:23:23.805950+0000    29     128      9942      9814   21.1488         8      1.1017    0.373727
2021-10-28T19:23:24.806017+0000    30     128     10070      9942   20.7105         8    0.865319    0.380504
2021-10-28T19:23:25.806087+0000    31     128     10242     10114   20.3892     10.75    0.848645    0.389411
2021-10-28T19:23:26.806156+0000    32     128     10370     10242    20.002         8    0.784269    0.394459
2021-10-28T19:23:27.806217+0000    33     128     10498     10370   19.6383         8    0.931738    0.399633
2021-10-28T19:23:28.806288+0000    34     128     10626     10498    19.296         8     1.08022     0.40787
2021-10-28T19:23:29.806355+0000    35     128     10754     10626   18.9733         8    0.834755     0.41388
2021-10-28T19:23:30.806430+0000    36     128     11350     11222   19.4809     37.25     0.20839    0.409237
2021-10-28T19:23:31.806553+0000    37     128     11862     11734   19.8191        32    0.266403    0.402316
2021-10-28T19:23:32.806669+0000    38     128     12290     12162   20.0014     26.75    0.225649    0.396959
2021-10-28T19:23:33.806768+0000    39     128     12758     12630   20.2385     29.25    0.342113    0.393931
2021-10-28T19:23:34.806880+0000 min lat: 0.0828392 max lat: 1.1021 avg lat: 0.387979
2021-10-28T19:23:34.806880+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T19:23:34.806880+0000    40     128     13270     13142   20.5325        32    0.216732    0.387979
2021-10-28T19:23:35.807007+0000    41     128     13826     13698   20.8791     34.75    0.241869    0.382301
2021-10-28T19:23:36.807126+0000    42     128     14338     14210   21.1438        32    0.208797    0.377458
2021-10-28T19:23:37.807227+0000    43     128     14806     14678   21.3323     29.25    0.227405    0.373963
2021-10-28T19:23:38.807298+0000    44     128     15234     15106   21.4554     26.75    0.242374    0.371202
2021-10-28T19:23:39.807389+0000    45     128     15746     15618   21.6896        32    0.274864    0.367981
2021-10-28T19:23:40.807463+0000    46     128     16258     16130   21.9137        32    0.200154    0.364475
2021-10-28T19:23:41.807569+0000    47     128     16386     16258   21.6177         8    0.891993    0.366846
2021-10-28T19:23:42.807685+0000    48     128     16514     16386   21.3339         8    0.827226    0.370788
2021-10-28T19:23:43.807770+0000    49     128     16642     16514   21.0618         8    0.879197    0.374711
2021-10-28T19:23:44.807883+0000    50     128     16726     16598   20.7455      5.25     1.11571    0.378461
2021-10-28T19:23:45.807985+0000    51     128     16770     16642   20.3927      2.75     1.59751    0.381684
2021-10-28T19:23:46.808095+0000    52     128     16770     16642   20.0005         0           -    0.381684
2021-10-28T19:23:47.808209+0000    53     128     16854     16726   19.7222     2.625     3.07611    0.395216
2021-10-28T19:23:48.808324+0000    54     128     16898     16770   19.4079      2.75     3.20118    0.402578
2021-10-28T19:23:49.808420+0000    55     128     16898     16770    19.055         0           -    0.402578
2021-10-28T19:23:50.808535+0000    56     128     16982     16854   18.8085     2.625     3.21704    0.416605
2021-10-28T19:23:51.808652+0000    57     128     17026     16898   18.5267      2.75     3.29904    0.424109
2021-10-28T19:23:52.808765+0000    58     128     17026     16898   18.2073         0           -    0.424109
2021-10-28T19:23:53.808863+0000    59     128     17110     16982   17.9877     2.625     2.91693     0.43644
2021-10-28T19:23:54.808978+0000 min lat: 0.0828392 max lat: 3.29938 avg lat: 0.442693
2021-10-28T19:23:54.808978+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T19:23:54.808978+0000    60     128     17154     17026   17.7337      2.75     2.85619    0.442693
2021-10-28T19:23:55.809103+0000    61     128     17154     17026    17.443         0           -    0.442693
2021-10-28T19:23:56.809213+0000    62      44     17154     17110   17.2463     2.625      3.1219    0.455848
2021-10-28T19:23:57.809340+0000 Total time run:         62.8425
Total writes made:      17154
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     17.0605
Stddev Bandwidth:       13.0842
Max bandwidth (MB/sec): 37.25
Min bandwidth (MB/sec): 0
Average IOPS:           272
Stddev IOPS:            209.347
Max IOPS:               596
Min IOPS:               0
Average Latency(s):     0.463147
Stddev Latency(s):      0.536977
Max latency(s):         3.30196
Min latency(s):         0.0828392

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:23:58,920026899-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:23:58,925532756-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 644868

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:23:58,930880684-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 300006
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:23:58,938723575-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 300006
[1] 12:23:59 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:23:59,125478418-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_64KB_write.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_64KB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:23:59,277080281-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:24:23,090804560-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.16k objects, 1.0 GiB
    usage:   4.5 GiB used, 395 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:24:23,098065604-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:24:31,947378960-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.16k objects, 1.0 GiB
    usage:   3.5 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:24:31,954806027-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:24:40,904316302-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.16k objects, 1.0 GiB
    usage:   3.2 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:24:40,911720115-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:24:49,594258895-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.16k objects, 1.0 GiB
    usage:   3.2 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:24:49,601490814-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:24:58,568341461-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.16k objects, 1.0 GiB
    usage:   3.2 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:24:58,575657358-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:24:58,581217267-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:24:58,584759392-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:24:58,591165966-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:24:58,596176219-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=646780
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:24:58,602585598-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:24:58,611658728-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'300773\n'
[1] 12:24:58 [SUCCESS] ljishen@10.10.2.2
300773

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:24:58,802630447-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_64KB_seq.log.1
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:24:58,822193917-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:24:58,825022798-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b25dffd8-3823-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b25dffd8-3823-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T19:25:01.837428+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T19:25:01.837428+0000     0       0         0         0         0         0           -           0
2021-10-28T19:25:02.837553+0000     1     128      1149      1021   63.7994   63.8125    0.153778    0.112111
2021-10-28T19:25:03.837630+0000     2     128      2358      2230   69.6777   75.5625   0.0983181    0.111691
2021-10-28T19:25:04.837702+0000     3     128      3761      3633   75.6786   87.6875    0.105691    0.103751
2021-10-28T19:25:05.837778+0000     4     128      4936      4808   75.1169   73.4375    0.121682    0.105149
2021-10-28T19:25:06.837851+0000     5     128      6219      6091   76.1298   80.1875    0.130093    0.102757
2021-10-28T19:25:07.837925+0000     6     128      7491      7363   76.6905      79.5    0.134296    0.103269
2021-10-28T19:25:08.837998+0000     7     128      8496      8368   74.7073   62.8125   0.0900496    0.106164
2021-10-28T19:25:09.838073+0000     8     128      9665      9537   74.5011   73.0625    0.188341    0.106141
2021-10-28T19:25:10.838146+0000     9     128     10969     10841    75.278      81.5    0.019205    0.104579
2021-10-28T19:25:11.838226+0000    10     128     12391     12263    76.637    88.875     0.14614     0.10375
2021-10-28T19:25:12.838302+0000    11     128     13858     13730   78.0046   91.6875    0.103863    0.101975
2021-10-28T19:25:13.838384+0000    12     128     14773     14645   76.2695   57.1875    0.328693    0.103352
2021-10-28T19:25:14.838467+0000    13     128     15838     15710   75.5223   66.5625    0.102942    0.105431
2021-10-28T19:25:15.838541+0000    14     128     17056     16928    75.565    76.125   0.0253592    0.104658
2021-10-28T19:25:16.838649+0000 Total time run:       14.1978
Total reads made:     17154
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   75.5135
Average IOPS:         1208
Stddev IOPS:          165.252
Max IOPS:             1467
Min IOPS:             915
Average Latency(s):   0.105648
Max latency(s):       0.498302
Min latency(s):       0.000627934

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:25:17,554559827-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:25:17,560248428-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 646780

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:25:17,565706505-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 300773
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:25:17,573318330-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 300773
[1] 12:25:17 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:25:17,757924052-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_64KB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_64KB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:25:17,909009160-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:25:41,757250903-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.16k objects, 1.0 GiB
    usage:   3.2 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:25:41,764304787-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:25:50,729603926-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.16k objects, 1.0 GiB
    usage:   3.2 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:25:50,736952254-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:25:59,762804358-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.16k objects, 1.0 GiB
    usage:   3.2 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:25:59,770267143-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:26:08,885538233-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.16k objects, 1.0 GiB
    usage:   3.2 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:26:08,893301513-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:26:17,811134273-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.16k objects, 1.0 GiB
    usage:   3.2 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:26:17,819010687-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:26:17,824809075-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T12:26:17,827119510-07:00][RUNNING][ROUND 2/3/21] object_size=64KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:26:17,830448093-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:26:17,839953196-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:26:18,236718206-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/b25dffd8-3823-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid b25dffd8-3823-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:26:18,247947896-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:26:18,251727612-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'b25dffd8-3823-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid b25dffd8-3823-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:26:18,260156147-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid b25dffd8-3823-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 12:26:23 [SUCCESS] 10.10.2.1\n[2] 12:26:24 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:26:24,130127874-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:26:24,142037593-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:26:24,146493470-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:26:24,294822896-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:26:24,299943592-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:26:24,455418732-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:26:24,739484250-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:26:24,744664378-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--50bb3222--1d7b--460c--b101--21be75c6eed5-osd--block--8553ab7a--cd37--4890--8b2a--97ca968617fb (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-50bb3222-1d7b-460c-b101-21be75c6eed5" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-8553ab7a-cd37-4890-8b2a-97ca968617fb"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-50bb3222-1d7b-460c-b101-21be75c6eed5" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-8553ab7a-cd37-4890-8b2a-97ca968617fb" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-50bb3222-1d7b-460c-b101-21be75c6eed5"\n'
10.10.2.1: b'  Volume group "ceph-50bb3222-1d7b-460c-b101-21be75c6eed5" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:26:25,081141305-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:26:25,090868251-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:26:25,094523022-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: f0e3cf98-3824-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\nWaiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid f0e3cf98-3824-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:27:24,717578075-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:27:44,724838299-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:27:44,734928818-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:27:44,738149353-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid f0e3cf98-3824-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/f0e3cf98-3824-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:27:53,398555957-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:27:53,408768336-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:27:53,412677194-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid f0e3cf98-3824-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/f0e3cf98-3824-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:28:02,014870773-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:28:02,020333602-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:28:02,234260205-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:28:02,237916148-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid f0e3cf98-3824-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/f0e3cf98-3824-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:28:11,760013070-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:28:31,764847645-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:28:31,771435199-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:28:31,780843967-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:28:31,784439356-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid f0e3cf98-3824-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/f0e3cf98-3824-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:28:56,220646452-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:29:16,226175957-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:29:16,236474378-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:29:16,240272479-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid f0e3cf98-3824-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/f0e3cf98-3824-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     f0e3cf98-3824-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.fqpyke(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 12:29:24 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:26:18,236718206-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/b25dffd8-3823-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid b25dffd8-3823-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:26:18,247947896-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:26:18,251727612-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'b25dffd8-3823-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid b25dffd8-3823-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:26:18,260156147-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid b25dffd8-3823-11ec-b51d-53e6e728d2d3'
[1] 12:26:23 [SUCCESS] 10.10.2.1
[2] 12:26:24 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:26:24,130127874-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:26:24,142037593-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:26:24,146493470-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:26:24,294822896-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:26:24,299943592-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:26:24,455418732-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:26:24,739484250-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:26:24,744664378-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--50bb3222--1d7b--460c--b101--21be75c6eed5-osd--block--8553ab7a--cd37--4890--8b2a--97ca968617fb (253:0)
  Archiving volume group "ceph-50bb3222-1d7b-460c-b101-21be75c6eed5" metadata (seqno 5).
  Releasing logical volume "osd-block-8553ab7a-cd37-4890-8b2a-97ca968617fb"
  Creating volume group backup "/etc/lvm/backup/ceph-50bb3222-1d7b-460c-b101-21be75c6eed5" (seqno 6).
  Logical volume "osd-block-8553ab7a-cd37-4890-8b2a-97ca968617fb" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-50bb3222-1d7b-460c-b101-21be75c6eed5"
  Volume group "ceph-50bb3222-1d7b-460c-b101-21be75c6eed5" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:26:25,081141305-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:26:25,090868251-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:26:25,094523022-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: f0e3cf98-3824-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid f0e3cf98-3824-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:27:24,717578075-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:27:44,724838299-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:27:44,734928818-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:27:44,738149353-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid f0e3cf98-3824-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/f0e3cf98-3824-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:27:53,398555957-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:27:53,408768336-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:27:53,412677194-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid f0e3cf98-3824-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/f0e3cf98-3824-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:28:02,014870773-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:28:02,020333602-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:28:02,234260205-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:28:02,237916148-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid f0e3cf98-3824-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/f0e3cf98-3824-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:28:11,760013070-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:28:31,764847645-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:28:31,771435199-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:28:31,780843967-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:28:31,784439356-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid f0e3cf98-3824-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/f0e3cf98-3824-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:28:56,220646452-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:29:16,226175957-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:29:16,236474378-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:29:16,240272479-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid f0e3cf98-3824-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/f0e3cf98-3824-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     f0e3cf98-3824-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.fqpyke(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:29:24,857758328-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:29:24,865430006-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 12:29:25 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:29:25,341382300-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:29:25,344768452-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:29:25,366632447-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:29:25,369295586-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f0e3cf98-3824-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f0e3cf98-3824-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:29:29,301950080-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:29:29,305074639-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f0e3cf98-3824-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f0e3cf98-3824-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:29:33,377169473-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:29:33,380308068-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f0e3cf98-3824-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f0e3cf98-3824-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:29:37,236048267-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:29:37,239028344-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f0e3cf98-3824-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f0e3cf98-3824-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:29:45,022583814-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:29:45,025760941-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f0e3cf98-3824-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f0e3cf98-3824-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:29:49,679728173-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:29:49,682582482-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f0e3cf98-3824-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f0e3cf98-3824-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:29:54,132868554-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:29:54,136101928-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f0e3cf98-3824-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f0e3cf98-3824-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:29:58,864603085-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:29:58,867562612-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f0e3cf98-3824-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f0e3cf98-3824-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:30:03,906969039-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:30:03,910236036-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f0e3cf98-3824-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f0e3cf98-3824-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:30:08,190946464-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:30:08,193935678-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f0e3cf98-3824-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f0e3cf98-3824-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:30:12,734783549-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:30:12,737799813-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f0e3cf98-3824-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f0e3cf98-3824-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/17 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 19 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:30:16,671292087-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:30:16,674451562-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f0e3cf98-3824-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f0e3cf98-3824-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.39059         -  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default   
-3         0.39059         -  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host node-1
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0  
                       TOTAL  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:30:20,580512308-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:30:44,351527592-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:30:53,333359163-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:31:02,236463461-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:31:11,160968076-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:31:11,167975032-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:31:20,059659581-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:31:20,067297905-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:31:28,983769364-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:31:28,990837465-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:31:37,908219896-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:31:37,916198102-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:31:46,808256204-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:31:46,815397842-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:31:46,821161845-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:31:46,824673704-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:31:46,831171179-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:31:46,836204725-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=654824
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:31:46,842884764-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:31:46,851611751-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'306541\n'
[1] 12:31:47 [SUCCESS] ljishen@10.10.2.2
306541

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:31:47,042416793-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_64KB_write.log.2
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:31:47,062323519-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:31:47,065191494-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f0e3cf98-3824-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f0e3cf98-3824-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T19:31:49.970898+0000 Maintaining 128 concurrent writes of 65536 bytes to objects of size 65536 for up to 60 seconds or 0 objects
2021-10-28T19:31:49.970910+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T19:31:49.973660+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T19:31:49.973660+0000     0       0         0         0         0         0           -           0
2021-10-28T19:31:50.973810+0000     1     128       642       514   32.1221    32.125   0.0833518    0.225728
2021-10-28T19:31:51.973944+0000     2     128      1154      1026   32.0589        32     0.18719    0.228513
2021-10-28T19:31:52.974098+0000     3     128      1666      1538   32.0376        32    0.258184    0.238924
2021-10-28T19:31:53.974200+0000     4     128      2050      1922   30.0277        24    0.208775    0.252704
2021-10-28T19:31:54.974322+0000     5     128      2658      2530   31.6212        38    0.224888    0.251315
2021-10-28T19:31:55.974442+0000     6     128      3170      3042   31.6837        32    0.199918    0.249328
2021-10-28T19:31:56.974560+0000     7     128      3586      3458   30.8713        26    0.216985    0.253337
2021-10-28T19:31:57.974661+0000     8     128      4098      3970    31.012        32    0.268731    0.250906
2021-10-28T19:31:58.974784+0000     9     128      4706      4578   31.7879        38     0.22383    0.250759
2021-10-28T19:31:59.974899+0000    10     128      5122      4994   31.2088        26    0.217436    0.252508
2021-10-28T19:32:00.975022+0000    11     128      5634      5506   31.2804        32    0.209078    0.252518
2021-10-28T19:32:01.975140+0000    12     128      6242      6114     31.84        38    0.245219    0.250049
2021-10-28T19:32:02.975272+0000    13     128      6754      6626    31.852        32    0.241127     0.24794
2021-10-28T19:32:03.975392+0000    14     128      7266      7138   31.8623        32      0.2328    0.250291
2021-10-28T19:32:04.975501+0000    15     128      7650      7522   31.3379        24    0.216879    0.253356
2021-10-28T19:32:05.975600+0000    16     128      8162      8034   31.3791        32    0.233703    0.251947
2021-10-28T19:32:06.975714+0000    17     128      8322      8194   30.1215        10    0.901975    0.261022
2021-10-28T19:32:07.975826+0000    18     128      8450      8322   28.8925         8     0.79112    0.269512
2021-10-28T19:32:08.975939+0000    19     128      8578      8450   27.7928         8    0.867235    0.277604
2021-10-28T19:32:09.976040+0000 min lat: 0.0833518 max lat: 0.902359 avg lat: 0.285931
2021-10-28T19:32:09.976040+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T19:32:09.976040+0000    20     128      8706      8578   26.8031         8    0.835268    0.285931
2021-10-28T19:32:10.976128+0000    21     128      8930      8802   26.1934        14    0.865315    0.302618
2021-10-28T19:32:11.976246+0000    22     128      9058      8930   25.3664         8    0.903066    0.311082
2021-10-28T19:32:12.976364+0000    23     128      9186      9058   24.6113         8     1.10894    0.321446
2021-10-28T19:32:13.976462+0000    24     128      9218      9090   23.6692         2     1.12572    0.324277
2021-10-28T19:32:14.976575+0000    25     128      9346      9218   23.0424         8     1.02294    0.334385
2021-10-28T19:32:15.976673+0000    26     128      9570      9442   22.6945        14    0.758432    0.347776
2021-10-28T19:32:16.976782+0000    27     128      9730      9602   22.2243        10    0.779423    0.355428
2021-10-28T19:32:17.976876+0000    28     128      9858      9730   21.7163         8    0.849503    0.361268
2021-10-28T19:32:18.976985+0000    29     128      9986      9858   21.2433         8     1.07389    0.368618
2021-10-28T19:32:19.977096+0000    30     128     10114      9986   20.8018         8    0.926716    0.378431
2021-10-28T19:32:20.977210+0000    31     128     10242     10114   20.3888         8     1.25037    0.388922
2021-10-28T19:32:21.977308+0000    32     128     10370     10242   20.0017         8    0.649329    0.392975
2021-10-28T19:32:22.977437+0000    33     128     10594     10466   19.8197        14    0.739597    0.400635
2021-10-28T19:32:23.977548+0000    34     128     10722     10594   19.4721         8    0.860703    0.405792
2021-10-28T19:32:24.977658+0000    35     128     11106     10978   19.6014        24    0.199628    0.407687
2021-10-28T19:32:25.977759+0000    36     128     11522     11394    19.779        26    0.366454    0.401988
2021-10-28T19:32:26.977874+0000    37     128     12034     11906   20.1092        32    0.200388    0.396482
2021-10-28T19:32:27.977983+0000    38     128     12546     12418   20.4221        32    0.200376    0.390204
2021-10-28T19:32:28.978094+0000    39     128     12930     12802   20.5137        24    0.266492    0.388018
2021-10-28T19:32:29.978193+0000 min lat: 0.0833518 max lat: 1.25083 avg lat: 0.379844
2021-10-28T19:32:29.978193+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T19:32:29.978193+0000    40     128     13570     13442   21.0008        40    0.192306    0.379844
2021-10-28T19:32:30.978271+0000    41     128     14082     13954    21.269        32    0.233354    0.375154
2021-10-28T19:32:31.978400+0000    42     128     14434     14306   21.2863        22    0.242222    0.374705
2021-10-28T19:32:32.978529+0000    43     128     14946     14818   21.5354        32    0.249768    0.370993
2021-10-28T19:32:33.978610+0000    44     128     15490     15362   21.8186        34     0.22554     0.36588
2021-10-28T19:32:34.978721+0000    45     128     15970     15842   22.0003        30    0.400083     0.36299
2021-10-28T19:32:35.978829+0000    46     128     16258     16130   21.9133        18    0.208965    0.361123
2021-10-28T19:32:36.978938+0000    47     128     16386     16258   21.6173         8    0.793678    0.363488
2021-10-28T19:32:37.979033+0000    48     128     16610     16482   21.4586        14    0.808165    0.371222
2021-10-28T19:32:38.979143+0000    49     128     16642     16514   21.0614         2    0.779592    0.372013
2021-10-28T19:32:39.979256+0000    50     128     16738     16610   20.7602         6     1.77106    0.380099
2021-10-28T19:32:40.979332+0000    51     128     16770     16642   20.3924         2     2.35549    0.383897
2021-10-28T19:32:41.979426+0000    52     128     16770     16642   20.0002         0           -    0.383897
2021-10-28T19:32:42.979539+0000    53     128     16770     16642   19.6228         0           -    0.383897
2021-10-28T19:32:43.979650+0000    54     128     16866     16738   19.3706         2     3.60515    0.402373
2021-10-28T19:32:44.979762+0000    55     128     16898     16770   19.0547         2     3.68462    0.408636
2021-10-28T19:32:45.979866+0000    56     128     16898     16770   18.7145         0           -    0.408636
2021-10-28T19:32:46.979978+0000    57     128     16994     16866   18.4914         3     3.43847    0.425881
2021-10-28T19:32:47.980089+0000    58     128     17026     16898   18.2071         2     3.38515    0.431485
2021-10-28T19:32:48.980202+0000    59     128     17026     16898   17.8985         0           -    0.431485
2021-10-28T19:32:49.980299+0000 min lat: 0.0833518 max lat: 3.6848 avg lat: 0.431485
2021-10-28T19:32:49.980299+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T19:32:49.980299+0000    60     128     17026     16898   17.6002         0           -    0.431485
2021-10-28T19:32:50.980397+0000 Total time run:         60.9589
Total writes made:      17026
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     17.4564
Stddev Bandwidth:       13.0115
Max bandwidth (MB/sec): 40
Min bandwidth (MB/sec): 0
Average IOPS:           279
Stddev IOPS:            208.184
Max IOPS:               640
Min IOPS:               0
Average Latency(s):     0.45362
Stddev Latency(s):      0.542089
Max latency(s):         3.6848
Min latency(s):         0.0833518

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:32:51,931447073-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:32:51,936973117-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 654824

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:32:51,942558554-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 306541
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:32:51,950027209-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 306541
[1] 12:32:52 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:32:52,137908460-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_64KB_write.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_64KB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:32:52,284886925-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:33:17,139024861-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.03k objects, 1.0 GiB
    usage:   4.5 GiB used, 395 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:33:17,146278391-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:33:26,095831987-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.03k objects, 1.0 GiB
    usage:   3.5 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:33:26,103365435-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:33:34,928902436-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.03k objects, 1.0 GiB
    usage:   3.1 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:33:34,936547995-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:33:43,793885794-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.03k objects, 1.0 GiB
    usage:   3.1 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:33:43,801543937-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:33:52,596947432-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.03k objects, 1.0 GiB
    usage:   3.1 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:33:52,604130538-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:33:52,609678645-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:33:52,613052633-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:33:52,619522246-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:33:52,624612068-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=657045
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:33:52,631130493-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:33:52,639832723-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'307274\n'
[1] 12:33:52 [SUCCESS] ljishen@10.10.2.2
307274

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:33:52,831070268-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_64KB_seq.log.2
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:33:52,850359210-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:33:52,853253264-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f0e3cf98-3824-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f0e3cf98-3824-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T19:33:55.769358+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T19:33:55.769358+0000     0       0         0         0         0         0           -           0
2021-10-28T19:33:56.769495+0000     1     128      1354      1226   76.6075    76.625   0.0428674   0.0976823
2021-10-28T19:33:57.769571+0000     2     128      2555      2427   75.8322   75.0625    0.103003    0.100162
2021-10-28T19:33:58.769642+0000     3     128      3981      3853   80.2608    89.125     0.06061   0.0980766
2021-10-28T19:33:59.769711+0000     4     128      5081      4953    77.382     68.75     0.10525    0.102098
2021-10-28T19:34:00.769779+0000     5     128      6254      6126   76.5672   73.3125   0.0270029    0.102885
2021-10-28T19:34:01.769883+0000     6     128      7623      7495   78.0649   85.5625   0.0544123    0.100721
2021-10-28T19:34:02.770013+0000     7     128      8536      8408   75.0634   57.0625    0.205125    0.104877
2021-10-28T19:34:03.770128+0000     8     128      9906      9778   76.3824    85.625    0.261133    0.103687
2021-10-28T19:34:04.770244+0000     9     128     11129     11001   76.3875   76.4375    0.157088    0.104074
2021-10-28T19:34:05.770338+0000    10     128     12337     12209   76.2981      75.5   0.0477581    0.103992
2021-10-28T19:34:06.770452+0000    11     128     13557     13429   76.2929     76.25   0.0826461    0.104421
2021-10-28T19:34:07.770562+0000    12     128     14841     14713   76.6219     80.25    0.185411    0.103826
2021-10-28T19:34:08.770672+0000    13     128     15930     15802   75.9629   68.0625   0.0788775     0.10493
2021-10-28T19:34:09.770792+0000 Total time run:       13.9014
Total reads made:     17026
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   76.5478
Average IOPS:         1224
Stddev IOPS:          135.26
Max IOPS:             1426
Min IOPS:             913
Average Latency(s):   0.104306
Max latency(s):       0.513968
Min latency(s):       0.000815356

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:34:10,371892626-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:34:10,377404463-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 657045

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:34:10,382897415-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 307274
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:34:10,390500654-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 307274
[1] 12:34:10 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:34:10,577244200-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_64KB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_64KB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:34:10,729423166-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:34:34,625915199-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.03k objects, 1.0 GiB
    usage:   3.1 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:34:34,633622013-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:34:43,590334980-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.03k objects, 1.0 GiB
    usage:   3.1 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:34:43,597668902-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:34:52,538783308-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.03k objects, 1.0 GiB
    usage:   3.1 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:34:52,546415702-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:35:01,498398548-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.03k objects, 1.0 GiB
    usage:   3.1 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:35:01,505779859-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:35:10,619391265-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.03k objects, 1.0 GiB
    usage:   3.1 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:35:10,626861633-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:35:10,632625206-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T12:35:10,634986035-07:00][RUNNING][ROUND 3/3/21] object_size=64KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:35:10,638389218-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:35:10,647613051-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:35:11,139464314-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/f0e3cf98-3824-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid f0e3cf98-3824-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:35:11,150374546-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:35:11,153881910-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'f0e3cf98-3824-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid f0e3cf98-3824-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:35:11,162515663-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid f0e3cf98-3824-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 12:35:16 [SUCCESS] 10.10.2.1\n[2] 12:35:17 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:35:17,454277909-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:35:17,466788830-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:35:17,471375072-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:35:17,623559320-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:35:17,628459042-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:35:17,782979152-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:35:18,075412379-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:35:18,080550539-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--f0c8ad79--ab44--45ad--a240--e9560bdc3c5f-osd--block--8c59f832--37c8--4a49--9168--fa3c1a31cc07 (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-f0c8ad79-ab44-45ad-a240-e9560bdc3c5f" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-8c59f832-37c8-4a49-9168-fa3c1a31cc07"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-f0c8ad79-ab44-45ad-a240-e9560bdc3c5f" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-8c59f832-37c8-4a49-9168-fa3c1a31cc07" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-f0c8ad79-ab44-45ad-a240-e9560bdc3c5f"\n'
10.10.2.1: b'  Volume group "ceph-f0c8ad79-ab44-45ad-a240-e9560bdc3c5f" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:35:18,421116198-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:35:18,430575973-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:35:18,433858465-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\npodman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 2ec91e70-3826-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\nExtracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\n'
10.10.2.1: b'Waiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 2ec91e70-3826-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\nPlease consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:36:19,963804325-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:36:39,970535829-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:36:39,980929149-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:36:39,984855702-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 2ec91e70-3826-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/2ec91e70-3826-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:36:48,766908185-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:36:48,776868651-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:36:48,780864914-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 2ec91e70-3826-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/2ec91e70-3826-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:36:58,392103114-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:36:58,397888050-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:36:58,613786906-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:36:58,617008673-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid 2ec91e70-3826-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/2ec91e70-3826-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:37:08,443001564-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:37:28,447884658-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:37:28,454293076-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:37:28,464584995-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:37:28,468733575-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 2ec91e70-3826-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/2ec91e70-3826-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:37:52,892857781-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:38:12,898212208-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:38:12,908819461-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:38:12,913037401-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 2ec91e70-3826-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/2ec91e70-3826-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     2ec91e70-3826-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.ehyiiu(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 12:38:21 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:35:11,139464314-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/f0e3cf98-3824-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid f0e3cf98-3824-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:35:11,150374546-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:35:11,153881910-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'f0e3cf98-3824-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid f0e3cf98-3824-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:35:11,162515663-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid f0e3cf98-3824-11ec-b51d-53e6e728d2d3'
[1] 12:35:16 [SUCCESS] 10.10.2.1
[2] 12:35:17 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:35:17,454277909-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:35:17,466788830-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:35:17,471375072-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:35:17,623559320-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:35:17,628459042-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:35:17,782979152-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:35:18,075412379-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:35:18,080550539-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--f0c8ad79--ab44--45ad--a240--e9560bdc3c5f-osd--block--8c59f832--37c8--4a49--9168--fa3c1a31cc07 (253:0)
  Archiving volume group "ceph-f0c8ad79-ab44-45ad-a240-e9560bdc3c5f" metadata (seqno 5).
  Releasing logical volume "osd-block-8c59f832-37c8-4a49-9168-fa3c1a31cc07"
  Creating volume group backup "/etc/lvm/backup/ceph-f0c8ad79-ab44-45ad-a240-e9560bdc3c5f" (seqno 6).
  Logical volume "osd-block-8c59f832-37c8-4a49-9168-fa3c1a31cc07" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-f0c8ad79-ab44-45ad-a240-e9560bdc3c5f"
  Volume group "ceph-f0c8ad79-ab44-45ad-a240-e9560bdc3c5f" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:35:18,421116198-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:35:18,430575973-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:35:18,433858465-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 2ec91e70-3826-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 2ec91e70-3826-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:36:19,963804325-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:36:39,970535829-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:36:39,980929149-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:36:39,984855702-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 2ec91e70-3826-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/2ec91e70-3826-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:36:48,766908185-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:36:48,776868651-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:36:48,780864914-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 2ec91e70-3826-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/2ec91e70-3826-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:36:58,392103114-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:36:58,397888050-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:36:58,613786906-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:36:58,617008673-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 2ec91e70-3826-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/2ec91e70-3826-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:37:08,443001564-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:37:28,447884658-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:37:28,454293076-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:37:28,464584995-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:37:28,468733575-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 2ec91e70-3826-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/2ec91e70-3826-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:37:52,892857781-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:38:12,898212208-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:38:12,908819461-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:38:12,913037401-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 2ec91e70-3826-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/2ec91e70-3826-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     2ec91e70-3826-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.ehyiiu(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:38:21,888889702-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:38:21,896788518-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 12:38:22 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:38:22,369216368-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:38:22,372904219-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:38:22,394615034-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:38:22,397555736-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ec91e70-3826-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ec91e70-3826-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:38:26,241857747-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:38:26,244882838-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ec91e70-3826-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ec91e70-3826-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:38:30,133553244-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:38:30,136698471-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ec91e70-3826-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ec91e70-3826-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:38:34,125597515-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:38:34,128422359-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ec91e70-3826-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ec91e70-3826-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:38:41,975355039-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:38:41,978566992-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ec91e70-3826-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ec91e70-3826-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:38:46,681604037-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:38:46,684799680-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ec91e70-3826-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ec91e70-3826-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:38:50,991404484-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:38:50,994430237-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ec91e70-3826-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ec91e70-3826-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:38:55,724247159-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:38:55,727053918-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ec91e70-3826-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ec91e70-3826-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:38:59,781534001-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:38:59,784431992-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ec91e70-3826-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ec91e70-3826-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:39:04,691170085-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:39:04,694283062-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ec91e70-3826-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ec91e70-3826-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:39:08,657058258-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:39:08,660079542-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ec91e70-3826-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ec91e70-3826-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 18 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 19 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:39:12,538870332-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:39:12,542007564-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ec91e70-3826-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ec91e70-3826-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.39059         -  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default   
-3         0.39059         -  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host node-1
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0  
                       TOTAL  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:39:16,557085918-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:39:40,536704882-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:39:49,469284269-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:39:58,353171017-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:40:07,375244546-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:40:07,382891277-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:40:16,190269542-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:40:16,197909711-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:40:25,214009755-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:40:25,221604128-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:40:34,103904362-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:40:34,111809380-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:40:43,003713592-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:40:43,010919682-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:40:43,016790325-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:40:43,020500087-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:40:43,026875632-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:40:43,032412617-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=664951
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:40:43,039107745-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:40:43,047675321-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'313052\n'
[1] 12:40:43 [SUCCESS] ljishen@10.10.2.2
313052

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:40:43,239290324-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_64KB_write.log.3
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:40:43,259050894-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:40:43,261940090-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ec91e70-3826-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ec91e70-3826-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T19:40:46.151583+0000 Maintaining 128 concurrent writes of 65536 bytes to objects of size 65536 for up to 60 seconds or 0 objects
2021-10-28T19:40:46.151596+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T19:40:46.154218+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T19:40:46.154218+0000     0       0         0         0         0         0           -           0
2021-10-28T19:40:47.154349+0000     1     128       641       513     32.06   32.0625     0.18071    0.191114
2021-10-28T19:40:48.154480+0000     2     128      1281      1153   36.0275        40    0.241261    0.216876
2021-10-28T19:40:49.154606+0000     3     128      1676      1548   32.2464   24.6875    0.316556     0.22445
2021-10-28T19:40:50.154713+0000     4     128      2188      2060   32.1839        32    0.258881    0.234308
2021-10-28T19:40:51.154838+0000     5     128      2828      2700   33.7462        40    0.195711    0.228967
2021-10-28T19:40:52.154959+0000     6     128      3457      3329   34.6731   39.3125    0.208275    0.226358
2021-10-28T19:40:53.155082+0000     7     128      3969      3841   34.2907        32    0.193808    0.232095
2021-10-28T19:40:54.155200+0000     8     128      4492      4364   34.0898   32.6875    0.266647    0.230182
2021-10-28T19:40:55.155313+0000     9     128      5121      4993   34.6696   39.3125    0.239879     0.22748
2021-10-28T19:40:56.155426+0000    10     128      5633      5505   34.4023        32    0.211242     0.23028
2021-10-28T19:40:57.155538+0000    11     128      6145      6017   34.1836        32    0.241137    0.232963
2021-10-28T19:40:58.155650+0000    12     128      6668      6540   34.0586   32.6875    0.191741    0.232973
2021-10-28T19:40:59.155766+0000    13     128      7180      7052   33.8999        32    0.208151     0.23301
2021-10-28T19:41:00.155880+0000    14     128      7692      7564    33.764        32    0.239186    0.232084
2021-10-28T19:41:01.155996+0000    15     128      8204      8076   33.6461        32    0.378525    0.235338
2021-10-28T19:41:02.156108+0000    16     128      8332      8204   32.0432         8    0.968413    0.245464
2021-10-28T19:41:03.156231+0000    17     128      8460      8332   30.6288         8      1.0341    0.257556
2021-10-28T19:41:04.156347+0000    18     128      8588      8460   29.3716         8    0.989733    0.269591
2021-10-28T19:41:05.156415+0000    19     128      8716      8588   28.2468         8    0.895588    0.278677
2021-10-28T19:41:06.156532+0000 min lat: 0.0971232 max lat: 1.0598 avg lat: 0.285657
2021-10-28T19:41:06.156532+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T19:41:06.156532+0000    20     128      8844      8716   27.2344         8    0.743174    0.285657
2021-10-28T19:41:07.156651+0000    21     128      8972      8844   26.3184         8     1.00251    0.296177
2021-10-28T19:41:08.156763+0000    22     128      9100      8972   25.4857         8    0.742133    0.302355
2021-10-28T19:41:09.156828+0000    23     128      9228      9100   24.7255         8     1.05773    0.311391
2021-10-28T19:41:10.156939+0000    24     128      9356      9228   24.0286         8    0.891845    0.321217
2021-10-28T19:41:11.157054+0000    25     128      9484      9356   23.3874         8    0.950249    0.329608
2021-10-28T19:41:12.157167+0000    26     128      9729      9601   23.0768   15.3125    0.821932    0.344701
2021-10-28T19:41:13.157243+0000    27     128      9868      9740   22.5438    8.6875    0.908465    0.352742
2021-10-28T19:41:14.157361+0000    28     128      9996      9868   22.0244         8    0.808165    0.358651
2021-10-28T19:41:15.157479+0000    29     128     10124      9996   21.5407         8    0.908207    0.364629
2021-10-28T19:41:16.157591+0000    30     128     10252     10124   21.0893         8    0.967726    0.373656
2021-10-28T19:41:17.157663+0000    31     128     10380     10252   20.6671         8     1.18197    0.383492
2021-10-28T19:41:18.157772+0000    32     128     10508     10380   20.2712         8    0.836235    0.388759
2021-10-28T19:41:19.157883+0000    33     128     10636     10508   19.8993         8    0.970547    0.395746
2021-10-28T19:41:20.157996+0000    34     128     11265     11137   20.4702   39.3125    0.216579    0.389693
2021-10-28T19:41:21.158062+0000    35     128     11788     11660   20.8192   32.6875    0.199939    0.383388
2021-10-28T19:41:22.158172+0000    36     128     12289     12161   21.1106   31.3125    0.308235    0.377767
2021-10-28T19:41:23.158292+0000    37     128     12812     12684   21.4233   32.6875    0.334114    0.371307
2021-10-28T19:41:24.158402+0000    38     128     13324     13196   21.7016        32    0.216559    0.367473
2021-10-28T19:41:25.158470+0000    39     128     13836     13708   21.9656        32     0.39187    0.363393
2021-10-28T19:41:26.158587+0000 min lat: 0.0971232 max lat: 1.18215 avg lat: 0.357368
2021-10-28T19:41:26.158587+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T19:41:26.158587+0000    40     128     14348     14220   22.2164        32    0.216751    0.357368
2021-10-28T19:41:27.158717+0000    41     128     14860     14732   22.4549        32    0.208363    0.354275
2021-10-28T19:41:28.158832+0000    42     128     15489     15361   22.8561   39.3125    0.300555    0.349292
2021-10-28T19:41:29.158899+0000    43     128     15756     15628   22.7127   16.6875    0.330404    0.349268
2021-10-28T19:41:30.159013+0000    44     128     16257     16129    22.908   31.3125    0.203337    0.348337
2021-10-28T19:41:31.159123+0000    45     128     16396     16268    22.592    8.6875    0.992972    0.352959
2021-10-28T19:41:32.159237+0000    46     128     16524     16396   22.2748         8    0.739724    0.355992
2021-10-28T19:41:33.159336+0000    47     128     16652     16524    21.971         8    0.761298    0.359249
2021-10-28T19:41:34.159449+0000    48     128     16780     16652     21.68         8     1.01935    0.362912
2021-10-28T19:41:35.159569+0000    49     128     16780     16652   21.2375         0           -    0.362912
2021-10-28T19:41:36.159681+0000    50     128     16780     16652   20.8127         0           -    0.362912
2021-10-28T19:41:37.159747+0000    51     128     16908     16780   20.5615   2.66667     3.38966    0.385972
2021-10-28T19:41:38.159862+0000    52     128     16908     16780   20.1661         0           -    0.385972
2021-10-28T19:41:39.159977+0000    53     128     16908     16780   19.7856         0           -    0.385972
2021-10-28T19:41:40.160088+0000    54     128     17036     16908   19.5673   2.66667     2.91244    0.405291
2021-10-28T19:41:41.160153+0000    55     128     17036     16908   19.2116         0           -    0.405291
2021-10-28T19:41:42.160278+0000    56     128     17036     16908   18.8685         0           -    0.405291
2021-10-28T19:41:43.160392+0000    57     128     17164     17036   18.6778   2.66667      3.1716    0.426052
2021-10-28T19:41:44.160506+0000    58     128     17164     17036   18.3558         0           -    0.426052
2021-10-28T19:41:45.160569+0000    59     128     17164     17036   18.0447         0           -    0.426052
2021-10-28T19:41:46.160678+0000 min lat: 0.0971232 max lat: 3.38966 avg lat: 0.426052
2021-10-28T19:41:46.160678+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T19:41:46.160678+0000    60     128     17164     17036   17.7439         0           -    0.426052
2021-10-28T19:41:47.160815+0000 Total time run:         60.5506
Total writes made:      17164
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     17.7166
Stddev Bandwidth:       14.3138
Max bandwidth (MB/sec): 40
Min bandwidth (MB/sec): 0
Average IOPS:           283
Stddev IOPS:            229.055
Max IOPS:               640
Min IOPS:               0
Average Latency(s):     0.449623
Stddev Latency(s):      0.562093
Max latency(s):         3.5905
Min latency(s):         0.0971232

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:41:48,559923563-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:41:48,565439248-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 664951

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:41:48,570885492-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 313052
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:41:48,578804286-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 313052
[1] 12:41:48 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:41:48,765266107-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_64KB_write.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_64KB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:41:48,912912749-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:42:12,938446491-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.16k objects, 1.0 GiB
    usage:   4.6 GiB used, 395 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:42:12,945567341-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:42:21,821522699-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.16k objects, 1.0 GiB
    usage:   3.5 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:42:21,829041059-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:42:30,711566035-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.16k objects, 1.0 GiB
    usage:   3.2 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:42:30,718844712-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:42:39,651903921-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.16k objects, 1.0 GiB
    usage:   3.2 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:42:39,659016194-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:42:48,541582803-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.16k objects, 1.0 GiB
    usage:   3.2 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:42:48,549425243-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:42:48,555030907-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:42:48,558560559-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:42:48,564961752-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:42:48,570256561-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=666850
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:42:48,576728968-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:42:48,585478558-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'313924\n'
[1] 12:42:48 [SUCCESS] ljishen@10.10.2.2
313924

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:42:48,774469525-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_64KB_seq.log.3
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:42:48,794344751-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:42:48,797269223-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2ec91e70-3826-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2ec91e70-3826-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T19:42:51.725408+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T19:42:51.725408+0000     0       0         0         0         0         0           -           0
2021-10-28T19:42:52.725533+0000     1     128      1099       971   60.6758   60.6875    0.103551    0.117732
2021-10-28T19:42:53.725611+0000     2     128      2324      2196   68.6157   76.5625    0.036717     0.11417
2021-10-28T19:42:54.725684+0000     3     128      3459      3331   69.3879   70.9375   0.0982178    0.109888
2021-10-28T19:42:55.725759+0000     4     128      4679      4551   71.1019     76.25   0.0107002     0.11047
2021-10-28T19:42:56.725842+0000     5     128      5881      5753   71.9053    75.125    0.143391    0.109837
2021-10-28T19:42:57.725922+0000     6     128      7260      7132   74.2845   86.1875   0.0677909    0.106695
2021-10-28T19:42:58.725999+0000     7     128      8246      8118   72.4753    61.625   0.0800638    0.109766
2021-10-28T19:42:59.726071+0000     8     128      9419      9291   72.5793   73.3125    0.163546    0.108647
2021-10-28T19:43:00.726187+0000     9     128     10672     10544   73.2153   78.3125     0.17078     0.10822
2021-10-28T19:43:01.726261+0000    10     127     11812     11685   73.0245   71.3125   0.0721851     0.10884
2021-10-28T19:43:02.726329+0000    11     128     13031     12903   73.3059    76.125   0.0466252    0.108428
2021-10-28T19:43:03.726403+0000    12     128     13975     13847   72.1134        59    0.782795    0.107904
2021-10-28T19:43:04.726469+0000    13     128     15486     15358   73.8301   94.4375    0.168315    0.107982
2021-10-28T19:43:05.726536+0000    14     128     16379     16251   72.5429   55.8125   0.0566679    0.109779
2021-10-28T19:43:06.726636+0000 Total time run:       14.8896
Total reads made:     17164
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   72.0471
Average IOPS:         1152
Stddev IOPS:          170.486
Max IOPS:             1511
Min IOPS:             893
Average Latency(s):   0.110253
Max latency(s):       0.899961
Min latency(s):       0.000491015

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:43:07,359950407-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:43:07,365498212-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 666850

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:43:07,371048773-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 313924
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:43:07,378576980-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 313924
[1] 12:43:07 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:43:07,561345711-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_64KB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_64KB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:43:07,713495138-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:43:31,556954172-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.16k objects, 1.0 GiB
    usage:   3.2 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:43:31,564522866-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:43:40,454550637-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.16k objects, 1.0 GiB
    usage:   3.2 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:43:40,462304680-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:43:49,552469320-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.16k objects, 1.0 GiB
    usage:   3.2 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:43:49,559934889-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:43:58,586711928-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.16k objects, 1.0 GiB
    usage:   3.2 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:43:58,594334864-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:44:07,516417866-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.16k objects, 1.0 GiB
    usage:   3.2 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:44:07,523791402-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:44:07,529684678-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T12:44:07,533365556-07:00][RUNNING][ROUND 1/4/21] object_size=256KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:44:07,537031775-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:44:07,546412824-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:44:07,985203522-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/2ec91e70-3826-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 2ec91e70-3826-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:44:07,996765419-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:44:08,000126188-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '2ec91e70-3826-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 2ec91e70-3826-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:44:08,009008557-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 2ec91e70-3826-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 12:44:13 [SUCCESS] 10.10.2.1\n[2] 12:44:14 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:44:14,223939717-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:44:14,236253448-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:44:14,240921274-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:44:14,391517714-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:44:14,396262625-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:44:14,546975764-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:44:14,835521266-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:44:14,840550702-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--0e3d1fae--6bca--40c8--b493--67aa703ac2cb-osd--block--9955745a--adf6--4aa0--b526--708a5cf65756 (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-0e3d1fae-6bca-40c8-b493-67aa703ac2cb" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-9955745a-adf6-4aa0-b526-708a5cf65756"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-0e3d1fae-6bca-40c8-b493-67aa703ac2cb" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-9955745a-adf6-4aa0-b526-708a5cf65756" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-0e3d1fae-6bca-40c8-b493-67aa703ac2cb"\n'
10.10.2.1: b'  Volume group "ceph-0e3d1fae-6bca-40c8-b493-67aa703ac2cb" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:44:15,176924825-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:44:15,186988365-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:44:15,190394879-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 6eb7a5dc-3827-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 6eb7a5dc-3827-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:45:16,848223776-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:45:36,855446913-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:45:36,865533356-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:45:36,869854460-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 6eb7a5dc-3827-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/6eb7a5dc-3827-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:45:45,769512807-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:45:45,779892201-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:45:45,783436524-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 6eb7a5dc-3827-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/6eb7a5dc-3827-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:45:55,450298542-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:45:55,456311477-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:45:55,670049686-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:45:55,673657610-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid 6eb7a5dc-3827-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/6eb7a5dc-3827-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:46:05,163857842-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:46:25,168348582-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:46:25,175425176-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:46:25,184967406-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:46:25,188814689-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 6eb7a5dc-3827-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/6eb7a5dc-3827-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:46:49,565200224-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:47:09,570661672-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:47:09,581030337-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:47:09,584857983-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 6eb7a5dc-3827-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/6eb7a5dc-3827-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     6eb7a5dc-3827-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.nojdxv(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 12:47:18 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:44:07,985203522-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/2ec91e70-3826-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 2ec91e70-3826-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:44:07,996765419-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:44:08,000126188-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '2ec91e70-3826-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 2ec91e70-3826-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:44:08,009008557-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 2ec91e70-3826-11ec-b51d-53e6e728d2d3'
[1] 12:44:13 [SUCCESS] 10.10.2.1
[2] 12:44:14 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:44:14,223939717-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:44:14,236253448-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:44:14,240921274-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:44:14,391517714-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:44:14,396262625-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:44:14,546975764-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:44:14,835521266-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:44:14,840550702-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--0e3d1fae--6bca--40c8--b493--67aa703ac2cb-osd--block--9955745a--adf6--4aa0--b526--708a5cf65756 (253:0)
  Archiving volume group "ceph-0e3d1fae-6bca-40c8-b493-67aa703ac2cb" metadata (seqno 5).
  Releasing logical volume "osd-block-9955745a-adf6-4aa0-b526-708a5cf65756"
  Creating volume group backup "/etc/lvm/backup/ceph-0e3d1fae-6bca-40c8-b493-67aa703ac2cb" (seqno 6).
  Logical volume "osd-block-9955745a-adf6-4aa0-b526-708a5cf65756" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-0e3d1fae-6bca-40c8-b493-67aa703ac2cb"
  Volume group "ceph-0e3d1fae-6bca-40c8-b493-67aa703ac2cb" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:44:15,176924825-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:44:15,186988365-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:44:15,190394879-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 6eb7a5dc-3827-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 6eb7a5dc-3827-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:45:16,848223776-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:45:36,855446913-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:45:36,865533356-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:45:36,869854460-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 6eb7a5dc-3827-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/6eb7a5dc-3827-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:45:45,769512807-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:45:45,779892201-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:45:45,783436524-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 6eb7a5dc-3827-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/6eb7a5dc-3827-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:45:55,450298542-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:45:55,456311477-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:45:55,670049686-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:45:55,673657610-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 6eb7a5dc-3827-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/6eb7a5dc-3827-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:46:05,163857842-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:46:25,168348582-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:46:25,175425176-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:46:25,184967406-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:46:25,188814689-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 6eb7a5dc-3827-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/6eb7a5dc-3827-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:46:49,565200224-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:47:09,570661672-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:47:09,581030337-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:47:09,584857983-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 6eb7a5dc-3827-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/6eb7a5dc-3827-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     6eb7a5dc-3827-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.nojdxv(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:47:18,176653517-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:47:18,184517768-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 12:47:18 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:47:18,664991010-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:47:18,668464807-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:47:18,690406347-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:47:18,693285323-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6eb7a5dc-3827-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6eb7a5dc-3827-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:47:22,719378597-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:47:22,722326262-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6eb7a5dc-3827-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6eb7a5dc-3827-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:47:26,653905289-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:47:26,656872822-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6eb7a5dc-3827-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6eb7a5dc-3827-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:47:30,570289529-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:47:30,573164588-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6eb7a5dc-3827-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6eb7a5dc-3827-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:47:38,455958062-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:47:38,458870581-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6eb7a5dc-3827-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6eb7a5dc-3827-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:47:42,602989850-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:47:42,606176246-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6eb7a5dc-3827-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6eb7a5dc-3827-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:47:47,038014859-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:47:47,041146290-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6eb7a5dc-3827-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6eb7a5dc-3827-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:47:51,571115015-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:47:51,574080965-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6eb7a5dc-3827-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6eb7a5dc-3827-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:47:56,556357902-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:47:56,559418941-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6eb7a5dc-3827-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6eb7a5dc-3827-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:48:01,584347226-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:48:01,587206183-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6eb7a5dc-3827-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6eb7a5dc-3827-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:48:05,636699220-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:48:05,639689015-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6eb7a5dc-3827-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6eb7a5dc-3827-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 18 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 19 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:48:09,643680351-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:48:09,646637935-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6eb7a5dc-3827-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6eb7a5dc-3827-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.39059         -  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default   
-3         0.39059         -  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host node-1
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0  
                       TOTAL  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:48:13,628791114-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:48:37,592020524-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:48:46,580873364-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:48:55,606260642-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:49:04,607520527-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:49:04,614787582-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:49:13,614201606-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:49:13,621902689-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:49:22,589251671-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:49:22,597066078-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:49:31,529623972-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:49:31,536869486-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:49:40,487038416-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:49:40,494783392-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:49:40,500616355-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:49:40,504093528-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:49:40,510564031-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:49:40,515656559-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=674470
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:49:40,522482743-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:49:40,531377506-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'319761\n'
[1] 12:49:40 [SUCCESS] ljishen@10.10.2.2
319761

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:49:40,723288162-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_256KB_write.log.1
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:49:40,743272254-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:49:40,746234166-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6eb7a5dc-3827-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6eb7a5dc-3827-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T19:49:43.819363+0000 Maintaining 128 concurrent writes of 262144 bytes to objects of size 262144 for up to 60 seconds or 0 objects
2021-10-28T19:49:43.819375+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T19:49:43.828086+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T19:49:43.828086+0000     0       0         0         0         0         0           -           0
2021-10-28T19:49:44.828179+0000     1     128       257       129   32.2482     32.25    0.367421    0.597978
2021-10-28T19:49:45.828217+0000     2     128       467       339    42.373      52.5    0.632895    0.668009
2021-10-28T19:49:46.828284+0000     3     128       641       513   42.7477      43.5    0.629707    0.648023
2021-10-28T19:49:47.828355+0000     4     128       851       723   45.1849      52.5    0.533455    0.656991
2021-10-28T19:49:48.828420+0000     5     128       979       851   42.5475        32    0.641062     0.65224
2021-10-28T19:49:49.828490+0000     6     128      1153      1025   42.7057      43.5    0.599232    0.680377
2021-10-28T19:49:50.828560+0000     7     128      1409      1281   45.7472        64    0.624686    0.677501
2021-10-28T19:49:51.828648+0000     8     128      1537      1409   44.0284        32    0.708237    0.681779
2021-10-28T19:49:52.828788+0000     9     128      1793      1665   46.2466        64    0.600704    0.671467
2021-10-28T19:49:53.828854+0000    10     128      1921      1793   44.8217        32    0.748931     0.67593
2021-10-28T19:49:54.828937+0000    11     128      2131      2003   45.5194      52.5    0.589029    0.688376
2021-10-28T19:49:55.829049+0000    12     128      2177      2049   42.6842      11.5    0.808277    0.691065
2021-10-28T19:49:56.829149+0000    13     128      2177      2049   39.4007         0           -    0.691065
2021-10-28T19:49:57.829247+0000    14     128      2259      2131   38.0505     10.25     2.44326    0.758493
2021-10-28T19:49:58.829311+0000    15     128      2305      2177   36.2805      11.5      2.9341    0.804463
2021-10-28T19:49:59.829420+0000    16     128      2305      2177   34.0129         0           -    0.804463
2021-10-28T19:50:00.829520+0000    17     128      2387      2259   33.2179     10.25     3.33071    0.896152
2021-10-28T19:50:01.829630+0000    18     128      2433      2305   32.0112      11.5     3.62628    0.950637
2021-10-28T19:50:02.829773+0000    19     128      2433      2305   30.3263         0           -    0.950637
2021-10-28T19:50:03.829837+0000 min lat: 0.367421 max lat: 3.62697 avg lat: 1.01535
2021-10-28T19:50:03.829837+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T19:50:03.829837+0000    20     128      2515      2387   29.8349     10.25     2.83429     1.01535
2021-10-28T19:50:04.829930+0000    21     128      2561      2433   28.9618      11.5     2.88876     1.05077
2021-10-28T19:50:05.830012+0000    22     128      2561      2433   27.6454         0           -     1.05077
2021-10-28T19:50:06.830117+0000    23     128      2643      2515   27.3346     10.25     3.05433     1.11609
2021-10-28T19:50:07.830229+0000    24     128      2689      2561   26.6747      11.5     2.98261     1.14961
2021-10-28T19:50:08.830294+0000    25     128      2689      2561   25.6078         0           -     1.14961
2021-10-28T19:50:09.830365+0000    26     128      2771      2643   25.4113     10.25     2.69645      1.1976
2021-10-28T19:50:10.830466+0000    27     128      2817      2689    24.896      11.5     2.48994      1.2197
2021-10-28T19:50:11.830532+0000    28     128      2899      2771   24.7389      20.5     2.64174     1.26178
2021-10-28T19:50:12.830603+0000    29     128      2945      2817   24.2824      11.5     2.64248     1.28432
2021-10-28T19:50:13.830668+0000    30     128      2945      2817    23.473         0           -     1.28432
2021-10-28T19:50:14.830736+0000    31     128      3027      2899   23.3771     10.25      2.9371     1.33107
2021-10-28T19:50:15.830806+0000    32     128      3073      2945   23.0059      11.5     2.74463     1.35315
2021-10-28T19:50:16.830872+0000    33     128      3073      2945   22.3087         0           -     1.35315
2021-10-28T19:50:17.830954+0000    34     128      3155      3027   22.2555     10.25     2.45769     1.38307
2021-10-28T19:50:18.831030+0000    35     128      3201      3073   21.9482      11.5     2.79772     1.40425
2021-10-28T19:50:19.831144+0000    36     128      3201      3073   21.3385         0           -     1.40425
2021-10-28T19:50:20.831247+0000    37     128      3283      3155   21.3158     10.25     3.21586     1.45133
2021-10-28T19:50:21.831351+0000    38     128      3329      3201   21.0574      11.5      3.1636     1.47593
2021-10-28T19:50:22.831450+0000    39     128      3329      3201   20.5175         0           -     1.47593
2021-10-28T19:50:23.831513+0000 min lat: 0.367421 max lat: 3.62697 avg lat: 1.50946
2021-10-28T19:50:23.831513+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T19:50:23.831513+0000    40     128      3411      3283    20.517     10.25     2.81846     1.50946
2021-10-28T19:50:24.831612+0000    41     128      3539      3411    20.797        32     1.42088     1.52404
2021-10-28T19:50:25.831723+0000    42     128      3713      3585   21.3375      43.5    0.801581     1.48834
2021-10-28T19:50:26.831822+0000    43     128      3923      3795   22.0621      52.5    0.593899     1.44046
2021-10-28T19:50:27.831933+0000    44     128      4097      3969   22.5492      43.5    0.634192     1.41088
2021-10-28T19:50:28.831996+0000    45     128      4179      4051   22.5036      20.5    0.693852     1.39637
2021-10-28T19:50:29.832074+0000    46     128      4225      4097   22.2644      11.5     1.13339     1.39341
2021-10-28T19:50:30.832184+0000    47     128      4307      4179   22.2268      20.5     2.29292     1.41105
2021-10-28T19:50:31.832291+0000    48     128      4353      4225   22.0033      11.5     2.79761     1.42615
2021-10-28T19:50:32.832400+0000    49     128      4353      4225   21.5542         0           -     1.42615
2021-10-28T19:50:33.832466+0000    50     128      4435      4307   21.5331     10.25     2.85479     1.45335
2021-10-28T19:50:34.832553+0000    51     128      4435      4307   21.1109         0           -     1.45335
2021-10-28T19:50:35.832627+0000    52     128      4481      4353   20.9261      5.75     3.92129     1.47943
2021-10-28T19:50:36.832732+0000    53     128      4481      4353   20.5312         0           -     1.47943
2021-10-28T19:50:37.832833+0000    54     128      4481      4353    20.151         0           -     1.47943
2021-10-28T19:50:38.832897+0000    55     128      4481      4353   19.7846         0           -     1.47943
2021-10-28T19:50:39.833008+0000    56     128      4563      4435   19.7974     5.125     5.74133     1.55822
2021-10-28T19:50:40.833104+0000    57     128      4609      4481   19.6518      11.5     5.17769     1.59538
2021-10-28T19:50:41.833189+0000    58     128      4609      4481    19.313         0           -     1.59538
2021-10-28T19:50:42.833320+0000    59     128      4609      4481   18.9856         0           -     1.59538
2021-10-28T19:50:43.833384+0000 min lat: 0.367421 max lat: 5.74173 avg lat: 1.59538
2021-10-28T19:50:43.833384+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T19:50:43.833384+0000    60     128      4609      4481   18.6692         0           -     1.59538
2021-10-28T19:50:44.833475+0000    61      46      4609      4563   18.6992     5.125      5.2045     1.66024
2021-10-28T19:50:45.833613+0000 Total time run:         61.9849
Total writes made:      4609
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     18.5892
Stddev Bandwidth:       18.0681
Max bandwidth (MB/sec): 64
Min bandwidth (MB/sec): 0
Average IOPS:           74
Stddev IOPS:            72.2829
Max IOPS:               256
Min IOPS:               0
Average Latency(s):     1.69384
Stddev Latency(s):      1.35281
Max latency(s):         5.74173
Min latency(s):         0.367421

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:50:46,568650548-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:50:46,574443615-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 674470

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:50:46,579961333-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 319761
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:50:46,587972451-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 319761
[1] 12:50:46 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:50:46,773737215-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_256KB_write.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_256KB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:50:46,925243677-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:51:11,680627582-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   4.8 GiB used, 395 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:51:11,687659604-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:51:20,597956161-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   3.5 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:51:20,605864424-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:51:29,630165777-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:51:29,637755200-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:51:38,614947017-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:51:38,622725926-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:51:47,579548385-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:51:47,587104014-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:51:47,593080597-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:51:47,596835012-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:51:47,603464426-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:51:47,608902384-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=676633
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:51:47,615645432-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:51:47,624663337-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'320545\n'
[1] 12:51:47 [SUCCESS] ljishen@10.10.2.2
320545

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:51:47,815177521-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_256KB_seq.log.1
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:51:47,835055111-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:51:47,837876698-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6eb7a5dc-3827-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6eb7a5dc-3827-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T19:51:50.772504+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T19:51:50.772504+0000     0       0         0         0         0         0           -           0
2021-10-28T19:51:51.772617+0000     1     128       444       316   78.9859        79    0.456403    0.322108
2021-10-28T19:51:52.772738+0000     2     128       871       743   92.8611    106.75    0.863822    0.299643
2021-10-28T19:51:53.772826+0000     3     128      1247      1119    93.238        94    0.318402    0.314244
2021-10-28T19:51:54.772936+0000     4     128      1709      1581   98.8002     115.5    0.285019    0.307575
2021-10-28T19:51:55.773004+0000     5     128      2056      1928   96.3891     86.75     0.14691    0.321433
2021-10-28T19:51:56.773117+0000     6     128      2457      2329   97.0307    100.25     0.17291    0.316728
2021-10-28T19:51:57.773204+0000     7     128      2859      2731   97.5251     100.5    0.267562    0.316778
2021-10-28T19:51:58.773314+0000     8     128      3368      3240   101.239    127.25    0.163673    0.311655
2021-10-28T19:51:59.773422+0000     9     128      3691      3563   98.9614     80.75    0.278307    0.314919
2021-10-28T19:52:00.773531+0000    10     128      4149      4021   100.514     114.5    0.694655     0.31074
2021-10-28T19:52:01.773618+0000    11     128      4460      4332    98.444     77.75    0.297307    0.319258
2021-10-28T19:52:02.773770+0000 Total time run:       11.6892
Total reads made:     4609
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   98.5742
Average IOPS:         394
Stddev IOPS:          66.1012
Max IOPS:             509
Min IOPS:             311
Average Latency(s):   0.320285
Max latency(s):       1.29378
Min latency(s):       0.00485202

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:52:03,444346223-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:52:03,450369294-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 676633

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:52:03,456132174-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 320545
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:52:03,463975054-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 320545
[1] 12:52:03 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:52:03,653847209-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_256KB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_256KB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:52:03,805190122-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:52:27,583313543-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:52:27,590597550-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:52:36,555399280-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:52:36,563167049-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:52:45,371657620-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:52:45,379120184-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:52:54,328445653-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:52:54,336015037-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:53:03,286374262-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:53:03,294005012-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:53:03,300134955-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T12:53:03,302512566-07:00][RUNNING][ROUND 2/4/21] object_size=256KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:53:03,305959652-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:53:03,315444587-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:53:03,759358465-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/6eb7a5dc-3827-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 6eb7a5dc-3827-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:53:03,770107715-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:53:03,773457474-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '6eb7a5dc-3827-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 6eb7a5dc-3827-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:53:03,781557313-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 6eb7a5dc-3827-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 12:53:09 [SUCCESS] 10.10.2.1\n[2] 12:53:10 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:53:10,020953125-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:53:10,032521976-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:53:10,037329264-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:53:10,186760448-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:53:10,191914228-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:53:10,347517025-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:53:10,635588244-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:53:10,640541406-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--5bf8f829--e0ca--4601--8a81--9b3071d6c496-osd--block--4fd9cd5f--b105--4754--a130--a90f7cd37fa6 (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-5bf8f829-e0ca-4601-8a81-9b3071d6c496" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-4fd9cd5f-b105-4754-a130-a90f7cd37fa6"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-5bf8f829-e0ca-4601-8a81-9b3071d6c496" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-4fd9cd5f-b105-4754-a130-a90f7cd37fa6" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-5bf8f829-e0ca-4601-8a81-9b3071d6c496"\n'
10.10.2.1: b'  Volume group "ceph-5bf8f829-e0ca-4601-8a81-9b3071d6c496" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:53:10,965803550-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:53:10,976033994-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:53:10,979989782-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\n'
10.10.2.1: b'Verifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\npodman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\nHost looks OK\n'
10.10.2.1: b'Cluster fsid: ae12df84-3828-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\nWaiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\nAdding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid ae12df84-3828-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:54:12,086007745-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:54:32,093699964-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:54:32,103581633-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:54:32,107560604-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid ae12df84-3828-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/ae12df84-3828-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:54:41,210474219-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:54:41,220210855-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:54:41,224112601-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid ae12df84-3828-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/ae12df84-3828-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:54:50,464834892-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:54:50,471211972-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:54:50,680956654-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:54:50,684434753-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid ae12df84-3828-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/ae12df84-3828-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:54:59,923600232-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:55:19,928655563-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:55:19,935892830-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:55:19,945978393-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:55:19,950006817-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid ae12df84-3828-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/ae12df84-3828-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:55:44,233125533-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:56:04,238835558-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:56:04,248528653-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T12:56:04,252630655-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid ae12df84-3828-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/ae12df84-3828-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     ae12df84-3828-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.vqujmc(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 12:56:13 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:53:03,759358465-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/6eb7a5dc-3827-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 6eb7a5dc-3827-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:53:03,770107715-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:53:03,773457474-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '6eb7a5dc-3827-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 6eb7a5dc-3827-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:53:03,781557313-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 6eb7a5dc-3827-11ec-b51d-53e6e728d2d3'
[1] 12:53:09 [SUCCESS] 10.10.2.1
[2] 12:53:10 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:53:10,020953125-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:53:10,032521976-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:53:10,037329264-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:53:10,186760448-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:53:10,191914228-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:53:10,347517025-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:53:10,635588244-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:53:10,640541406-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--5bf8f829--e0ca--4601--8a81--9b3071d6c496-osd--block--4fd9cd5f--b105--4754--a130--a90f7cd37fa6 (253:0)
  Archiving volume group "ceph-5bf8f829-e0ca-4601-8a81-9b3071d6c496" metadata (seqno 5).
  Releasing logical volume "osd-block-4fd9cd5f-b105-4754-a130-a90f7cd37fa6"
  Creating volume group backup "/etc/lvm/backup/ceph-5bf8f829-e0ca-4601-8a81-9b3071d6c496" (seqno 6).
  Logical volume "osd-block-4fd9cd5f-b105-4754-a130-a90f7cd37fa6" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-5bf8f829-e0ca-4601-8a81-9b3071d6c496"
  Volume group "ceph-5bf8f829-e0ca-4601-8a81-9b3071d6c496" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:53:10,965803550-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:53:10,976033994-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:53:10,979989782-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: ae12df84-3828-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid ae12df84-3828-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:54:12,086007745-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:54:32,093699964-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:54:32,103581633-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:54:32,107560604-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid ae12df84-3828-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/ae12df84-3828-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:54:41,210474219-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:54:41,220210855-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:54:41,224112601-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid ae12df84-3828-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/ae12df84-3828-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:54:50,464834892-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:54:50,471211972-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:54:50,680956654-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:54:50,684434753-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid ae12df84-3828-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/ae12df84-3828-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:54:59,923600232-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:55:19,928655563-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:55:19,935892830-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:55:19,945978393-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:55:19,950006817-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid ae12df84-3828-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/ae12df84-3828-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:55:44,233125533-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:56:04,238835558-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:56:04,248528653-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:56:04,252630655-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid ae12df84-3828-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/ae12df84-3828-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     ae12df84-3828-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.vqujmc(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:56:13,173406231-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:56:13,181106552-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 12:56:13 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:56:13,656699778-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:56:13,660101699-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:56:13,682031597-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:56:13,684798041-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ae12df84-3828-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ae12df84-3828-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:56:17,710505243-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:56:17,713460773-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ae12df84-3828-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ae12df84-3828-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:56:21,742851746-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:56:21,746020358-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ae12df84-3828-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ae12df84-3828-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:56:25,604503468-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:56:25,607445662-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ae12df84-3828-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ae12df84-3828-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:56:33,504712555-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:56:33,507560903-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ae12df84-3828-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ae12df84-3828-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:56:38,106355861-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:56:38,109368899-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ae12df84-3828-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ae12df84-3828-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:56:42,589074368-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:56:42,592077377-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ae12df84-3828-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ae12df84-3828-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:56:46,556568250-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:56:46,559554257-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ae12df84-3828-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ae12df84-3828-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:56:51,331350502-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:56:51,334418624-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ae12df84-3828-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ae12df84-3828-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:56:55,598541856-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:56:55,601629334-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ae12df84-3828-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ae12df84-3828-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:56:59,918450999-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:56:59,921391200-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ae12df84-3828-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ae12df84-3828-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 20 lfor 0/0/17 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 21 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:57:03,817151134-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:57:03,820176766-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ae12df84-3828-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ae12df84-3828-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.39059         -  400 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default   
-3         0.39059         -  400 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host node-1
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0  
                       TOTAL  400 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  400 GiB  0.00                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:57:07,703346954-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:57:31,626775597-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:57:40,581195972-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:57:49,546095452-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:57:58,452688731-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:57:58,460292841-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:58:07,317900190-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:58:07,325632782-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:58:16,150517241-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:58:16,158288517-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:58:25,040090576-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:58:25,047728081-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:58:34,029751129-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:58:34,037674764-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:58:34,043121380-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:58:34,046365224-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:58:34,053230514-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:58:34,058770165-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=684298
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:58:34,065726567-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:58:34,074548354-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'326310\n'
[1] 12:58:34 [SUCCESS] ljishen@10.10.2.2
326310

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:58:34,266761695-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_256KB_write.log.2
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:58:34,287103937-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:58:34,289985959-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ae12df84-3828-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ae12df84-3828-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T19:58:37.300192+0000 Maintaining 128 concurrent writes of 262144 bytes to objects of size 262144 for up to 60 seconds or 0 objects
2021-10-28T19:58:37.300204+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T19:58:37.309089+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T19:58:37.309089+0000     0       0         0         0         0         0           -           0
2021-10-28T19:58:38.309224+0000     1     128       257       129   32.2474     32.25    0.387017    0.674994
2021-10-28T19:58:39.309318+0000     2     128       385       257   32.1222        32    0.863464    0.678742
2021-10-28T19:58:40.309407+0000     3     128       595       467   38.9133      52.5    0.658273    0.732854
2021-10-28T19:58:41.309478+0000     4     128       769       641   40.0592      43.5    0.740621    0.711274
2021-10-28T19:58:42.309572+0000     5     128       979       851   42.5464      52.5    0.634745    0.703214
2021-10-28T19:58:43.309662+0000     6     128      1153      1025   42.7046      43.5    0.664955     0.69413
2021-10-28T19:58:44.309754+0000     7     128      1363      1235   44.1033      52.5    0.877566    0.706871
2021-10-28T19:58:45.309828+0000     8     128      1491      1363   42.5901        32    0.659001    0.701466
2021-10-28T19:58:46.309920+0000     9     128      1747      1619   44.9683        64    0.618882    0.702356
2021-10-28T19:58:47.310015+0000    10     128      1921      1793   44.8211      43.5    0.624472    0.699766
2021-10-28T19:58:48.310104+0000    11     128      2131      2003   45.5188      52.5    0.682898    0.694829
2021-10-28T19:58:49.310178+0000    12     128      2177      2049   42.6838      11.5     0.73221    0.695671
2021-10-28T19:58:50.310268+0000    13     128      2177      2049   39.4004         0           -    0.695671
2021-10-28T19:58:51.310359+0000    14     128      2259      2131   38.0503     10.25     2.63037    0.770151
2021-10-28T19:58:52.310447+0000    15     128      2305      2177   36.2802      11.5      3.1677    0.820807
2021-10-28T19:58:53.310517+0000    16     128      2305      2177   34.0127         0           -    0.820807
2021-10-28T19:58:54.310608+0000    17     128      2387      2259   33.2177     10.25      2.9958    0.899747
2021-10-28T19:58:55.310701+0000    18     128      2387      2259   31.3723         0           -    0.899747
2021-10-28T19:58:56.310787+0000    19     128      2433      2305   30.3263      5.75     3.46888    0.950819
2021-10-28T19:58:57.310856+0000 min lat: 0.387017 max lat: 3.46926 avg lat: 1.02686
2021-10-28T19:58:57.310856+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T19:58:57.310856+0000    20     128      2515      2387   29.8349      20.5     3.16909     1.02686
2021-10-28T19:58:58.310951+0000    21     128      2515      2387   28.4142         0           -     1.02686
2021-10-28T19:58:59.311042+0000    22     128      2561      2433   27.6453      5.75     3.05588     1.06542
2021-10-28T19:59:00.311134+0000    23     128      2643      2515   27.3346      20.5     2.76766     1.12068
2021-10-28T19:59:01.311208+0000    24     128      2643      2515   26.1957         0           -     1.12068
2021-10-28T19:59:02.311301+0000    25     128      2689      2561   25.6078      5.75     2.93583     1.15328
2021-10-28T19:59:03.311384+0000    26     128      2771      2643   25.4113      20.5     3.08811     1.21367
2021-10-28T19:59:04.311472+0000    27     128      2817      2689    24.896      11.5     2.65293     1.23814
2021-10-28T19:59:05.311542+0000    28     128      2817      2689   24.0069         0           -     1.23814
2021-10-28T19:59:06.311632+0000    29     128      2899      2771   23.8859     10.25     2.65314     1.27985
2021-10-28T19:59:07.311719+0000    30     128      2945      2817    23.473      11.5     2.58509     1.30131
2021-10-28T19:59:08.311809+0000    31     128      2945      2817   22.7158         0           -     1.30131
2021-10-28T19:59:09.311879+0000    32     128      3027      2899   22.6465     10.25     2.99561     1.34938
2021-10-28T19:59:10.311971+0000    33     128      3073      2945   22.3087      11.5     2.93943     1.37421
2021-10-28T19:59:11.312048+0000    34     128      3155      3027   22.2555      20.5     2.33183     1.40015
2021-10-28T19:59:12.312139+0000    35     128      3201      3073   21.9481      11.5     2.33826     1.41419
2021-10-28T19:59:13.312211+0000    36     128      3201      3073   21.3385         0           -     1.41419
2021-10-28T19:59:14.312298+0000    37     128      3283      3155   21.3158     10.25     2.79197        1.45
2021-10-28T19:59:15.312388+0000    38     128      3329      3201   21.0574      11.5     2.84848      1.4701
2021-10-28T19:59:16.312480+0000    39     128      3411      3283   21.0431      20.5     2.41584     1.49371
2021-10-28T19:59:17.312557+0000 min lat: 0.387017 max lat: 3.46926 avg lat: 1.4835
2021-10-28T19:59:17.312557+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T19:59:17.312557+0000    40     128      3539      3411   21.3169        32    0.900601      1.4835
2021-10-28T19:59:18.312658+0000    41     128      3713      3585   21.8579      43.5    0.703643     1.45312
2021-10-28T19:59:19.312746+0000    42     128      3841      3713   22.0993        32    0.830636     1.43336
2021-10-28T19:59:20.312834+0000    43     128      4097      3969   23.0736        64    0.623064     1.37965
2021-10-28T19:59:21.312911+0000    44     128      4225      4097   23.2764        32    0.932051     1.36138
2021-10-28T19:59:22.313002+0000    45     128      4225      4097   22.7592         0           -     1.36138
2021-10-28T19:59:23.313093+0000    46     128      4307      4179     22.71     10.25     2.01988      1.3743
2021-10-28T19:59:24.313183+0000    47     128      4353      4225   22.4715      11.5     2.90935     1.39101
2021-10-28T19:59:25.313258+0000    48     128      4353      4225   22.0033         0           -     1.39101
2021-10-28T19:59:26.313343+0000    49     128      4435      4307   21.9726     10.25     3.08274     1.42322
2021-10-28T19:59:27.313431+0000    50     128      4435      4307   21.5332         0           -     1.42322
2021-10-28T19:59:28.313519+0000    51     128      4481      4353   21.3364      5.75     3.86145     1.44898
2021-10-28T19:59:29.313594+0000    52     128      4481      4353   20.9261         0           -     1.44898
2021-10-28T19:59:30.313681+0000    53     128      4481      4353   20.5313         0           -     1.44898
2021-10-28T19:59:31.313767+0000    54     128      4481      4353   20.1511         0           -     1.44898
2021-10-28T19:59:32.313854+0000    55     128      4563      4435   20.1574     5.125      5.8373     1.52986
2021-10-28T19:59:33.313929+0000    56     128      4563      4435   19.7974         0           -     1.52986
2021-10-28T19:59:34.314021+0000    57     128      4609      4481   19.6518      5.75     5.47258     1.57033
2021-10-28T19:59:35.314111+0000    58     128      4609      4481    19.313         0           -     1.57033
2021-10-28T19:59:36.314197+0000    59     128      4609      4481   18.9857         0           -     1.57033
2021-10-28T19:59:37.314266+0000 min lat: 0.387017 max lat: 5.83755 avg lat: 1.63405
2021-10-28T19:59:37.314266+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T19:59:37.314266+0000    60     128      4691      4563   19.0109   6.83333     5.11317     1.63405
2021-10-28T19:59:38.314364+0000    61     128      4691      4563   18.6992         0           -     1.63405
2021-10-28T19:59:39.314453+0000    62      82      4691      4609   18.5831      5.75     5.15844     1.66923
2021-10-28T19:59:40.314540+0000    63      82      4691      4609   18.2881         0           -     1.66923
2021-10-28T19:59:41.314610+0000    64      82      4691      4609   18.0024         0           -     1.66923
2021-10-28T19:59:42.314744+0000 Total time run:         64.8625
Total writes made:      4691
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     18.0806
Stddev Bandwidth:       18.2368
Max bandwidth (MB/sec): 64
Min bandwidth (MB/sec): 0
Average IOPS:           72
Stddev IOPS:            72.9546
Max IOPS:               256
Min IOPS:               0
Average Latency(s):     1.73486
Stddev Latency(s):      1.4271
Max latency(s):         5.83755
Min latency(s):         0.387017

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:59:43,675304893-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:59:43,681045112-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 684298

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:59:43,686737652-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 326310
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:59:43,694423158-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 326310
[1] 12:59:43 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:59:43,881957909-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_256KB_write.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_256KB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T12:59:44,029504479-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:00:08,022387080-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.69k objects, 1.1 GiB
    usage:   4.8 GiB used, 395 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:00:08,029518952-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:00:17,079081127-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.69k objects, 1.1 GiB
    usage:   3.7 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:00:17,086513606-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:00:26,001675584-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.69k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:00:26,009470025-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:00:34,885823331-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.69k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:00:34,893116186-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:00:43,958039899-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.69k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:00:43,965502284-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:00:43,971028660-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:00:43,974538546-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:00:43,981168272-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:00:43,986771102-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=686261
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:00:43,993524832-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:00:44,002300522-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'327227\n'
[1] 13:00:44 [SUCCESS] ljishen@10.10.2.2
327227

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:00:44,190180093-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_256KB_seq.log.2
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:00:44,209982146-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:00:44,212971721-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ae12df84-3828-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ae12df84-3828-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T20:00:47.295300+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T20:00:47.295300+0000     0       0         0         0         0         0           -           0
2021-10-28T20:00:48.295429+0000     1     128       539       411    102.73    102.75    0.276495    0.258965
2021-10-28T20:00:49.295510+0000     2     128       947       819   102.361       102    0.373136    0.281912
2021-10-28T20:00:50.295578+0000     3     128      1383      1255   104.571       109    0.194452    0.285454
2021-10-28T20:00:51.295683+0000     4     128      1747      1619   101.176        91    0.243494    0.298617
2021-10-28T20:00:52.295798+0000     5     128      2196      2068   103.388    112.25    0.162054    0.289562
2021-10-28T20:00:53.295874+0000     6     128      2519      2391   99.6144     80.75    0.138511    0.302134
2021-10-28T20:00:54.295946+0000     7     128      2991      2863    102.24       118    0.126336    0.306284
2021-10-28T20:00:55.296060+0000     8     128      3420      3292   102.864    107.25    0.101276    0.295836
2021-10-28T20:00:56.296175+0000     9     128      3841      3713   103.128    105.25     1.47717    0.294894
2021-10-28T20:00:57.296252+0000    10     128      4143      4015   100.365      75.5    0.222074    0.301306
2021-10-28T20:00:58.296362+0000    11     128      4463      4335   98.5126        80    0.193106    0.306064
2021-10-28T20:00:59.296503+0000 Total time run:       11.7693
Total reads made:     4691
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   99.6448
Average IOPS:         398
Stddev IOPS:          57.6133
Max IOPS:             472
Min IOPS:             302
Average Latency(s):   0.317291
Max latency(s):       1.94792
Min latency(s):       0.00260775

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:00:59,926756243-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:00:59,932751061-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 686261

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:00:59,938715494-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 327227
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:00:59,946425285-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 327227
[1] 13:01:00 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:01:00,133857672-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_256KB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_256KB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:01:00,282357509-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:01:24,132817316-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.69k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:01:24,140291133-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:01:33,236967103-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.69k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:01:33,244645937-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:01:42,060762367-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.69k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:01:42,068230873-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:01:51,109610269-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.69k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:01:51,117478439-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:02:00,182614937-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.69k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:02:00,190572346-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:02:00,196470833-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T13:02:00,198784885-07:00][RUNNING][ROUND 3/4/21] object_size=256KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:02:00,202169414-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:02:00,211810516-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:02:00,670820641-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/ae12df84-3828-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid ae12df84-3828-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:02:00,681926632-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:02:00,685732838-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'ae12df84-3828-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid ae12df84-3828-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:02:00,694868965-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid ae12df84-3828-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 13:02:06 [SUCCESS] 10.10.2.1\n[2] 13:02:06 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:02:06,788584275-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:02:06,800170538-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:02:06,804914077-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:02:06,954999286-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:02:06,959436709-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:02:07,115657764-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:02:07,403373717-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:02:07,408603369-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--4c97967f--c2be--4e48--b5c2--c309b63dd136-osd--block--9bd141bb--371a--4741--8112--1ac4c4d4e5e4 (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-4c97967f-c2be-4e48-b5c2-c309b63dd136" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-9bd141bb-371a-4741-8112-1ac4c4d4e5e4"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-4c97967f-c2be-4e48-b5c2-c309b63dd136" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-9bd141bb-371a-4741-8112-1ac4c4d4e5e4" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-4c97967f-c2be-4e48-b5c2-c309b63dd136"\n'
10.10.2.1: b'  Volume group "ceph-4c97967f-c2be-4e48-b5c2-c309b63dd136" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:02:07,740921860-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:02:07,750675809-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:02:07,754705414-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: ee03f06e-3829-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\n'
10.10.2.1: b'Waiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid ee03f06e-3829-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:03:08,528179258-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:03:28,535875408-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:03:28,546055257-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:03:28,549825816-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid ee03f06e-3829-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/ee03f06e-3829-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:03:37,499530129-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:03:37,509018819-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:03:37,512781573-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid ee03f06e-3829-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/ee03f06e-3829-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:03:46,588627764-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:03:46,594528949-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:03:46,810973421-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:03:46,814919911-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid ee03f06e-3829-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/ee03f06e-3829-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:03:55,497699615-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:04:15,502959489-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:04:15,509528349-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:04:15,520172251-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:04:15,524127397-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid ee03f06e-3829-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/ee03f06e-3829-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:04:39,893054859-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:04:59,898769946-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:04:59,908774506-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:04:59,912573438-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid ee03f06e-3829-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/ee03f06e-3829-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     ee03f06e-3829-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.pnfpwy(active, since 2m)\n    osd: 1 osds: 1 up (since 19s), 1 in (since 41s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 13:05:08 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:02:00,670820641-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/ae12df84-3828-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid ae12df84-3828-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:02:00,681926632-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:02:00,685732838-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'ae12df84-3828-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid ae12df84-3828-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:02:00,694868965-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid ae12df84-3828-11ec-b51d-53e6e728d2d3'
[1] 13:02:06 [SUCCESS] 10.10.2.1
[2] 13:02:06 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:02:06,788584275-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:02:06,800170538-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:02:06,804914077-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:02:06,954999286-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:02:06,959436709-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:02:07,115657764-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:02:07,403373717-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:02:07,408603369-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--4c97967f--c2be--4e48--b5c2--c309b63dd136-osd--block--9bd141bb--371a--4741--8112--1ac4c4d4e5e4 (253:0)
  Archiving volume group "ceph-4c97967f-c2be-4e48-b5c2-c309b63dd136" metadata (seqno 5).
  Releasing logical volume "osd-block-9bd141bb-371a-4741-8112-1ac4c4d4e5e4"
  Creating volume group backup "/etc/lvm/backup/ceph-4c97967f-c2be-4e48-b5c2-c309b63dd136" (seqno 6).
  Logical volume "osd-block-9bd141bb-371a-4741-8112-1ac4c4d4e5e4" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-4c97967f-c2be-4e48-b5c2-c309b63dd136"
  Volume group "ceph-4c97967f-c2be-4e48-b5c2-c309b63dd136" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:02:07,740921860-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:02:07,750675809-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:02:07,754705414-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: ee03f06e-3829-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid ee03f06e-3829-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:03:08,528179258-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:03:28,535875408-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:03:28,546055257-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:03:28,549825816-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid ee03f06e-3829-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/ee03f06e-3829-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:03:37,499530129-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:03:37,509018819-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:03:37,512781573-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid ee03f06e-3829-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/ee03f06e-3829-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:03:46,588627764-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:03:46,594528949-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:03:46,810973421-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:03:46,814919911-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid ee03f06e-3829-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/ee03f06e-3829-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:03:55,497699615-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:04:15,502959489-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:04:15,509528349-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:04:15,520172251-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:04:15,524127397-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid ee03f06e-3829-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/ee03f06e-3829-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:04:39,893054859-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:04:59,898769946-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:04:59,908774506-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:04:59,912573438-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid ee03f06e-3829-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/ee03f06e-3829-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     ee03f06e-3829-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.pnfpwy(active, since 2m)
    osd: 1 osds: 1 up (since 19s), 1 in (since 41s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:05:08,618530875-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:05:08,626207904-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 13:05:08 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:05:09,100670807-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:05:09,104312040-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:05:09,125821942-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:05:09,128799043-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ee03f06e-3829-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ee03f06e-3829-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:05:13,315017187-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:05:13,317854715-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ee03f06e-3829-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ee03f06e-3829-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:05:17,355244986-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:05:17,358269837-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ee03f06e-3829-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ee03f06e-3829-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:05:21,372279158-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:05:21,375596892-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ee03f06e-3829-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ee03f06e-3829-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:05:29,414365402-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:05:29,417353424-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ee03f06e-3829-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ee03f06e-3829-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:05:33,646556681-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:05:33,649677473-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ee03f06e-3829-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ee03f06e-3829-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:05:37,654194394-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:05:37,657116131-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ee03f06e-3829-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ee03f06e-3829-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:05:42,620954456-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:05:42,623869891-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ee03f06e-3829-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ee03f06e-3829-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:05:47,787528684-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:05:47,790548395-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ee03f06e-3829-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ee03f06e-3829-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:05:52,253579017-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:05:52,256720488-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ee03f06e-3829-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ee03f06e-3829-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:05:56,618857714-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:05:56,622071141-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ee03f06e-3829-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ee03f06e-3829-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 21 lfor 0/0/17 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:06:00,486035907-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:06:00,489130169-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ee03f06e-3829-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ee03f06e-3829-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.39059         -  400 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default   
-3         0.39059         -  400 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host node-1
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0  
                       TOTAL  400 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  400 GiB  0.00                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:06:04,355575209-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:06:28,183066967-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:06:37,132658789-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:06:46,239406944-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:06:55,161754926-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:06:55,169071337-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:07:04,160640863-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:07:04,168521105-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:07:13,088200408-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:07:13,096043590-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:07:22,000068283-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:07:22,008007597-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:07:31,073148245-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:07:31,080777244-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:07:31,086935000-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:07:31,090538623-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:07:31,097448316-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:07:31,103284245-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=692540
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:07:31,110371875-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:07:31,119435197-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'333008\n'
[1] 13:07:31 [SUCCESS] ljishen@10.10.2.2
333008

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:07:31,311134071-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_256KB_write.log.3
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:07:31,331228164-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:07:31,334122820-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ee03f06e-3829-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ee03f06e-3829-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T20:07:34.311111+0000 Maintaining 128 concurrent writes of 262144 bytes to objects of size 262144 for up to 60 seconds or 0 objects
2021-10-28T20:07:34.311126+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T20:07:34.319849+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T20:07:34.319849+0000     0       0         0         0         0         0           -           0
2021-10-28T20:07:35.319987+0000     1     128       257       129   32.2467     32.25    0.507728    0.658109
2021-10-28T20:07:36.320101+0000     2     128       467       339   42.3704      52.5    0.674917    0.662636
2021-10-28T20:07:37.320174+0000     3     128       641       513   42.7459      43.5    0.638328    0.660842
2021-10-28T20:07:38.320287+0000     4     128       769       641   40.0585        32    0.941109    0.720366
2021-10-28T20:07:39.320355+0000     5     128       979       851    42.546      52.5     0.72815    0.701027
2021-10-28T20:07:40.320466+0000     6     128      1153      1025   42.7042      43.5    0.636328    0.689732
2021-10-28T20:07:41.320538+0000     7     128      1363      1235    44.103      52.5    0.624542    0.680406
2021-10-28T20:07:42.320650+0000     8     128      1537      1409    44.027      43.5      0.5914    0.689396
2021-10-28T20:07:43.320760+0000     9     128      1665      1537   42.6903        32    0.650147    0.683681
2021-10-28T20:07:44.320824+0000    10     128      1875      1747   43.6709      52.5    0.590976    0.707136
2021-10-28T20:07:45.320898+0000    11     128      2049      1921   43.6551      43.5    0.592599    0.696207
2021-10-28T20:07:46.321011+0000    12     128      2177      2049   42.6835        32    0.849546     0.70092
2021-10-28T20:07:47.321126+0000    13     128      2177      2049   39.4001         0           -     0.70092
2021-10-28T20:07:48.321238+0000    14     128      2259      2131   38.0499     10.25     2.58392    0.773374
2021-10-28T20:07:49.321306+0000    15     128      2305      2177   36.2799      11.5     3.25139    0.825736
2021-10-28T20:07:50.321422+0000    16     128      2305      2177   34.0124         0           -    0.825736
2021-10-28T20:07:51.321532+0000    17     128      2387      2259   33.2174     10.25     3.16613    0.910686
2021-10-28T20:07:52.321643+0000    18     128      2387      2259   31.3719         0           -    0.910686
2021-10-28T20:07:53.321717+0000    19     128      2433      2305    30.326      5.75     3.39481    0.960255
2021-10-28T20:07:54.321789+0000 min lat: 0.40192 max lat: 3.39525 avg lat: 1.03059
2021-10-28T20:07:54.321789+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T20:07:54.321789+0000    20     128      2515      2387   29.8347      20.5     3.00761     1.03059
2021-10-28T20:07:55.321915+0000    21     128      2515      2387   28.4139         0           -     1.03059
2021-10-28T20:07:56.322027+0000    22     128      2561      2433    27.645      5.75     3.16125     1.07087
2021-10-28T20:07:57.322103+0000    23     128      2643      2515   27.3343      20.5     2.92488     1.13132
2021-10-28T20:07:58.322213+0000    24     128      2643      2515   26.1954         0           -     1.13132
2021-10-28T20:07:59.322276+0000    25     128      2689      2561   25.6076      5.75     2.82683     1.16177
2021-10-28T20:08:00.322388+0000    26     128      2771      2643    25.411      20.5     3.09412     1.22172
2021-10-28T20:08:01.322465+0000    27     128      2817      2689   24.8958      11.5     2.77638     1.24831
2021-10-28T20:08:02.322578+0000    28     128      2817      2689   24.0066         0           -     1.24831
2021-10-28T20:08:03.322689+0000    29     128      2899      2771   23.8856     10.25     2.82837     1.29507
2021-10-28T20:08:04.322754+0000    30     128      2945      2817   23.4728      11.5     2.74075     1.31867
2021-10-28T20:08:05.322831+0000    31     128      2945      2817   22.7156         0           -     1.31867
2021-10-28T20:08:06.322940+0000    32     128      3027      2899   22.6463     10.25      2.8309     1.36145
2021-10-28T20:08:07.323052+0000    33     128      3073      2945   22.3085      11.5     2.76989     1.38345
2021-10-28T20:08:08.323169+0000    34     128      3073      2945   21.6523         0           -     1.38345
2021-10-28T20:08:09.323230+0000    35     128      3155      3027   21.6194     10.25     2.66147     1.41806
2021-10-28T20:08:10.323348+0000    36     128      3201      3073   21.3382      11.5     2.87454     1.43986
2021-10-28T20:08:11.323465+0000    37     128      3201      3073   20.7615         0           -     1.43986
2021-10-28T20:08:12.323574+0000    38     128      3283      3155   20.7546     10.25     3.07762     1.48242
2021-10-28T20:08:13.323646+0000    39     128      3329      3201   20.5173      11.5     3.30298     1.50858
2021-10-28T20:08:14.323711+0000 min lat: 0.40192 max lat: 3.39525 avg lat: 1.50858
2021-10-28T20:08:14.323711+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T20:08:14.323711+0000    40     128      3329      3201   20.0043         0           -     1.50858
2021-10-28T20:08:15.323833+0000    41     128      3411      3283   20.0164     10.25     2.86564     1.54248
2021-10-28T20:08:16.323944+0000    42     128      3585      3457   20.5754      43.5     0.63432     1.53683
2021-10-28T20:08:17.324017+0000    43     128      3785      3657   21.2596        50    0.896557     1.49464
2021-10-28T20:08:18.324130+0000    44     128      3969      3841   21.8218        46    0.586458     1.46011
2021-10-28T20:08:19.324203+0000    45     128      4097      3969   22.0479        32    0.760433     1.43506
2021-10-28T20:08:20.324321+0000    46     128      4225      4097   22.2642        32    0.929348     1.41755
2021-10-28T20:08:21.324391+0000    47     128      4225      4097   21.7905         0           -     1.41755
2021-10-28T20:08:22.324501+0000    48     128      4307      4179   21.7635     10.25     1.86084     1.42625
2021-10-28T20:08:23.324612+0000    49     128      4353      4225    21.554      11.5      2.8249     1.44148
2021-10-28T20:08:24.324677+0000    50     128      4353      4225    21.123         0           -     1.44148
2021-10-28T20:08:25.324751+0000    51     128      4435      4307   21.1107     10.25     3.11897     1.47341
2021-10-28T20:08:26.324864+0000    52     128      4481      4353   20.9259      11.5     3.19842     1.49164
2021-10-28T20:08:27.324983+0000    53     128      4481      4353    20.531         0           -     1.49164
2021-10-28T20:08:28.325103+0000    54     128      4481      4353   20.1508         0           -     1.49164
2021-10-28T20:08:29.325172+0000    55     128      4481      4353   19.7845         0           -     1.49164
2021-10-28T20:08:30.325281+0000    56     128      4563      4435   19.7972     5.125     5.07015     1.55769
2021-10-28T20:08:31.325391+0000    57     128      4609      4481   19.6516      11.5     5.29736     1.59608
2021-10-28T20:08:32.325502+0000    58     128      4609      4481   19.3128         0           -     1.59608
2021-10-28T20:08:33.325577+0000    59     128      4609      4481   18.9855         0           -     1.59608
2021-10-28T20:08:34.325643+0000 min lat: 0.40192 max lat: 5.29773 avg lat: 1.59608
2021-10-28T20:08:34.325643+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T20:08:34.325643+0000    60     128      4609      4481    18.669         0           -     1.59608
2021-10-28T20:08:35.325749+0000    61      46      4609      4563    18.699     5.125     4.96915     1.65682
2021-10-28T20:08:36.325890+0000 Total time run:         61.6636
Total writes made:      4609
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     18.6861
Stddev Bandwidth:       17.5123
Max bandwidth (MB/sec): 52.5
Min bandwidth (MB/sec): 0
Average IOPS:           74
Stddev IOPS:            70.0598
Max IOPS:               210
Min IOPS:               0
Average Latency(s):     1.68679
Stddev Latency(s):      1.30326
Max latency(s):         5.29773
Min latency(s):         0.40192

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:08:38,294373203-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:08:38,300197451-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 692540

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:08:38,305932981-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 333008
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:08:38,313880701-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 333008
[1] 13:08:38 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:08:38,501923704-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_256KB_write.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_256KB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:08:38,649588572-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:09:03,146791819-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   4.6 GiB used, 395 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:09:03,154217315-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:09:12,055380315-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   4.9 GiB used, 395 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:09:12,063160028-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:09:21,088323008-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:09:21,096119222-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:09:29,993474185-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:09:30,001110448-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:09:38,882295850-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:09:38,890548846-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:09:38,896559525-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:09:38,900155151-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:09:38,907046180-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:09:38,912610167-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=693906
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:09:38,919775292-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:09:38,928767610-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'333697\n'
[1] 13:09:39 [SUCCESS] ljishen@10.10.2.2
333697

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:09:39,119082494-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_256KB_seq.log.3
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:09:39,139158233-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:09:39,142130285-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ee03f06e-3829-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ee03f06e-3829-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T20:09:42.076994+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T20:09:42.076994+0000     0       0         0         0         0         0           -           0
2021-10-28T20:09:43.077125+0000     1     128       530       402    100.48     100.5    0.310239    0.241885
2021-10-28T20:09:44.077239+0000     2     128       982       854   106.733       113    0.222649     0.28199
2021-10-28T20:09:45.077331+0000     3     127      1391      1264   105.319     102.5    0.463981    0.279416
2021-10-28T20:09:46.077438+0000     4     128      1732      1604   100.237        85    0.418723    0.299242
2021-10-28T20:09:47.077550+0000     5     128      2136      2008   100.387       101     0.54917    0.299415
2021-10-28T20:09:48.077659+0000     6     128      2549      2421   100.863    103.25    0.258562    0.310701
2021-10-28T20:09:49.077769+0000     7     128      2900      2772    98.988     87.75    0.553486    0.311024
2021-10-28T20:09:50.077871+0000     8     128      3350      3222   100.676     112.5    0.369119    0.310334
2021-10-28T20:09:51.077952+0000     9     128      3828      3700   102.766     119.5    0.162624    0.306535
2021-10-28T20:09:52.078055+0000    10     128      4187      4059   101.463     89.75    0.356326    0.310206
2021-10-28T20:09:53.078166+0000    11     128      4520      4392   99.8069     83.25    0.713605    0.310374
2021-10-28T20:09:54.078303+0000 Total time run:       11.6561
Total reads made:     4609
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   98.8542
Average IOPS:         395
Stddev IOPS:          48.7259
Max IOPS:             478
Min IOPS:             333
Average Latency(s):   0.319568
Max latency(s):       1.0571
Min latency(s):       0.00887884

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:09:54,669282082-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:09:54,675121238-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 693906

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:09:54,681255098-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 333697
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:09:54,689287588-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 333697
[1] 13:09:54 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:09:54,878070975-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_256KB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_256KB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:09:55,029185525-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:10:18,930936491-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:10:18,938881936-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:10:27,907496078-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:10:27,915456812-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:10:36,863992877-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:10:36,871622437-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:10:45,818753830-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:10:45,826625467-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:10:54,769025056-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:10:54,776926819-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:10:54,782961463-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T13:10:54,786460879-07:00][RUNNING][ROUND 1/5/21] object_size=1MB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:10:54,790107953-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:10:54,799787507-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:10:55,477404937-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/ee03f06e-3829-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid ee03f06e-3829-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:10:55,488153074-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:10:55,491688161-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'ee03f06e-3829-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid ee03f06e-3829-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:10:55,500650611-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid ee03f06e-3829-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 13:11:00 [SUCCESS] 10.10.2.1\n[2] 13:11:01 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:11:01,530129439-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:11:01,541991612-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:11:01,546502192-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:11:01,700189220-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:11:01,704993231-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:11:01,859463561-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:11:02,147519747-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:11:02,152537791-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--5435efaa--4e90--4598--88e7--4c836eebcc23-osd--block--a3168242--3ad2--4864--a8d5--fe7fc52cbcbe (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-5435efaa-4e90-4598-88e7-4c836eebcc23" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-a3168242-3ad2-4864-a8d5-fe7fc52cbcbe"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-5435efaa-4e90-4598-88e7-4c836eebcc23" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-a3168242-3ad2-4864-a8d5-fe7fc52cbcbe" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-5435efaa-4e90-4598-88e7-4c836eebcc23"\n'
10.10.2.1: b'  Volume group "ceph-5435efaa-4e90-4598-88e7-4c836eebcc23" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:11:02,509258443-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:11:02,518809890-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:11:02,522389941-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\npodman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 2cc309c4-382b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\nPulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\n'
10.10.2.1: b'Waiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 2cc309c4-382b-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:12:03,634978892-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:12:23,642420531-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:12:23,652813972-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:12:23,657126970-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 2cc309c4-382b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/2cc309c4-382b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:12:32,800241830-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:12:32,810334064-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:12:32,814547205-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 2cc309c4-382b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/2cc309c4-382b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:12:42,279512922-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:12:42,285869343-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:12:42,503460646-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:12:42,507265720-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid 2cc309c4-382b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/2cc309c4-382b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:12:51,692216387-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:13:11,697091040-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:13:11,703939396-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:13:11,714068990-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:13:11,718091313-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 2cc309c4-382b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/2cc309c4-382b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:13:37,082248315-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:13:57,088132277-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:13:57,097970845-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:13:57,101631237-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 2cc309c4-382b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/2cc309c4-382b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     2cc309c4-382b-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.nkchnv(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 41s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 13:14:05 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:10:55,477404937-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/ee03f06e-3829-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid ee03f06e-3829-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:10:55,488153074-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:10:55,491688161-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'ee03f06e-3829-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid ee03f06e-3829-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:10:55,500650611-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid ee03f06e-3829-11ec-b51d-53e6e728d2d3'
[1] 13:11:00 [SUCCESS] 10.10.2.1
[2] 13:11:01 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:11:01,530129439-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:11:01,541991612-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:11:01,546502192-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:11:01,700189220-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:11:01,704993231-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:11:01,859463561-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:11:02,147519747-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:11:02,152537791-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--5435efaa--4e90--4598--88e7--4c836eebcc23-osd--block--a3168242--3ad2--4864--a8d5--fe7fc52cbcbe (253:0)
  Archiving volume group "ceph-5435efaa-4e90-4598-88e7-4c836eebcc23" metadata (seqno 5).
  Releasing logical volume "osd-block-a3168242-3ad2-4864-a8d5-fe7fc52cbcbe"
  Creating volume group backup "/etc/lvm/backup/ceph-5435efaa-4e90-4598-88e7-4c836eebcc23" (seqno 6).
  Logical volume "osd-block-a3168242-3ad2-4864-a8d5-fe7fc52cbcbe" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-5435efaa-4e90-4598-88e7-4c836eebcc23"
  Volume group "ceph-5435efaa-4e90-4598-88e7-4c836eebcc23" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:11:02,509258443-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:11:02,518809890-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:11:02,522389941-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 2cc309c4-382b-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 2cc309c4-382b-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:12:03,634978892-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:12:23,642420531-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:12:23,652813972-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:12:23,657126970-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 2cc309c4-382b-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/2cc309c4-382b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:12:32,800241830-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:12:32,810334064-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:12:32,814547205-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 2cc309c4-382b-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/2cc309c4-382b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:12:42,279512922-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:12:42,285869343-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:12:42,503460646-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:12:42,507265720-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 2cc309c4-382b-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/2cc309c4-382b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:12:51,692216387-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:13:11,697091040-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:13:11,703939396-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:13:11,714068990-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:13:11,718091313-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 2cc309c4-382b-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/2cc309c4-382b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:13:37,082248315-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:13:57,088132277-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:13:57,097970845-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:13:57,101631237-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 2cc309c4-382b-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/2cc309c4-382b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     2cc309c4-382b-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.nkchnv(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 41s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:14:05,987006630-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:14:05,994785001-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 13:14:06 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:14:06,468911214-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:14:06,472744549-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:14:06,494850013-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:14:06,497981575-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2cc309c4-382b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2cc309c4-382b-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:14:10,420739234-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:14:10,423760167-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2cc309c4-382b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2cc309c4-382b-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:14:14,480980878-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:14:14,484274436-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2cc309c4-382b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2cc309c4-382b-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:14:18,373986539-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:14:18,377119324-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2cc309c4-382b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2cc309c4-382b-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:14:26,242938073-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:14:26,245988822-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2cc309c4-382b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2cc309c4-382b-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:14:30,499583621-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:14:30,502726916-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2cc309c4-382b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2cc309c4-382b-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:14:34,767102613-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:14:34,770184461-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2cc309c4-382b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2cc309c4-382b-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:14:38,877272396-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:14:38,880341139-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2cc309c4-382b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2cc309c4-382b-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:14:43,753458577-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:14:43,756459002-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2cc309c4-382b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2cc309c4-382b-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:14:48,687172904-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:14:48,690057481-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2cc309c4-382b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2cc309c4-382b-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:14:53,061068608-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:14:53,064170414-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2cc309c4-382b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2cc309c4-382b-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:14:57,155588911-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:14:57,158662263-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2cc309c4-382b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2cc309c4-382b-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default   
-3         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host node-1
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0  
                       TOTAL  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:15:00,961482285-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:15:24,955292796-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:15:33,846871432-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:15:42,671929117-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:15:51,620709396-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:15:51,628770049-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:16:00,498411716-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:16:00,506099426-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:16:09,386984199-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:16:09,394593371-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:16:18,303730239-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:16:18,311449458-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:16:27,228044489-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:16:27,235879466-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:16:27,241883923-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:16:27,245431169-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:16:27,252341433-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:16:27,257812074-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=699673
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:16:27,264490391-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:16:27,273473953-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'339150\n'
[1] 13:16:27 [SUCCESS] ljishen@10.10.2.2
339150

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:16:27,462535551-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_1MB_write.log.1
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:16:27,482873984-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:16:27,485824936-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2cc309c4-382b-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2cc309c4-382b-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T20:16:30.561574+0000 Maintaining 128 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 60 seconds or 0 objects
2021-10-28T20:16:30.561586+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T20:16:30.593762+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T20:16:30.593762+0000     0       0         0         0         0         0           -           0
2021-10-28T20:16:31.593907+0000     1      54        54         0         0         0           -           0
2021-10-28T20:16:32.594021+0000     2     100       100         0         0         0           -           0
2021-10-28T20:16:33.594112+0000     3     127       153        26   8.66581   8.66667       2.824     2.74492
2021-10-28T20:16:34.594226+0000     4     127       199        72   17.9982        46     2.47236     2.73902
2021-10-28T20:16:35.594298+0000     5     127       232       105    20.998        33     2.22211     2.71827
2021-10-28T20:16:36.594409+0000     6     127       285       158   26.3307        53     2.54585     2.73702
2021-10-28T20:16:37.594505+0000     7     127       331       204     29.14        46     2.66602     2.78045
2021-10-28T20:16:38.594618+0000     8     127       384       257   32.1218        53     2.48584     2.78907
2021-10-28T20:16:39.594733+0000     9     127       430       303   33.6632        46     2.32903      2.7712
2021-10-28T20:16:40.594802+0000    10     127       483       356   35.5965        53     2.52298     2.75316
2021-10-28T20:16:41.594892+0000    11     127       529       402   36.5419        46     2.44914     2.74972
2021-10-28T20:16:42.595001+0000    12     127       549       422   35.1632        20     2.73339     2.76313
2021-10-28T20:16:43.595112+0000    13     127       549       422   32.4583         0           -     2.76313
2021-10-28T20:16:44.595222+0000    14     127       562       435   31.0683       6.5     3.89482     2.80349
2021-10-28T20:16:45.595287+0000    15     127       582       455   30.3304        20     4.81201     2.90197
2021-10-28T20:16:46.595396+0000    16     127       595       468   29.2471        13     5.39358     2.98079
2021-10-28T20:16:47.595508+0000    17     127       595       468   27.5267         0           -     2.98079
2021-10-28T20:16:48.595619+0000    18     127       615       488   27.1084        10     6.89616     3.15223
2021-10-28T20:16:49.595711+0000    19     127       628       501   26.3658        13     7.83037      3.2791
2021-10-28T20:16:50.595778+0000 min lat: 2.22117 max lat: 8.13586 avg lat: 3.2791
2021-10-28T20:16:50.595778+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T20:16:50.595778+0000    20     127       628       501   25.0475         0           -      3.2791
2021-10-28T20:16:51.595903+0000    21     127       648       521   24.8071        10     9.30515     3.51993
2021-10-28T20:16:52.596013+0000    22     127       661       534   24.2703        13     9.82208     3.68459
2021-10-28T20:16:53.596104+0000    23     127       681       554   24.0846        20     9.76788     3.94689
2021-10-28T20:16:54.596217+0000    24     127       694       567   23.6226        13     9.44605     4.09213
2021-10-28T20:16:55.596284+0000    25     127       694       567   22.6778         0           -     4.09213
2021-10-28T20:16:56.596392+0000    26     127       714       587   22.5747        10     10.1664     4.32857
2021-10-28T20:16:57.596483+0000    27     127       727       600     22.22        13     9.29112     4.46366
2021-10-28T20:16:58.596591+0000    28     127       727       600   21.4264         0           -     4.46366
2021-10-28T20:16:59.596704+0000    29     127       747       620   21.3772        10     9.63295     4.66237
2021-10-28T20:17:00.596772+0000    30     127       760       633   21.0979        13     9.50859     4.78736
2021-10-28T20:17:01.596869+0000    31     127       780       653   21.0624        20     9.37983       4.957
2021-10-28T20:17:02.596979+0000    32     127       826       699   21.8416        46      6.2275     5.18086
2021-10-28T20:17:03.597090+0000    33     127       892       765   23.1795        66     2.21827     5.15627
2021-10-28T20:17:04.597205+0000    34     127       925       798   23.4682        33     2.36809     5.04802
2021-10-28T20:17:05.597278+0000    35     127       978       851   24.3119        53     2.62598     4.91119
2021-10-28T20:17:06.597388+0000    36     127      1011       884   24.5531        33     2.66463     4.83469
2021-10-28T20:17:07.597498+0000    37     127      1057       930   25.1326        46     2.35971     4.73898
2021-10-28T20:17:08.597606+0000    38     127      1057       930   24.4712         0           -     4.73898
2021-10-28T20:17:09.597696+0000    39     127      1077       950   24.3566        10     3.67438     4.72189
2021-10-28T20:17:10.597762+0000 min lat: 2.21826 max lat: 11.3004 avg lat: 4.71965
2021-10-28T20:17:10.597762+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T20:17:10.597762+0000    40     127      1090       963   24.0726        13     4.31928     4.71965
2021-10-28T20:17:11.597879+0000    41     127      1090       963   23.4855         0           -     4.71965
2021-10-28T20:17:12.597990+0000    42     127      1090       963   22.9263         0           -     4.71965
2021-10-28T20:17:13.598082+0000    43     127      1110       983   22.8582   6.66667     6.86967     4.76789
2021-10-28T20:17:14.598193+0000    44     127      1110       983   22.3387         0           -     4.76789
2021-10-28T20:17:15.598260+0000    45     127      1123       996   22.1311       6.5     8.81892     4.82408
2021-10-28T20:17:16.598370+0000    46     127      1123       996     21.65         0           -     4.82408
2021-10-28T20:17:17.598462+0000    47     127      1123       996   21.1894         0           -     4.82408
2021-10-28T20:17:18.598571+0000    48     127      1143      1016   21.1646   6.66667     11.3265     4.95837
2021-10-28T20:17:19.598685+0000    49     127      1143      1016   20.7326         0           -     4.95837
2021-10-28T20:17:20.598756+0000    50     127      1156      1029    20.578       6.5     12.9962     5.06257
2021-10-28T20:17:21.598847+0000    51     127      1156      1029   20.1745         0           -     5.06257
2021-10-28T20:17:22.598958+0000    52     127      1156      1029   19.7865         0           -     5.06257
2021-10-28T20:17:23.599071+0000    53     127      1176      1049   19.7905   6.66667     16.2942     5.28165
2021-10-28T20:17:24.599183+0000    54     127      1176      1049    19.424         0           -     5.28165
2021-10-28T20:17:25.599248+0000    55     127      1189      1062   19.3072       6.5     16.3645     5.43121
2021-10-28T20:17:26.599363+0000    56     127      1189      1062   18.9624         0           -     5.43121
2021-10-28T20:17:27.599480+0000    57     127      1189      1062   18.6297         0           -     5.43121
2021-10-28T20:17:28.599593+0000    58     127      1189      1062   18.3085         0           -     5.43121
2021-10-28T20:17:29.599682+0000    59     127      1209      1082   18.3372         5     18.8997     5.69477
2021-10-28T20:17:30.599749+0000 min lat: 2.21826 max lat: 19.8873 avg lat: 5.69477
2021-10-28T20:17:30.599749+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T20:17:30.599749+0000    60     127      1209      1082   18.0316         0           -     5.69477
2021-10-28T20:17:31.599872+0000    61      21      1210      1189   19.4899      53.5      5.8023      6.3541
2021-10-28T20:17:32.599982+0000    62      21      1210      1189   19.1755         0           -      6.3541
2021-10-28T20:17:33.600073+0000    63      21      1210      1189   18.8711         0           -      6.3541
2021-10-28T20:17:34.600218+0000 Total time run:         63.4124
Total writes made:      1210
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     19.0814
Stddev Bandwidth:       19.1069
Max bandwidth (MB/sec): 66
Min bandwidth (MB/sec): 0
Average IOPS:           19
Stddev IOPS:            19.1266
Max IOPS:               66
Min IOPS:               0
Average Latency(s):     6.33341
Stddev Latency(s):      4.71904
Max latency(s):         21.178
Min latency(s):         2.21826

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:17:35,375971448-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:17:35,381829459-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 699673

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:17:35,387836892-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 339150
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:17:35,395558545-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 339150
[1] 13:17:35 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:17:35,582230047-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_1MB_write.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_1MB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:17:35,729440264-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:17:59,614966178-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:17:59,623107593-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:18:08,581218936-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:18:08,589211690-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:18:17,596968709-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:18:17,604977794-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:18:26,519860052-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:18:26,527851754-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:18:35,504377261-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:18:35,512081963-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:18:35,517998534-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:18:35,521970871-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:18:35,528705043-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:18:35,534068282-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=701019
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:18:35,540818083-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:18:35,550181611-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'339838\n'
[1] 13:18:35 [SUCCESS] ljishen@10.10.2.2
339838

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:18:35,742498964-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_1MB_seq.log.1
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:18:35,762779658-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:18:35,765527718-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2cc309c4-382b-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2cc309c4-382b-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T20:18:38.741700+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T20:18:38.741700+0000     0       0         0         0         0         0           -           0
2021-10-28T20:18:39.741799+0000     1     127       237       110   109.983       110    0.854032    0.541655
2021-10-28T20:18:40.741883+0000     2     127       371       244   121.985       134     1.65011    0.741015
2021-10-28T20:18:41.741959+0000     3     127       463       336   111.988        92    0.593847    0.859445
2021-10-28T20:18:42.742040+0000     4     127       591       464   115.989       128    0.455095    0.938237
2021-10-28T20:18:43.742111+0000     5     127       707       580   115.989       116     1.40673    0.940802
2021-10-28T20:18:44.742190+0000     6     127       831       704   117.323       124    0.422209    0.962568
2021-10-28T20:18:45.742274+0000     7     127       945       818   116.847       114    0.910422    0.982928
2021-10-28T20:18:46.742352+0000     8     127      1061       934    116.74       116    0.540577     1.00458
2021-10-28T20:18:47.742436+0000     9     127      1167      1040   115.545       106     1.71421     1.02135
2021-10-28T20:18:48.742514+0000    10      17      1210      1193    119.29       153     1.96521     1.01771
2021-10-28T20:18:49.742616+0000 Total time run:       10.1668
Total reads made:     1210
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   119.015
Average IOPS:         119
Stddev IOPS:          16.6936
Max IOPS:             153
Min IOPS:             92
Average Latency(s):   1.02059
Max latency(s):       2.44125
Min latency(s):       0.127365

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:18:50,349544716-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:18:50,355474854-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 701019

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:18:50,361425970-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 339838
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:18:50,369490680-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 339838
[1] 13:18:50 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:18:50,553716001-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_1MB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_1MB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:18:50,706041709-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:19:14,604653275-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:19:14,612370180-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:19:23,597863497-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:19:23,605723070-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:19:32,401742790-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:19:32,409418798-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:19:41,334655274-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:19:41,342694576-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:19:50,274986078-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:19:50,283197925-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:19:50,289437586-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T13:19:50,292016316-07:00][RUNNING][ROUND 2/5/21] object_size=1MB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:19:50,295748971-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:19:50,305212598-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:19:50,707825275-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/2cc309c4-382b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 2cc309c4-382b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:19:50,718531113-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:19:50,722443929-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '2cc309c4-382b-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 2cc309c4-382b-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:19:50,730788989-07:00] DEBUG: command from stdin\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 2cc309c4-382b-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 13:19:56 [SUCCESS] 10.10.2.1\n[2] 13:19:56 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:19:56,869590393-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:19:56,881902270-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:19:56,887048036-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:19:57,038942840-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:19:57,043817855-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:19:57,199155022-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:19:57,491252078-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:19:57,496371523-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--f056ac86--219a--4594--8f7c--0aee31804c62-osd--block--1512ef57--164c--4832--a31b--c8837c0d7afe (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-f056ac86-219a-4594-8f7c-0aee31804c62" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-1512ef57-164c-4832-a31b-c8837c0d7afe"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-f056ac86-219a-4594-8f7c-0aee31804c62" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-1512ef57-164c-4832-a31b-c8837c0d7afe" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-f056ac86-219a-4594-8f7c-0aee31804c62"\n'
10.10.2.1: b'  Volume group "ceph-f056ac86-219a-4594-8f7c-0aee31804c62" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:19:57,837130277-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:19:57,846900376-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:19:57,850238131-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\n'
10.10.2.1: b'Verifying lvm2 is present...\nVerifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\npodman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 6bd79c00-382c-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 6bd79c00-382c-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:20:58,706433677-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:21:18,713957108-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:21:18,724720354-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:21:18,728461147-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 6bd79c00-382c-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/6bd79c00-382c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:21:27,580904429-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:21:27,590768343-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:21:27,594479050-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 6bd79c00-382c-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/6bd79c00-382c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:21:36,628961838-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:21:36,635135344-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:21:36,850139939-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:21:36,853798758-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid 6bd79c00-382c-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/6bd79c00-382c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:21:46,499300882-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:22:06,504391714-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:22:06,511092603-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:22:06,521066918-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:22:06,524706020-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 6bd79c00-382c-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/6bd79c00-382c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:22:31,967771945-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:22:51,973000116-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:22:51,983127459-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:22:51,986906264-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 6bd79c00-382c-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/6bd79c00-382c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     6bd79c00-382c-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.voqnbn(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 41s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 13:23:00 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:19:50,707825275-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/2cc309c4-382b-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 2cc309c4-382b-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:19:50,718531113-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:19:50,722443929-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '2cc309c4-382b-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 2cc309c4-382b-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:19:50,730788989-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 2cc309c4-382b-11ec-b51d-53e6e728d2d3'
[1] 13:19:56 [SUCCESS] 10.10.2.1
[2] 13:19:56 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:19:56,869590393-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:19:56,881902270-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:19:56,887048036-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:19:57,038942840-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:19:57,043817855-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:19:57,199155022-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:19:57,491252078-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:19:57,496371523-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--f056ac86--219a--4594--8f7c--0aee31804c62-osd--block--1512ef57--164c--4832--a31b--c8837c0d7afe (253:0)
  Archiving volume group "ceph-f056ac86-219a-4594-8f7c-0aee31804c62" metadata (seqno 5).
  Releasing logical volume "osd-block-1512ef57-164c-4832-a31b-c8837c0d7afe"
  Creating volume group backup "/etc/lvm/backup/ceph-f056ac86-219a-4594-8f7c-0aee31804c62" (seqno 6).
  Logical volume "osd-block-1512ef57-164c-4832-a31b-c8837c0d7afe" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-f056ac86-219a-4594-8f7c-0aee31804c62"
  Volume group "ceph-f056ac86-219a-4594-8f7c-0aee31804c62" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:19:57,837130277-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:19:57,846900376-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:19:57,850238131-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 6bd79c00-382c-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 6bd79c00-382c-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:20:58,706433677-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:21:18,713957108-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:21:18,724720354-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:21:18,728461147-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 6bd79c00-382c-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/6bd79c00-382c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:21:27,580904429-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:21:27,590768343-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:21:27,594479050-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 6bd79c00-382c-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/6bd79c00-382c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:21:36,628961838-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:21:36,635135344-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:21:36,850139939-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:21:36,853798758-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 6bd79c00-382c-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/6bd79c00-382c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:21:46,499300882-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:22:06,504391714-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:22:06,511092603-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:22:06,521066918-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:22:06,524706020-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 6bd79c00-382c-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/6bd79c00-382c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:22:31,967771945-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:22:51,973000116-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:22:51,983127459-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:22:51,986906264-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 6bd79c00-382c-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/6bd79c00-382c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     6bd79c00-382c-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.voqnbn(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 41s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:23:00,781354632-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:23:00,789035058-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 13:23:00 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:23:01,265290711-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:23:01,268793563-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:23:01,290600614-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:23:01,293319929-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6bd79c00-382c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6bd79c00-382c-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:23:05,369741740-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:23:05,372740011-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6bd79c00-382c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6bd79c00-382c-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:23:09,362158160-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:23:09,365067012-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6bd79c00-382c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6bd79c00-382c-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:23:13,279536932-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:23:13,282544982-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6bd79c00-382c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6bd79c00-382c-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:23:21,080207179-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:23:21,083318583-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6bd79c00-382c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6bd79c00-382c-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:23:24,752968566-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:23:24,756130385-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6bd79c00-382c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6bd79c00-382c-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:23:28,363794570-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:23:28,366712119-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6bd79c00-382c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6bd79c00-382c-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:23:32,779278092-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:23:32,782276323-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6bd79c00-382c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6bd79c00-382c-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:23:36,989309767-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:23:36,992389792-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6bd79c00-382c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6bd79c00-382c-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:23:41,321079047-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:23:41,324167808-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6bd79c00-382c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6bd79c00-382c-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:23:45,828967666-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:23:45,832088348-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6bd79c00-382c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6bd79c00-382c-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/17 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:23:49,995300948-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:23:49,998236421-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6bd79c00-382c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6bd79c00-382c-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default   
-3         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host node-1
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0  
                       TOTAL  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:23:53,804602846-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:24:17,702698526-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:24:26,557337814-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:24:35,514697740-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:24:44,579774061-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:24:44,587439077-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:24:53,545988178-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:24:53,553935747-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:25:02,677955203-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:25:02,685829645-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:25:11,586362963-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:25:11,594528182-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:25:20,608910383-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:25:20,616884922-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:25:20,622788158-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:25:20,626379637-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:25:20,633390030-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:25:20,638750072-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=706846
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:25:20,645742331-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:25:20,654672041-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'345674\n'
[1] 13:25:20 [SUCCESS] ljishen@10.10.2.2
345674

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:25:20,846994200-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_1MB_write.log.2
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:25:20,867213227-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:25:20,870028484-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6bd79c00-382c-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6bd79c00-382c-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T20:25:23.927791+0000 Maintaining 128 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 60 seconds or 0 objects
2021-10-28T20:25:23.927804+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T20:25:23.959707+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T20:25:23.959707+0000     0       0         0         0         0         0           -           0
2021-10-28T20:25:24.959844+0000     1      67        67         0         0         0           -           0
2021-10-28T20:25:25.959939+0000     2     120       120         0         0         0           -           0
2021-10-28T20:25:26.960051+0000     3     127       166        39   12.9987        13       2.356     2.41782
2021-10-28T20:25:27.960168+0000     4     127       219        92   22.9976        53     2.38386     2.54882
2021-10-28T20:25:28.960233+0000     5     127       265       138   27.5973        46     2.60588     2.63039
2021-10-28T20:25:29.960326+0000     6     127       318       191   31.8303        53     2.47435     2.66169
2021-10-28T20:25:30.960442+0000     7     127       364       237   33.8538        46     2.27755      2.6751
2021-10-28T20:25:31.960559+0000     8     127       397       270   33.7466        33     2.32658     2.66763
2021-10-28T20:25:32.960672+0000     9     127       450       323   35.8852        53     2.43327     2.66896
2021-10-28T20:25:33.960738+0000    10     127       496       369   36.8963        46     2.44614     2.67259
2021-10-28T20:25:34.960854+0000    11     127       529       402   36.5418        33     2.59597     2.68028
2021-10-28T20:25:35.960969+0000    12     127       549       422   35.1631        20     2.74686     2.69635
2021-10-28T20:25:36.961082+0000    13     127       562       435   33.4581        13      3.6144     2.73149
2021-10-28T20:25:37.961177+0000    14     127       582       455   32.4967        20     4.78546     2.83114
2021-10-28T20:25:38.961245+0000    15     127       595       468   31.1969        13     5.27311     2.90739
2021-10-28T20:25:39.961366+0000    16     127       595       468    29.247         0           -     2.90739
2021-10-28T20:25:40.961483+0000    17     127       615       488    28.703        10     6.81125     3.07936
2021-10-28T20:25:41.961581+0000    18     127       628       501   27.8305        13     7.47967     3.19913
2021-10-28T20:25:42.961697+0000    19     127       628       501   26.3657         0           -     3.19913
2021-10-28T20:25:43.961764+0000 min lat: 2.02343 max lat: 9.51025 avg lat: 3.43777
2021-10-28T20:25:43.961764+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T20:25:43.961764+0000    20     127       648       521   26.0474        10     9.03637     3.43777
2021-10-28T20:25:44.961883+0000    21     127       661       534    25.426        13      9.8841      3.6029
2021-10-28T20:25:45.961975+0000    22     127       661       534   24.2703         0           -      3.6029
2021-10-28T20:25:46.962084+0000    23     127       681       554   24.0845        10     10.1224     3.87407
2021-10-28T20:25:47.962197+0000    24     127       694       567   23.6226        13     9.66579     4.02976
2021-10-28T20:25:48.962267+0000    25     127       694       567   22.6777         0           -     4.02976
2021-10-28T20:25:49.962361+0000    26     127       714       587   22.5747        10     11.2134     4.29975
2021-10-28T20:25:50.962473+0000    27     127       714       587   21.7385         0           -     4.29975
2021-10-28T20:25:51.962585+0000    28     127       727       600   21.4264       6.5     10.3884     4.46034
2021-10-28T20:25:52.962695+0000    29     127       727       600   20.6876         0           -     4.46034
2021-10-28T20:25:53.962758+0000    30     127       747       620   20.6646        10     11.4945     4.71255
2021-10-28T20:25:54.962868+0000    31     127       760       633   20.4173        13     10.5087     4.86054
2021-10-28T20:25:55.962979+0000    32     127       793       666   20.8104        33     9.46762     5.17625
2021-10-28T20:25:56.963094+0000    33     127       826       699   21.1797        33     6.99173     5.37555
2021-10-28T20:25:57.963188+0000    34     127       879       752   22.1154        53     3.80723     5.41512
2021-10-28T20:25:58.963254+0000    35     127       912       785   22.4263        33     2.86638     5.32795
2021-10-28T20:25:59.963365+0000    36     127       958       831    23.081        46     2.37856     5.18995
2021-10-28T20:26:00.963473+0000    37     127      1011       884   23.8895        53     2.59155     5.04598
2021-10-28T20:26:01.963571+0000    38     127      1044       917   24.1292        33     2.63916     4.96548
2021-10-28T20:26:02.963687+0000    39     127      1057       930   23.8437        13     2.63618     4.93615
2021-10-28T20:26:03.963754+0000 min lat: 2.02343 max lat: 12.5449 avg lat: 4.92364
2021-10-28T20:26:03.963754+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T20:26:03.963754+0000    40     127      1077       950   23.7476        20     4.10399     4.92364
2021-10-28T20:26:04.963870+0000    41     127      1090       963   23.4854        13     4.79031     4.92527
2021-10-28T20:26:05.963968+0000    42     127      1090       963   22.9263         0           -     4.92527
2021-10-28T20:26:06.964082+0000    43     127      1090       963   22.3931         0           -     4.92527
2021-10-28T20:26:07.964157+0000    44     127      1090       963   21.8842         0           -     4.92527
2021-10-28T20:26:08.964231+0000    45     127      1110       983   21.8423         5     7.51194     4.98514
2021-10-28T20:26:09.964328+0000    46     127      1110       983   21.3674         0           -     4.98514
2021-10-28T20:26:10.964440+0000    47     127      1123       996   21.1894       6.5     9.47999     5.04688
2021-10-28T20:26:11.964553+0000    48     127      1123       996   20.7479         0           -     5.04688
2021-10-28T20:26:12.964669+0000    49     127      1123       996   20.3245         0           -     5.04688
2021-10-28T20:26:13.964736+0000    50     127      1143      1016    20.318   6.66667        12.2     5.19198
2021-10-28T20:26:14.964850+0000    51     127      1143      1016   19.9196         0           -     5.19198
2021-10-28T20:26:15.964966+0000    52     127      1156      1029   19.7865       6.5     13.6521      5.3038
2021-10-28T20:26:16.965078+0000    53     127      1156      1029   19.4131         0           -      5.3038
2021-10-28T20:26:17.965170+0000    54     127      1156      1029   19.0536         0           -      5.3038
2021-10-28T20:26:18.965234+0000    55     127      1176      1049   19.0708   6.66667     16.5952     5.52406
2021-10-28T20:26:19.965345+0000    56     127      1176      1049   18.7303         0           -     5.52406
2021-10-28T20:26:20.965459+0000    57     127      1189      1062   18.6297       6.5     16.8884     5.67822
2021-10-28T20:26:21.965552+0000    58     127      1189      1062   18.3085         0           -     5.67822
2021-10-28T20:26:22.965663+0000    59     127      1189      1062   17.9982         0           -     5.67822
2021-10-28T20:26:23.965734+0000 min lat: 2.02343 max lat: 18.6632 avg lat: 5.67822
2021-10-28T20:26:23.965734+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T20:26:23.965734+0000    60     127      1189      1062   17.6982         0           -     5.67822
2021-10-28T20:26:24.965861+0000    61      14      1190      1176   19.2768      28.5     5.55437     6.41746
2021-10-28T20:26:25.965960+0000    62      14      1190      1176   18.9658         0           -     6.41746
2021-10-28T20:26:26.966101+0000 Total time run:         62.8272
Total writes made:      1190
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     18.9408
Stddev Bandwidth:       17.602
Max bandwidth (MB/sec): 53
Min bandwidth (MB/sec): 0
Average IOPS:           18
Stddev IOPS:            17.6237
Max IOPS:               53
Min IOPS:               0
Average Latency(s):     6.41306
Stddev Latency(s):      4.73835
Max latency(s):         20.3653
Min latency(s):         2.02343

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:26:27,995861492-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:26:28,001877090-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 706846

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:26:28,007901044-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 345674
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:26:28,015723006-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 345674
[1] 13:26:28 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:26:28,201855237-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_1MB_write.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_1MB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:26:28,349208700-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:26:52,259305430-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.19k objects, 1.2 GiB
    usage:   3.7 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:26:52,267433960-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:27:01,276431270-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.19k objects, 1.2 GiB
    usage:   3.5 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:27:01,284279061-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:27:10,117170183-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.19k objects, 1.2 GiB
    usage:   3.5 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:27:10,125225164-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:27:19,067171634-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.19k objects, 1.2 GiB
    usage:   3.5 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:27:19,074943421-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:27:27,987524381-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.19k objects, 1.2 GiB
    usage:   3.5 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:27:27,995659123-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:27:28,001488199-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:27:28,005340019-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:27:28,012750826-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:27:28,018427675-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=708214
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:27:28,025364880-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:27:28,034481663-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'346405\n'
[1] 13:27:28 [SUCCESS] ljishen@10.10.2.2
346405

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:27:28,222639241-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_1MB_seq.log.2
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:27:28,242503430-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:27:28,245392214-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6bd79c00-382c-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6bd79c00-382c-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T20:27:31.257437+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T20:27:31.257437+0000     0       0         0         0         0         0           -           0
2021-10-28T20:27:32.257578+0000     1     127       250       123   122.974       123    0.853008    0.535438
2021-10-28T20:27:33.257664+0000     2     127       367       240   119.982       117    0.835941    0.727213
2021-10-28T20:27:34.257783+0000     3     127       502       375   124.983       135    0.691843    0.841417
2021-10-28T20:27:35.257895+0000     4     127       629       502   125.483       127    0.783915    0.831625
2021-10-28T20:27:36.258005+0000     5     127       743       616   123.184       114     1.82934    0.865774
2021-10-28T20:27:37.258074+0000     6     127       866       739   123.152       123    0.444305    0.897832
2021-10-28T20:27:38.258191+0000     7     127       957       830   118.557        91    0.755369    0.935071
2021-10-28T20:27:39.258303+0000     8     127      1054       927   115.861        97     2.70597    0.978159
2021-10-28T20:27:40.258414+0000     9     127      1156      1029    114.32       102     1.10868     1.01421
2021-10-28T20:27:41.258485+0000    10      21      1190      1169   116.887       140     2.09139     1.02327
2021-10-28T20:27:42.258640+0000 Total time run:       10.3142
Total reads made:     1190
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   115.375
Average IOPS:         115
Stddev IOPS:          16.1069
Max IOPS:             140
Min IOPS:             91
Average Latency(s):   1.02948
Max latency(s):       3.13918
Min latency(s):       0.0929215

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:27:42,841715527-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:27:42,847932123-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 708214

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:27:42,853914318-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 346405
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:27:42,862064359-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 346405
[1] 13:27:43 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:27:43,049618660-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_1MB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_1MB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:27:43,197018580-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:28:07,094982191-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.19k objects, 1.2 GiB
    usage:   3.5 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:28:07,102905355-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:28:16,069414054-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.19k objects, 1.2 GiB
    usage:   3.5 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:28:16,077301099-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:28:24,991809408-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.19k objects, 1.2 GiB
    usage:   3.5 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:28:24,999897042-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:28:33,948198248-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.19k objects, 1.2 GiB
    usage:   3.5 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:28:33,956418301-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:28:42,238543468-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.19k objects, 1.2 GiB
    usage:   3.5 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:28:42,246292963-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:28:42,252181772-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T13:28:42,254517855-07:00][RUNNING][ROUND 3/5/21] object_size=1MB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:28:42,258230503-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:28:42,267622063-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:28:42,752621307-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/6bd79c00-382c-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 6bd79c00-382c-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:28:42,764238861-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:28:42,767809584-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '6bd79c00-382c-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 6bd79c00-382c-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:28:42,776264973-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 6bd79c00-382c-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 13:28:48 [SUCCESS] 10.10.2.1\n[2] 13:28:48 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:28:48,856321946-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:28:48,868148223-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:28:48,872813234-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:28:49,023071488-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:28:49,027887904-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:28:49,183412510-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:28:49,471821123-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:28:49,476758095-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--434905a7--16e8--465e--b4e1--febb86118579-osd--block--047f5553--6135--4748--8185--1edbc514fc6b (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-434905a7-16e8-465e-b4e1-febb86118579" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-047f5553-6135-4748-8185-1edbc514fc6b"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-434905a7-16e8-465e-b4e1-febb86118579" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-047f5553-6135-4748-8185-1edbc514fc6b" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-434905a7-16e8-465e-b4e1-febb86118579"\n'
10.10.2.1: b'  Volume group "ceph-434905a7-16e8-465e-b4e1-febb86118579" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:28:49,840584974-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:28:49,850633567-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:28:49,854259866-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: a8f13604-382d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid a8f13604-382d-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:29:51,329161351-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:30:11,336264079-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:30:11,346612526-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:30:11,350313154-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid a8f13604-382d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/a8f13604-382d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:30:20,623059493-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:30:20,632937196-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:30:20,636519251-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid a8f13604-382d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/a8f13604-382d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:30:30,375153902-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:30:30,381297444-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:30:30,593932601-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:30:30,597742645-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid a8f13604-382d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/a8f13604-382d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:30:40,084868813-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:31:00,089731840-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:31:00,096483184-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:31:00,106558919-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:31:00,110329399-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid a8f13604-382d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/a8f13604-382d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:31:25,261013164-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:31:45,266480203-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:31:45,276510984-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:31:45,280365521-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid a8f13604-382d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/a8f13604-382d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     a8f13604-382d-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.gvwjqv(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 41s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 13:31:54 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:28:42,752621307-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/6bd79c00-382c-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 6bd79c00-382c-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:28:42,764238861-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:28:42,767809584-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '6bd79c00-382c-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 6bd79c00-382c-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:28:42,776264973-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 6bd79c00-382c-11ec-b51d-53e6e728d2d3'
[1] 13:28:48 [SUCCESS] 10.10.2.1
[2] 13:28:48 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:28:48,856321946-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:28:48,868148223-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:28:48,872813234-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:28:49,023071488-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:28:49,027887904-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:28:49,183412510-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:28:49,471821123-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:28:49,476758095-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--434905a7--16e8--465e--b4e1--febb86118579-osd--block--047f5553--6135--4748--8185--1edbc514fc6b (253:0)
  Archiving volume group "ceph-434905a7-16e8-465e-b4e1-febb86118579" metadata (seqno 5).
  Releasing logical volume "osd-block-047f5553-6135-4748-8185-1edbc514fc6b"
  Creating volume group backup "/etc/lvm/backup/ceph-434905a7-16e8-465e-b4e1-febb86118579" (seqno 6).
  Logical volume "osd-block-047f5553-6135-4748-8185-1edbc514fc6b" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-434905a7-16e8-465e-b4e1-febb86118579"
  Volume group "ceph-434905a7-16e8-465e-b4e1-febb86118579" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:28:49,840584974-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:28:49,850633567-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:28:49,854259866-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: a8f13604-382d-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid a8f13604-382d-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:29:51,329161351-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:30:11,336264079-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:30:11,346612526-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:30:11,350313154-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid a8f13604-382d-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/a8f13604-382d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:30:20,623059493-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:30:20,632937196-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:30:20,636519251-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid a8f13604-382d-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/a8f13604-382d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:30:30,375153902-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:30:30,381297444-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:30:30,593932601-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:30:30,597742645-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid a8f13604-382d-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/a8f13604-382d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:30:40,084868813-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:31:00,089731840-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:31:00,096483184-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:31:00,106558919-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:31:00,110329399-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid a8f13604-382d-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/a8f13604-382d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:31:25,261013164-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:31:45,266480203-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:31:45,276510984-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:31:45,280365521-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid a8f13604-382d-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/a8f13604-382d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     a8f13604-382d-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.gvwjqv(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 41s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:31:54,280352145-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:31:54,288120616-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 13:31:54 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:31:54,757269103-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:31:54,761145459-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:31:54,783501674-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:31:54,786326388-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a8f13604-382d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a8f13604-382d-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:31:58,817268084-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:31:58,820516236-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a8f13604-382d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a8f13604-382d-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:32:02,842580328-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:32:02,845782534-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a8f13604-382d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a8f13604-382d-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:32:06,918128493-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:32:06,921132565-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a8f13604-382d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a8f13604-382d-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:32:14,857604125-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:32:14,860909885-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a8f13604-382d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a8f13604-382d-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:32:19,916760390-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:32:19,920073865-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a8f13604-382d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a8f13604-382d-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:32:24,182505411-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:32:24,185676397-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a8f13604-382d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a8f13604-382d-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:32:28,866843645-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:32:28,869825244-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a8f13604-382d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a8f13604-382d-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:32:33,281383091-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:32:33,284598982-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a8f13604-382d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a8f13604-382d-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:32:38,238643376-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:32:38,241741435-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a8f13604-382d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a8f13604-382d-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:32:42,520886869-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:32:42,523916740-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a8f13604-382d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a8f13604-382d-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:32:46,348591661-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:32:46,351585774-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a8f13604-382d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a8f13604-382d-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default   
-3         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host node-1
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0  
                       TOTAL  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:32:50,346340960-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:33:14,278129958-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:33:23,192776365-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:33:32,107540490-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:33:40,980243908-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:33:40,988264695-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:33:49,996381427-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:33:50,004090757-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:33:58,954832099-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:33:58,963003751-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:34:07,789406376-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:34:07,796726392-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:34:16,670345521-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:34:16,678422575-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:34:16,684681712-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:34:16,688564841-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:34:16,695511795-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:34:16,701528144-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=714042
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:34:16,708419865-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:34:16,717281056-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'351681\n'
[1] 13:34:16 [SUCCESS] ljishen@10.10.2.2
351681

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:34:16,910985557-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_1MB_write.log.3
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:34:16,931233912-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:34:16,934181758-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a8f13604-382d-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a8f13604-382d-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T20:34:19.875393+0000 Maintaining 128 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 60 seconds or 0 objects
2021-10-28T20:34:19.875406+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T20:34:19.907560+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T20:34:19.907560+0000     0       0         0         0         0         0           -           0
2021-10-28T20:34:20.907713+0000     1      67        67         0         0         0           -           0
2021-10-28T20:34:21.907816+0000     2     120       120         0         0         0           -           0
2021-10-28T20:34:22.907930+0000     3     127       166        39   12.9986        13     2.43424     2.61945
2021-10-28T20:34:23.908004+0000     4     127       199        72   17.9983        33     2.50162      2.6967
2021-10-28T20:34:24.908125+0000     5     127       252       125   24.9975        53     2.48758     2.73208
2021-10-28T20:34:25.908235+0000     6     127       298       171   28.4971        46      2.3527      2.7189
2021-10-28T20:34:26.908352+0000     7     127       351       224   31.9966        53     2.48326     2.72065
2021-10-28T20:34:27.908466+0000     8     127       397       270   33.7464        46     2.37722     2.71288
2021-10-28T20:34:28.908539+0000     9     127       450       323   35.8852        53     2.52577     2.71373
2021-10-28T20:34:29.908643+0000    10     127       483       356   35.5963        33     2.55645     2.72442
2021-10-28T20:34:30.908756+0000    11     127       529       402   36.5417        46     2.64976     2.74442
2021-10-28T20:34:31.908869+0000    12     127       549       422    35.163        20     2.82797     2.76149
2021-10-28T20:34:32.908984+0000    13     127       562       435    33.458        13     3.53413     2.79359
2021-10-28T20:34:33.909060+0000    14     127       582       455   32.4967        20     4.48201     2.88156
2021-10-28T20:34:34.909181+0000    15     127       595       468   31.1967        13     5.19805     2.95278
2021-10-28T20:34:35.909298+0000    16     127       595       468   29.2469         0           -     2.95278
2021-10-28T20:34:36.909414+0000    17     127       615       488   28.7028        10     6.75689     3.11845
2021-10-28T20:34:37.909519+0000    18     127       628       501   27.8304        13     7.27221     3.23573
2021-10-28T20:34:38.909593+0000    19     127       628       501   26.3657         0           -     3.23573
2021-10-28T20:34:39.909708+0000 min lat: 2.28555 max lat: 9.18736 avg lat: 3.46173
2021-10-28T20:34:39.909708+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T20:34:39.909708+0000    20     127       648       521   26.0473        10     8.90026     3.46173
2021-10-28T20:34:40.909837+0000    21     127       648       521   24.8069         0           -     3.46173
2021-10-28T20:34:41.909939+0000    22     127       661       534   24.2702       6.5     9.90582     3.62745
2021-10-28T20:34:42.910051+0000    23     127       681       554   24.0844        20     10.2573     3.89977
2021-10-28T20:34:43.910120+0000    24     127       694       567   23.6225        13     9.87181     4.05796
2021-10-28T20:34:44.910234+0000    25     127       694       567   22.6776         0           -     4.05796
2021-10-28T20:34:45.910338+0000    26     127       714       587   22.5746        10     10.8453     4.31845
2021-10-28T20:34:46.910456+0000    27     127       727       600   22.2199        13     10.0012     4.46943
2021-10-28T20:34:47.910572+0000    28     127       727       600   21.4263         0           -     4.46943
2021-10-28T20:34:48.910641+0000    29     127       747       620   21.3771        10      10.644     4.69555
2021-10-28T20:34:49.910747+0000    30     127       760       633   21.0978        13     10.2083     4.83599
2021-10-28T20:34:50.910861+0000    31     127       760       633   20.4172         0           -     4.83599
2021-10-28T20:34:51.910973+0000    32     127       793       666   20.8103      16.5     9.01904     5.13602
2021-10-28T20:34:52.911089+0000    33     127       846       719   21.7856        53     6.04878     5.35089
2021-10-28T20:34:53.911162+0000    34     127       892       765   22.4977        46     2.43909     5.31378
2021-10-28T20:34:54.911278+0000    35     127       925       798   22.7976        33     2.48048     5.20866
2021-10-28T20:34:55.911395+0000    36     127       991       864   23.9975        66      2.4702     5.02419
2021-10-28T20:34:56.911512+0000    37     127      1024       897   24.2407        33     2.50819      4.9444
2021-10-28T20:34:57.911626+0000    38     127      1057       930   24.4711        33     2.56735     4.87153
2021-10-28T20:34:58.911703+0000    39     127      1057       930   23.8437         0           -     4.87153
2021-10-28T20:34:59.911818+0000 min lat: 2.28555 max lat: 12.1234 avg lat: 4.85395
2021-10-28T20:34:59.911818+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T20:34:59.911818+0000    40     127      1077       950   23.7475        10     3.78489     4.85395
2021-10-28T20:35:00.911901+0000    41     127      1090       963   23.4854        13     4.39233     4.85069
2021-10-28T20:35:01.912002+0000    42     127      1090       963   22.9262         0           -     4.85069
2021-10-28T20:35:02.912121+0000    43     127      1090       963    22.393         0           -     4.85069
2021-10-28T20:35:03.912193+0000    44     127      1110       983   22.3386   6.66667     7.17711     4.90289
2021-10-28T20:35:04.912305+0000    45     127      1110       983   21.8422         0           -     4.90289
2021-10-28T20:35:05.912407+0000    46     127      1123       996   21.6499       6.5     9.24949     4.96426
2021-10-28T20:35:06.912521+0000    47     127      1123       996   21.1893         0           -     4.96426
2021-10-28T20:35:07.912635+0000    48     127      1123       996   20.7478         0           -     4.96426
2021-10-28T20:35:08.912743+0000    49     127      1143      1016   20.7325   6.66667     11.5517      5.0989
2021-10-28T20:35:09.912846+0000    50     127      1143      1016   20.3179         0           -      5.0989
2021-10-28T20:35:10.912957+0000    51     127      1156      1029   20.1744       6.5     13.4233     5.20817
2021-10-28T20:35:11.913069+0000    52     127      1156      1029   19.7864         0           -     5.20817
2021-10-28T20:35:12.913180+0000    53     127      1156      1029   19.4131         0           -     5.20817
2021-10-28T20:35:13.913281+0000    54     127      1156      1029   19.0536         0           -     5.20817
2021-10-28T20:35:14.913392+0000    55     127      1176      1049   19.0707         5     16.5551     5.42937
2021-10-28T20:35:15.913507+0000    56     127      1176      1049   18.7302         0           -     5.42937
2021-10-28T20:35:16.913622+0000    57     127      1189      1062   18.6296       6.5     16.8884     5.58262
2021-10-28T20:35:17.913727+0000    58     127      1189      1062   18.3084         0           -     5.58262
2021-10-28T20:35:18.913836+0000    59     127      1189      1062   17.9981         0           -     5.58262
2021-10-28T20:35:19.913946+0000 min lat: 2.28555 max lat: 20.4421 avg lat: 5.85385
2021-10-28T20:35:19.913946+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T20:35:19.913946+0000    60     127      1209      1082   18.0314   6.66667     19.5158     5.85385
2021-10-28T20:35:20.914068+0000    61     127      1209      1082   17.7358         0           -     5.85385
2021-10-28T20:35:21.914173+0000    62     127      1209      1082   17.4498         0           -     5.85385
2021-10-28T20:35:22.914273+0000    63      21      1210      1189    18.871   35.6667     6.07078     6.54144
2021-10-28T20:35:23.914387+0000    64      21      1210      1189   18.5762         0           -     6.54144
2021-10-28T20:35:24.914501+0000    65       1      1210      1209    18.598        10     5.25234     6.52306
2021-10-28T20:35:25.914637+0000 Total time run:         65.2908
Total writes made:      1210
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     18.5325
Stddev Bandwidth:       17.9051
Max bandwidth (MB/sec): 66
Min bandwidth (MB/sec): 0
Average IOPS:           18
Stddev IOPS:            17.9215
Max IOPS:               66
Min IOPS:               0
Average Latency(s):     6.52237
Stddev Latency(s):      4.91903
Max latency(s):         22.0321
Min latency(s):         2.28555

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:35:27,290995684-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:35:27,297049585-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 714042

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:35:27,303039625-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 351681
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:35:27,310752313-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 351681
[1] 13:35:27 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:35:27,498450823-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_1MB_write.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_1MB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:35:27,649421766-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:35:51,512489116-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:35:51,520553967-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:36:00,496912604-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:36:00,504800241-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:36:09,362754956-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:36:09,370720610-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:36:18,285704535-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:36:18,294199036-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:36:27,252611153-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:36:27,260514019-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:36:27,266960120-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:36:27,271006897-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:36:27,277965353-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:36:27,283619180-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=715429
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:36:27,290718682-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:36:27,299854021-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'351804\n'
[1] 13:36:27 [SUCCESS] ljishen@10.10.2.2
351804

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:36:27,491503217-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_1MB_seq.log.3
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:36:27,511269654-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:36:27,514144102-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a8f13604-382d-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a8f13604-382d-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T20:36:30.504440+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T20:36:30.504440+0000     0       0         0         0         0         0           -           0
2021-10-28T20:36:31.504568+0000     1     127       229       102    101.98       102    0.997746    0.583518
2021-10-28T20:36:32.504671+0000     2     127       370       243   121.482       141    0.830506    0.768904
2021-10-28T20:36:33.504759+0000     3     127       488       361   120.318       118      1.3594      0.8574
2021-10-28T20:36:34.504853+0000     4     127       617       490   122.485       129    0.416828     0.88153
2021-10-28T20:36:35.504939+0000     5     127       716       589   117.787        99     1.62987    0.918738
2021-10-28T20:36:36.505032+0000     6     127       852       725    120.82       136    0.944886    0.962611
2021-10-28T20:36:37.505159+0000     7     127       967       840   119.987       115    0.605882    0.962714
2021-10-28T20:36:38.505250+0000     8     127      1076       949   118.612       109     0.78525    0.981645
2021-10-28T20:36:39.505337+0000     9     127      1179      1052   116.876       103      1.0817     1.00415
2021-10-28T20:36:40.505431+0000    10      36      1210      1174   117.388       122     1.21484     1.02059
2021-10-28T20:36:41.505573+0000 Total time run:       10.2917
Total reads made:     1210
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   117.57
Average IOPS:         117
Stddev IOPS:          14.5998
Max IOPS:             141
Min IOPS:             99
Average Latency(s):   1.0257
Max latency(s):       2.21486
Min latency(s):       0.115056

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:36:42,114459957-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:36:42,120798925-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 715429

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:36:42,126854970-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 351804
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:36:42,135187045-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 351804
[1] 13:36:42 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:36:42,318494037-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_1MB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_1MB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:36:42,469335606-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:37:06,363805572-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:37:06,371453297-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:37:15,370078278-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:37:15,378167115-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:37:24,369749838-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:37:24,377776768-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:37:33,284759726-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:37:33,292952098-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:37:42,262674248-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:37:42,270650964-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:37:42,277203114-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T13:37:42,280874774-07:00][RUNNING][ROUND 1/6/21] object_size=4MB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:37:42,284671210-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:37:42,294597700-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:37:42,723312519-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/a8f13604-382d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid a8f13604-382d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:37:42,734579614-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:37:42,738156990-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'a8f13604-382d-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid a8f13604-382d-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:37:42,746220942-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid a8f13604-382d-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 13:37:48 [SUCCESS] 10.10.2.1\n[2] 13:37:48 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:37:48,970673564-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:37:48,982714644-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:37:48,987609207-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:37:49,139206686-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:37:49,143659799-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:37:49,299289598-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:37:49,587725896-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:37:49,592656216-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--44664fc1--bfd8--4b24--ba30--a399ef7e9a8b-osd--block--ce460814--f21b--40dd--b417--78073dc0953a (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-44664fc1-bfd8-4b24-ba30-a399ef7e9a8b" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-ce460814-f21b-40dd-b417-78073dc0953a"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-44664fc1-bfd8-4b24-ba30-a399ef7e9a8b" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-ce460814-f21b-40dd-b417-78073dc0953a" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-44664fc1-bfd8-4b24-ba30-a399ef7e9a8b"\n'
10.10.2.1: b'  Volume group "ceph-44664fc1-bfd8-4b24-ba30-a399ef7e9a8b" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:37:49,953041794-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:37:49,962922853-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:37:49,966533892-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: eadfc2dc-382e-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid eadfc2dc-382e-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:38:50,281576185-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:39:10,288851749-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:39:10,299208712-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:39:10,303159140-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid eadfc2dc-382e-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/eadfc2dc-382e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:39:19,225608245-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:39:19,235796471-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:39:19,239021144-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid eadfc2dc-382e-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/eadfc2dc-382e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:39:28,311502991-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:39:28,317804289-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:39:28,537857550-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:39:28,541390994-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid eadfc2dc-382e-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/eadfc2dc-382e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:39:38,174784721-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:39:58,179892086-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:39:58,186453864-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:39:58,196676174-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:39:58,200477131-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid eadfc2dc-382e-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/eadfc2dc-382e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:40:23,378951364-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:40:43,385011551-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:40:43,395095331-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:40:43,399364227-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid eadfc2dc-382e-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/eadfc2dc-382e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     eadfc2dc-382e-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.vlewlg(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 13:40:51 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:37:42,723312519-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/a8f13604-382d-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid a8f13604-382d-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:37:42,734579614-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:37:42,738156990-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'a8f13604-382d-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid a8f13604-382d-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:37:42,746220942-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid a8f13604-382d-11ec-b51d-53e6e728d2d3'
[1] 13:37:48 [SUCCESS] 10.10.2.1
[2] 13:37:48 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:37:48,970673564-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:37:48,982714644-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:37:48,987609207-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:37:49,139206686-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:37:49,143659799-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:37:49,299289598-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:37:49,587725896-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:37:49,592656216-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--44664fc1--bfd8--4b24--ba30--a399ef7e9a8b-osd--block--ce460814--f21b--40dd--b417--78073dc0953a (253:0)
  Archiving volume group "ceph-44664fc1-bfd8-4b24-ba30-a399ef7e9a8b" metadata (seqno 5).
  Releasing logical volume "osd-block-ce460814-f21b-40dd-b417-78073dc0953a"
  Creating volume group backup "/etc/lvm/backup/ceph-44664fc1-bfd8-4b24-ba30-a399ef7e9a8b" (seqno 6).
  Logical volume "osd-block-ce460814-f21b-40dd-b417-78073dc0953a" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-44664fc1-bfd8-4b24-ba30-a399ef7e9a8b"
  Volume group "ceph-44664fc1-bfd8-4b24-ba30-a399ef7e9a8b" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:37:49,953041794-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:37:49,962922853-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:37:49,966533892-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: eadfc2dc-382e-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid eadfc2dc-382e-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:38:50,281576185-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:39:10,288851749-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:39:10,299208712-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:39:10,303159140-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid eadfc2dc-382e-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/eadfc2dc-382e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:39:19,225608245-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:39:19,235796471-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:39:19,239021144-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid eadfc2dc-382e-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/eadfc2dc-382e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:39:28,311502991-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:39:28,317804289-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:39:28,537857550-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:39:28,541390994-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid eadfc2dc-382e-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/eadfc2dc-382e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:39:38,174784721-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:39:58,179892086-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:39:58,186453864-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:39:58,196676174-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:39:58,200477131-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid eadfc2dc-382e-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/eadfc2dc-382e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:40:23,378951364-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:40:43,385011551-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:40:43,395095331-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:40:43,399364227-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid eadfc2dc-382e-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/eadfc2dc-382e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     eadfc2dc-382e-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.vlewlg(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:40:51,817964276-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:40:51,825809213-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 13:40:52 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:40:52,304909444-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:40:52,308548243-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:40:52,330628220-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:40:52,333632954-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'eadfc2dc-382e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid eadfc2dc-382e-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:40:56,364575152-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:40:56,367794380-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'eadfc2dc-382e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid eadfc2dc-382e-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:41:00,506904871-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:41:00,509921877-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'eadfc2dc-382e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid eadfc2dc-382e-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:41:04,366406684-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:41:04,369556391-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'eadfc2dc-382e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid eadfc2dc-382e-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:41:12,101301884-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:41:12,104352404-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'eadfc2dc-382e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid eadfc2dc-382e-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:41:17,033537167-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:41:17,036638803-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'eadfc2dc-382e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid eadfc2dc-382e-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:41:21,312190091-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:41:21,314968669-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'eadfc2dc-382e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid eadfc2dc-382e-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:41:25,937526018-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:41:25,940703026-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'eadfc2dc-382e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid eadfc2dc-382e-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:41:30,881205327-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:41:30,884252811-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'eadfc2dc-382e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid eadfc2dc-382e-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:41:35,463488703-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:41:35,466449033-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'eadfc2dc-382e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid eadfc2dc-382e-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:41:39,654124514-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:41:39,657009782-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'eadfc2dc-382e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid eadfc2dc-382e-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:41:43,630064320-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:41:43,632891499-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'eadfc2dc-382e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid eadfc2dc-382e-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default   
-3         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host node-1
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0  
                       TOTAL  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:41:47,450634105-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:42:11,435683756-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:42:20,481172983-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:42:29,295973299-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:42:38,260320626-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:42:38,268020760-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:42:47,183596034-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:42:47,191920595-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:42:56,201025978-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:42:56,208836760-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:43:05,105241042-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:43:05,113260799-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:43:14,135958063-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:43:14,144208584-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:43:14,150177093-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:43:14,153905992-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:43:14,160868125-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:43:14,166491204-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=722981
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:43:14,173570207-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:43:14,182629822-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'355524\n'
[1] 13:43:14 [SUCCESS] ljishen@10.10.2.2
355524

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:43:14,374893964-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4MB_write.log.1
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:43:14,395430903-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:43:14,398309930-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'eadfc2dc-382e-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid eadfc2dc-382e-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T20:43:17.352972+0000 Maintaining 128 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 60 seconds or 0 objects
2021-10-28T20:43:17.352986+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T20:43:17.478478+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T20:43:17.478478+0000     0       0         0         0         0         0           -           0
2021-10-28T20:43:18.478614+0000     1      17        17         0         0         0           -           0
2021-10-28T20:43:19.478733+0000     2      30        30         0         0         0           -           0
2021-10-28T20:43:20.478819+0000     3      46        46         0         0         0           -           0
2021-10-28T20:43:21.478930+0000     4      57        57         0         0         0           -           0
2021-10-28T20:43:22.479017+0000     5      65        65         0         0         0           -           0
2021-10-28T20:43:23.479132+0000     6      78        78         0         0         0           -           0
2021-10-28T20:43:24.479219+0000     7      89        89         0         0         0           -           0
2021-10-28T20:43:25.479302+0000     8     102       102         0         0         0           -           0
2021-10-28T20:43:26.479380+0000     9     118       118         0         0         0           -           0
2021-10-28T20:43:27.479494+0000    10     127       129         2  0.799924       0.8     9.95758     9.95704
2021-10-28T20:43:28.479578+0000    11     127       137        10   3.63602        32     10.6972     10.5401
2021-10-28T20:43:29.479665+0000    12     127       137        10   3.33302         0           -     10.5401
2021-10-28T20:43:30.479747+0000    13     127       137        10   3.07664         0           -     10.5401
2021-10-28T20:43:31.479816+0000    14     127       142        15   4.28532   6.66667     13.3847      11.488
2021-10-28T20:43:32.479909+0000    15     127       142        15   3.99964         0           -      11.488
2021-10-28T20:43:33.479992+0000    16     127       145        18   4.49959         6     15.0502     12.0815
2021-10-28T20:43:34.480078+0000    17     127       145        18   4.23491         0           -     12.0815
2021-10-28T20:43:35.480191+0000    18     127       145        18   3.99963         0           -     12.0815
2021-10-28T20:43:36.480266+0000    19     127       150        23   4.84167   6.66667      17.485     13.2564
2021-10-28T20:43:37.480351+0000 min lat: 9.9565 max lat: 17.4861 avg lat: 13.2564
2021-10-28T20:43:37.480351+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T20:43:37.480351+0000    20     127       150        23   4.59958         0           -     13.2564
2021-10-28T20:43:38.480441+0000    21     127       153        26   4.95193         6     19.2395     13.9468
2021-10-28T20:43:39.480559+0000    22     127       153        26   4.72684         0           -     13.9468
2021-10-28T20:43:40.480644+0000    23     127       161        34    5.9125        16     20.7292     15.5512
2021-10-28T20:43:41.480724+0000    24     127       174        47   7.83262        52     20.9374     17.0056
2021-10-28T20:43:42.480815+0000    25     127       185        58   9.27916        44     20.7706     17.7476
2021-10-28T20:43:43.480935+0000    26     127       198        71   10.9221        52     20.5349     18.2868
2021-10-28T20:43:44.481017+0000    27     127       209        82    12.147        44     20.8791     18.6241
2021-10-28T20:43:45.481101+0000    28     127       222        95   13.5702        52     20.9465      18.919
2021-10-28T20:43:46.481180+0000    29     127       233       106   14.6194        44     20.7543     19.1232
2021-10-28T20:43:47.481268+0000    30     127       246       119   15.8652        52      20.653     19.2969
2021-10-28T20:43:48.481355+0000    31     127       262       135   17.4178        64     20.3209     19.4569
2021-10-28T20:43:49.481441+0000    32     127       265       138   17.2484        12     20.3618     19.4766
2021-10-28T20:43:50.481520+0000    33     127       265       138   16.7258         0           -     19.4766
2021-10-28T20:43:51.481598+0000    34     127       265       138   16.2338         0           -     19.4766
2021-10-28T20:43:52.481713+0000    35     127       265       138     15.77         0           -     19.4766
2021-10-28T20:43:53.481806+0000    36     127       270       143   15.8874         5     21.3042     19.5405
2021-10-28T20:43:54.481895+0000    37     127       270       143   15.4581         0           -     19.5405
2021-10-28T20:43:55.482023+0000    38     127       270       143   15.0513         0           -     19.5405
2021-10-28T20:43:56.482116+0000    39     127       273       146    14.973         4     22.6934     19.6053
2021-10-28T20:43:57.482217+0000 min lat: 9.9565 max lat: 22.6949 avg lat: 19.6053
2021-10-28T20:43:57.482217+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T20:43:57.482217+0000    40     127       273       146   14.5987         0           -     19.6053
2021-10-28T20:43:58.482319+0000    41     127       273       146   14.2426         0           -     19.6053
2021-10-28T20:43:59.482435+0000    42     127       273       146   13.9035         0           -     19.6053
2021-10-28T20:44:00.482513+0000    43     127       273       146   13.5801         0           -     19.6053
2021-10-28T20:44:01.482596+0000    44     127       278       151    13.726         4     24.4991     19.7673
2021-10-28T20:44:02.482685+0000    45     127       278       151    13.421         0           -     19.7673
2021-10-28T20:44:03.482799+0000    46     127       278       151   13.1292         0           -     19.7673
2021-10-28T20:44:04.482881+0000    47     127       281       154   13.1052         4     25.8319     19.8855
2021-10-28T20:44:05.482968+0000    48     127       281       154   12.8322         0           -     19.8855
2021-10-28T20:44:06.483049+0000    49     127       281       154   12.5703         0           -     19.8855
2021-10-28T20:44:07.483168+0000    50     127       281       154   12.3189         0           -     19.8855
2021-10-28T20:44:08.483249+0000    51     127       286       159   12.4694         5      28.073     20.1429
2021-10-28T20:44:09.483331+0000    52     127       286       159   12.2296         0           -     20.1429
2021-10-28T20:44:10.483408+0000    53     127       289       162   12.2253         6     29.2722      20.312
2021-10-28T20:44:11.483487+0000    54     127       289       162   11.9989         0           -      20.312
2021-10-28T20:44:12.483577+0000    55     127       294       167   12.1443        10     31.3002      20.641
2021-10-28T20:44:13.483663+0000    56     127       294       167   11.9275         0           -      20.641
2021-10-28T20:44:14.483744+0000    57     127       297       170   11.9287         6     32.6666     20.8532
2021-10-28T20:44:15.483862+0000    58     127       297       170   11.7231         0           -     20.8532
2021-10-28T20:44:16.483937+0000    59     127       302       175   11.8633        10     34.8427     21.2529
2021-10-28T20:44:17.484020+0000 min lat: 9.9565 max lat: 34.8437 avg lat: 21.2529
2021-10-28T20:44:17.484020+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T20:44:17.484020+0000    60     127       302       175   11.6656         0           -     21.2529
2021-10-28T20:44:18.484107+0000    61       6       303       297   19.4736       244     4.37703     24.0155
2021-10-28T20:44:19.484224+0000    62       6       303       297   19.1595         0           -     24.0155
2021-10-28T20:44:20.484302+0000    63       1       303       302   19.1729        10     4.10522     23.6947
2021-10-28T20:44:21.484413+0000 Total time run:         63.068
Total writes made:      303
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     19.2173
Stddev Bandwidth:       34.125
Max bandwidth (MB/sec): 244
Min bandwidth (MB/sec): 0
Average IOPS:           4
Stddev IOPS:            8.54835
Max IOPS:               61
Min IOPS:               0
Average Latency(s):     23.6307
Stddev Latency(s):      8.0691
Max latency(s):         36.3086
Min latency(s):         4.10522

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:44:22,158303738-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:44:22,164537548-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 722981

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:44:22,170686338-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 355524
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:44:22,178413843-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 355524
[1] 13:44:22 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:44:22,365838041-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4MB_write.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4MB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:44:22,513645578-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:44:46,403048042-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 304 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:44:46,412377046-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:44:55,362137932-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 304 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:44:55,370669523-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:45:04,586581873-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 304 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:45:04,594568186-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:45:13,625882191-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 304 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:45:13,633801757-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:45:22,614504524-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 304 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:45:22,622726411-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:45:22,629446226-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:45:22,633287417-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:45:22,640314241-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:45:22,645941588-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=725364
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:45:22,653081746-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:45:22,662300943-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'355875\n'
[1] 13:45:22 [SUCCESS] ljishen@10.10.2.2
355875

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:45:22,851657563-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4MB_seq.log.1
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:45:22,871763871-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:45:22,874642276-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'eadfc2dc-382e-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid eadfc2dc-382e-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T20:45:25.959823+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T20:45:25.959823+0000     0       0         0         0         0         0           -           0
2021-10-28T20:45:26.959967+0000     1      63        63         0         0         0           -           0
2021-10-28T20:45:27.960079+0000     2     103       103         0         0         0           -           0
2021-10-28T20:45:28.960195+0000     3     127       144        17   22.6634   22.6667     2.98536     2.76799
2021-10-28T20:45:29.960304+0000     4     127       183        56   55.9923       156     3.19147     3.03611
2021-10-28T20:45:30.960419+0000     5     127       222        95     75.99       156      3.2671     3.10338
2021-10-28T20:45:31.960532+0000     6     127       262       135   89.9884       160     3.27174     3.13362
2021-10-28T20:45:32.960650+0000     7     127       299       172   98.2732       148     3.32327     3.15894
2021-10-28T20:45:33.960785+0000 Total time run:       7.79398
Total reads made:     303
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   155.505
Average IOPS:         38
Stddev IOPS:          19.9117
Max IOPS:             40
Min IOPS:             0
Average Latency(s):   2.56519
Max latency(s):       3.32867
Min latency(s):       0.254608

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:45:34,612680558-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:45:34,618771859-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 725364

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:45:34,624884861-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 355875
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:45:34,633192540-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 355875
[1] 13:45:34 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:45:34,814741023-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4MB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4MB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:45:34,969485774-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:45:58,805718101-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 304 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:45:58,814096322-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:46:07,871387137-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 304 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:46:07,879600238-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:46:16,744335870-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 304 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:46:16,752873272-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:46:25,735433083-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 304 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:46:25,743302174-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:46:34,582918294-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 304 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:46:34,591393899-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:46:34,598056617-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T13:46:34,600764932-07:00][RUNNING][ROUND 2/6/21] object_size=4MB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:46:34,604545468-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:46:34,613639919-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:46:35,002882215-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/eadfc2dc-382e-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid eadfc2dc-382e-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:46:35,014159408-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:46:35,017785076-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'eadfc2dc-382e-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid eadfc2dc-382e-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:46:35,026042091-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid eadfc2dc-382e-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 13:46:40 [SUCCESS] 10.10.2.1\n[2] 13:46:41 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:46:41,253541650-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:46:41,265539849-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:46:41,270116414-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:46:41,419514785-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:46:41,424419166-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:46:41,579608125-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:46:41,871486720-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:46:41,876537626-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--3fd7d845--69cc--4d08--ac6a--1eb46941c29e-osd--block--59b34d48--cd32--45dc--a4e4--942e947e06d2 (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-3fd7d845-69cc-4d08-ac6a-1eb46941c29e" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-59b34d48-cd32-45dc-a4e4-942e947e06d2"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-3fd7d845-69cc-4d08-ac6a-1eb46941c29e" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-59b34d48-cd32-45dc-a4e4-942e947e06d2" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-3fd7d845-69cc-4d08-ac6a-1eb46941c29e"\n'
10.10.2.1: b'  Volume group "ceph-3fd7d845-69cc-4d08-ac6a-1eb46941c29e" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:46:42,200667190-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:46:42,210656122-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:46:42,214498877-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 281e3916-3830-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\n'
10.10.2.1: b'Waiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 281e3916-3830-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:47:43,511970753-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:48:03,518601184-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:48:03,528937569-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:48:03,532811152-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 281e3916-3830-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/281e3916-3830-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:48:12,237879414-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:48:12,248303363-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:48:12,252193928-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 281e3916-3830-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/281e3916-3830-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:48:21,329524458-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:48:21,335765001-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:48:21,550167303-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:48:21,553941178-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid 281e3916-3830-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/281e3916-3830-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:48:31,072930517-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:48:51,077838583-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:48:51,084718679-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:48:51,094183685-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:48:51,097941200-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 281e3916-3830-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/281e3916-3830-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:49:16,386454102-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:49:36,392407619-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:49:36,402605062-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:49:36,406404295-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 281e3916-3830-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/281e3916-3830-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     281e3916-3830-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.gmzvos(active, since 2m)\n    osd: 1 osds: 1 up (since 17s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 13:49:44 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:46:35,002882215-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/eadfc2dc-382e-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid eadfc2dc-382e-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:46:35,014159408-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:46:35,017785076-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'eadfc2dc-382e-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid eadfc2dc-382e-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:46:35,026042091-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid eadfc2dc-382e-11ec-b51d-53e6e728d2d3'
[1] 13:46:40 [SUCCESS] 10.10.2.1
[2] 13:46:41 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:46:41,253541650-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:46:41,265539849-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:46:41,270116414-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:46:41,419514785-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:46:41,424419166-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:46:41,579608125-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:46:41,871486720-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:46:41,876537626-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--3fd7d845--69cc--4d08--ac6a--1eb46941c29e-osd--block--59b34d48--cd32--45dc--a4e4--942e947e06d2 (253:0)
  Archiving volume group "ceph-3fd7d845-69cc-4d08-ac6a-1eb46941c29e" metadata (seqno 5).
  Releasing logical volume "osd-block-59b34d48-cd32-45dc-a4e4-942e947e06d2"
  Creating volume group backup "/etc/lvm/backup/ceph-3fd7d845-69cc-4d08-ac6a-1eb46941c29e" (seqno 6).
  Logical volume "osd-block-59b34d48-cd32-45dc-a4e4-942e947e06d2" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-3fd7d845-69cc-4d08-ac6a-1eb46941c29e"
  Volume group "ceph-3fd7d845-69cc-4d08-ac6a-1eb46941c29e" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:46:42,200667190-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:46:42,210656122-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:46:42,214498877-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 281e3916-3830-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 281e3916-3830-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:47:43,511970753-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:48:03,518601184-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:48:03,528937569-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:48:03,532811152-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 281e3916-3830-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/281e3916-3830-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:48:12,237879414-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:48:12,248303363-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:48:12,252193928-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 281e3916-3830-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/281e3916-3830-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:48:21,329524458-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:48:21,335765001-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:48:21,550167303-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:48:21,553941178-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 281e3916-3830-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/281e3916-3830-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:48:31,072930517-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:48:51,077838583-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:48:51,084718679-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:48:51,094183685-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:48:51,097941200-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 281e3916-3830-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/281e3916-3830-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:49:16,386454102-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:49:36,392407619-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:49:36,402605062-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:49:36,406404295-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 281e3916-3830-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/281e3916-3830-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     281e3916-3830-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.gmzvos(active, since 2m)
    osd: 1 osds: 1 up (since 17s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:49:44,825018353-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:49:44,832959070-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 13:49:45 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:49:45,309307493-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:49:45,313067450-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:49:45,336280002-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:49:45,339261301-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '281e3916-3830-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 281e3916-3830-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:49:49,236749471-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:49:49,239931128-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '281e3916-3830-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 281e3916-3830-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:49:53,293150004-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:49:53,296148246-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '281e3916-3830-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 281e3916-3830-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:49:57,285748841-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:49:57,288807185-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '281e3916-3830-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 281e3916-3830-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:50:05,453940186-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:50:05,456868857-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '281e3916-3830-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 281e3916-3830-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:50:10,062209714-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:50:10,065158412-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '281e3916-3830-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 281e3916-3830-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:50:14,334144693-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:50:14,337227914-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '281e3916-3830-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 281e3916-3830-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:50:18,967743350-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:50:18,970705042-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '281e3916-3830-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 281e3916-3830-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:50:23,893983139-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:50:23,897123407-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '281e3916-3830-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 281e3916-3830-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:50:28,718067880-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:50:28,721173333-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '281e3916-3830-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 281e3916-3830-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:50:32,925850669-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:50:32,928735436-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '281e3916-3830-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 281e3916-3830-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:50:36,850257908-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:50:36,853622460-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '281e3916-3830-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 281e3916-3830-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default   
-3         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host node-1
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0  
                       TOTAL  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:50:40,836053985-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:51:04,781766563-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:51:13,689449389-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:51:22,655159985-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:51:31,502439133-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:51:31,510209929-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:51:40,486058848-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:51:40,494300862-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:51:49,416504625-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:51:49,424783399-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:51:58,347768200-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:51:58,356373540-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:52:07,362592881-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:52:07,370492010-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:52:07,376597347-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:52:07,380585304-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:52:07,388179628-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:52:07,394111539-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=734220
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:52:07,401601446-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:52:07,410582344-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'361636\n'
[1] 13:52:07 [SUCCESS] ljishen@10.10.2.2
361636

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:52:07,599458865-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4MB_write.log.2
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:52:07,619887580-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:52:07,622799899-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '281e3916-3830-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 281e3916-3830-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T20:52:10.617698+0000 Maintaining 128 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 60 seconds or 0 objects
2021-10-28T20:52:10.617711+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T20:52:10.742797+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T20:52:10.742797+0000     0       0         0         0         0         0           -           0
2021-10-28T20:52:11.742909+0000     1      17        17         0         0         0           -           0
2021-10-28T20:52:12.743019+0000     2      30        30         0         0         0           -           0
2021-10-28T20:52:13.743097+0000     3      46        46         0         0         0           -           0
2021-10-28T20:52:14.743176+0000     4      57        57         0         0         0           -           0
2021-10-28T20:52:15.743251+0000     5      70        70         0         0         0           -           0
2021-10-28T20:52:16.743318+0000     6      81        81         0         0         0           -           0
2021-10-28T20:52:17.743386+0000     7      94        94         0         0         0           -           0
2021-10-28T20:52:18.743464+0000     8     105       105         0         0         0           -           0
2021-10-28T20:52:19.743545+0000     9     121       121         0         0         0           -           0
2021-10-28T20:52:20.743626+0000    10     127       129         2  0.799937       0.8     9.79289     9.79228
2021-10-28T20:52:21.743712+0000    11     127       137        10   3.63608        32     10.2976      10.184
2021-10-28T20:52:22.743791+0000    12     127       137        10   3.33307         0           -      10.184
2021-10-28T20:52:23.743867+0000    13     127       137        10   3.07668         0           -      10.184
2021-10-28T20:52:24.743941+0000    14     127       142        15   4.28538   6.66667     12.8672     11.0786
2021-10-28T20:52:25.744012+0000    15     127       142        15   3.99969         0           -     11.0786
2021-10-28T20:52:26.744085+0000    16     127       145        18   4.49965         6     14.5726     11.6609
2021-10-28T20:52:27.744157+0000    17     127       145        18   4.23497         0           -     11.6609
2021-10-28T20:52:28.744229+0000    18     127       145        18   3.99969         0           -     11.6609
2021-10-28T20:52:29.744302+0000    19     127       150        23   4.84173   6.66667     17.0546     12.8336
2021-10-28T20:52:30.744379+0000 min lat: 9.79166 max lat: 17.0559 avg lat: 12.8336
2021-10-28T20:52:30.744379+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T20:52:30.744379+0000    20     127       150        23   4.59965         0           -     12.8336
2021-10-28T20:52:31.744475+0000    21     127       153        26     4.952         6     18.6557     13.5053
2021-10-28T20:52:32.744563+0000    22     127       158        31   5.63592        20     20.0869     14.5669
2021-10-28T20:52:33.744641+0000    23     127       169        42   7.30378        44     20.2382      16.031
2021-10-28T20:52:34.744720+0000    24     127       182        55   9.16595        52      20.345     17.0398
2021-10-28T20:52:35.744791+0000    25     127       198        71   11.3591        64     20.0283     17.7595
2021-10-28T20:52:36.744864+0000    26     127       206        79   12.1529        32     20.2534     18.0045
2021-10-28T20:52:37.744939+0000    27     127       217        90   13.3323        44     20.1866     18.2809
2021-10-28T20:52:38.745007+0000    28     127       230       103   14.7131        52     20.4789      18.541
2021-10-28T20:52:39.745083+0000    29     127       246       119   16.4125        64      20.339     18.7829
2021-10-28T20:52:40.745156+0000    30     127       254       127    16.932        32     20.5866     18.8902
2021-10-28T20:52:41.745233+0000    31     127       265       138   17.8051        44     20.2369     18.9957
2021-10-28T20:52:42.745315+0000    32     127       265       138   17.2487         0           -     18.9957
2021-10-28T20:52:43.745400+0000    33     127       265       138    16.726         0           -     18.9957
2021-10-28T20:52:44.745475+0000    34     127       265       138    16.234         0           -     18.9957
2021-10-28T20:52:45.745547+0000    35     127       270       143   16.3416         5     21.4579     19.0818
2021-10-28T20:52:46.745620+0000    36     127       270       143   15.8877         0           -     19.0818
2021-10-28T20:52:47.745700+0000    37     127       270       143   15.4583         0           -     19.0818
2021-10-28T20:52:48.745773+0000    38     127       270       143   15.0515         0           -     19.0818
2021-10-28T20:52:49.745848+0000    39     127       273       146   14.9732         3     22.7905      19.158
2021-10-28T20:52:50.745924+0000 min lat: 9.79166 max lat: 22.7906 avg lat: 19.158
2021-10-28T20:52:50.745924+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T20:52:50.745924+0000    40     127       273       146   14.5989         0           -      19.158
2021-10-28T20:52:51.746013+0000    41     127       273       146   14.2428         0           -      19.158
2021-10-28T20:52:52.746092+0000    42     127       273       146   13.9037         0           -      19.158
2021-10-28T20:52:53.746167+0000    43     127       278       151   14.0454         5     24.7121     19.3419
2021-10-28T20:52:54.746240+0000    44     127       278       151   13.7262         0           -     19.3419
2021-10-28T20:52:55.746313+0000    45     127       278       151   13.4212         0           -     19.3419
2021-10-28T20:52:56.746389+0000    46     127       278       151   13.1294         0           -     19.3419
2021-10-28T20:52:57.746458+0000    47     127       281       154   13.1054         3     26.1888     19.4753
2021-10-28T20:52:58.746532+0000    48     127       281       154   12.8323         0           -     19.4753
2021-10-28T20:52:59.746612+0000    49     127       281       154   12.5705         0           -     19.4753
2021-10-28T20:53:00.746687+0000    50     127       281       154   12.3191         0           -     19.4753
2021-10-28T20:53:01.746772+0000    51     127       286       159   12.4696         5     28.4958     19.7589
2021-10-28T20:53:02.746881+0000    52     127       289       162   12.4606        12     29.5976     19.9411
2021-10-28T20:53:03.746965+0000    53     127       289       162   12.2255         0           -     19.9411
2021-10-28T20:53:04.747043+0000    54     127       289       162   11.9991         0           -     19.9411
2021-10-28T20:53:05.747115+0000    55     127       294       167   12.1445   6.66667     31.7766     20.2955
2021-10-28T20:53:06.747190+0000    56     127       297       170   12.1419        12     32.9957     20.5196
2021-10-28T20:53:07.747268+0000    57     127       297       170   11.9289         0           -     20.5196
2021-10-28T20:53:08.747342+0000    58     127       297       170   11.7232         0           -     20.5196
2021-10-28T20:53:09.747426+0000    59     127       302       175   11.8635   6.66667     35.5995     20.9504
2021-10-28T20:53:10.747508+0000 min lat: 9.79166 max lat: 35.5996 avg lat: 20.9504
2021-10-28T20:53:10.747508+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T20:53:10.747508+0000    60     127       302       175   11.6658         0           -     20.9504
2021-10-28T20:53:11.747591+0000    61       6       303       297   19.4739       244     4.30287     23.9462
2021-10-28T20:53:12.747680+0000    62       6       303       297   19.1598         0           -     23.9462
2021-10-28T20:53:13.747757+0000    63       1       303       302   19.1731        10     3.97465     23.6251
2021-10-28T20:53:14.747865+0000 Total time run:         63.0278
Total writes made:      303
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     19.2296
Stddev Bandwidth:       34.1992
Max bandwidth (MB/sec): 244
Min bandwidth (MB/sec): 0
Average IOPS:           4
Stddev IOPS:            8.57577
Max IOPS:               61
Min IOPS:               0
Average Latency(s):     23.5612
Stddev Latency(s):      8.34647
Max latency(s):         36.735
Min latency(s):         3.97351

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:53:15,467388401-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:53:15,473364355-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 734220

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:53:15,479659249-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 361636
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:53:15,487500619-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 361636
[1] 13:53:15 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:53:15,674496967-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4MB_write.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4MB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:53:15,825693257-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:53:39,654714797-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 304 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:53:39,662774869-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:53:48,629433157-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 304 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:53:48,637358755-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:53:57,611626252-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 304 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:53:57,620259013-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:54:06,667343905-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 304 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:54:06,675416460-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:54:15,673856061-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 304 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:54:15,682363566-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:54:15,688679240-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:54:15,692284184-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:54:15,699452365-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:54:15,705648434-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=736015
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:54:15,713412628-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:54:15,722600596-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'362154\n'
[1] 13:54:15 [SUCCESS] ljishen@10.10.2.2
362154

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:54:15,911267602-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4MB_seq.log.2
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:54:15,931283709-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:54:15,934313249-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '281e3916-3830-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 281e3916-3830-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T20:54:18.506548+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T20:54:18.506548+0000     0       0         0         0         0         0           -           0
2021-10-28T20:54:19.506689+0000     1      57        57         0         0         0           -           0
2021-10-28T20:54:20.506805+0000     2      94        94         0         0         0           -           0
2021-10-28T20:54:21.506879+0000     3     127       130         3   3.99945         4     2.97631      2.9589
2021-10-28T20:54:22.506991+0000     4     127       166        39   38.9949       144     3.44371     3.37996
2021-10-28T20:54:23.507105+0000     5     127       201        74   59.1925       140     3.54532     3.44772
2021-10-28T20:54:24.507217+0000     6     127       241       114   75.9905       160     3.34513     3.46804
2021-10-28T20:54:25.507295+0000     7     127       276       149   85.1328       140     3.46126     3.45586
2021-10-28T20:54:26.507411+0000     8      14       303       289   144.483       560     1.62325     2.82599
2021-10-28T20:54:27.507554+0000 Total time run:       8.4756
Total reads made:     303
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   142.999
Average IOPS:         35
Stddev IOPS:          45.7491
Max IOPS:             140
Min IOPS:             0
Average Latency(s):   2.74518
Max latency(s):       3.66538
Min latency(s):       0.265113

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:54:28,087141217-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:54:28,093686765-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 736015

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:54:28,100098920-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 362154
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:54:28,107971610-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 362154
[1] 13:54:28 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:54:28,294292423-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4MB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4MB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:54:28,445199127-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:54:52,316271726-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 304 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:54:52,324276704-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:55:01,280214169-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 304 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:55:01,288256257-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:55:10,394433394-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 304 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:55:10,402493146-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:55:19,469346381-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 304 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:55:19,476964831-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:55:28,419718185-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 304 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:55:28,428050841-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:55:28,434260215-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T13:55:28,436933303-07:00][RUNNING][ROUND 3/6/21] object_size=4MB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:55:28,440682119-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:55:28,450212101-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:55:28,887435950-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/281e3916-3830-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 281e3916-3830-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:55:28,898786351-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:55:28,902139946-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '281e3916-3830-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 281e3916-3830-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:55:28,911093891-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 281e3916-3830-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 13:55:34 [SUCCESS] 10.10.2.1\n[2] 13:55:35 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:55:35,208548624-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:55:35,220445794-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:55:35,225013602-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:55:35,375316771-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:55:35,379472064-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:55:35,535631534-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:55:35,823782344-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:55:35,829128285-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--5afd6251--df3d--4063--8261--c10cd0f40c1f-osd--block--73ddab88--8d27--41a2--ae29--a3d35fdf0d78 (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-5afd6251-df3d-4063-8261-c10cd0f40c1f" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-73ddab88-8d27-41a2-ae29-a3d35fdf0d78"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-5afd6251-df3d-4063-8261-c10cd0f40c1f" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-73ddab88-8d27-41a2-ae29-a3d35fdf0d78" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-5afd6251-df3d-4063-8261-c10cd0f40c1f"\n'
10.10.2.1: b'  Volume group "ceph-5afd6251-df3d-4063-8261-c10cd0f40c1f" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:55:36,201334506-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:55:36,211253445-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:55:36,214946709-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 666889a0-3831-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 666889a0-3831-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:56:36,936519811-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:56:56,943924863-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:56:56,953837021-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:56:56,957312235-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 666889a0-3831-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/666889a0-3831-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:57:05,628353136-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:57:05,638341205-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:57:05,641855683-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 666889a0-3831-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/666889a0-3831-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:57:14,637686910-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:57:14,643484801-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:57:14,854282802-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:57:14,857985744-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid 666889a0-3831-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/666889a0-3831-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:57:24,339203737-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:57:44,344284569-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:57:44,350871544-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:57:44,361025435-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:57:44,364686308-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 666889a0-3831-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/666889a0-3831-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:58:09,715785332-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:58:29,721550026-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:58:29,732480968-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T13:58:29,735990316-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 666889a0-3831-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/666889a0-3831-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     666889a0-3831-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.kecdqo(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 41s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 13:58:38 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:55:28,887435950-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/281e3916-3830-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 281e3916-3830-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:55:28,898786351-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:55:28,902139946-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '281e3916-3830-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 281e3916-3830-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:55:28,911093891-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 281e3916-3830-11ec-b51d-53e6e728d2d3'
[1] 13:55:34 [SUCCESS] 10.10.2.1
[2] 13:55:35 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:55:35,208548624-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:55:35,220445794-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:55:35,225013602-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:55:35,375316771-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:55:35,379472064-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:55:35,535631534-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:55:35,823782344-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:55:35,829128285-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--5afd6251--df3d--4063--8261--c10cd0f40c1f-osd--block--73ddab88--8d27--41a2--ae29--a3d35fdf0d78 (253:0)
  Archiving volume group "ceph-5afd6251-df3d-4063-8261-c10cd0f40c1f" metadata (seqno 5).
  Releasing logical volume "osd-block-73ddab88-8d27-41a2-ae29-a3d35fdf0d78"
  Creating volume group backup "/etc/lvm/backup/ceph-5afd6251-df3d-4063-8261-c10cd0f40c1f" (seqno 6).
  Logical volume "osd-block-73ddab88-8d27-41a2-ae29-a3d35fdf0d78" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-5afd6251-df3d-4063-8261-c10cd0f40c1f"
  Volume group "ceph-5afd6251-df3d-4063-8261-c10cd0f40c1f" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:55:36,201334506-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:55:36,211253445-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:55:36,214946709-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 666889a0-3831-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 666889a0-3831-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:56:36,936519811-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:56:56,943924863-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:56:56,953837021-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:56:56,957312235-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 666889a0-3831-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/666889a0-3831-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:57:05,628353136-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:57:05,638341205-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:57:05,641855683-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 666889a0-3831-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/666889a0-3831-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:57:14,637686910-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:57:14,643484801-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:57:14,854282802-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:57:14,857985744-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 666889a0-3831-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/666889a0-3831-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:57:24,339203737-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:57:44,344284569-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:57:44,350871544-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:57:44,361025435-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:57:44,364686308-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 666889a0-3831-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/666889a0-3831-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:58:09,715785332-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:58:29,721550026-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:58:29,732480968-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:58:29,735990316-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 666889a0-3831-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/666889a0-3831-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     666889a0-3831-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.kecdqo(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 41s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:58:38,332148554-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:58:38,340084813-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 13:58:38 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:58:38,816929370-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:58:38,820712741-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:58:38,842676419-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:58:38,845513386-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '666889a0-3831-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 666889a0-3831-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:58:42,812217622-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:58:42,815346639-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '666889a0-3831-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 666889a0-3831-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:58:46,777567073-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:58:46,780635086-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '666889a0-3831-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 666889a0-3831-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:58:50,764688890-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:58:50,767626677-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '666889a0-3831-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 666889a0-3831-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:58:58,707117608-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:58:58,710127611-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '666889a0-3831-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 666889a0-3831-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:59:03,252258881-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:59:03,255283702-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '666889a0-3831-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 666889a0-3831-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:59:07,432534687-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:59:07,435410748-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '666889a0-3831-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 666889a0-3831-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:59:12,149949083-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:59:12,152942094-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '666889a0-3831-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 666889a0-3831-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:59:17,049951163-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:59:17,052960515-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '666889a0-3831-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 666889a0-3831-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:59:21,517443589-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:59:21,520553240-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '666889a0-3831-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 666889a0-3831-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:59:26,067923510-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:59:26,070976715-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '666889a0-3831-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 666889a0-3831-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:59:29,919747960-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:59:29,922633589-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '666889a0-3831-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 666889a0-3831-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default   
-3         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host node-1
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0  
                       TOTAL  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:59:33,804949500-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T13:59:57,685216477-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:00:06,817997976-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:00:15,833932887-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:00:24,825940655-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:00:24,834460424-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:00:33,867646844-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:00:33,876078146-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:00:42,917935546-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:00:42,925805610-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:00:51,716175630-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:00:51,723985320-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:01:00,865470023-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:01:00,873418445-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:01:00,879460984-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:01:00,883454752-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:01:00,891013489-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:01:00,896826715-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=741817
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:01:00,904257962-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:01:00,913689900-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'365886\n'
[1] 14:01:01 [SUCCESS] ljishen@10.10.2.2
365886

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:01:01,103774204-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4MB_write.log.3
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:01:01,124480000-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:01:01,127387240-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '666889a0-3831-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 666889a0-3831-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T21:01:04.135285+0000 Maintaining 128 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 60 seconds or 0 objects
2021-10-28T21:01:04.135299+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T21:01:04.261126+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T21:01:04.261126+0000     0       0         0         0         0         0           -           0
2021-10-28T21:01:05.261240+0000     1      17        17         0         0         0           -           0
2021-10-28T21:01:06.261334+0000     2      30        30         0         0         0           -           0
2021-10-28T21:01:07.261450+0000     3      41        41         0         0         0           -           0
2021-10-28T21:01:08.261532+0000     4      57        57         0         0         0           -           0
2021-10-28T21:01:09.261648+0000     5      65        65         0         0         0           -           0
2021-10-28T21:01:10.261768+0000     6      78        78         0         0         0           -           0
2021-10-28T21:01:11.261884+0000     7      94        94         0         0         0           -           0
2021-10-28T21:01:12.261997+0000     8     105       105         0         0         0           -           0
2021-10-28T21:01:13.262080+0000     9     118       118         0         0         0           -           0
2021-10-28T21:01:14.262194+0000    10     127       129         2  0.799919       0.8     9.85458     9.85464
2021-10-28T21:01:15.262313+0000    11     127       137        10   3.63599        32     10.2561     10.2451
2021-10-28T21:01:16.262433+0000    12     127       137        10   3.33299         0           -     10.2451
2021-10-28T21:01:17.262549+0000    13     127       137        10    3.0766         0           -     10.2451
2021-10-28T21:01:18.262633+0000    14     127       142        15   4.28527   6.66667     12.9399     11.1434
2021-10-28T21:01:19.262736+0000    15     127       142        15   3.99959         0           -     11.1434
2021-10-28T21:01:20.262848+0000    16     127       145        18   4.49953         6     14.6228     11.7233
2021-10-28T21:01:21.262959+0000    17     127       145        18   4.23485         0           -     11.7233
2021-10-28T21:01:22.263070+0000    18     127       145        18   3.99958         0           -     11.7233
2021-10-28T21:01:23.263144+0000    19     127       150        23   4.84161   6.66667     17.2306     12.9205
2021-10-28T21:01:24.263257+0000 min lat: 9.85458 max lat: 17.2306 avg lat: 12.9205
2021-10-28T21:01:24.263257+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T21:01:24.263257+0000    20     127       150        23   4.59952         0           -     12.9205
2021-10-28T21:01:25.263374+0000    21     127       153        26   4.95187         6     18.7678     13.5952
2021-10-28T21:01:26.263488+0000    22     127       158        31   5.63577        20     19.8452     14.6032
2021-10-28T21:01:27.263602+0000    23     127       169        42   7.30358        44     19.9792     16.0236
2021-10-28T21:01:28.263683+0000    24     127       182        55   9.16571        52       20.07     16.9728
2021-10-28T21:01:29.263794+0000    25     127       198        71   11.3588        64     19.8409     17.6501
2021-10-28T21:01:30.263908+0000    26     127       206        79   12.1526        32     20.0773     17.8926
2021-10-28T21:01:31.264024+0000    27     127       222        95   14.0726        64     19.9717     18.2363
2021-10-28T21:01:32.264141+0000    28     127       230       103   14.7127        32     20.3006      18.396
2021-10-28T21:01:33.264222+0000    29     127       246       119   16.4121        64      20.196     18.6435
2021-10-28T21:01:34.264341+0000    30     127       257       130   17.3315        44     20.0952     18.7854
2021-10-28T21:01:35.264457+0000    31     127       265       138   17.8046        32     20.1616     18.8586
2021-10-28T21:01:36.264574+0000    32     127       265       138   17.2482         0           -     18.8586
2021-10-28T21:01:37.264688+0000    33     127       265       138   16.7255         0           -     18.8586
2021-10-28T21:01:38.264764+0000    34     127       265       138   16.2336         0           -     18.8586
2021-10-28T21:01:39.264875+0000    35     127       270       143   16.3411         5     21.3455     18.9456
2021-10-28T21:01:40.264986+0000    36     127       270       143   15.8872         0           -     18.9456
2021-10-28T21:01:41.265096+0000    37     127       270       143   15.4578         0           -     18.9456
2021-10-28T21:01:42.265206+0000    38     127       270       143    15.051         0           -     18.9456
2021-10-28T21:01:43.265287+0000    39     127       273       146   14.9728         3     23.0654     19.0302
2021-10-28T21:01:44.265406+0000 min lat: 9.85458 max lat: 23.0654 avg lat: 19.0302
2021-10-28T21:01:44.265406+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T21:01:44.265406+0000    40     127       273       146   14.5985         0           -     19.0302
2021-10-28T21:01:45.265526+0000    41     127       273       146   14.2424         0           -     19.0302
2021-10-28T21:01:46.265642+0000    42     127       273       146   13.9033         0           -     19.0302
2021-10-28T21:01:47.265757+0000    43     127       273       146     13.58         0           -     19.0302
2021-10-28T21:01:48.265837+0000    44     127       278       151   13.7258         4     25.0788     19.2306
2021-10-28T21:01:49.265948+0000    45     127       278       151   13.4208         0           -     19.2306
2021-10-28T21:01:50.266061+0000    46     127       278       151    13.129         0           -     19.2306
2021-10-28T21:01:51.266174+0000    47     127       281       154    13.105         4     26.1617     19.3656
2021-10-28T21:01:52.266288+0000    48     127       281       154    12.832         0           -     19.3656
2021-10-28T21:01:53.266369+0000    49     127       281       154   12.5701         0           -     19.3656
2021-10-28T21:01:54.266484+0000    50     127       281       154   12.3187         0           -     19.3656
2021-10-28T21:01:55.266596+0000    51     127       286       159   12.4693         5     28.6356     19.6571
2021-10-28T21:01:56.266713+0000    52     127       289       162   12.4602        12     29.5951     19.8411
2021-10-28T21:01:57.266826+0000    53     127       289       162   12.2251         0           -     19.8411
2021-10-28T21:01:58.266906+0000    54     127       289       162   11.9987         0           -     19.8411
2021-10-28T21:01:59.267024+0000    55     127       294       167   12.1442   6.66667     31.8431     20.2005
2021-10-28T21:02:00.267140+0000    56     127       297       170   12.1416        12     33.1379     20.4288
2021-10-28T21:02:01.267253+0000    57     127       297       170   11.9286         0           -     20.4288
2021-10-28T21:02:02.267366+0000    58     127       297       170   11.7229         0           -     20.4288
2021-10-28T21:02:03.267445+0000    59     127       302       175   11.8631   6.66667     35.5831     20.8618
2021-10-28T21:02:04.267564+0000 min lat: 9.85458 max lat: 35.5832 avg lat: 20.8618
2021-10-28T21:02:04.267564+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T21:02:04.267564+0000    60     127       302       175   11.6654         0           -     20.8618
2021-10-28T21:02:05.267688+0000    61       6       303       297   19.4733       244     4.40323       23.93
2021-10-28T21:02:06.267803+0000    62       6       303       297   19.1592         0           -       23.93
2021-10-28T21:02:07.267952+0000 Total time run:         62.9332
Total writes made:      303
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     19.2585
Stddev Bandwidth:       34.5897
Max bandwidth (MB/sec): 244
Min bandwidth (MB/sec): 0
Average IOPS:           4
Stddev IOPS:            8.66787
Max IOPS:               61
Min IOPS:               0
Average Latency(s):     23.5468
Stddev Latency(s):      8.42518
Max latency(s):         36.7589
Min latency(s):         4.08236

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:02:07,937081943-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:02:07,943552048-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 741817

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:02:07,949490932-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 365886
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:02:07,957359673-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 365886
[1] 14:02:08 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:02:08,142307355-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4MB_write.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4MB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:02:08,293690314-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:02:32,083288713-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 304 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:02:32,091303439-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:02:40,957161776-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 304 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:02:40,964933755-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:02:49,999855211-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 304 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:02:50,007849318-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:02:59,022382924-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 304 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:02:59,031005967-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:03:07,989458137-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 304 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:03:07,997530622-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:03:08,003641861-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:03:08,007453245-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:03:08,015254108-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:03:08,021116828-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=743160
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:03:08,028914916-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:03:08,038136687-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'365984\n'
[1] 14:03:08 [SUCCESS] ljishen@10.10.2.2
365984

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:03:08,227365227-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4MB_seq.log.3
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:03:08,247270124-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:03:08,250109917-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '666889a0-3831-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 666889a0-3831-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T21:03:11.127193+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T21:03:11.127193+0000     0       0         0         0         0         0           -           0
2021-10-28T21:03:12.127333+0000     1      64        64         0         0         0           -           0
2021-10-28T21:03:13.127447+0000     2     104       104         0         0         0           -           0
2021-10-28T21:03:14.127556+0000     3     127       145        18   23.9965        24     2.98971     2.78007
2021-10-28T21:03:15.127675+0000     4     127       187        60   59.9917       168     3.09716     2.99239
2021-10-28T21:03:16.127786+0000     5     127       222        95   75.9899       140       3.249     3.07765
2021-10-28T21:03:17.127901+0000     6     127       261       134   89.3217       156     3.26639      3.1247
2021-10-28T21:03:18.128014+0000     7     127       302       175   99.9872       164     3.24894     3.15837
2021-10-28T21:03:19.128127+0000     8       1       303       302   150.981       508    0.987778     2.57505
2021-10-28T21:03:20.128279+0000 Total time run:       8.00179
Total reads made:     303
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   151.466
Average IOPS:         37
Stddev IOPS:          41.1331
Max IOPS:             127
Min IOPS:             0
Average Latency(s):   2.57001
Max latency(s):       5.03158
Min latency(s):       0.20581

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:03:20,770073486-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:03:20,776315962-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 743160

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:03:20,782461225-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 365984
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:03:20,790501559-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 365984
[1] 14:03:20 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:03:20,974532643-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4MB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4MB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:03:21,125394991-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:03:45,048107674-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 304 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:03:45,055906553-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:03:53,957893492-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 304 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:03:53,965869635-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:04:02,791527045-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 304 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:04:02,799377782-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:04:11,774734253-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 304 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:04:11,782840532-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:04:20,754854421-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 304 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:04:20,763077019-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:04:20,769470789-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T14:04:20,773207713-07:00][RUNNING][ROUND 1/7/21] object_size=16MB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:04:20,777056337-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:04:20,786738756-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:04:21,221947972-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/666889a0-3831-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 666889a0-3831-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:04:21,232766192-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:04:21,236357395-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '666889a0-3831-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 666889a0-3831-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:04:21,244816690-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 666889a0-3831-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 14:04:26 [SUCCESS] 10.10.2.1\n[2] 14:04:27 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:04:27,548741352-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:04:27,560369955-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:04:27,565081704-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:04:27,715450724-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:04:27,719906973-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:04:27,875390729-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:04:28,167851166-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:04:28,172690765-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--6a29bfd3--b6cf--4d0b--8700--14d0883ee3ea-osd--block--ac63ad36--339a--40f7--b049--beb36dfcb51e (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-6a29bfd3-b6cf-4d0b-8700-14d0883ee3ea" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-ac63ad36-339a-40f7-b049-beb36dfcb51e"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-6a29bfd3-b6cf-4d0b-8700-14d0883ee3ea" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-ac63ad36-339a-40f7-b049-beb36dfcb51e" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-6a29bfd3-b6cf-4d0b-8700-14d0883ee3ea"\n'
10.10.2.1: b'  Volume group "ceph-6a29bfd3-b6cf-4d0b-8700-14d0883ee3ea" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:04:28,509124299-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:04:28,518183061-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:04:28,521736212-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\npodman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: a3afff04-3832-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid a3afff04-3832-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:05:30,334455077-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:05:50,341671368-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:05:50,351887185-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:05:50,355713099-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid a3afff04-3832-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/a3afff04-3832-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:05:59,688005008-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:05:59,697695608-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:05:59,701485213-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid a3afff04-3832-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/a3afff04-3832-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:06:08,811830722-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:06:08,817666664-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:06:09,029921380-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:06:09,033525867-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid a3afff04-3832-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/a3afff04-3832-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:06:18,378009493-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:06:38,383152056-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:06:38,389725435-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:06:38,400364959-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:06:38,404240235-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid a3afff04-3832-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/a3afff04-3832-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:07:03,346712939-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:07:23,352349053-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:07:23,362634962-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:07:23,366196799-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid a3afff04-3832-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/a3afff04-3832-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     a3afff04-3832-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.isxsqn(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 14:07:32 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:04:21,221947972-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/666889a0-3831-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 666889a0-3831-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:04:21,232766192-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:04:21,236357395-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '666889a0-3831-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 666889a0-3831-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:04:21,244816690-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 666889a0-3831-11ec-b51d-53e6e728d2d3'
[1] 14:04:26 [SUCCESS] 10.10.2.1
[2] 14:04:27 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:04:27,548741352-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:04:27,560369955-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:04:27,565081704-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:04:27,715450724-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:04:27,719906973-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:04:27,875390729-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:04:28,167851166-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:04:28,172690765-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--6a29bfd3--b6cf--4d0b--8700--14d0883ee3ea-osd--block--ac63ad36--339a--40f7--b049--beb36dfcb51e (253:0)
  Archiving volume group "ceph-6a29bfd3-b6cf-4d0b-8700-14d0883ee3ea" metadata (seqno 5).
  Releasing logical volume "osd-block-ac63ad36-339a-40f7-b049-beb36dfcb51e"
  Creating volume group backup "/etc/lvm/backup/ceph-6a29bfd3-b6cf-4d0b-8700-14d0883ee3ea" (seqno 6).
  Logical volume "osd-block-ac63ad36-339a-40f7-b049-beb36dfcb51e" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-6a29bfd3-b6cf-4d0b-8700-14d0883ee3ea"
  Volume group "ceph-6a29bfd3-b6cf-4d0b-8700-14d0883ee3ea" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:04:28,509124299-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:04:28,518183061-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:04:28,521736212-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: a3afff04-3832-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid a3afff04-3832-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:05:30,334455077-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:05:50,341671368-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:05:50,351887185-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:05:50,355713099-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid a3afff04-3832-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/a3afff04-3832-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:05:59,688005008-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:05:59,697695608-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:05:59,701485213-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid a3afff04-3832-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/a3afff04-3832-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:06:08,811830722-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:06:08,817666664-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:06:09,029921380-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:06:09,033525867-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid a3afff04-3832-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/a3afff04-3832-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:06:18,378009493-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:06:38,383152056-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:06:38,389725435-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:06:38,400364959-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:06:38,404240235-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid a3afff04-3832-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/a3afff04-3832-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:07:03,346712939-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:07:23,352349053-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:07:23,362634962-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:07:23,366196799-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid a3afff04-3832-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/a3afff04-3832-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     a3afff04-3832-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.isxsqn(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:07:32,035900149-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:07:32,043852688-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 14:07:32 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:07:32,520737242-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:07:32,524443197-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:07:32,546313679-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:07:32,549218824-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a3afff04-3832-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a3afff04-3832-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:07:36,640496601-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:07:36,643404963-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a3afff04-3832-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a3afff04-3832-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:07:40,773387059-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:07:40,776521868-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a3afff04-3832-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a3afff04-3832-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:07:44,665628440-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:07:44,668485866-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a3afff04-3832-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a3afff04-3832-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:07:52,483238520-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:07:52,486273179-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a3afff04-3832-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a3afff04-3832-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:07:57,174463697-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:07:57,177816436-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a3afff04-3832-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a3afff04-3832-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:08:02,163469040-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:08:02,166433497-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a3afff04-3832-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a3afff04-3832-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:08:07,172662513-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:08:07,175937505-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a3afff04-3832-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a3afff04-3832-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:08:11,556223331-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:08:11,559168422-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a3afff04-3832-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a3afff04-3832-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:08:16,038037857-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:08:16,040880495-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a3afff04-3832-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a3afff04-3832-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:08:20,504425433-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:08:20,507513794-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a3afff04-3832-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a3afff04-3832-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 18 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 19 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:08:24,405514993-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:08:24,408878452-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a3afff04-3832-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a3afff04-3832-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.39059         -  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default   
-3         0.39059         -  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host node-1
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0  
                       TOTAL  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:08:28,597029599-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:08:52,661858082-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:09:01,561102640-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:09:10,542254229-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:09:19,697620936-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:09:19,706447052-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:09:28,771364893-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:09:28,780076714-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:09:38,086318794-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:09:38,094076997-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:09:47,368174297-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:09:47,376358683-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:09:56,685879479-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:09:56,694420647-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:09:56,700661962-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:09:56,704479177-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:09:56,711958815-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:09:56,718180802-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=749093
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:09:56,725595367-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:09:56,735155668-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'369625\n'
[1] 14:09:56 [SUCCESS] ljishen@10.10.2.2
369625

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:09:56,928740527-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16MB_write.log.1
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:09:56,949838975-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:09:56,952774378-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a3afff04-3832-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a3afff04-3832-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T21:09:59.988154+0000 Maintaining 128 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 60 seconds or 0 objects
2021-10-28T21:09:59.988193+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T21:10:00.662816+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T21:10:00.662816+0000     0       0         0         0         0         0           -           0
2021-10-28T21:10:01.662918+0000     1       4         4         0         0         0           -           0
2021-10-28T21:10:02.662994+0000     2       8         8         0         0         0           -           0
2021-10-28T21:10:03.663062+0000     3      11        11         0         0         0           -           0
2021-10-28T21:10:04.663138+0000     4      15        15         0         0         0           -           0
2021-10-28T21:10:05.663213+0000     5      18        18         0         0         0           -           0
2021-10-28T21:10:06.663301+0000     6      21        21         0         0         0           -           0
2021-10-28T21:10:07.663379+0000     7      24        24         0         0         0           -           0
2021-10-28T21:10:08.663455+0000     8      27        27         0         0         0           -           0
2021-10-28T21:10:09.663535+0000     9      30        30         0         0         0           -           0
2021-10-28T21:10:10.663618+0000    10      33        33         0         0         0           -           0
2021-10-28T21:10:11.663694+0000    11      34        34         0         0         0           -           0
2021-10-28T21:10:12.663781+0000    12      34        34         0         0         0           -           0
2021-10-28T21:10:13.663851+0000    13      34        34         0         0         0           -           0
2021-10-28T21:10:14.663928+0000    14      35        35         0         0         0           -           0
2021-10-28T21:10:15.663998+0000    15      35        35         0         0         0           -           0
2021-10-28T21:10:16.664076+0000    16      35        35         0         0         0           -           0
2021-10-28T21:10:17.664155+0000    17      35        35         0         0         0           -           0
2021-10-28T21:10:18.664251+0000    18      36        36         0         0         0           -           0
2021-10-28T21:10:19.664331+0000    19      36        36         0         0         0           -           0
2021-10-28T21:10:20.664423+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-28T21:10:20.664423+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T21:10:20.664423+0000    20      38        38         0         0         0           -           0
2021-10-28T21:10:21.664521+0000    21      42        42         0         0         0           -           0
2021-10-28T21:10:22.664612+0000    22      44        44         0         0         0           -           0
2021-10-28T21:10:23.664702+0000    23      47        47         0         0         0           -           0
2021-10-28T21:10:24.664780+0000    24      51        51         0         0         0           -           0
2021-10-28T21:10:25.664856+0000    25      55        55         0         0         0           -           0
2021-10-28T21:10:26.664941+0000    26      58        58         0         0         0           -           0
2021-10-28T21:10:27.665016+0000    27      61        61         0         0         0           -           0
2021-10-28T21:10:28.665090+0000    28      64        64         0         0         0           -           0
2021-10-28T21:10:29.665162+0000    29      66        66         0         0         0           -           0
2021-10-28T21:10:30.665243+0000    30      66        66         0         0         0           -           0
2021-10-28T21:10:31.665311+0000    31      67        67         0         0         0           -           0
2021-10-28T21:10:32.665385+0000    32      67        67         0         0         0           -           0
2021-10-28T21:10:33.665464+0000    33      67        67         0         0         0           -           0
2021-10-28T21:10:34.665551+0000    34      67        67         0         0         0           -           0
2021-10-28T21:10:35.665622+0000    35      67        67         0         0         0           -           0
2021-10-28T21:10:36.665699+0000    36      67        67         0         0         0           -           0
2021-10-28T21:10:37.665769+0000    37      68        68         0         0         0           -           0
2021-10-28T21:10:38.665851+0000    38      68        68         0         0         0           -           0
2021-10-28T21:10:39.665925+0000    39      68        68         0         0         0           -           0
2021-10-28T21:10:40.665997+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-28T21:10:40.665997+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T21:10:40.665997+0000    40      68        68         0         0         0           -           0
2021-10-28T21:10:41.666071+0000    41      68        68         0         0         0           -           0
2021-10-28T21:10:42.666146+0000    42      68        68         0         0         0           -           0
2021-10-28T21:10:43.666218+0000    43      69        69         0         0         0           -           0
2021-10-28T21:10:44.666300+0000    44      69        69         0         0         0           -           0
2021-10-28T21:10:45.666368+0000    45      70        70         0         0         0           -           0
2021-10-28T21:10:46.666445+0000    46      70        70         0         0         0           -           0
2021-10-28T21:10:47.666515+0000    47      70        70         0         0         0           -           0
2021-10-28T21:10:48.666589+0000    48      71        71         0         0         0           -           0
2021-10-28T21:10:49.666666+0000    49      71        71         0         0         0           -           0
2021-10-28T21:10:50.666750+0000    50      71        71         0         0         0           -           0
2021-10-28T21:10:51.666818+0000    51      72        72         0         0         0           -           0
2021-10-28T21:10:52.666892+0000    52      72        72         0         0         0           -           0
2021-10-28T21:10:53.666963+0000    53      73        73         0         0         0           -           0
2021-10-28T21:10:54.667040+0000    54      73        73         0         0         0           -           0
2021-10-28T21:10:55.667113+0000    55      75        75         0         0         0           -           0
2021-10-28T21:10:56.667187+0000    56      78        78         0         0         0           -           0
2021-10-28T21:10:57.667254+0000    57      82        82         0         0         0           -           0
2021-10-28T21:10:58.667328+0000    58      85        85         0         0         0           -           0
2021-10-28T21:10:59.667395+0000    59      88        88         0         0         0           -           0
2021-10-28T21:11:00.667468+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-28T21:11:00.667468+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T21:11:00.667468+0000    60      91        91         0         0         0           -           0
2021-10-28T21:11:01.667543+0000    61      95        95         0         0         0           -           0
2021-10-28T21:11:02.667617+0000    62      98        98         0         0         0           -           0
2021-10-28T21:11:03.667686+0000    63      98        98         0         0         0           -           0
2021-10-28T21:11:04.667765+0000    64      98        98         0         0         0           -           0
2021-10-28T21:11:05.667833+0000    65      98        98         0         0         0           -           0
2021-10-28T21:11:06.667922+0000    66      98        98         0         0         0           -           0
2021-10-28T21:11:07.668045+0000    67      99        99         0         0         0           -           0
2021-10-28T21:11:08.668180+0000    68      99        99         0         0         0           -           0
2021-10-28T21:11:09.668280+0000    69      99        99         0         0         0           -           0
2021-10-28T21:11:10.668402+0000    70      99        99         0         0         0           -           0
2021-10-28T21:11:11.668515+0000    71      99        99         0         0         0           -           0
2021-10-28T21:11:12.668636+0000    72      99        99         0         0         0           -           0
2021-10-28T21:11:13.668727+0000    73     100       100         0         0         0           -           0
2021-10-28T21:11:14.668812+0000    74     100       100         0         0         0           -           0
2021-10-28T21:11:15.668895+0000    75     103       103         0         0         0           -           0
2021-10-28T21:11:16.669014+0000    76     106       106         0         0         0           -           0
2021-10-28T21:11:17.669106+0000    77     108       108         0         0         0           -           0
2021-10-28T21:11:18.669233+0000    78     112       112         0         0         0           -           0
2021-10-28T21:11:19.669341+0000    79     115       115         0         0         0           -           0
2021-10-28T21:11:20.669461+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-28T21:11:20.669461+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T21:11:20.669461+0000    80     118       118         0         0         0           -           0
2021-10-28T21:11:21.669566+0000    81     122       122         0         0         0           -           0
2021-10-28T21:11:22.669688+0000    82     125       125         0         0         0           -           0
2021-10-28T21:11:23.669797+0000    83     125       125         0         0         0           -           0
2021-10-28T21:11:24.669917+0000    84     125       125         0         0         0           -           0
2021-10-28T21:11:25.670002+0000    85     125       125         0         0         0           -           0
2021-10-28T21:11:26.670122+0000    86     126       126         0         0         0           -           0
2021-10-28T21:11:27.670230+0000    87     126       126         0         0         0           -           0
2021-10-28T21:11:28.670349+0000    88     126       126         0         0         0           -           0
2021-10-28T21:11:29.670442+0000    89       2       128       126   22.6498   22.6517     6.78716     50.9346
2021-10-28T21:11:30.670611+0000 Total time run:         89.3563
Total writes made:      128
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     22.9195
Stddev Bandwidth:       2.40107
Max bandwidth (MB/sec): 22.6517
Min bandwidth (MB/sec): 0
Average IOPS:           1
Stddev IOPS:            0.106
Max IOPS:               1
Min IOPS:               0
Average Latency(s):     50.1789
Stddev Latency(s):      28.6291
Max latency(s):         88.7785
Min latency(s):         1.06607

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:11:31,354924964-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:11:31,361231621-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 749093

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:11:31,367618821-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 369625
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:11:31,375930688-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 369625
[1] 14:11:31 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:11:31,562582078-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16MB_write.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16MB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:11:31,713566722-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:11:56,075095956-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.7 GiB used, 393 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:11:56,083347969-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:12:05,373201188-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:12:05,381242044-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:12:14,697989252-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:12:14,706336596-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:12:23,988143232-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:12:23,996605944-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:12:33,403310553-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:12:33,411200945-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:12:33,417635223-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:12:33,421534993-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:12:33,429259683-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:12:33,435232822-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=751171
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:12:33,442836423-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:12:33,452529864-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'369739\n'
[1] 14:12:33 [SUCCESS] ljishen@10.10.2.2
369739

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:12:33,644113070-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16MB_seq.log.1
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:12:33,665215185-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:12:33,668199861-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'a3afff04-3832-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid a3afff04-3832-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T21:12:36.729053+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T21:12:36.729053+0000     0       0         0         0         0         0           -           0
2021-10-28T21:12:37.729161+0000     1      12        12         0         0         0           -           0
2021-10-28T21:12:38.729241+0000     2      24        24         0         0         0           -           0
2021-10-28T21:12:39.729324+0000     3      35        35         0         0         0           -           0
2021-10-28T21:12:40.729399+0000     4      44        44         0         0         0           -           0
2021-10-28T21:12:41.729468+0000     5      52        52         0         0         0           -           0
2021-10-28T21:12:42.729542+0000     6      64        64         0         0         0           -           0
2021-10-28T21:12:43.729623+0000     7      73        73         0         0         0           -           0
2021-10-28T21:12:44.729701+0000     8      81        81         0         0         0           -           0
2021-10-28T21:12:45.729782+0000     9      91        91         0         0         0           -           0
2021-10-28T21:12:46.729856+0000    10     100       100         0         0         0           -           0
2021-10-28T21:12:47.729930+0000    11     112       112         0         0         0           -           0
2021-10-28T21:12:48.730006+0000    12     119       119         0         0         0           -           0
2021-10-28T21:12:49.895187+0000    13       4       128       124   150.688   152.615    0.646733     7.18622
2021-10-28T21:12:50.895286+0000 Total time run:       13.6706
Total reads made:     128
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   149.811
Average IOPS:         9
Stddev IOPS:          2.49615
Max IOPS:             9
Min IOPS:             0
Average Latency(s):   6.99644
Max latency(s):       12.9665
Min latency(s):       0.646733

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:12:51,631174066-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:12:51,637651465-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 751171

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:12:51,644069412-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 369739
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:12:51,652359958-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 369739
[1] 14:12:51 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:12:51,838924585-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16MB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16MB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:12:51,989648828-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:13:16,417752287-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:13:16,425933176-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:13:25,741603886-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:13:25,750207372-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:13:35,296739198-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:13:35,305240352-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:13:44,438534103-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:13:44,446794622-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:13:53,856221520-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:13:53,864440452-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:13:53,870642592-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T14:13:53,873022178-07:00][RUNNING][ROUND 2/7/21] object_size=16MB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:13:53,876786773-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:13:53,886143048-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:13:54,313986985-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/a3afff04-3832-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid a3afff04-3832-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:13:54,324781751-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:13:54,328960427-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'a3afff04-3832-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid a3afff04-3832-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:13:54,337735927-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid a3afff04-3832-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 14:14:00 [SUCCESS] 10.10.2.1\n[2] 14:14:01 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:14:01,433139928-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:14:01,444499836-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:14:01,449317955-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:14:01,599372622-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:14:01,603706801-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:14:01,759592281-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:14:02,051410757-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:14:02,056615853-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--393abb36--2d0c--4699--a33b--3d971729a3fc-osd--block--e66a6a04--3086--45a1--ba0a--8fd3856bc37a (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-393abb36-2d0c-4699-a33b-3d971729a3fc" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-e66a6a04-3086-45a1-ba0a-8fd3856bc37a"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-393abb36-2d0c-4699-a33b-3d971729a3fc" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-e66a6a04-3086-45a1-ba0a-8fd3856bc37a" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-393abb36-2d0c-4699-a33b-3d971729a3fc"\n'
10.10.2.1: b'  Volume group "ceph-393abb36-2d0c-4699-a33b-3d971729a3fc" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:14:02,405138412-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:14:02,415057992-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:14:02,418723213-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\n'
10.10.2.1: b'Verifying lvm2 is present...\nVerifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: f9c21584-3833-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\nWaiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\nCreating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid f9c21584-3833-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:15:03,240113094-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:15:23,246745256-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:15:23,257352820-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:15:23,261044941-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid f9c21584-3833-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/f9c21584-3833-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:15:32,374445280-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:15:32,384655947-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:15:32,388180393-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid f9c21584-3833-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/f9c21584-3833-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:15:41,889372243-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:15:41,895330284-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:15:42,110214876-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:15:42,113833970-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid f9c21584-3833-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/f9c21584-3833-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:15:51,528484082-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:16:11,533450948-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:16:11,539970235-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:16:11,549932875-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:16:11,553687434-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid f9c21584-3833-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/f9c21584-3833-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:16:36,618726327-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:16:56,624007039-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:16:56,634128498-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:16:56,637988936-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid f9c21584-3833-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/f9c21584-3833-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     f9c21584-3833-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.cgwpgh(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 41s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 14:17:05 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:13:54,313986985-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/a3afff04-3832-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid a3afff04-3832-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:13:54,324781751-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:13:54,328960427-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'a3afff04-3832-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid a3afff04-3832-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:13:54,337735927-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid a3afff04-3832-11ec-b51d-53e6e728d2d3'
[1] 14:14:00 [SUCCESS] 10.10.2.1
[2] 14:14:01 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:14:01,433139928-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:14:01,444499836-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:14:01,449317955-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:14:01,599372622-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:14:01,603706801-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:14:01,759592281-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:14:02,051410757-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:14:02,056615853-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--393abb36--2d0c--4699--a33b--3d971729a3fc-osd--block--e66a6a04--3086--45a1--ba0a--8fd3856bc37a (253:0)
  Archiving volume group "ceph-393abb36-2d0c-4699-a33b-3d971729a3fc" metadata (seqno 5).
  Releasing logical volume "osd-block-e66a6a04-3086-45a1-ba0a-8fd3856bc37a"
  Creating volume group backup "/etc/lvm/backup/ceph-393abb36-2d0c-4699-a33b-3d971729a3fc" (seqno 6).
  Logical volume "osd-block-e66a6a04-3086-45a1-ba0a-8fd3856bc37a" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-393abb36-2d0c-4699-a33b-3d971729a3fc"
  Volume group "ceph-393abb36-2d0c-4699-a33b-3d971729a3fc" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:14:02,405138412-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:14:02,415057992-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:14:02,418723213-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: f9c21584-3833-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid f9c21584-3833-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:15:03,240113094-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:15:23,246745256-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:15:23,257352820-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:15:23,261044941-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid f9c21584-3833-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/f9c21584-3833-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:15:32,374445280-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:15:32,384655947-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:15:32,388180393-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid f9c21584-3833-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/f9c21584-3833-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:15:41,889372243-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:15:41,895330284-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:15:42,110214876-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:15:42,113833970-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid f9c21584-3833-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/f9c21584-3833-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:15:51,528484082-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:16:11,533450948-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:16:11,539970235-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:16:11,549932875-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:16:11,553687434-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid f9c21584-3833-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/f9c21584-3833-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:16:36,618726327-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:16:56,624007039-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:16:56,634128498-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:16:56,637988936-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid f9c21584-3833-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/f9c21584-3833-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     f9c21584-3833-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.cgwpgh(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 41s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:17:05,666276619-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:17:05,674839188-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 14:17:05 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:17:06,153580833-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:17:06,157442631-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:17:06,179742884-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:17:06,182518335-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f9c21584-3833-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f9c21584-3833-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:17:10,456084847-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:17:10,459025650-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f9c21584-3833-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f9c21584-3833-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:17:15,213908710-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:17:15,217045572-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f9c21584-3833-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f9c21584-3833-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:17:19,632113442-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:17:19,635118867-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f9c21584-3833-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f9c21584-3833-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:17:28,720642130-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:17:28,723628329-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f9c21584-3833-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f9c21584-3833-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:17:33,386213278-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:17:33,389143762-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f9c21584-3833-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f9c21584-3833-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:17:38,094445445-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:17:38,097453905-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f9c21584-3833-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f9c21584-3833-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:17:43,768226210-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:17:43,771398920-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f9c21584-3833-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f9c21584-3833-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:17:49,218409047-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:17:49,221184317-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f9c21584-3833-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f9c21584-3833-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:17:54,608354985-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:17:54,611291160-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f9c21584-3833-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f9c21584-3833-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:17:59,540313318-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:17:59,543192395-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f9c21584-3833-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f9c21584-3833-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 17 lfor 0/0/15 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 19 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:18:03,949010645-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:18:03,951881827-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f9c21584-3833-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f9c21584-3833-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.39059         -  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default   
-3         0.39059         -  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host node-1
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0  
                       TOTAL  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:18:08,133373025-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:18:32,488993279-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:18:41,887401383-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:18:51,576662137-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:18:51,584723110-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:19:01,041962698-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:19:01,049851197-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:19:10,465258655-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:19:10,473413716-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:19:19,798351992-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:19:19,806436510-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:19:29,300532129-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:19:29,308932211-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:19:29,315120455-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:19:29,318823234-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:19:29,326707645-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:19:29,332976682-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=759169
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:19:29,340940752-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:19:29,350081822-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'373347\n'
[1] 14:19:29 [SUCCESS] ljishen@10.10.2.2
373347

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:19:29,541341085-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16MB_write.log.2
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:19:29,561910545-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:19:29,564864303-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f9c21584-3833-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f9c21584-3833-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T21:19:32.975476+0000 Maintaining 128 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 60 seconds or 0 objects
2021-10-28T21:19:32.975510+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T21:19:33.652480+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T21:19:33.652480+0000     0       0         0         0         0         0           -           0
2021-10-28T21:19:34.652612+0000     1       3         3         0         0         0           -           0
2021-10-28T21:19:35.652734+0000     2       6         6         0         0         0           -           0
2021-10-28T21:19:36.652815+0000     3       9         9         0         0         0           -           0
2021-10-28T21:19:37.652913+0000     4      13        13         0         0         0           -           0
2021-10-28T21:19:38.653028+0000     5      17        17         0         0         0           -           0
2021-10-28T21:19:39.653143+0000     6      20        20         0         0         0           -           0
2021-10-28T21:19:40.653225+0000     7      22        22         0         0         0           -           0
2021-10-28T21:19:41.653330+0000     8      26        26         0         0         0           -           0
2021-10-28T21:19:42.653434+0000     9      29        29         0         0         0           -           0
2021-10-28T21:19:43.653536+0000    10      33        33         0         0         0           -           0
2021-10-28T21:19:44.653647+0000    11      34        34         0         0         0           -           0
2021-10-28T21:19:45.653742+0000    12      34        34         0         0         0           -           0
2021-10-28T21:19:46.653844+0000    13      34        34         0         0         0           -           0
2021-10-28T21:19:47.653928+0000    14      34        34         0         0         0           -           0
2021-10-28T21:19:48.654013+0000    15      35        35         0         0         0           -           0
2021-10-28T21:19:49.654113+0000    16      35        35         0         0         0           -           0
2021-10-28T21:19:50.654218+0000    17      35        35         0         0         0           -           0
2021-10-28T21:19:51.654320+0000    18      35        35         0         0         0           -           0
2021-10-28T21:19:52.654423+0000    19      35        35         0         0         0           -           0
2021-10-28T21:19:53.654505+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-28T21:19:53.654505+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T21:19:53.654505+0000    20      36        36         0         0         0           -           0
2021-10-28T21:19:54.654593+0000    21      38        38         0         0         0           -           0
2021-10-28T21:19:55.654696+0000    22      41        41         0         0         0           -           0
2021-10-28T21:19:56.654802+0000    23      44        44         0         0         0           -           0
2021-10-28T21:19:57.654896+0000    24      48        48         0         0         0           -           0
2021-10-28T21:19:58.654992+0000    25      51        51         0         0         0           -           0
2021-10-28T21:19:59.655079+0000    26      55        55         0         0         0           -           0
2021-10-28T21:20:00.655170+0000    27      58        58         0         0         0           -           0
2021-10-28T21:20:01.655268+0000    28      61        61         0         0         0           -           0
2021-10-28T21:20:02.655377+0000    29      64        64         0         0         0           -           0
2021-10-28T21:20:03.655460+0000    30      66        66         0         0         0           -           0
2021-10-28T21:20:04.655544+0000    31      66        66         0         0         0           -           0
2021-10-28T21:20:05.655640+0000    32      67        67         0         0         0           -           0
2021-10-28T21:20:06.655746+0000    33      67        67         0         0         0           -           0
2021-10-28T21:20:07.655859+0000    34      67        67         0         0         0           -           0
2021-10-28T21:20:08.655964+0000    35      67        67         0         0         0           -           0
2021-10-28T21:20:09.656060+0000    36      67        67         0         0         0           -           0
2021-10-28T21:20:10.656168+0000    37      67        67         0         0         0           -           0
2021-10-28T21:20:11.656275+0000    38      68        68         0         0         0           -           0
2021-10-28T21:20:12.656389+0000    39      68        68         0         0         0           -           0
2021-10-28T21:20:13.656502+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-28T21:20:13.656502+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T21:20:13.656502+0000    40      68        68         0         0         0           -           0
2021-10-28T21:20:14.656633+0000    41      68        68         0         0         0           -           0
2021-10-28T21:20:15.656738+0000    42      68        68         0         0         0           -           0
2021-10-28T21:20:16.656845+0000    43      68        68         0         0         0           -           0
2021-10-28T21:20:17.656937+0000    44      69        69         0         0         0           -           0
2021-10-28T21:20:18.657019+0000    45      69        69         0         0         0           -           0
2021-10-28T21:20:19.657111+0000    46      70        70         0         0         0           -           0
2021-10-28T21:20:20.657191+0000    47      70        70         0         0         0           -           0
2021-10-28T21:20:21.657274+0000    48      71        71         0         0         0           -           0
2021-10-28T21:20:22.657385+0000    49      71        71         0         0         0           -           0
2021-10-28T21:20:23.657503+0000    50      72        72         0         0         0           -           0
2021-10-28T21:20:24.657581+0000    51      72        72         0         0         0           -           0
2021-10-28T21:20:25.657666+0000    52      73        73         0         0         0           -           0
2021-10-28T21:20:26.657766+0000    53      73        73         0         0         0           -           0
2021-10-28T21:20:27.657871+0000    54      73        73         0         0         0           -           0
2021-10-28T21:20:28.657974+0000    55      74        74         0         0         0           -           0
2021-10-28T21:20:29.658061+0000    56      76        76         0         0         0           -           0
2021-10-28T21:20:30.658140+0000    57      80        80         0         0         0           -           0
2021-10-28T21:20:31.658215+0000    58      83        83         0         0         0           -           0
2021-10-28T21:20:32.658327+0000    59      86        86         0         0         0           -           0
2021-10-28T21:20:33.658435+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-28T21:20:33.658435+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T21:20:33.658435+0000    60      89        89         0         0         0           -           0
2021-10-28T21:20:34.658550+0000    61      92        92         0         0         0           -           0
2021-10-28T21:20:35.658657+0000    62      96        96         0         0         0           -           0
2021-10-28T21:20:36.658760+0000    63      98        98         0         0         0           -           0
2021-10-28T21:20:37.658859+0000    64      98        98         0         0         0           -           0
2021-10-28T21:20:38.658975+0000    65      98        98         0         0         0           -           0
2021-10-28T21:20:39.659045+0000    66      98        98         0         0         0           -           0
2021-10-28T21:20:40.659137+0000    67      99        99         0         0         0           -           0
2021-10-28T21:20:41.659239+0000    68      99        99         0         0         0           -           0
2021-10-28T21:20:42.659356+0000    69      99        99         0         0         0           -           0
2021-10-28T21:20:43.659475+0000    70      99        99         0         0         0           -           0
2021-10-28T21:20:44.659582+0000    71      99        99         0         0         0           -           0
2021-10-28T21:20:45.659684+0000    72      99        99         0         0         0           -           0
2021-10-28T21:20:46.659800+0000    73      99        99         0         0         0           -           0
2021-10-28T21:20:47.659906+0000    74     100       100         0         0         0           -           0
2021-10-28T21:20:48.660016+0000    75     100       100         0         0         0           -           0
2021-10-28T21:20:49.660111+0000    76     102       102         0         0         0           -           0
2021-10-28T21:20:50.660217+0000    77     105       105         0         0         0           -           0
2021-10-28T21:20:51.660329+0000    78     108       108         0         0         0           -           0
2021-10-28T21:20:52.660454+0000    79     111       111         0         0         0           -           0
2021-10-28T21:20:53.660561+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-28T21:20:53.660561+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T21:20:53.660561+0000    80     114       114         0         0         0           -           0
2021-10-28T21:20:54.660658+0000    81     118       118         0         0         0           -           0
2021-10-28T21:20:55.660761+0000    82     121       121         0         0         0           -           0
2021-10-28T21:20:56.660869+0000    83     124       124         0         0         0           -           0
2021-10-28T21:20:57.660977+0000    84     125       125         0         0         0           -           0
2021-10-28T21:20:58.661075+0000    85     125       125         0         0         0           -           0
2021-10-28T21:20:59.661184+0000    86     125       125         0         0         0           -           0
2021-10-28T21:21:00.661298+0000    87     126       126         0         0         0           -           0
2021-10-28T21:21:01.661395+0000    88     126       126         0         0         0           -           0
2021-10-28T21:21:02.661504+0000    89     126       126         0         0         0           -           0
2021-10-28T21:21:03.661615+0000    90     127       127         0         0         0           -           0
2021-10-28T21:21:04.661733+0000 Total time run:         90.7949
Total writes made:      128
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     22.5563
Stddev Bandwidth:       0
Max bandwidth (MB/sec): 0
Min bandwidth (MB/sec): 0
Average IOPS:           1
Stddev IOPS:            0
Max IOPS:               0
Min IOPS:               0
Average Latency(s):     50.8059
Stddev Latency(s):      28.8744
Max latency(s):         90.1617
Min latency(s):         0.991397

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:21:05,936147303-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:21:05,942675759-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 759169

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:21:05,949157084-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 373347
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:21:05,957139931-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 373347
[1] 14:21:06 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:21:06,146154613-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16MB_write.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16MB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:21:06,297961685-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:21:30,624249718-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.4 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:21:30,632465564-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:21:39,931184707-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:21:39,938941717-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:21:49,330173620-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:21:49,338201080-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:21:58,924803943-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:21:58,932916995-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:22:08,631019163-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:22:08,639116566-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:22:08,645610986-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:22:08,649561102-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:22:08,657236038-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:22:08,663783058-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=761419
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:22:08,671056968-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:22:08,680579436-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'373457\n'
[1] 14:22:08 [SUCCESS] ljishen@10.10.2.2
373457

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:22:08,872272256-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16MB_seq.log.2
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:22:08,893311943-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:22:08,896201439-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f9c21584-3833-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f9c21584-3833-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T21:22:12.580961+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T21:22:12.580961+0000     0       0         0         0         0         0           -           0
2021-10-28T21:22:13.581080+0000     1      10        10         0         0         0           -           0
2021-10-28T21:22:14.581196+0000     2      23        23         0         0         0           -           0
2021-10-28T21:22:15.581268+0000     3      33        33         0         0         0           -           0
2021-10-28T21:22:16.581334+0000     4      43        43         0         0         0           -           0
2021-10-28T21:22:17.581401+0000     5      55        55         0         0         0           -           0
2021-10-28T21:22:18.581466+0000     6      65        65         0         0         0           -           0
2021-10-28T21:22:19.581535+0000     7      74        74         0         0         0           -           0
2021-10-28T21:22:20.581605+0000     8      86        86         0         0         0           -           0
2021-10-28T21:22:21.581677+0000     9      94        94         0         0         0           -           0
2021-10-28T21:22:22.581755+0000    10     103       103         0         0         0           -           0
2021-10-28T21:22:23.581827+0000    11     115       115         0         0         0           -           0
2021-10-28T21:22:24.581911+0000    12     122       122         0         0         0           -           0
2021-10-28T21:22:25.581977+0000    13       4       128       124   152.602   152.615    0.724754     6.69703
2021-10-28T21:22:26.582075+0000 Total time run:       13.2709
Total reads made:     128
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   154.323
Average IOPS:         9
Stddev IOPS:          2.49615
Max IOPS:             9
Min IOPS:             0
Average Latency(s):   6.5248
Max latency(s):       12.3526
Min latency(s):       0.550485

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:22:27,317296869-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:22:27,323719604-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 761419

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:22:27,329919670-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 373457
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:22:27,337924178-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 373457
[1] 14:22:27 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:22:27,522122662-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16MB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16MB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:22:27,673740557-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:22:52,246577293-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:22:52,254948201-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:23:01,643353268-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:23:01,651333750-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:23:11,267186657-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:23:11,275213256-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:23:20,799928000-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:23:20,808306111-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:23:30,499794434-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:23:30,508031690-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:23:30,514634224-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T14:23:30,517413784-07:00][RUNNING][ROUND 3/7/21] object_size=16MB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:23:30,521220809-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:23:30,531114076-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:23:30,999462534-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/f9c21584-3833-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid f9c21584-3833-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:23:31,010602829-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:23:31,014230319-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'f9c21584-3833-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid f9c21584-3833-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:23:31,022549419-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid f9c21584-3833-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 14:23:36 [SUCCESS] 10.10.2.1\n[2] 14:23:37 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:23:37,781468439-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:23:37,792933684-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:23:37,798150713-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:23:37,951513067-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:23:37,956157679-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:23:38,111419313-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:23:38,399947450-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:23:38,404792549-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--135d92c9--4bc8--4596--927f--869335af75e0-osd--block--d30dd553--331d--41be--827c--2c53c6ff4677 (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-135d92c9-4bc8-4596-927f-869335af75e0" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-d30dd553-331d-41be-827c-2c53c6ff4677"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-135d92c9-4bc8-4596-927f-869335af75e0" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-d30dd553-331d-41be-827c-2c53c6ff4677" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-135d92c9-4bc8-4596-927f-869335af75e0"\n'
10.10.2.1: b'  Volume group "ceph-135d92c9-4bc8-4596-927f-869335af75e0" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:23:38,721053546-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:23:38,731205652-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:23:38,734574025-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\npodman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 5144aa46-3835-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\nAdding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 5144aa46-3835-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:24:40,532197069-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:25:00,539511735-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:25:00,549814575-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:25:00,553557733-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 5144aa46-3835-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/5144aa46-3835-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:25:09,670573132-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:25:09,680932058-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:25:09,684419484-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 5144aa46-3835-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/5144aa46-3835-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:25:18,984949936-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:25:18,990949315-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:25:19,201842231-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:25:19,205543911-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid 5144aa46-3835-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/5144aa46-3835-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:25:28,174537826-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:25:48,179774643-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:25:48,186113150-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:25:48,196601057-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:25:48,200199743-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 5144aa46-3835-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/5144aa46-3835-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:26:12,860049887-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:26:32,865316195-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:26:32,875632510-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T14:26:32,879210146-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 5144aa46-3835-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/5144aa46-3835-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     5144aa46-3835-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.gckquv(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 41s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 14:26:41 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:23:30,999462534-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/f9c21584-3833-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid f9c21584-3833-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:23:31,010602829-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:23:31,014230319-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'f9c21584-3833-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid f9c21584-3833-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:23:31,022549419-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid f9c21584-3833-11ec-b51d-53e6e728d2d3'
[1] 14:23:36 [SUCCESS] 10.10.2.1
[2] 14:23:37 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:23:37,781468439-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:23:37,792933684-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:23:37,798150713-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:23:37,951513067-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:23:37,956157679-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:23:38,111419313-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:23:38,399947450-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:23:38,404792549-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--135d92c9--4bc8--4596--927f--869335af75e0-osd--block--d30dd553--331d--41be--827c--2c53c6ff4677 (253:0)
  Archiving volume group "ceph-135d92c9-4bc8-4596-927f-869335af75e0" metadata (seqno 5).
  Releasing logical volume "osd-block-d30dd553-331d-41be-827c-2c53c6ff4677"
  Creating volume group backup "/etc/lvm/backup/ceph-135d92c9-4bc8-4596-927f-869335af75e0" (seqno 6).
  Logical volume "osd-block-d30dd553-331d-41be-827c-2c53c6ff4677" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-135d92c9-4bc8-4596-927f-869335af75e0"
  Volume group "ceph-135d92c9-4bc8-4596-927f-869335af75e0" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:23:38,721053546-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:23:38,731205652-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:23:38,734574025-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 5144aa46-3835-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 5144aa46-3835-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:24:40,532197069-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:25:00,539511735-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:25:00,549814575-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:25:00,553557733-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 5144aa46-3835-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/5144aa46-3835-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:25:09,670573132-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:25:09,680932058-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:25:09,684419484-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 5144aa46-3835-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/5144aa46-3835-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:25:18,984949936-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:25:18,990949315-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:25:19,201842231-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:25:19,205543911-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 5144aa46-3835-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/5144aa46-3835-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:25:28,174537826-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:25:48,179774643-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:25:48,186113150-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:25:48,196601057-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:25:48,200199743-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 5144aa46-3835-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/5144aa46-3835-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:26:12,860049887-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:26:32,865316195-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:26:32,875632510-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:26:32,879210146-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 5144aa46-3835-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/5144aa46-3835-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     5144aa46-3835-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.gckquv(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 41s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:26:41,500755231-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:26:41,509055996-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 14:26:41 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:26:41,985521413-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:26:41,989570114-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:26:42,012543204-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:26:42,015394809-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5144aa46-3835-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5144aa46-3835-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:26:46,651917530-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:26:46,654786017-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5144aa46-3835-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5144aa46-3835-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:26:51,333348054-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:26:51,336583693-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5144aa46-3835-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5144aa46-3835-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:26:55,657877299-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:26:55,660886441-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5144aa46-3835-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5144aa46-3835-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:27:04,715869288-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:27:04,718859985-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5144aa46-3835-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5144aa46-3835-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:27:10,123490566-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:27:10,126408486-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5144aa46-3835-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5144aa46-3835-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:27:14,864829781-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:27:14,867777737-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5144aa46-3835-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5144aa46-3835-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:27:20,256005876-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:27:20,259095719-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5144aa46-3835-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5144aa46-3835-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:27:24,779639255-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:27:24,782668063-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5144aa46-3835-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5144aa46-3835-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:27:30,286518464-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:27:30,289619408-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5144aa46-3835-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5144aa46-3835-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:27:35,336746814-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:27:35,339859010-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5144aa46-3835-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5144aa46-3835-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 18 lfor 0/0/15 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:27:39,670146816-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:27:39,673127694-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5144aa46-3835-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5144aa46-3835-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default   
-3         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host node-1
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0  
                       TOTAL  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:27:44,145934042-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:28:08,617860674-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:28:17,940861328-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:28:27,613733287-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:28:27,621975471-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:28:37,112647693-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:28:37,121043527-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:28:46,654422860-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:28:46,662432416-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:28:56,319902621-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:28:56,328271704-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:29:05,725805165-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:29:05,733754807-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:29:05,740326322-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:29:05,744331250-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:29:05,752481160-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:29:05,758816190-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=769370
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:29:05,766503238-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:29:05,775883005-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'377073\n'
[1] 14:29:05 [SUCCESS] ljishen@10.10.2.2
377073

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:29:05,967923920-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16MB_write.log.3
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:29:05,988949557-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:29:05,991840856-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5144aa46-3835-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5144aa46-3835-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T21:29:09.276624+0000 Maintaining 128 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 60 seconds or 0 objects
2021-10-28T21:29:09.276654+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-28T21:29:09.950883+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T21:29:09.950883+0000     0       0         0         0         0         0           -           0
2021-10-28T21:29:10.951006+0000     1       4         4         0         0         0           -           0
2021-10-28T21:29:11.951089+0000     2       7         7         0         0         0           -           0
2021-10-28T21:29:12.951167+0000     3       9         9         0         0         0           -           0
2021-10-28T21:29:13.951244+0000     4      13        13         0         0         0           -           0
2021-10-28T21:29:14.951322+0000     5      17        17         0         0         0           -           0
2021-10-28T21:29:15.951398+0000     6      21        21         0         0         0           -           0
2021-10-28T21:29:16.951474+0000     7      24        24         0         0         0           -           0
2021-10-28T21:29:17.951552+0000     8      27        27         0         0         0           -           0
2021-10-28T21:29:18.951635+0000     9      30        30         0         0         0           -           0
2021-10-28T21:29:19.951717+0000    10      33        33         0         0         0           -           0
2021-10-28T21:29:20.951796+0000    11      34        34         0         0         0           -           0
2021-10-28T21:29:21.951874+0000    12      34        34         0         0         0           -           0
2021-10-28T21:29:22.951953+0000    13      34        34         0         0         0           -           0
2021-10-28T21:29:23.952031+0000    14      35        35         0         0         0           -           0
2021-10-28T21:29:24.952112+0000    15      35        35         0         0         0           -           0
2021-10-28T21:29:25.952192+0000    16      35        35         0         0         0           -           0
2021-10-28T21:29:26.952267+0000    17      35        35         0         0         0           -           0
2021-10-28T21:29:27.952343+0000    18      36        36         0         0         0           -           0
2021-10-28T21:29:28.952424+0000    19      36        36         0         0         0           -           0
2021-10-28T21:29:29.952505+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-28T21:29:29.952505+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T21:29:29.952505+0000    20      38        38         0         0         0           -           0
2021-10-28T21:29:30.952589+0000    21      41        41         0         0         0           -           0
2021-10-28T21:29:31.952665+0000    22      45        45         0         0         0           -           0
2021-10-28T21:29:32.952741+0000    23      48        48         0         0         0           -           0
2021-10-28T21:29:33.952819+0000    24      51        51         0         0         0           -           0
2021-10-28T21:29:34.952899+0000    25      54        54         0         0         0           -           0
2021-10-28T21:29:35.952974+0000    26      58        58         0         0         0           -           0
2021-10-28T21:29:36.953049+0000    27      61        61         0         0         0           -           0
2021-10-28T21:29:37.953125+0000    28      64        64         0         0         0           -           0
2021-10-28T21:29:38.953200+0000    29      66        66         0         0         0           -           0
2021-10-28T21:29:39.953275+0000    30      66        66         0         0         0           -           0
2021-10-28T21:29:40.953349+0000    31      67        67         0         0         0           -           0
2021-10-28T21:29:41.953426+0000    32      67        67         0         0         0           -           0
2021-10-28T21:29:42.953503+0000    33      67        67         0         0         0           -           0
2021-10-28T21:29:43.953578+0000    34      67        67         0         0         0           -           0
2021-10-28T21:29:44.953654+0000    35      67        67         0         0         0           -           0
2021-10-28T21:29:45.953729+0000    36      67        67         0         0         0           -           0
2021-10-28T21:29:46.953810+0000    37      68        68         0         0         0           -           0
2021-10-28T21:29:47.953888+0000    38      68        68         0         0         0           -           0
2021-10-28T21:29:48.953965+0000    39      68        68         0         0         0           -           0
2021-10-28T21:29:49.954038+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-28T21:29:49.954038+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T21:29:49.954038+0000    40      68        68         0         0         0           -           0
2021-10-28T21:29:50.954120+0000    41      68        68         0         0         0           -           0
2021-10-28T21:29:51.954194+0000    42      68        68         0         0         0           -           0
2021-10-28T21:29:52.954267+0000    43      69        69         0         0         0           -           0
2021-10-28T21:29:53.954340+0000    44      69        69         0         0         0           -           0
2021-10-28T21:29:54.954416+0000    45      70        70         0         0         0           -           0
2021-10-28T21:29:55.954488+0000    46      70        70         0         0         0           -           0
2021-10-28T21:29:56.954564+0000    47      71        71         0         0         0           -           0
2021-10-28T21:29:57.954641+0000    48      71        71         0         0         0           -           0
2021-10-28T21:29:58.954717+0000    49      71        71         0         0         0           -           0
2021-10-28T21:29:59.954796+0000    50      72        72         0         0         0           -           0
2021-10-28T21:30:00.954874+0000    51      72        72         0         0         0           -           0
2021-10-28T21:30:01.954949+0000    52      73        73         0         0         0           -           0
2021-10-28T21:30:02.955024+0000    53      73        73         0         0         0           -           0
2021-10-28T21:30:03.955103+0000    54      74        74         0         0         0           -           0
2021-10-28T21:30:04.955179+0000    55      77        77         0         0         0           -           0
2021-10-28T21:30:05.955255+0000    56      80        80         0         0         0           -           0
2021-10-28T21:30:06.955332+0000    57      83        83         0         0         0           -           0
2021-10-28T21:30:07.955404+0000    58      87        87         0         0         0           -           0
2021-10-28T21:30:08.955477+0000    59      90        90         0         0         0           -           0
2021-10-28T21:30:09.955552+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-28T21:30:09.955552+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T21:30:09.955552+0000    60      93        93         0         0         0           -           0
2021-10-28T21:30:10.955643+0000    61      97        97         0         0         0           -           0
2021-10-28T21:30:11.955720+0000    62      98        98         0         0         0           -           0
2021-10-28T21:30:12.955799+0000    63      98        98         0         0         0           -           0
2021-10-28T21:30:13.955878+0000    64      98        98         0         0         0           -           0
2021-10-28T21:30:14.955951+0000    65      98        98         0         0         0           -           0
2021-10-28T21:30:15.956023+0000    66      99        99         0         0         0           -           0
2021-10-28T21:30:16.956094+0000    67      99        99         0         0         0           -           0
2021-10-28T21:30:17.956168+0000    68      99        99         0         0         0           -           0
2021-10-28T21:30:18.956240+0000    69      99        99         0         0         0           -           0
2021-10-28T21:30:19.956314+0000    70      99        99         0         0         0           -           0
2021-10-28T21:30:20.956386+0000    71      99        99         0         0         0           -           0
2021-10-28T21:30:21.956460+0000    72     100       100         0         0         0           -           0
2021-10-28T21:30:22.956539+0000    73     100       100         0         0         0           -           0
2021-10-28T21:30:23.956614+0000    74     102       102         0         0         0           -           0
2021-10-28T21:30:24.956686+0000    75     105       105         0         0         0           -           0
2021-10-28T21:30:25.956758+0000    76     108       108         0         0         0           -           0
2021-10-28T21:30:26.956832+0000    77     112       112         0         0         0           -           0
2021-10-28T21:30:27.956904+0000    78     115       115         0         0         0           -           0
2021-10-28T21:30:28.956978+0000    79     118       118         0         0         0           -           0
2021-10-28T21:30:29.957064+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-28T21:30:29.957064+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T21:30:29.957064+0000    80     121       121         0         0         0           -           0
2021-10-28T21:30:30.957148+0000    81     124       124         0         0         0           -           0
2021-10-28T21:30:31.957219+0000    82     125       125         0         0         0           -           0
2021-10-28T21:30:32.957293+0000    83     125       125         0         0         0           -           0
2021-10-28T21:30:33.957367+0000    84     125       125         0         0         0           -           0
2021-10-28T21:30:34.957439+0000    85     126       126         0         0         0           -           0
2021-10-28T21:30:35.957515+0000    86     126       126         0         0         0           -           0
2021-10-28T21:30:36.957592+0000    87     127       127         0         0         0           -           0
2021-10-28T21:30:37.957664+0000    88       2       128       126   22.9073   22.9091     6.76507     50.3003
2021-10-28T21:30:38.957765+0000 Total time run:         88.4663
Total writes made:      128
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     23.1501
Stddev Bandwidth:       2.44212
Max bandwidth (MB/sec): 22.9091
Min bandwidth (MB/sec): 0
Average IOPS:           1
Stddev IOPS:            0.1066
Max IOPS:               1
Min IOPS:               0
Average Latency(s):     49.5577
Stddev Latency(s):      28.2182
Max latency(s):         87.8582
Min latency(s):         1.61896

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:30:39,697618267-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:30:39,704985622-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 769370

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:30:39,711484941-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 377073
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:30:39,719469279-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 377073
[1] 14:30:39 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:30:39,905493554-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16MB_write.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16MB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:30:40,053450926-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:31:03,986977286-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.6 GiB used, 393 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:31:03,995633941-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:31:12,943207308-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:31:12,951772120-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:31:21,891138861-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:31:21,899658797-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:31:30,998001889-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:31:31,006823114-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:31:40,047199197-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:31:40,055552791-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:31:40,062074161-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:31:40,066340021-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:31:40,074177723-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:31:40,080423894-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=770726
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:31:40,088025832-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:31:40,097385652-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'377183\n'
[1] 14:31:40 [SUCCESS] ljishen@10.10.2.2
377183

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:31:40,287537157-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16MB_seq.log.3
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:31:40,308045899-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:31:40,310894758-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5144aa46-3835-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5144aa46-3835-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-28T21:31:43.335109+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-28T21:31:43.335109+0000     0       0         0         0         0         0           -           0
2021-10-28T21:31:44.335217+0000     1      11        11         0         0         0           -           0
2021-10-28T21:31:45.335288+0000     2      21        21         0         0         0           -           0
2021-10-28T21:31:46.335361+0000     3      32        32         0         0         0           -           0
2021-10-28T21:31:47.335433+0000     4      41        41         0         0         0           -           0
2021-10-28T21:31:48.335505+0000     5      51        51         0         0         0           -           0
2021-10-28T21:31:49.335574+0000     6      62        62         0         0         0           -           0
2021-10-28T21:31:50.335644+0000     7      73        73         0         0         0           -           0
2021-10-28T21:31:51.335719+0000     8      82        82         0         0         0           -           0
2021-10-28T21:31:52.335793+0000     9      94        94         0         0         0           -           0
2021-10-28T21:31:53.335861+0000    10     104       104         0         0         0           -           0
2021-10-28T21:31:54.335934+0000    11     114       114         0         0         0           -           0
2021-10-28T21:31:55.336022+0000    12     122       122         0         0         0           -           0
2021-10-28T21:31:56.336096+0000    13       4       128       124   152.602   152.615     1.04257     6.70864
2021-10-28T21:31:57.336201+0000 Total time run:       13.3946
Total reads made:     128
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   152.898
Average IOPS:         9
Stddev IOPS:          2.49615
Max IOPS:             9
Min IOPS:             0
Average Latency(s):   6.53712
Max latency(s):       12.4632
Min latency(s):       0.573579

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:31:57,999445453-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:31:58,005988876-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 770726

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:31:58,012671360-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 377183
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:31:58,021172933-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 377183
[1] 14:31:58 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:31:58,205946491-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16MB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16MB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:31:58,357764119-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:32:22,436013791-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:32:22,444747541-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:32:31,534233857-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:32:31,542834987-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:32:40,557894794-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:32:40,566164670-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:32:49,590647878-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:32:49,599573651-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:32:58,564419943-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:32:58,572933589-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T14:32:58,579393413-07:00] INFO: > The cluster is idle now.[0m
