[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:20:36,117305078-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 mkdir --parents /tmp/bench-rados
[1] 18:20:36 [SUCCESS] ljishen@10.10.2.1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:20:36,303295292-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 mkdir --parents /tmp/bench-rados
[1] 18:20:36 [SUCCESS] ljishen@10.10.2.2
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:20:36,494682674-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
[1] 18:20:36 [SUCCESS] ljishen@10.10.2.2


[1;7;39;49m[2021-10-28T18:20:36,678200059-07:00][RUNNING][ROUND 1/1/21] object_size=4KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:20:36,680537382-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:20:36,685174379-07:00] INFO: > Get OSD hostname[0m
## ./benchmarks/bench-rados:178 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 uname --nodename
## ./benchmarks/bench-rados:178 - launch_ceph_cluster() > tail -n1
# ./benchmarks/bench-rados:178 - launch_ceph_cluster() > OSD_HOSTNAME=node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:20:36,877916424-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:20:37,877461798-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:20:37,882824157-07:00] INFO: > Deploy a new cluster\x1b[0m\n# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:20:37,894370101-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:20:37,898708676-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:20:38,043303178-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:20:38,047680306-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:20:38,194387468-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:20:38,470971817-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:20:38,475998727-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:20:38,752527251-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:20:38,762529733-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:20:38,766408144-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 6d10e6d8-3856-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\n'
10.10.2.1: b'Waiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\nWaiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\nAdding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 6d10e6d8-3856-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:21:40,541702142-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:22:00,548515817-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:22:00,558325337-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:22:00,561973074-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 6d10e6d8-3856-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/6d10e6d8-3856-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:22:10,203914501-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:22:10,212976025-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:22:10,216759416-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 6d10e6d8-3856-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/6d10e6d8-3856-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:22:18,954278557-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:22:18,960495814-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:22:19,204048933-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:22:19,207788021-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid 6d10e6d8-3856-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/6d10e6d8-3856-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:22:28,988985604-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:22:48,993781621-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:22:49,000671983-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:22:49,010511018-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:22:49,014527688-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 6d10e6d8-3856-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/6d10e6d8-3856-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:23:15,842236221-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:23:35,847964264-07:00] STAGE: Show Cluster Status\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:23:35,857673645-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:23:35,861127798-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 6d10e6d8-3856-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/6d10e6d8-3856-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     6d10e6d8-3856-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.ccnocr(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 42s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 18:23:44 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:20:37,877461798-07:00] INFO: > Remove existing clusters[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:20:37,882824157-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:20:37,894370101-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:20:37,898708676-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:20:38,043303178-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:20:38,047680306-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:20:38,194387468-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:20:38,470971817-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:20:38,475998727-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:20:38,752527251-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:20:38,762529733-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:20:38,766408144-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 6d10e6d8-3856-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 6d10e6d8-3856-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:21:40,541702142-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:22:00,548515817-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:22:00,558325337-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:22:00,561973074-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 6d10e6d8-3856-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/6d10e6d8-3856-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:22:10,203914501-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:22:10,212976025-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:22:10,216759416-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 6d10e6d8-3856-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/6d10e6d8-3856-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:22:18,954278557-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:22:18,960495814-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:22:19,204048933-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:22:19,207788021-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 6d10e6d8-3856-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/6d10e6d8-3856-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:22:28,988985604-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:22:48,993781621-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:22:49,000671983-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:22:49,010511018-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:22:49,014527688-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 6d10e6d8-3856-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/6d10e6d8-3856-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:23:15,842236221-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:23:35,847964264-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:23:35,857673645-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:23:35,861127798-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 6d10e6d8-3856-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/6d10e6d8-3856-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     6d10e6d8-3856-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.ccnocr(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 42s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:23:44,648646488-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:23:44,655986638-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 18:23:44 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:23:45,132360510-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:23:45,134898282-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:23:45,158054302-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:23:45,160858225-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6d10e6d8-3856-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6d10e6d8-3856-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:23:49,391704145-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:23:49,394636400-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6d10e6d8-3856-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6d10e6d8-3856-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:23:53,473634175-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:23:53,476906771-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6d10e6d8-3856-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6d10e6d8-3856-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:23:57,650115905-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:23:57,653152998-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6d10e6d8-3856-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6d10e6d8-3856-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:24:05,610368569-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:24:05,613510559-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6d10e6d8-3856-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6d10e6d8-3856-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:24:10,201936608-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:24:10,205170050-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6d10e6d8-3856-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6d10e6d8-3856-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:24:14,495970517-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:24:14,498872144-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6d10e6d8-3856-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6d10e6d8-3856-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:24:19,136436739-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:24:19,139323037-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6d10e6d8-3856-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6d10e6d8-3856-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:24:23,985027871-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:24:23,988038804-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6d10e6d8-3856-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6d10e6d8-3856-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:24:28,328867383-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:24:28,331678420-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6d10e6d8-3856-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6d10e6d8-3856-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:24:32,978466905-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:24:32,981803752-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6d10e6d8-3856-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6d10e6d8-3856-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 17 lfor 0/0/15 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:24:37,035883472-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:24:37,038830825-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6d10e6d8-3856-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6d10e6d8-3856-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default   
-3         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host node-1
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0  
                       TOTAL  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:24:40,995254958-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:25:05,348829822-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:25:14,271193880-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:25:23,320292387-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:25:23,326250222-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:25:32,369372431-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:25:32,375213205-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:25:41,365998084-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:25:41,371938707-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:25:50,466432583-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:25:50,472734346-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:25:59,388813872-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:25:59,394995590-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:25:59,399886705-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:25:59,402603014-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:25:59,407384622-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:25:59,411499395-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=986919
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:25:59,416882047-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:25:59,425113948-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'13149\n'
[1] 18:25:59 [SUCCESS] ljishen@10.10.2.2
13149

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:25:59,612540589-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4KB_write.log.1
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:25:59,632275533-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:25:59,635082422-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6d10e6d8-3856-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6d10e6d8-3856-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T01:26:02.720553+0000 Maintaining 128 concurrent writes of 4096 bytes to objects of size 4096 for up to 60 seconds or 0 objects
2021-10-29T01:26:02.720565+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-29T01:26:02.721094+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T01:26:02.721094+0000     0       0         0         0         0         0           -           0
2021-10-29T01:26:03.721209+0000     1     128      1415      1287   5.02704   5.02734   0.0666905   0.0983011
2021-10-29T01:26:04.721327+0000     2     128      3355      3227   6.30217   7.57812   0.0637543   0.0779437
2021-10-29T01:26:05.721419+0000     3     128      5275      5147   6.70122       7.5   0.0667855   0.0733422
2021-10-29T01:26:06.721531+0000     4     128      7195      7067   6.90071       7.5   0.0501341   0.0718115
2021-10-29T01:26:07.721647+0000     5     128      9095      8967   7.00477   7.42188   0.0587625   0.0709843
2021-10-29T01:26:08.721760+0000     6     128     10375     10247   6.67054         5   0.0664561   0.0746132
2021-10-29T01:26:09.721850+0000     7     128     12039     11911    6.6461       6.5   0.0752694   0.0751041
2021-10-29T01:26:10.721962+0000     8     128     13831     13703   6.69024         7   0.0669036   0.0746327
2021-10-29T01:26:11.722071+0000     9     128     15515     15387    6.6777   6.57812    0.189395   0.0740404
2021-10-29T01:26:12.722181+0000    10     128     17287     17159   6.70204   6.92188     0.10038   0.0745202
2021-10-29T01:26:13.722270+0000    11     128     18843     18715   6.64527   6.07812   0.0751358   0.0748968
2021-10-29T01:26:14.722382+0000    12     128     20763     20635   6.71643       7.5   0.0667642   0.0741764
2021-10-29T01:26:15.722493+0000    13     128     22555     22427   6.73819         7    0.174872   0.0740028
2021-10-29T01:26:16.722608+0000    14     128     24583     24455   6.82267   7.92188   0.0590298   0.0732285
2021-10-29T01:26:17.722697+0000    15     128     26395     26267   6.83966   7.07812   0.0662474   0.0728153
2021-10-29T01:26:18.722806+0000    16     128     28167     28039   6.84475   6.92188   0.0589155   0.0728919
2021-10-29T01:26:19.722928+0000    17     128     29723     29595   6.79961   6.07812   0.0670846    0.073285
2021-10-29T01:26:20.723046+0000    18     128     31515     31387    6.8107         7   0.0669521   0.0732814
2021-10-29T01:26:21.723133+0000    19     128     33307     33179   6.82063         7    0.133706   0.0731196
2021-10-29T01:26:22.723243+0000 min lat: 0.0328896 max lat: 0.399775 avg lat: 0.0729661
2021-10-29T01:26:22.723243+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T01:26:22.723243+0000    20     127     35173     35046   6.84421   7.29297   0.0675286   0.0729661
2021-10-29T01:26:23.723368+0000    21     128     37019     36891   6.86144   7.20703   0.0667023   0.0726822
2021-10-29T01:26:24.723480+0000    22     127     38590     38463   6.82864   6.14062   0.0670866   0.0730718
2021-10-29T01:26:25.723571+0000    23     128     40491     40363   6.85441   7.42188    0.108953   0.0727008
2021-10-29T01:26:26.723684+0000    24     128     42283     42155   6.86044         7    0.104898    0.072682
2021-10-29T01:26:27.723797+0000    25     128     43931     43803   6.84349    6.4375   0.0667176   0.0729825
2021-10-29T01:26:28.723911+0000    26     128     45483     45355   6.81343    6.0625   0.0750427   0.0732161
2021-10-29T01:26:29.723999+0000    27     128     47387     47259   6.83652    7.4375   0.0658251   0.0730665
2021-10-29T01:26:30.724107+0000    28     128     48939     48811   6.80885    6.0625   0.0667794   0.0733441
2021-10-29T01:26:31.724216+0000    29     128     50603     50475   6.79818       6.5    0.125736    0.073402
2021-10-29T01:26:32.724338+0000    30     128     52635     52507   6.83612    7.9375   0.0584811   0.0731027
2021-10-29T01:26:33.724422+0000    31     128     54443     54315   6.84341    7.0625   0.0670149   0.0729291
2021-10-29T01:26:34.724533+0000    32     128     56235     56107   6.84827         7   0.0667193   0.0728996
2021-10-29T01:26:35.724642+0000    33     128     57771     57643   6.82255         6    0.266992   0.0732026
2021-10-29T01:26:36.724753+0000    34     128     59726     59598   6.84647   7.63672   0.0504222   0.0729367
2021-10-29T01:26:37.724842+0000    35     128     61774     61646   6.87941         8   0.0581041   0.0725924
2021-10-29T01:26:38.724931+0000    36     128     63566     63438   6.88274         7   0.0662983   0.0725459
2021-10-29T01:26:39.725042+0000    37     128     65486     65358    6.8994       7.5   0.0741553   0.0724246
2021-10-29T01:26:40.725153+0000    38     128     67371     67243   6.91159   7.36328   0.0671235   0.0723008
2021-10-29T01:26:41.725240+0000    39     128     68942     68814    6.8917   6.13672    0.144387   0.0724215
2021-10-29T01:26:42.725351+0000 min lat: 0.0328896 max lat: 0.399775 avg lat: 0.0724519
2021-10-29T01:26:42.725351+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T01:26:42.725351+0000    40     128     70734     70606   6.89439         7   0.0586206   0.0724519
2021-10-29T01:26:43.725466+0000    41     128     72782     72654   6.92134         8   0.0584427   0.0721592
2021-10-29T01:26:44.725579+0000    42     128     74795     74667   6.94374   7.86328   0.0673365   0.0719561
2021-10-29T01:26:45.725666+0000    43     128     76715     76587   6.95666       7.5   0.0668024   0.0718402
2021-10-29T01:26:46.725775+0000    44     128     78251     78123   6.93491         6   0.0583368   0.0720715
2021-10-29T01:26:47.725887+0000    45     128     79950     79822   6.92826   6.63672   0.0585896   0.0721156
2021-10-29T01:26:48.726000+0000    46     128     81963     81835   6.94857   7.86328   0.0660504    0.071935
2021-10-29T01:26:49.726089+0000    47     128     83755     83627   6.94965         7    0.116798   0.0719012
2021-10-29T01:26:50.726198+0000    48     128     85838     85710   6.97436   8.13672   0.0670473   0.0716404
2021-10-29T01:26:51.726308+0000    49     128     87467     87339   6.96188   6.36328    0.185548   0.0716371
2021-10-29T01:26:52.726416+0000    50     128     89038     88910   6.94536   6.13672   0.0666953     0.07194
2021-10-29T01:26:53.726518+0000    51     128     90795     90667   6.94374   6.86328    0.161452   0.0719355
2021-10-29T01:26:54.726640+0000    52     128     92843     92715   6.96403         8   0.0746972   0.0717602
2021-10-29T01:26:55.726747+0000    53     128     94763     94635   6.97413       7.5   0.0676983   0.0716744
2021-10-29T01:26:56.726856+0000    54     128     96718     96590   6.98639   7.63672   0.0671298   0.0715319
2021-10-29T01:26:57.726941+0000    55     128     98091     97963   6.95687   5.36328    0.242964   0.0715755
2021-10-29T01:26:58.727050+0000    56     128     99790     99662   6.95114   6.63672   0.0590448   0.0718662
2021-10-29T01:26:59.727158+0000    57     128    101675    101547   6.95835   7.36328   0.0896769   0.0718277
2021-10-29T01:27:00.727268+0000    58     128    103723    103595    6.9763         8   0.0505057   0.0716383
2021-10-29T01:27:01.727356+0000    59     128    105515    105387   6.97669         7   0.0754543   0.0716318
2021-10-29T01:27:02.727479+0000 min lat: 0.0328896 max lat: 0.399775 avg lat: 0.0719238
2021-10-29T01:27:02.727479+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T01:27:02.727479+0000    60     128    106574    106446   6.92935   4.13672    0.275124   0.0719238
2021-10-29T01:27:03.727637+0000 Total time run:         60.0691
Total writes made:      106574
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     6.93043
Stddev Bandwidth:       0.823005
Max bandwidth (MB/sec): 8.13672
Min bandwidth (MB/sec): 4.13672
Average IOPS:           1774
Stddev IOPS:            210.689
Max IOPS:               2083
Min IOPS:               1059
Average Latency(s):     0.0720891
Stddev Latency(s):      0.0316154
Max latency(s):         0.399775
Min latency(s):         0.0328896

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:27:04,569134463-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:27:04,573326932-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 986919

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:27:04,577816952-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 13149
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:27:04,585347361-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 13149
[1] 18:27:04 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:27:04,764081414-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4KB_write.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4KB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:27:04,951789917-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:27:29,038932529-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 106.58k objects, 416 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:27:29,044979753-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:27:38,138771791-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 106.58k objects, 416 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:27:38,145066961-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:27:47,278003986-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 106.58k objects, 416 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:27:47,284453307-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:27:56,317099651-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 106.58k objects, 416 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:27:56,323332154-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:28:05,407445190-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 106.58k objects, 416 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:28:05,413350026-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:28:05,417535131-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:28:05,420666691-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:28:05,425616919-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:28:05,429667621-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=988845
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:28:05,435902278-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:28:05,444361015-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'13981\n'
[1] 18:28:05 [SUCCESS] ljishen@10.10.2.2
13981

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:28:05,629447328-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4KB_seq.log.1
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:28:05,648524122-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:28:05,651379391-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6d10e6d8-3856-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6d10e6d8-3856-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T01:28:08.678554+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T01:28:08.678554+0000     0       0         0         0         0         0           -           0
2021-10-29T01:28:09.678678+0000     1     128      3982      3854   15.0517   15.0547  0.00477618   0.0329629
2021-10-29T01:28:10.678791+0000     2     128      9737      9609   18.7646   22.4805  0.00694675   0.0264164
2021-10-29T01:28:11.678903+0000     3     128     14578     14450   18.8124   18.9102    0.133719   0.0261723
2021-10-29T01:28:12.679015+0000     4     128     19498     19370   18.9135   19.2188  0.00121057   0.0262927
2021-10-29T01:28:13.679102+0000     5     128     23526     23398   18.2774   15.7344   0.0147948   0.0272611
2021-10-29T01:28:14.679211+0000     6     128     28009     27881   18.1495   17.5117   0.0534808   0.0274732
2021-10-29T01:28:15.679284+0000     7     128     33174     33046   18.4387   20.1758     0.00393   0.0270536
2021-10-29T01:28:16.679362+0000     8     128     38475     38347   18.7221    20.707 0.000426403   0.0265537
2021-10-29T01:28:17.679437+0000     9     128     43098     42970   18.6482   18.0586   0.0177468   0.0267482
2021-10-29T01:28:18.679505+0000    10     128     48039     47911   18.7133   19.3008   0.0313406   0.0266261
2021-10-29T01:28:19.679575+0000    11     128     52745     52617   18.6832   18.3828   0.0407791   0.0266492
2021-10-29T01:28:20.679661+0000    12     128     56059     55931   18.2049   12.9453   0.0116182   0.0274247
2021-10-29T01:28:21.679740+0000    13     128     60857     60729   18.2461   18.7422   0.0308272   0.0273222
2021-10-29T01:28:22.679816+0000    14     128     66566     66438   18.5356   22.3008  0.00100909   0.0269211
2021-10-29T01:28:23.679891+0000    15     128     71390     71262   18.5561   18.8438   0.0070208    0.026922
2021-10-29T01:28:24.679965+0000    16     128     76813     76685   18.7202   21.1836   0.0809951   0.0266342
2021-10-29T01:28:25.680049+0000    17     128     81617     81489   18.7228   18.7656    0.048756   0.0263767
2021-10-29T01:28:26.680130+0000    18     128     85978     85850   18.6289   17.0352   0.0320499    0.026802
2021-10-29T01:28:27.680207+0000    19     128     91412     91284   18.7656   21.2266   0.0123529    0.026623
2021-10-29T01:28:28.680323+0000 min lat: 0.000204896 max lat: 0.464231 avg lat: 0.0267255
2021-10-29T01:28:28.680323+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T01:28:28.680323+0000    20     128     95764     95636   18.6772        17   0.0227603   0.0267255
2021-10-29T01:28:29.680418+0000    21     128    100177    100049   18.6086   17.2383   0.0356097   0.0267592
2021-10-29T01:28:30.680530+0000    22     128    105475    105347   18.7033   20.6953   0.0290598   0.0267173
2021-10-29T01:28:31.680667+0000 Total time run:       22.2063
Total reads made:     106574
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   18.7471
Average IOPS:         4799
Stddev IOPS:          600.683
Max IOPS:             5755
Min IOPS:             3314
Average Latency(s):   0.0266595
Max latency(s):       0.464231
Min latency(s):       0.000204896

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:28:32,349443530-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:28:32,354224337-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 988845

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:28:32,359290142-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 13981
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:28:32,366707097-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 13981
[1] 18:28:32 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:28:32,548443217-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4KB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4KB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:28:32,704870251-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:28:56,687021424-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 106.58k objects, 416 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:28:56,693910545-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:29:05,943421852-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 106.58k objects, 416 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:29:05,950793832-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:29:15,115913734-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 106.58k objects, 416 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:29:15,123061071-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:29:24,089488496-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 106.58k objects, 416 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:29:24,096066780-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:29:33,154740845-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 106.58k objects, 416 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:29:33,161762355-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:29:33,166658520-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T18:29:33,168812138-07:00][RUNNING][ROUND 2/1/21] object_size=4KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:29:33,171952434-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:29:33,180934989-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:29:33,913255155-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/6d10e6d8-3856-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 6d10e6d8-3856-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:29:33,924825014-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:29:33,928279127-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '6d10e6d8-3856-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 6d10e6d8-3856-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:29:33,936860357-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 6d10e6d8-3856-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 18:29:39 [SUCCESS] 10.10.2.1\n[2] 18:29:39 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:29:39,729523292-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:29:39,741058045-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:29:39,746061720-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:29:39,895185415-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:29:39,899853309-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:29:40,047343764-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:29:40,326926148-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:29:40,331951715-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--bb1fbe99--d831--4f75--a851--838990afffc2-osd--block--1ba5ba1d--b8dd--4358--805e--a6f008ace5a0 (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-bb1fbe99-d831-4f75-a851-838990afffc2" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-1ba5ba1d-b8dd-4358-805e-a6f008ace5a0"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-bb1fbe99-d831-4f75-a851-838990afffc2" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-1ba5ba1d-b8dd-4358-805e-a6f008ace5a0" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-bb1fbe99-d831-4f75-a851-838990afffc2"\n'
10.10.2.1: b'  Volume group "ceph-bb1fbe99-d831-4f75-a851-838990afffc2" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:29:40,681153245-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:29:40,690656057-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:29:40,694278116-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\npodman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: b0145d2e-3857-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\nWaiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid b0145d2e-3857-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:30:44,942478996-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:31:04,949878063-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:31:04,960236865-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:31:04,963921271-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid b0145d2e-3857-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/b0145d2e-3857-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:31:14,368908663-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:31:14,378381018-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:31:14,382368944-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid b0145d2e-3857-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/b0145d2e-3857-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:31:24,273977450-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:31:24,280069281-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:31:24,489773531-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:31:24,493493574-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid b0145d2e-3857-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/b0145d2e-3857-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:31:33,943250920-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:31:53,948010621-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:31:53,954663196-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:31:53,964985991-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:31:53,969012099-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid b0145d2e-3857-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/b0145d2e-3857-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:32:19,178083205-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:32:39,183546022-07:00] STAGE: Show Cluster Status\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:32:39,193863586-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:32:39,197603586-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid b0145d2e-3857-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/b0145d2e-3857-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     b0145d2e-3857-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.qgssyn(active, since 2m)\n    osd: 1 osds: 1 up (since 17s), 1 in (since 41s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 18:32:48 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:29:33,913255155-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/6d10e6d8-3856-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 6d10e6d8-3856-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:29:33,924825014-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:29:33,928279127-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '6d10e6d8-3856-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 6d10e6d8-3856-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:29:33,936860357-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 6d10e6d8-3856-11ec-b51d-53e6e728d2d3'
[1] 18:29:39 [SUCCESS] 10.10.2.1
[2] 18:29:39 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:29:39,729523292-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:29:39,741058045-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:29:39,746061720-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:29:39,895185415-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:29:39,899853309-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:29:40,047343764-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:29:40,326926148-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:29:40,331951715-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--bb1fbe99--d831--4f75--a851--838990afffc2-osd--block--1ba5ba1d--b8dd--4358--805e--a6f008ace5a0 (253:0)
  Archiving volume group "ceph-bb1fbe99-d831-4f75-a851-838990afffc2" metadata (seqno 5).
  Releasing logical volume "osd-block-1ba5ba1d-b8dd-4358-805e-a6f008ace5a0"
  Creating volume group backup "/etc/lvm/backup/ceph-bb1fbe99-d831-4f75-a851-838990afffc2" (seqno 6).
  Logical volume "osd-block-1ba5ba1d-b8dd-4358-805e-a6f008ace5a0" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-bb1fbe99-d831-4f75-a851-838990afffc2"
  Volume group "ceph-bb1fbe99-d831-4f75-a851-838990afffc2" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:29:40,681153245-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:29:40,690656057-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:29:40,694278116-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: b0145d2e-3857-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid b0145d2e-3857-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:30:44,942478996-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:31:04,949878063-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:31:04,960236865-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:31:04,963921271-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid b0145d2e-3857-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/b0145d2e-3857-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:31:14,368908663-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:31:14,378381018-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:31:14,382368944-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid b0145d2e-3857-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/b0145d2e-3857-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:31:24,273977450-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:31:24,280069281-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:31:24,489773531-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:31:24,493493574-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid b0145d2e-3857-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/b0145d2e-3857-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:31:33,943250920-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:31:53,948010621-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:31:53,954663196-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:31:53,964985991-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:31:53,969012099-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid b0145d2e-3857-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/b0145d2e-3857-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:32:19,178083205-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:32:39,183546022-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:32:39,193863586-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:32:39,197603586-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid b0145d2e-3857-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/b0145d2e-3857-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     b0145d2e-3857-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.qgssyn(active, since 2m)
    osd: 1 osds: 1 up (since 17s), 1 in (since 41s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:32:48,315976940-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:32:48,324855519-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 18:32:48 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:32:48,804839880-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:32:48,807818592-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:32:48,829397409-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:32:48,832271935-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b0145d2e-3857-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b0145d2e-3857-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:32:53,048922061-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:32:53,052082185-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b0145d2e-3857-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b0145d2e-3857-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:32:57,083102978-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:32:57,085960763-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b0145d2e-3857-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b0145d2e-3857-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:33:01,190157040-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:33:01,193289532-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b0145d2e-3857-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b0145d2e-3857-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:33:09,257712307-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:33:09,260781760-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b0145d2e-3857-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b0145d2e-3857-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:33:13,932979506-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:33:13,936414979-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b0145d2e-3857-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b0145d2e-3857-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:33:18,390390453-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:33:18,393578921-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b0145d2e-3857-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b0145d2e-3857-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:33:22,940657434-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:33:22,943615157-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b0145d2e-3857-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b0145d2e-3857-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:33:27,263868529-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:33:27,266858102-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b0145d2e-3857-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b0145d2e-3857-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:33:32,072232494-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:33:32,075331703-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b0145d2e-3857-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b0145d2e-3857-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:33:36,452503068-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:33:36,455425104-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b0145d2e-3857-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b0145d2e-3857-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:33:40,488374123-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:33:40,491217140-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b0145d2e-3857-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b0145d2e-3857-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default   
-3         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host node-1
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0  
                       TOTAL  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:33:44,579153304-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:34:08,542967896-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:34:17,615555182-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:34:26,671828932-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:34:26,678427355-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:34:35,880129836-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:34:35,886905171-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:34:44,951714287-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:34:44,958401927-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:34:53,942914152-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:34:53,949740114-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:35:03,181667858-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:35:03,188650404-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:35:03,193630016-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:35:03,196761286-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:35:03,202868073-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:35:03,207579590-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=997824
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:35:03,213643605-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:35:03,222132670-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'19985\n'
[1] 18:35:03 [SUCCESS] ljishen@10.10.2.2
19985

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:35:03,405136857-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4KB_write.log.2
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:35:03,424892239-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:35:03,427731990-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b0145d2e-3857-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b0145d2e-3857-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T01:35:06.491368+0000 Maintaining 128 concurrent writes of 4096 bytes to objects of size 4096 for up to 60 seconds or 0 objects
2021-10-29T01:35:06.491379+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-29T01:35:06.491903+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T01:35:06.491903+0000     0       0         0         0         0         0           -           0
2021-10-29T01:35:07.492039+0000     1     128      2055      1927   7.52665   7.52734    0.058644   0.0651856
2021-10-29T01:35:08.492152+0000     2     128      3734      3606   7.04225   6.55859   0.0750496   0.0688383
2021-10-29T01:35:09.492239+0000     3     128      5270      5142   6.69466         6   0.0496025   0.0743153
2021-10-29T01:35:10.492342+0000     4     128      7332      7204   7.03446   8.05469   0.0418365   0.0706909
2021-10-29T01:35:11.492453+0000     5     128      9135      9007   7.03601   7.04297   0.0901207   0.0694039
2021-10-29T01:35:12.492564+0000     6     128     11003     10875   7.07935   7.29688   0.0669698   0.0701032
2021-10-29T01:35:13.492645+0000     7     128     12795     12667   7.06793         7   0.0667114   0.0705424
2021-10-29T01:35:14.492750+0000     8     128     14203     14075   6.87187       5.5   0.0667934    0.072429
2021-10-29T01:35:15.492855+0000     9     128     16303     16175   7.01969   8.20312    0.144404   0.0708943
2021-10-29T01:35:16.492956+0000    10     128     18299     18171   7.09733   7.79688   0.0511643   0.0701618
2021-10-29T01:35:17.493040+0000    11     128     20219     20091   7.13388       7.5   0.0752229   0.0698372
2021-10-29T01:35:18.493145+0000    12     128     22139     22011   7.16432       7.5   0.0585792   0.0696304
2021-10-29T01:35:19.493244+0000    13     128     23727     23599   7.09034   6.20312    0.183199   0.0699302
2021-10-29T01:35:20.493342+0000    14     128     25339     25211   7.03362   6.29688   0.0587951   0.0708713
2021-10-29T01:35:21.493424+0000    15     128     27259     27131   7.06467       7.5    0.058505   0.0706946
2021-10-29T01:35:22.493520+0000    16     128     29179     29051   7.09183       7.5   0.0674232   0.0704323
2021-10-29T01:35:23.493616+0000    17     128     31099     30971    7.1158       7.5   0.0505044   0.0701819
2021-10-29T01:35:24.493715+0000    18     128     32943     32815   7.12061   7.20312    0.125145    0.070008
2021-10-29T01:35:25.493843+0000    19     128     34299     34171   7.02459   5.29688    0.125158   0.0709484
2021-10-29T01:35:26.493949+0000 min lat: 0.0327623 max lat: 0.358811 avg lat: 0.0711118
2021-10-29T01:35:26.493949+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T01:35:26.493949+0000    20     128     36091     35963   7.02332         7   0.0665759   0.0711118
2021-10-29T01:35:27.494056+0000    21     128     37755     37627   6.99837       6.5   0.0589083   0.0713716
2021-10-29T01:35:28.494156+0000    22     128     39471     39343   6.98492   6.70312   0.0584295    0.071402
2021-10-29T01:35:29.494239+0000    23     128     41339     41211   6.99845   7.29688    0.095615   0.0713233
2021-10-29T01:35:30.494346+0000    24     128     42747     42619   6.93599       5.5   0.0587427   0.0719926
2021-10-29T01:35:31.494447+0000    25     128     44539     44411   6.93852         7   0.0585223   0.0720106
2021-10-29T01:35:32.494550+0000    26     128     46459     46331   6.96009       7.5    0.050976   0.0718029
2021-10-29T01:35:33.494630+0000    27     128     48175     48047   6.95055   6.70312   0.0994504   0.0716184
2021-10-29T01:35:34.494734+0000    28     128     50095     49967   6.97015       7.5   0.0734655   0.0716246
2021-10-29T01:35:35.494844+0000    29     128     51247     51119   6.88495       4.5   0.0585061   0.0725213
2021-10-29T01:35:36.494946+0000    30     128     53167     53039   6.90543       7.5   0.0839955   0.0723161
2021-10-29T01:35:37.495029+0000    31     128     55163     55035   6.93416   7.79688   0.0669448   0.0720401
2021-10-29T01:35:38.495125+0000    32     128     57007     56879   6.94255   7.20312   0.0669859    0.071905
2021-10-29T01:35:39.495221+0000    33     128     58875     58747   6.95326   7.29688   0.0669441   0.0718331
2021-10-29T01:35:40.495316+0000    34     128     60795     60667   6.96932       7.5   0.0589743   0.0716728
2021-10-29T01:35:41.495394+0000    35     128     62383     62255   6.94742   6.20312   0.0581956   0.0718519
2021-10-29T01:35:42.495493+0000    36     128     64303     64175   6.96275       7.5   0.0500635   0.0717165
2021-10-29T01:35:43.495595+0000    37     128     66223     66095   6.97725       7.5   0.0667952   0.0715816
2021-10-29T01:35:44.495702+0000    38     128     68015     67887   6.97783         7    0.059051    0.071574
2021-10-29T01:35:45.495825+0000    39     128     69935     69807   6.99119       7.5   0.0736396    0.071449
2021-10-29T01:35:46.495931+0000 min lat: 0.0327623 max lat: 0.358811 avg lat: 0.0717711
2021-10-29T01:35:46.495931+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T01:35:46.495931+0000    40     128     71419     71291   6.96132   5.79688   0.0760681   0.0717711
2021-10-29T01:35:47.496047+0000    41     128     73339     73211   6.97444       7.5   0.0581217   0.0716405
2021-10-29T01:35:48.496145+0000    42     128     75311     75183   6.99177   7.70312   0.0590207   0.0714514
2021-10-29T01:35:49.496226+0000    43     128     77307     77179   7.01048   7.79688   0.0676346   0.0712686
2021-10-29T01:35:50.496327+0000    44     128     79099     78971   7.01022         7   0.0503588    0.071295
2021-10-29T01:35:51.496429+0000    45     128     80559     80431   6.98116   5.70312   0.0754079   0.0715662
2021-10-29T01:35:52.496527+0000    46     128     82299     82171   6.97714   6.79688    0.075395   0.0716089
2021-10-29T01:35:53.496620+0000    47     127     84126     83999   6.98061   7.14062   0.0672803   0.0715806
2021-10-29T01:35:54.496727+0000    48     128     85883     85755   6.97806   6.85938   0.0584446   0.0716068
2021-10-29T01:35:55.496828+0000    49     128     87547     87419   6.96829       6.5    0.116756   0.0717156
2021-10-29T01:35:56.496926+0000    50     128     89391     89263   6.97298   7.20312   0.0671492   0.0716466
2021-10-29T01:35:57.497002+0000    51     128     90927     90799   6.95389         6    0.126954   0.0716845
2021-10-29T01:35:58.497099+0000    52     128     92667     92539   6.95086   6.79688   0.0586271   0.0718713
2021-10-29T01:35:59.497198+0000    53     128     94587     94459   6.96121       7.5   0.0588416   0.0717798
2021-10-29T01:36:00.497300+0000    54     128     96251     96123   6.95265       6.5    0.075601   0.0718496
2021-10-29T01:36:01.497376+0000    55     128     98043     97915    6.9535         7    0.113115   0.0718311
2021-10-29T01:36:02.497474+0000    56     128     99963     99835   6.96325       7.5   0.0670832   0.0717595
2021-10-29T01:36:03.497569+0000    57     128    101371    101243   6.93757       5.5    0.080893   0.0718974
2021-10-29T01:36:04.497655+0000    58     128    103087    102959   6.93352   6.70312   0.0667525   0.0720495
2021-10-29T01:36:05.497741+0000    59     128    104879    104751   6.93463         7   0.0586734    0.072079
2021-10-29T01:36:06.497847+0000 min lat: 0.0327623 max lat: 0.358811 avg lat: 0.0721797
2021-10-29T01:36:06.497847+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T01:36:06.497847+0000    60     128    106415    106287   6.91905         6    0.166955   0.0721797
2021-10-29T01:36:07.498013+0000 Total time run:         60.1234
Total writes made:      106415
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     6.91384
Stddev Bandwidth:       0.760425
Max bandwidth (MB/sec): 8.20312
Min bandwidth (MB/sec): 4.5
Average IOPS:           1769
Stddev IOPS:            194.669
Max IOPS:               2100
Min IOPS:               1152
Average Latency(s):     0.0722746
Stddev Latency(s):      0.0302672
Max latency(s):         0.358811
Min latency(s):         0.0327623

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:36:08,245034648-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:36:08,250035801-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 997824

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:36:08,254946483-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 19985
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:36:08,262417400-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 19985
[1] 18:36:08 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:36:08,445243511-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4KB_write.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4KB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:36:08,613090951-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:36:32,606844716-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 106.42k objects, 416 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:36:32,613447406-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:36:41,772741283-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 106.42k objects, 416 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:36:41,779699023-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:36:50,754511010-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 106.42k objects, 416 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:36:50,761533602-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:36:59,894443961-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 106.42k objects, 416 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:36:59,901143845-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:37:09,001807940-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 106.42k objects, 416 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:37:09,008880225-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:37:09,014223923-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:37:09,017572943-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:37:09,023141946-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:37:09,027841050-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=999955
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:37:09,034030572-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:37:09,042568068-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'20717\n'
[1] 18:37:09 [SUCCESS] ljishen@10.10.2.2
20717

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:37:09,229087218-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4KB_seq.log.2
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:37:09,248447967-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:37:09,251371755-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b0145d2e-3857-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b0145d2e-3857-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T01:37:12.306493+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T01:37:12.306493+0000     0       0         0         0         0         0           -           0
2021-10-29T01:37:13.306594+0000     1     128      4730      4602   17.9735   17.9766 0.000392339   0.0271596
2021-10-29T01:37:14.306674+0000     2     128     10305     10177   19.8745   21.7773  0.00090745   0.0249105
2021-10-29T01:37:15.306748+0000     3     128     15000     14872   19.3625   18.3398   0.0784027   0.0255749
2021-10-29T01:37:16.306821+0000     4     128     19352     19224   18.7716        17  0.00344499   0.0265356
2021-10-29T01:37:17.306894+0000     5     128     24407     24279   18.9662   19.7461  0.00441534   0.0262314
2021-10-29T01:37:18.306994+0000     6     127     28739     28612   18.6258   16.9258   0.0110189   0.0267546
2021-10-29T01:37:19.307120+0000     7     128     34015     33887   18.9083   20.6055   0.0214917    0.026328
2021-10-29T01:37:20.307235+0000     8     128     40067     39939   19.4995   23.6406   0.0161533   0.0255578
2021-10-29T01:37:21.307308+0000     9     128     44505     44377    19.259   17.3359  0.00415101   0.0257844
2021-10-29T01:37:22.307406+0000    10     128     50193     50065   19.5547   22.2188   0.0819111   0.0254821
2021-10-29T01:37:23.307506+0000    11     127     55706     55579   19.7349   21.5391   0.0836203   0.0252905
2021-10-29T01:37:24.307621+0000    12     128     59003     58875   19.1631    12.875   0.0543847    0.026016
2021-10-29T01:37:25.307692+0000    13     127     64646     64519   19.3848   22.0469   0.0802554   0.0257343
2021-10-29T01:37:26.307809+0000    14     128     69630     69502   19.3904   19.4648   0.0461423   0.0257572
2021-10-29T01:37:27.307926+0000    15     128     75173     75045    19.541   21.6523 0.000395805   0.0255397
2021-10-29T01:37:28.308007+0000    16     128     80248     80120   19.5586   19.8242 0.000356081   0.0255188
2021-10-29T01:37:29.308078+0000    17     128     84242     84114   19.3258   15.6016   0.0278911   0.0258389
2021-10-29T01:37:30.308169+0000    18     128     90342     90214   19.5758   23.8281   0.0247199   0.0254518
2021-10-29T01:37:31.308293+0000    19     128     95564     95436    19.619   20.3984 0.000432776   0.0254496
2021-10-29T01:37:32.308386+0000 min lat: 0.00022799 max lat: 0.302347 avg lat: 0.0255927
2021-10-29T01:37:32.308386+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T01:37:32.308386+0000    20     128    100102     99974   19.5243   17.7266  0.00240127   0.0255927
2021-10-29T01:37:33.308471+0000    21     128    104605    104477   19.4321   17.5898   0.0370332   0.0257064
2021-10-29T01:37:34.308611+0000 Total time run:       21.4779
Total reads made:     106415
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   19.354
Average IOPS:         4954
Stddev IOPS:          707.935
Max IOPS:             6100
Min IOPS:             3296
Average Latency(s):   0.0258167
Max latency(s):       0.302347
Min latency(s):       0.00022799

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:37:34,941375597-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:37:34,946290047-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 999955

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:37:34,951294627-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 20717
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:37:34,958843289-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 20717
[1] 18:37:35 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:37:35,141085080-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4KB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4KB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:37:35,295341272-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:37:59,322006874-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 106.42k objects, 416 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:37:59,329219273-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:38:08,527875048-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 106.42k objects, 416 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:38:08,535015161-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:38:17,409692607-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 106.42k objects, 416 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:38:17,416268287-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:38:26,589556459-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 106.42k objects, 416 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:38:26,596740664-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:38:35,671947668-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 106.42k objects, 416 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:38:35,679125011-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:38:35,684454352-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T18:38:35,686494366-07:00][RUNNING][ROUND 3/1/21] object_size=4KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:38:35,689526990-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:38:35,698873941-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:38:36,092551239-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/b0145d2e-3857-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid b0145d2e-3857-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:38:36,103711688-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:38:36,107417825-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'b0145d2e-3857-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid b0145d2e-3857-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:38:36,115941518-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid b0145d2e-3857-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 18:38:41 [SUCCESS] 10.10.2.1\n[2] 18:38:42 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:38:42,063883687-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:38:42,076231779-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:38:42,080974604-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:38:42,231473993-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:38:42,236204135-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:38:42,382608356-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:38:42,663206343-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:38:42,668457434-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--ebec326a--4afb--4f39--97f8--00a0b274c192-osd--block--07c53712--32b3--4846--9969--c3cedb386a38 (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-ebec326a-4afb-4f39-97f8-00a0b274c192" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-07c53712-32b3-4846-9969-c3cedb386a38"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-ebec326a-4afb-4f39-97f8-00a0b274c192" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-07c53712-32b3-4846-9969-c3cedb386a38" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-ebec326a-4afb-4f39-97f8-00a0b274c192"\n'
10.10.2.1: b'  Volume group "ceph-ebec326a-4afb-4f39-97f8-00a0b274c192" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:38:43,004754866-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:38:43,014575728-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:38:43,018212394-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\nHost looks OK\n'
10.10.2.1: b'Cluster fsid: f354a228-3858-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\nExtracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid f354a228-3858-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\nPlease consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:39:46,969101264-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:40:06,976064848-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:40:06,986121312-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:40:06,989829202-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid f354a228-3858-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/f354a228-3858-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:40:16,967718346-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:40:16,978087779-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:40:16,981525491-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid f354a228-3858-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/f354a228-3858-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:40:26,816840714-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:40:26,823030910-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:40:27,030196239-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:40:27,033772541-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid f354a228-3858-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/f354a228-3858-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:40:36,787392332-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:40:56,792196888-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:40:56,798848892-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:40:56,808761767-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:40:56,812823893-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid f354a228-3858-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/f354a228-3858-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:41:22,364040988-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:41:42,369318858-07:00] STAGE: Show Cluster Status\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:41:42,379271557-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:41:42,383243854-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid f354a228-3858-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/f354a228-3858-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     f354a228-3858-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.uzynty(active, since 2m)\n    osd: 1 osds: 1 up (since 16s), 1 in (since 41s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 18:41:51 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:38:36,092551239-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/b0145d2e-3857-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid b0145d2e-3857-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:38:36,103711688-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:38:36,107417825-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'b0145d2e-3857-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid b0145d2e-3857-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:38:36,115941518-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid b0145d2e-3857-11ec-b51d-53e6e728d2d3'
[1] 18:38:41 [SUCCESS] 10.10.2.1
[2] 18:38:42 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:38:42,063883687-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:38:42,076231779-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:38:42,080974604-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:38:42,231473993-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:38:42,236204135-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:38:42,382608356-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:38:42,663206343-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:38:42,668457434-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--ebec326a--4afb--4f39--97f8--00a0b274c192-osd--block--07c53712--32b3--4846--9969--c3cedb386a38 (253:0)
  Archiving volume group "ceph-ebec326a-4afb-4f39-97f8-00a0b274c192" metadata (seqno 5).
  Releasing logical volume "osd-block-07c53712-32b3-4846-9969-c3cedb386a38"
  Creating volume group backup "/etc/lvm/backup/ceph-ebec326a-4afb-4f39-97f8-00a0b274c192" (seqno 6).
  Logical volume "osd-block-07c53712-32b3-4846-9969-c3cedb386a38" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-ebec326a-4afb-4f39-97f8-00a0b274c192"
  Volume group "ceph-ebec326a-4afb-4f39-97f8-00a0b274c192" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:38:43,004754866-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:38:43,014575728-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:38:43,018212394-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: f354a228-3858-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid f354a228-3858-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:39:46,969101264-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:40:06,976064848-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:40:06,986121312-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:40:06,989829202-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid f354a228-3858-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/f354a228-3858-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:40:16,967718346-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:40:16,978087779-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:40:16,981525491-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid f354a228-3858-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/f354a228-3858-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:40:26,816840714-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:40:26,823030910-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:40:27,030196239-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:40:27,033772541-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid f354a228-3858-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/f354a228-3858-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:40:36,787392332-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:40:56,792196888-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:40:56,798848892-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:40:56,808761767-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:40:56,812823893-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid f354a228-3858-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/f354a228-3858-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:41:22,364040988-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:41:42,369318858-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:41:42,379271557-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:41:42,383243854-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid f354a228-3858-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/f354a228-3858-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     f354a228-3858-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.uzynty(active, since 2m)
    osd: 1 osds: 1 up (since 16s), 1 in (since 41s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:41:51,093969345-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:41:51,101435833-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 18:41:51 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:41:51,572297159-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:41:51,575472512-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:41:51,597071818-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:41:51,599912441-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f354a228-3858-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f354a228-3858-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:41:55,765590847-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:41:55,768510108-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f354a228-3858-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f354a228-3858-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:41:59,898884216-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:41:59,902154708-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f354a228-3858-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f354a228-3858-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:42:03,940297319-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:42:03,943271433-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f354a228-3858-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f354a228-3858-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:42:11,927498472-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:42:11,930518131-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f354a228-3858-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f354a228-3858-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:42:16,192945089-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:42:16,195968025-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f354a228-3858-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f354a228-3858-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:42:20,553695073-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:42:20,556868352-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f354a228-3858-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f354a228-3858-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:42:24,994584883-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:42:24,997685696-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f354a228-3858-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f354a228-3858-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:42:29,568966042-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:42:29,572033552-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f354a228-3858-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f354a228-3858-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:42:34,702013286-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:42:34,705296462-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f354a228-3858-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f354a228-3858-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:42:38,995985211-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:42:38,998821485-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f354a228-3858-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f354a228-3858-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 21 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:42:43,005249923-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:42:43,008461234-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f354a228-3858-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f354a228-3858-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.39059         -  400 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default   
-3         0.39059         -  400 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host node-1
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0  
                       TOTAL  400 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  400 GiB  0.00                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:42:47,038586000-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:43:10,978143461-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:43:20,069075014-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:43:29,193864442-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:43:29,200863459-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:43:38,249580025-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:43:38,256563143-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:43:47,192332800-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:43:47,199463095-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:43:56,157220172-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:43:56,164282518-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:44:05,281910408-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:44:05,288489444-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:44:05,293695263-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:44:05,297032671-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:44:05,303045931-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:44:05,307834543-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=1008117
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:44:05,314310195-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:44:05,323102250-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'26565\n'
[1] 18:44:05 [SUCCESS] ljishen@10.10.2.2
26565

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:44:05,509045437-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4KB_write.log.3
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:44:05,528893243-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:44:05,531797956-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f354a228-3858-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f354a228-3858-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T01:44:08.648955+0000 Maintaining 128 concurrent writes of 4096 bytes to objects of size 4096 for up to 60 seconds or 0 objects
2021-10-29T01:44:08.648965+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-29T01:44:08.649495+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T01:44:08.649495+0000     0       0         0         0         0         0           -           0
2021-10-29T01:44:09.649618+0000     1     128      1937      1809   7.06584   7.06641   0.0666069   0.0667849
2021-10-29T01:44:10.649732+0000     2     128      3714      3586   7.00323   6.94141   0.0674281   0.0702725
2021-10-29T01:44:11.649844+0000     3     128      5265      5137   6.68812   6.05859    0.050222   0.0737282
2021-10-29T01:44:12.649961+0000     4     128      7185      7057   6.89087       7.5    0.183587   0.0714302
2021-10-29T01:44:13.650030+0000     5     128      9233      9105   7.11258         8    0.058199   0.0696132
2021-10-29T01:44:14.650122+0000     6     128     11153     11025   7.17703       7.5   0.0673202   0.0692042
2021-10-29T01:44:15.650233+0000     7     128     12945     12817   7.15163         7    0.116848    0.069635
2021-10-29T01:44:16.650346+0000     8     128     14276     14148   6.90751   5.19922   0.0766748    0.072039
2021-10-29T01:44:17.650444+0000     9     128     16068     15940   6.91771         7   0.0582243   0.0720121
2021-10-29T01:44:18.650512+0000    10     128     17809     17681   6.90597   6.80078   0.0584716   0.0721375
2021-10-29T01:44:19.650624+0000    11     128     19652     19524   6.93255   7.19922   0.0586163   0.0719287
2021-10-29T01:44:20.650733+0000    12     128     21265     21137   6.87985   6.30078   0.0745755   0.0725363
2021-10-29T01:44:21.650826+0000    13     128     22980     22852   6.86591   6.69922   0.0587002   0.0725667
2021-10-29T01:44:22.650941+0000    14     128     24388     24260   6.76829       5.5    0.131127   0.0732823
2021-10-29T01:44:23.651004+0000    15     128     26257     26129   6.80376   7.30078   0.0751339   0.0732609
2021-10-29T01:44:24.651115+0000    16     128     28049     27921   6.81598         7   0.0747666   0.0732601
2021-10-29T01:44:25.651217+0000    17     128     29713     29585   6.79735       6.5    0.108663   0.0733119
2021-10-29T01:44:26.651336+0000    18     128     31300     31172   6.76408   6.19922   0.0748586   0.0737904
2021-10-29T01:44:27.651455+0000    19     128     32785     32657   6.71334   5.80078   0.0667667   0.0742725
2021-10-29T01:44:28.651521+0000 min lat: 0.0304437 max lat: 0.340455 avg lat: 0.0742196
2021-10-29T01:44:28.651521+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T01:44:28.651521+0000    20     128     34577     34449   6.72765         7   0.0753101   0.0742196
2021-10-29T01:44:29.651620+0000    21     128     36292     36164   6.72627   6.69922   0.0672136   0.0742006
2021-10-29T01:44:30.651733+0000    22     128     37905     37777   6.70689   6.30078   0.0669311   0.0744336
2021-10-29T01:44:31.651846+0000    23     128     39697     39569    6.7196         7   0.0585372   0.0742747
2021-10-29T01:44:32.651954+0000    24     128     41412     41284   6.71872   6.69922    0.108534   0.0741678
2021-10-29T01:44:33.652017+0000    25     128     42769     42641     6.662   5.30078    0.125329   0.0748889
2021-10-29T01:44:34.652130+0000    26     128     44561     44433   6.67497         7   0.0673074   0.0748283
2021-10-29T01:44:35.652242+0000    27     128     46276     46148   6.67584   6.69922   0.0671187   0.0747574
2021-10-29T01:44:36.652359+0000    28     128     47940     47812   6.66953       6.5    0.058976    0.074871
2021-10-29T01:44:37.652452+0000    29     128     49604     49476   6.66366       6.5   0.0583252   0.0749655
2021-10-29T01:44:38.652515+0000    30     128     51217     51089   6.65155   6.30078   0.0668976   0.0748601
2021-10-29T01:44:39.652623+0000    31     128     52881     52753   6.64664       6.5   0.0663235   0.0751206
2021-10-29T01:44:40.652734+0000    32     128     54724     54596   6.66389   7.19922   0.0665389   0.0749431
2021-10-29T01:44:41.652831+0000    33     128     56388     56260    6.6589       6.5    0.125537   0.0749808
2021-10-29T01:44:42.652941+0000    34     128     58308     58180   6.68361       7.5   0.0763141   0.0747397
2021-10-29T01:44:43.653005+0000    35     128     60049     59921   6.68695   6.80078   0.0667164   0.0747301
2021-10-29T01:44:44.653116+0000    36     128     61636     61508   6.67338   6.19922   0.0666917   0.0748725
2021-10-29T01:44:45.653209+0000    37     128     63428     63300   6.68219         7   0.0665335   0.0747261
2021-10-29T01:44:46.653320+0000    38     128     65092     64964   6.67738       6.5   0.0666529   0.0747376
2021-10-29T01:44:47.653432+0000    39     128     66961     66833   6.69334   7.30078   0.0595748   0.0746451
2021-10-29T01:44:48.653499+0000 min lat: 0.0304437 max lat: 0.340455 avg lat: 0.0745918
2021-10-29T01:44:48.653499+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T01:44:48.653499+0000    40     128     68676     68548   6.69348   6.69922   0.0749988   0.0745918
2021-10-29T01:44:49.653599+0000    41     128     70033     69905    6.6595   5.30078   0.0749783   0.0749982
2021-10-29T01:44:50.653711+0000    42     128     71697     71569   6.65568       6.5   0.0673257   0.0750457
2021-10-29T01:44:51.653824+0000    43     128     73540     73412    6.6683   7.19922   0.0671303   0.0748955
2021-10-29T01:44:52.653935+0000    44     128     75281     75153    6.6713   6.80078   0.0667234   0.0748922
2021-10-29T01:44:53.653999+0000    45     127     76882     76755    6.6621   6.25781   0.0750608   0.0749664
2021-10-29T01:44:54.654111+0000    46     128     77892     77764   6.60294   3.94141    0.333377   0.0756341
2021-10-29T01:44:55.654228+0000    47     128     79684     79556   6.61137         7   0.0667712   0.0755549
2021-10-29T01:44:56.654342+0000    48     128     81169     81041   6.59447   5.80078   0.0672875    0.075789
2021-10-29T01:44:57.654438+0000    49     128     82884     82756    6.5966   6.69922   0.0752564   0.0757364
2021-10-29T01:44:58.654506+0000    50     128     84625     84497   6.60067   6.80078   0.0589051   0.0756909
2021-10-29T01:44:59.654631+0000    51     128     86340     86212   6.60259   6.69922   0.0751014   0.0756875
2021-10-29T01:45:00.654752+0000    52     128     87697     87569   6.57754   5.30078   0.0671266   0.0759499
2021-10-29T01:45:01.654845+0000    53     128     89489     89361    6.5855         7   0.0669584   0.0758263
2021-10-29T01:45:02.654918+0000    54     128     91153     91025   6.58391       6.5   0.0669379   0.0758959
2021-10-29T01:45:03.654997+0000    55     127     92779     92652   6.57974   6.35547   0.0744214     0.07594
2021-10-29T01:45:04.655111+0000    56     128     94532     94404   6.58445   6.84375   0.0932252   0.0759034
2021-10-29T01:45:05.655209+0000    57     128     95633     95505   6.54437   4.30078    0.453368   0.0762698
2021-10-29T01:45:06.655321+0000    58     128     97220     97092   6.53841   6.19922   0.0668658   0.0764327
2021-10-29T01:45:07.655435+0000    59     128     98961     98833   6.54284   6.80078   0.0755812   0.0763714
2021-10-29T01:45:08.655500+0000 min lat: 0.0304437 max lat: 0.454281 avg lat: 0.0763027
2021-10-29T01:45:08.655500+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T01:45:08.655500+0000    60     119    100744    100625   6.55046         7   0.0668051   0.0763027
2021-10-29T01:45:09.655637+0000 Total time run:         60.0659
Total writes made:      100744
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     6.55166
Stddev Bandwidth:       0.733445
Max bandwidth (MB/sec): 8
Min bandwidth (MB/sec): 3.94141
Average IOPS:           1677
Stddev IOPS:            187.762
Max IOPS:               2048
Min IOPS:               1009
Average Latency(s):     0.0762911
Stddev Latency(s):      0.0320344
Max latency(s):         0.454281
Min latency(s):         0.0304437

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:45:10,288170248-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:45:10,293467299-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 1008117

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:45:10,298571506-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 26565
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:45:10,305646817-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 26565
[1] 18:45:10 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:45:10,488963866-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4KB_write.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4KB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_write.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:45:10,657737652-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:45:34,751237819-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 100.75k objects, 394 MiB
    usage:   1.3 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:45:34,757903158-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:45:43,958683273-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 100.75k objects, 394 MiB
    usage:   1.3 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:45:43,965223455-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:45:53,008384192-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 100.75k objects, 394 MiB
    usage:   1.3 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:45:53,015289382-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:46:01,991631464-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 100.75k objects, 394 MiB
    usage:   1.3 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:46:01,998598651-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:46:11,151098853-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 100.75k objects, 394 MiB
    usage:   1.3 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:46:11,157740558-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:46:11,163023582-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:46:11,166198774-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:46:11,172430295-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:46:11,177647746-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=1010120
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:46:11,183953387-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:46:11,192717089-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'27365\n'
[1] 18:46:11 [SUCCESS] ljishen@10.10.2.2
27365

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:46:11,377093664-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4KB_seq.log.3
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:46:11,396407354-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:46:11,399208542-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f354a228-3858-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f354a228-3858-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T01:46:14.538348+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T01:46:14.538348+0000     0       0         0         0         0         0           -           0
2021-10-29T01:46:15.538468+0000     1     128      4575      4447   17.3678   17.3711   0.0290028   0.0280679
2021-10-29T01:46:16.538582+0000     2     128      9642      9514   18.5792    19.793   0.0472338   0.0265332
2021-10-29T01:46:17.538680+0000     3     128     14704     14576   18.9766   19.7734    0.169829    0.025297
2021-10-29T01:46:18.538790+0000     4     128     19739     19611   19.1489    19.668 0.000424991   0.0259411
2021-10-29T01:46:19.538902+0000     5     128     24743     24615   19.2281   19.5469  0.00555727   0.0258795
2021-10-29T01:46:20.539013+0000     6     128     29267     29139   18.9684   17.6719   0.0368748   0.0262698
2021-10-29T01:46:21.539109+0000     7     128     35014     34886   19.4653   22.4492 0.000447523   0.0256142
2021-10-29T01:46:22.539225+0000     8     128     39865     39737   19.4005   18.9492   0.0501009   0.0256774
2021-10-29T01:46:23.539335+0000     9     128     44283     44155   19.1622   17.2578   0.0427348   0.0260309
2021-10-29T01:46:24.539446+0000    10     128     48516     48388   18.8994   16.5352   0.0922314   0.0263776
2021-10-29T01:46:25.539538+0000    11     128     53328     53200   18.8899   18.7969  0.00353224   0.0264099
2021-10-29T01:46:26.539648+0000    12     127     57869     57742   18.7941   17.7422   0.0010503   0.0265499
2021-10-29T01:46:27.539766+0000    13     128     65076     64948   19.5134   28.1484   0.0394399   0.0255946
2021-10-29T01:46:28.539877+0000    14     128     70569     70441   19.6521    21.457   0.0382295   0.0253848
2021-10-29T01:46:29.539969+0000    15     128     75742     75614   19.6889    20.207 0.000548704   0.0253038
2021-10-29T01:46:30.540079+0000    16     128     81266     81138   19.8069   21.5781 0.000259539   0.0252008
2021-10-29T01:46:31.540193+0000    17     128     86155     86027    19.765   19.0977  0.00274092   0.0252758
2021-10-29T01:46:32.540308+0000    18     128     92529     92401     20.05   24.8984  0.00654559   0.0249216
2021-10-29T01:46:33.540401+0000    19     128     97163     97035   19.9474   18.1016  0.00207718    0.025049
2021-10-29T01:46:34.540548+0000 Total time run:       19.9765
Total reads made:     100744
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   19.6997
Average IOPS:         5043
Stddev IOPS:          727.418
Max IOPS:             7206
Min IOPS:             4233
Average Latency(s):   0.0253464
Max latency(s):       0.282781
Min latency(s):       0.000237087

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:46:35,242220452-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:46:35,247133780-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 1010120

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:46:35,252053659-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 27365
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:46:35,259804142-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 27365
[1] 18:46:35 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:46:35,440913191-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4KB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4KB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_4KB_seq.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:46:35,598833438-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:46:59,645447475-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 100.75k objects, 394 MiB
    usage:   1.3 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:46:59,652752630-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:47:08,728254022-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 100.75k objects, 394 MiB
    usage:   1.3 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:47:08,736221634-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:47:17,753836411-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 100.75k objects, 394 MiB
    usage:   1.3 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:47:17,760400599-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:47:26,811177458-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 100.75k objects, 394 MiB
    usage:   1.3 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:47:26,818364059-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:47:36,136600397-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 100.75k objects, 394 MiB
    usage:   1.3 GiB used, 399 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:47:36,143935647-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:47:36,149336003-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T18:47:36,152843632-07:00][RUNNING][ROUND 1/2/21] object_size=16KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:47:36,155897025-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:47:36,165335428-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:47:36,568269030-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/f354a228-3858-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid f354a228-3858-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:47:36,577989342-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:47:36,580949125-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'f354a228-3858-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid f354a228-3858-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:47:36,589330400-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid f354a228-3858-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 18:47:42 [SUCCESS] 10.10.2.1\n[2] 18:47:42 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:47:42,532296378-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:47:42,543533552-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:47:42,547609504-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:47:42,696071381-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:47:42,700770144-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:47:42,846374610-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:47:43,126886433-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:47:43,131151200-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--54a6165b--08d9--459c--a735--04cd221ac9bc-osd--block--6dd016a7--46fe--4655--9c95--12b96b2bbf0a (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-54a6165b-08d9-459c-a735-04cd221ac9bc" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-6dd016a7-46fe-4655-9c95-12b96b2bbf0a"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-54a6165b-08d9-459c-a735-04cd221ac9bc" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-6dd016a7-46fe-4655-9c95-12b96b2bbf0a" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-54a6165b-08d9-459c-a735-04cd221ac9bc"\n'
10.10.2.1: b'  Volume group "ceph-54a6165b-08d9-459c-a735-04cd221ac9bc" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:47:43,536418527-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:47:43,545921370-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:47:43,549424446-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 3583dbea-385a-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\nExtracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\n'
10.10.2.1: b'Waiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 3583dbea-385a-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:48:43,707721831-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:49:03,714679437-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:49:03,724571322-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:49:03,729017740-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 3583dbea-385a-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/3583dbea-385a-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:49:13,037203632-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:49:13,046637987-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:49:13,049975540-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 3583dbea-385a-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/3583dbea-385a-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:49:22,274117968-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:49:22,280157160-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:49:22,484234573-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:49:22,488143951-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid 3583dbea-385a-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/3583dbea-385a-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:49:31,916476351-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:49:51,920828021-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:49:51,927459217-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:49:51,937777252-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:49:51,941541448-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 3583dbea-385a-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/3583dbea-385a-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:50:17,743215508-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:50:37,748305341-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:50:37,757476531-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:50:37,760849151-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 3583dbea-385a-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/3583dbea-385a-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     3583dbea-385a-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.ivwkpz(active, since 2m)\n    osd: 1 osds: 1 up (since 16s), 1 in (since 41s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 18:50:46 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:47:36,568269030-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/f354a228-3858-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid f354a228-3858-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:47:36,577989342-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:47:36,580949125-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'f354a228-3858-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid f354a228-3858-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:47:36,589330400-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid f354a228-3858-11ec-b51d-53e6e728d2d3'
[1] 18:47:42 [SUCCESS] 10.10.2.1
[2] 18:47:42 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:47:42,532296378-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:47:42,543533552-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:47:42,547609504-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:47:42,696071381-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:47:42,700770144-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:47:42,846374610-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:47:43,126886433-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:47:43,131151200-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--54a6165b--08d9--459c--a735--04cd221ac9bc-osd--block--6dd016a7--46fe--4655--9c95--12b96b2bbf0a (253:0)
  Archiving volume group "ceph-54a6165b-08d9-459c-a735-04cd221ac9bc" metadata (seqno 5).
  Releasing logical volume "osd-block-6dd016a7-46fe-4655-9c95-12b96b2bbf0a"
  Creating volume group backup "/etc/lvm/backup/ceph-54a6165b-08d9-459c-a735-04cd221ac9bc" (seqno 6).
  Logical volume "osd-block-6dd016a7-46fe-4655-9c95-12b96b2bbf0a" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-54a6165b-08d9-459c-a735-04cd221ac9bc"
  Volume group "ceph-54a6165b-08d9-459c-a735-04cd221ac9bc" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:47:43,536418527-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:47:43,545921370-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:47:43,549424446-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 3583dbea-385a-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 3583dbea-385a-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:48:43,707721831-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:49:03,714679437-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:49:03,724571322-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:49:03,729017740-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 3583dbea-385a-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/3583dbea-385a-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:49:13,037203632-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:49:13,046637987-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:49:13,049975540-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 3583dbea-385a-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/3583dbea-385a-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:49:22,274117968-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:49:22,280157160-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:49:22,484234573-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:49:22,488143951-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 3583dbea-385a-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/3583dbea-385a-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:49:31,916476351-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:49:51,920828021-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:49:51,927459217-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:49:51,937777252-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:49:51,941541448-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 3583dbea-385a-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/3583dbea-385a-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:50:17,743215508-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:50:37,748305341-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:50:37,757476531-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:50:37,760849151-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 3583dbea-385a-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/3583dbea-385a-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     3583dbea-385a-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.ivwkpz(active, since 2m)
    osd: 1 osds: 1 up (since 16s), 1 in (since 41s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:50:46,852717209-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:50:46,860114097-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 18:50:47 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:50:47,337306397-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:50:47,340552383-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:50:47,362099655-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:50:47,364859325-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3583dbea-385a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3583dbea-385a-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:50:51,572725556-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:50:51,575782987-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3583dbea-385a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3583dbea-385a-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:50:55,708490694-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:50:55,711681727-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3583dbea-385a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3583dbea-385a-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:50:59,748641813-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:50:59,751618171-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3583dbea-385a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3583dbea-385a-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:51:07,871458911-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:51:07,874319401-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3583dbea-385a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3583dbea-385a-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:51:12,453400031-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:51:12,456514881-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3583dbea-385a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3583dbea-385a-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:51:16,845752572-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:51:16,848590360-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3583dbea-385a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3583dbea-385a-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:51:21,560911638-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:51:21,564106759-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3583dbea-385a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3583dbea-385a-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:51:26,483539944-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:51:26,486645396-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3583dbea-385a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3583dbea-385a-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:51:30,918665743-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:51:30,921810599-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3583dbea-385a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3583dbea-385a-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:51:35,090885179-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:51:35,094036076-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3583dbea-385a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3583dbea-385a-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:51:39,084865082-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:51:39,087805022-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3583dbea-385a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3583dbea-385a-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default   
-3         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host node-1
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0  
                       TOTAL  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:51:42,966983189-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:52:06,920675999-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:52:15,903603446-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:52:24,907766986-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:52:34,041874361-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:52:34,048733155-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:52:43,147507660-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:52:43,154594624-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:52:52,303148517-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:52:52,310495811-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:53:01,402621633-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:53:01,410006318-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:53:10,476847322-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:53:10,483901574-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:53:10,489269439-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:53:10,492353941-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:53:10,498312920-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:53:10,503128784-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=1018089
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:53:10,509287890-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:53:10,518102089-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'33205\n'
[1] 18:53:10 [SUCCESS] ljishen@10.10.2.2
33205

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:53:10,705385052-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16KB_write.log.1
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:53:10,725346275-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:53:10,728187760-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3583dbea-385a-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3583dbea-385a-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T01:53:13.830847+0000 Maintaining 128 concurrent writes of 16384 bytes to objects of size 16384 for up to 60 seconds or 0 objects
2021-10-29T01:53:13.830857+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-29T01:53:13.831833+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T01:53:13.831833+0000     0       0         0         0         0         0           -           0
2021-10-29T01:53:14.831992+0000     1     128      1243      1115   17.4202   17.4219   0.0919009    0.109808
2021-10-29T01:53:15.832116+0000     2     128      2433      2305   18.0058   18.5938    0.174585    0.106651
2021-10-29T01:53:16.832231+0000     3     128      3675      3547   18.4719   19.4062    0.099544    0.107503
2021-10-29T01:53:17.832308+0000     4     128      4481      4353   17.0021   12.5938    0.383546    0.111832
2021-10-29T01:53:18.832427+0000     5     128      5723      5595   17.4825   19.4062   0.0837937    0.113205
2021-10-29T01:53:19.832544+0000     6     128      6785      6657   17.3341   16.5938    0.100262    0.114256
2021-10-29T01:53:20.832659+0000     7     128      7899      7771   17.3441   17.4062     0.10047    0.114271
2021-10-29T01:53:21.832754+0000     8     128      8961      8833   17.2501   16.5938    0.166872      0.1149
2021-10-29T01:53:22.832828+0000     9     128     10203     10075   17.4895   19.4062     0.09212    0.113578
2021-10-29T01:53:23.832939+0000    10     128     11099     10971   17.1404        14    0.108486    0.116008
2021-10-29T01:53:24.833050+0000    11     128     12251     12123   17.2184        18    0.108372    0.115807
2021-10-29T01:53:25.833146+0000    12     128     13313     13185   17.1662   16.5938    0.208827    0.115428
2021-10-29T01:53:26.833256+0000    13     128     14427     14299   17.1845   17.4062    0.124424    0.116062
2021-10-29T01:53:27.833329+0000    14     128     15489     15361   17.1422   16.5938    0.126736    0.116043
2021-10-29T01:53:28.833440+0000    15     128     16347     16219    16.893   13.4062    0.123667    0.117563
2021-10-29T01:53:29.833539+0000    16     128     17409     17281   16.8742   16.5938    0.109143    0.118216
2021-10-29T01:53:30.833654+0000    17     128     18523     18395   16.9054   17.4062    0.108197    0.117966
2021-10-29T01:53:31.833765+0000    18     128     19419     19291   16.7439        14    0.100553    0.119291
2021-10-29T01:53:32.833838+0000    19     128     20481     20353    16.736   16.5938    0.122839    0.118608
2021-10-29T01:53:33.833935+0000 min lat: 0.059813 max lat: 0.384172 avg lat: 0.119571
2021-10-29T01:53:33.833935+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T01:53:33.833935+0000    20     128     21467     21339   16.6694   15.4062    0.283427    0.119571
2021-10-29T01:53:34.834051+0000    21     128     22529     22401   16.6657   16.5938    0.116891    0.119648
2021-10-29T01:53:35.834162+0000    22     128     23643     23515   16.6993   17.4062    0.167419    0.119449
2021-10-29T01:53:36.834275+0000    23     128     24667     24539   16.6688        16    0.164201     0.11938
2021-10-29T01:53:37.834349+0000    24     128     25819     25691   16.7242        18    0.100369    0.119406
2021-10-29T01:53:38.834460+0000    25     128     26971     26843   16.7752        18    0.116478    0.118962
2021-10-29T01:53:39.834571+0000    26     128     27905     27777   16.6912   14.5938     0.10822    0.118771
2021-10-29T01:53:40.834683+0000    27     128     28929     28801   16.6655        16    0.100073    0.119502
2021-10-29T01:53:41.834786+0000    28     128     29953     29825   16.6417        16    0.136247    0.119801
2021-10-29T01:53:42.834862+0000    29     128     30977     30849   16.6195        16    0.125089    0.119893
2021-10-29T01:53:43.834973+0000    30     128     31451     31323   16.3124   7.40625    0.332664    0.122211
2021-10-29T01:53:44.835086+0000    31     128     31745     31617   15.9343   4.59375    0.304868    0.124063
2021-10-29T01:53:45.835186+0000    32     128     32129     32001   15.6239         6    0.295378    0.127082
2021-10-29T01:53:46.835302+0000    33     128     32513     32385   15.3322         6     0.25833    0.129046
2021-10-29T01:53:47.835380+0000    34     128     32987     32859   15.0991   7.40625    0.274987    0.131943
2021-10-29T01:53:48.835491+0000    35     127     33345     33218   14.8279   5.60938    0.451766    0.134152
2021-10-29T01:53:49.835589+0000    36     128     33793     33665     14.61   6.98438    0.230703    0.136301
2021-10-29T01:53:50.835706+0000    37     128     34139     34011   14.3613   5.40625    0.352358    0.138644
2021-10-29T01:53:51.835824+0000    38     128     34523     34395   14.1412         6    0.332523    0.140932
2021-10-29T01:53:52.835902+0000    39     128     34817     34689   13.8964   4.59375    0.331825    0.142743
2021-10-29T01:53:53.836001+0000 min lat: 0.059813 max lat: 0.501941 avg lat: 0.145271
2021-10-29T01:53:53.836001+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T01:53:53.836001+0000    40     128     35291     35163   13.7341   7.40625    0.266369    0.145271
2021-10-29T01:53:54.836117+0000    41     128     35585     35457   13.5112   4.59375    0.452563    0.146908
2021-10-29T01:53:55.836231+0000    42     128     35969     35841   13.3323         6    0.338756    0.149135
2021-10-29T01:53:56.836349+0000    43     128     36353     36225   13.1618         6     0.31986    0.150967
2021-10-29T01:53:57.836427+0000    44     128     36737     36609    12.999         6    0.271908    0.153368
2021-10-29T01:53:58.836538+0000    45     128     37121     36993   12.8435         6    0.389308    0.154855
2021-10-29T01:53:59.836650+0000    46     128     37505     37377   12.6947         6    0.312903    0.156899
2021-10-29T01:54:00.836760+0000    47     128     37889     37761   12.5522         6    0.384861    0.158191
2021-10-29T01:54:01.836855+0000    48     128     38363     38235    12.445   7.40625    0.294044    0.160412
2021-10-29T01:54:02.836933+0000    49     128     38657     38529   12.2848   4.59375    0.316755    0.162146
2021-10-29T01:54:03.837048+0000    50     128     39041     38913   12.1591         6    0.258588    0.164097
2021-10-29T01:54:04.837161+0000    51     128     40027     39899   12.2227   15.4062   0.0997592    0.163547
2021-10-29T01:54:05.837264+0000    52     128     41089     40961   12.3067   16.5938    0.183614    0.162182
2021-10-29T01:54:06.837382+0000    53     128     42203     42075   12.4029   17.4062    0.142134    0.161147
2021-10-29T01:54:07.837461+0000    54     128     42971     42843   12.3954        12    0.116552    0.161267
2021-10-29T01:54:08.837536+0000    55     128     44033     43905   12.4717   16.5938    0.100003    0.160166
2021-10-29T01:54:09.837632+0000    56     127     45183     45056   12.5701   17.9844    0.174725    0.158883
2021-10-29T01:54:10.837743+0000    57     128     46337     46209   12.6656   18.0156    0.139257    0.157744
2021-10-29T01:54:11.837853+0000    58     128     47361     47233   12.7231        16     0.10833    0.157078
2021-10-29T01:54:12.837926+0000    59     128     48001     47873   12.6769        10    0.170117    0.156506
2021-10-29T01:54:13.838022+0000 min lat: 0.059813 max lat: 0.591557 avg lat: 0.156889
2021-10-29T01:54:13.838022+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T01:54:13.838022+0000    60     128     49025     48897   12.7323        16    0.136446    0.156889
2021-10-29T01:54:14.838173+0000 Total time run:         60.0664
Total writes made:      49025
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     12.7528
Stddev Bandwidth:       5.26382
Max bandwidth (MB/sec): 19.4062
Min bandwidth (MB/sec): 4.59375
Average IOPS:           816
Stddev IOPS:            336.885
Max IOPS:               1242
Min IOPS:               294
Average Latency(s):     0.156764
Stddev Latency(s):      0.0942308
Max latency(s):         0.591557
Min latency(s):         0.059813

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:54:15,555804989-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:54:15,561033872-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 1018089

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:54:15,566201630-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 33205
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:54:15,573925103-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 33205
[1] 18:54:15 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:54:15,757647607-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16KB_write.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16KB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:54:15,930489728-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:54:39,919560819-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 49.03k objects, 766 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:54:39,926788538-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:54:49,010200685-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 49.03k objects, 766 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:54:49,017408708-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:54:57,972390649-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 49.03k objects, 766 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:54:57,979105151-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:55:07,020443102-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 49.03k objects, 766 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:55:07,027624415-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:55:16,202183136-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 49.03k objects, 766 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:55:16,209260702-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:55:16,214597709-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:55:16,217955987-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:55:16,223902222-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:55:16,228840727-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=1019440
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:55:16,235059215-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:55:16,243469002-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'33980\n'
[1] 18:55:16 [SUCCESS] ljishen@10.10.2.2
33980

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:55:16,429654316-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16KB_seq.log.1
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:55:16,449082344-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:55:16,451845020-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3583dbea-385a-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3583dbea-385a-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T01:55:19.490938+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T01:55:19.490938+0000     0       0         0         0         0         0           -           0
2021-10-29T01:55:20.491080+0000     1     128      3335      3207   50.0985   50.1094   0.0339723   0.0391031
2021-10-29T01:55:21.491163+0000     2     128      6145      6017   47.0008   43.9062   0.0262939   0.0420986
2021-10-29T01:55:22.491279+0000     3     128      8998      8870   46.1915   44.5781  0.00643331   0.0430271
2021-10-29T01:55:23.491359+0000     4     128     11844     11716     45.76   44.4688   0.0218083   0.0435916
2021-10-29T01:55:24.491475+0000     5     128     14955     14827   46.3287   48.6094    0.096349   0.0430384
2021-10-29T01:55:25.491566+0000     6     128     17950     17822    46.406   46.7969   0.0331984   0.0429296
2021-10-29T01:55:26.491689+0000     7     128     20534     20406   45.5437    40.375   0.0473565   0.0437467
2021-10-29T01:55:27.491811+0000     8     128     22776     22648   44.2291   35.0312   0.0362528   0.0449619
2021-10-29T01:55:28.491937+0000     9     128     25705     25577   44.3992   45.7656   0.0128365   0.0449891
2021-10-29T01:55:29.492035+0000    10     128     28141     28013   43.7652   38.0625   0.0284957   0.0454449
2021-10-29T01:55:30.492144+0000    11     128     30780     30652   43.5347   41.2344   0.0947354   0.0455632
2021-10-29T01:55:31.492263+0000    12     128     34673     34545   44.9752   60.8281   0.0664918   0.0442859
2021-10-29T01:55:32.492384+0000    13     128     38516     38388    46.134   60.0469    0.150607   0.0431596
2021-10-29T01:55:33.492471+0000    14     128     40502     40374   45.0551   31.0312   0.0663073    0.044292
2021-10-29T01:55:34.492586+0000    15     128     43654     43526   45.3344     49.25 0.000628304   0.0439968
2021-10-29T01:55:35.492702+0000    16     128     46521     46393   45.3005   44.7969   0.0221528   0.0440605
2021-10-29T01:55:36.492862+0000 Total time run:       16.9479
Total reads made:     49025
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   45.1982
Average IOPS:         2892
Stddev IOPS:          500.225
Max IOPS:             3893
Min IOPS:             1986
Average Latency(s):   0.0441776
Max latency(s):       0.496769
Min latency(s):       0.000320905

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:55:37,178172714-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:55:37,184009502-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 1019440

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:55:37,189073705-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 33980
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:55:37,196729340-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 33980
[1] 18:55:37 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:55:37,381345788-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16KB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16KB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:55:37,533632195-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:56:01,541921801-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 49.03k objects, 766 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:56:01,549363514-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:56:10,640189823-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 49.03k objects, 766 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:56:10,647650812-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:56:19,649310771-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 49.03k objects, 766 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:56:19,656644140-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:56:28,730266307-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 49.03k objects, 766 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:56:28,737224709-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:56:37,900898271-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 49.03k objects, 766 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:56:37,908525994-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:56:37,914053539-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T18:56:37,916330430-07:00][RUNNING][ROUND 2/2/21] object_size=16KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:56:37,919455218-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:56:37,928383532-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:56:38,332525254-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/3583dbea-385a-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 3583dbea-385a-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:56:38,343509994-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:56:38,347021555-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '3583dbea-385a-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 3583dbea-385a-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:56:38,356081356-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 3583dbea-385a-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 18:56:44 [SUCCESS] 10.10.2.2\n[2] 18:56:44 [SUCCESS] 10.10.2.1\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:56:44,285771054-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:56:44,296481658-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:56:44,301063592-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:56:44,450876758-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:56:44,455907205-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:56:44,606480951-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:56:44,886934973-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:56:44,891929131-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--e74d371c--246e--42eb--b235--a92386a121d4-osd--block--7e3af60a--367e--4a47--b4f6--ae33981e21a5 (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-e74d371c-246e-42eb-b235-a92386a121d4" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-7e3af60a-367e-4a47-b4f6-ae33981e21a5"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-e74d371c-246e-42eb-b235-a92386a121d4" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-7e3af60a-367e-4a47-b4f6-ae33981e21a5" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-e74d371c-246e-42eb-b235-a92386a121d4"\n'
10.10.2.1: b'  Volume group "ceph-e74d371c-246e-42eb-b235-a92386a121d4" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:56:45,216879780-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:56:45,226827971-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:56:45,230902852-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\nRepeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\nHost looks OK\n'
10.10.2.1: b'Cluster fsid: 786109a0-385b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\nExtracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\nAdding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 786109a0-385b-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:57:47,661072933-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:58:07,668499861-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:58:07,678398719-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:58:07,682159298-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 786109a0-385b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/786109a0-385b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:58:16,640700412-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:58:16,651065517-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:58:16,654880228-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 786109a0-385b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/786109a0-385b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:58:26,133012283-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:58:26,139116639-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:58:26,349939213-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:58:26,353315720-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid 786109a0-385b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/786109a0-385b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:58:36,135546762-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:58:56,140235246-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:58:56,146929290-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:58:56,156451120-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:58:56,160234592-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 786109a0-385b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/786109a0-385b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:59:22,217191854-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:59:42,222574276-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:59:42,232312942-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T18:59:42,236144475-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 786109a0-385b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/786109a0-385b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     786109a0-385b-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.tqkdbc(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 41s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 18:59:50 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:56:38,332525254-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/3583dbea-385a-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 3583dbea-385a-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:56:38,343509994-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:56:38,347021555-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '3583dbea-385a-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 3583dbea-385a-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:56:38,356081356-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 3583dbea-385a-11ec-b51d-53e6e728d2d3'
[1] 18:56:44 [SUCCESS] 10.10.2.2
[2] 18:56:44 [SUCCESS] 10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:56:44,285771054-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:56:44,296481658-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:56:44,301063592-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:56:44,450876758-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:56:44,455907205-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:56:44,606480951-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:56:44,886934973-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:56:44,891929131-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--e74d371c--246e--42eb--b235--a92386a121d4-osd--block--7e3af60a--367e--4a47--b4f6--ae33981e21a5 (253:0)
  Archiving volume group "ceph-e74d371c-246e-42eb-b235-a92386a121d4" metadata (seqno 5).
  Releasing logical volume "osd-block-7e3af60a-367e-4a47-b4f6-ae33981e21a5"
  Creating volume group backup "/etc/lvm/backup/ceph-e74d371c-246e-42eb-b235-a92386a121d4" (seqno 6).
  Logical volume "osd-block-7e3af60a-367e-4a47-b4f6-ae33981e21a5" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-e74d371c-246e-42eb-b235-a92386a121d4"
  Volume group "ceph-e74d371c-246e-42eb-b235-a92386a121d4" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:56:45,216879780-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:56:45,226827971-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:56:45,230902852-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 786109a0-385b-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 786109a0-385b-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:57:47,661072933-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:58:07,668499861-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:58:07,678398719-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:58:07,682159298-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 786109a0-385b-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/786109a0-385b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:58:16,640700412-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:58:16,651065517-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:58:16,654880228-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 786109a0-385b-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/786109a0-385b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:58:26,133012283-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:58:26,139116639-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:58:26,349939213-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:58:26,353315720-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 786109a0-385b-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/786109a0-385b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:58:36,135546762-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:58:56,140235246-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:58:56,146929290-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:58:56,156451120-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:58:56,160234592-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 786109a0-385b-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/786109a0-385b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:59:22,217191854-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:59:42,222574276-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:59:42,232312942-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:59:42,236144475-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 786109a0-385b-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/786109a0-385b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     786109a0-385b-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.tqkdbc(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 41s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:59:50,794471109-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:59:50,802079025-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 18:59:50 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:59:51,281055580-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:59:51,284355969-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:59:51,305881350-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:59:51,308668311-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '786109a0-385b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 786109a0-385b-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:59:55,180310511-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:59:55,183363755-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '786109a0-385b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 786109a0-385b-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:59:59,311781742-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T18:59:59,314689722-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '786109a0-385b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 786109a0-385b-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:00:03,545623361-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:00:03,548640337-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '786109a0-385b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 786109a0-385b-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:00:11,928991261-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:00:11,932003046-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '786109a0-385b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 786109a0-385b-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:00:16,343090091-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:00:16,346312964-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '786109a0-385b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 786109a0-385b-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:00:20,818630386-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:00:20,821663210-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '786109a0-385b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 786109a0-385b-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:00:25,283864750-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:00:25,286883027-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '786109a0-385b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 786109a0-385b-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:00:29,724277195-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:00:29,727361036-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '786109a0-385b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 786109a0-385b-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:00:34,724502894-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:00:34,727384474-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '786109a0-385b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 786109a0-385b-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:00:38,938364701-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:00:38,941565532-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '786109a0-385b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 786109a0-385b-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 18 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 19 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:00:43,140445228-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:00:43,143355983-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '786109a0-385b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 786109a0-385b-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.39059         -  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default   
-3         0.39059         -  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host node-1
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0  
                       TOTAL  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:00:47,123231565-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:01:11,200797042-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:01:20,226078147-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:01:29,342159947-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:01:29,349050621-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:01:38,421967924-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:01:38,428806691-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:01:47,563893088-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:01:47,571245473-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:01:56,601432504-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:01:56,608601232-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:02:05,707354252-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:02:05,714270785-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:02:05,719707630-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:02:05,723131922-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:02:05,729452422-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:02:05,734273326-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=1025123
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:02:05,740576373-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:02:05,749375764-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'39821\n'
[1] 19:02:05 [SUCCESS] ljishen@10.10.2.2
39821

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:02:05,937909532-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16KB_write.log.2
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:02:05,958112420-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:02:05,961146217-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '786109a0-385b-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 786109a0-385b-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T02:02:09.032501+0000 Maintaining 128 concurrent writes of 16384 bytes to objects of size 16384 for up to 60 seconds or 0 objects
2021-10-29T02:02:09.032510+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-29T02:02:09.033470+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T02:02:09.033470+0000     0       0         0         0         0         0           -           0
2021-10-29T02:02:10.033584+0000     1     128      1352      1224    19.124    19.125    0.121912    0.100992
2021-10-29T02:02:11.033663+0000     2     128      2307      2179   17.0223   14.9219    0.156807    0.112701
2021-10-29T02:02:12.033739+0000     3     128      3400      3272   17.0405   17.0781    0.116902    0.115929
2021-10-29T02:02:13.033823+0000     4     128      4227      4099   16.0106   12.9219    0.239998    0.117984
2021-10-29T02:02:14.033903+0000     5     127      5320      5193   16.2269   17.0938    0.126823    0.120817
2021-10-29T02:02:15.034022+0000     6     128      6344      6216   16.1862   15.9844    0.117019     0.12253
2021-10-29T02:02:16.034133+0000     7     128      7368      7240   16.1593        16    0.141595    0.122796
2021-10-29T02:02:17.034198+0000     8     128      8392      8264   16.1393        16    0.166898    0.123023
2021-10-29T02:02:18.034307+0000     9     128      9416      9288   16.1236        16    0.208964    0.122785
2021-10-29T02:02:19.034387+0000    10     128     10371     10243   16.0033   14.9219    0.109131    0.124029
2021-10-29T02:02:20.034500+0000    11     128     11464     11336   16.1009   17.0781    0.117102    0.123507
2021-10-29T02:02:21.034567+0000    12     128     12360     12232   15.9257        14    0.117191    0.124841
2021-10-29T02:02:22.034680+0000    13     128     13384     13256   15.9313        16    0.162275    0.125128
2021-10-29T02:02:23.034790+0000    14     128     14408     14280   15.9361        16    0.116606    0.124604
2021-10-29T02:02:24.034899+0000    15     128     15363     15235   15.8683   14.9219    0.191843    0.125161
2021-10-29T02:02:25.034963+0000    16     128     16259     16131   15.7515        14    0.116993     0.12654
2021-10-29T02:02:26.035074+0000    17     128     17224     17096   15.7118   15.0781     0.22156    0.126691
2021-10-29T02:02:27.035189+0000    18     128     18307     18179   15.7789   16.9219    0.109651    0.126111
2021-10-29T02:02:28.035304+0000    19     128     19400     19272   15.8472   17.0781    0.116944    0.125967
2021-10-29T02:02:29.035374+0000 min lat: 0.056184 max lat: 0.313513 avg lat: 0.126266
2021-10-29T02:02:29.035374+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T02:02:29.035374+0000    20     128     20355     20227   15.8009   14.9219    0.116696    0.126266
2021-10-29T02:02:30.035496+0000    21     128     21379     21251   15.8103        16    0.108457    0.126285
2021-10-29T02:02:31.035605+0000    22     128     22275     22147   15.7279        14     0.17845    0.126591
2021-10-29T02:02:32.035713+0000    23     128     23171     23043   15.6527        14    0.166209     0.12731
2021-10-29T02:02:33.035778+0000    24     128     24264     24136   15.7121   17.0781    0.141204    0.127065
2021-10-29T02:02:34.035892+0000    25     128     25288     25160   15.7235        16     0.10808    0.126895
2021-10-29T02:02:35.036002+0000    26     128     26115     25987   15.6157   12.9219    0.124905     0.12774
2021-10-29T02:02:36.036117+0000    27     128     27139     27011   15.6299        16    0.249905    0.127533
2021-10-29T02:02:37.036188+0000    28     128     28035     27907   15.5716        14    0.183412    0.127489
2021-10-29T02:02:38.036300+0000    29     128     29059     28931   15.5863        16    0.166819    0.128029
2021-10-29T02:02:39.036413+0000    30     128     30152     30024    15.636   17.0781    0.125476    0.127616
2021-10-29T02:02:40.036523+0000    31     128     31048     30920   15.5832        14    0.150216    0.127928
2021-10-29T02:02:41.036589+0000    32     128     31432     31304   15.2837         6    0.409927    0.130083
2021-10-29T02:02:42.036701+0000    33     128     31816     31688   15.0023         6    0.344943    0.132022
2021-10-29T02:02:43.036817+0000    34     128     32200     32072   14.7375         6    0.418175    0.134667
2021-10-29T02:02:44.036930+0000    35     128     32584     32456   14.4879         6    0.345649    0.137106
2021-10-29T02:02:45.037001+0000    36     128     33027     32899   14.2777   6.92188    0.287398    0.139741
2021-10-29T02:02:46.037114+0000    37     128     33283     33155   13.9999         4    0.334129    0.141152
2021-10-29T02:02:47.037231+0000    38     128     33736     33608   13.8177   7.07812    0.305618    0.143886
2021-10-29T02:02:48.037345+0000    39     128     34120     33992   13.6173         6    0.358729     0.14602
2021-10-29T02:02:49.037415+0000 min lat: 0.056184 max lat: 0.541805 avg lat: 0.148066
2021-10-29T02:02:49.037415+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T02:02:49.037415+0000    40     128     34435     34307   13.3999   4.92188    0.325007    0.148066
2021-10-29T02:02:50.037533+0000    41     128     34819     34691   13.2194         6    0.306135    0.150391
2021-10-29T02:02:51.037648+0000    42     128     35203     35075   13.0475         6    0.441497    0.152696
2021-10-29T02:02:52.037760+0000    43     128     35528     35400   12.8621   5.07812    0.418982    0.154301
2021-10-29T02:02:53.037826+0000    44     128     35971     35843   12.7271   6.92188    0.283541    0.156527
2021-10-29T02:02:54.037938+0000    45     128     36296     36168   12.5571   5.07812    0.343554    0.157968
2021-10-29T02:02:55.038049+0000    46     128     36611     36483   12.3911   4.92188    0.580885    0.160734
2021-10-29T02:02:56.038164+0000    47     128     36995     36867   12.2551         6    0.308104    0.162186
2021-10-29T02:02:57.038230+0000    48     128     37379     37251   12.1248         6    0.275626    0.164283
2021-10-29T02:02:58.038340+0000    49     128     37763     37635   11.9998         6    0.371346    0.165708
2021-10-29T02:02:59.038454+0000    50     128     38216     38088   11.9013   7.07812    0.202121    0.167578
2021-10-29T02:03:00.038572+0000    51     128     38984     38856   11.9032        12    0.241931    0.167298
2021-10-29T02:03:01.038645+0000    52     128     39939     39811   11.9613   14.9219    0.137101    0.166933
2021-10-29T02:03:02.038760+0000    53     128     40904     40776     12.02   15.0781    0.142587    0.165849
2021-10-29T02:03:03.038873+0000    54     128     41928     41800   12.0937        16    0.108368    0.165104
2021-10-29T02:03:04.038989+0000    55     128     42824     42696   12.1283        14    0.253727    0.164417
2021-10-29T02:03:05.039058+0000    56     128     43907     43779   12.2139   16.9219    0.116738    0.163525
2021-10-29T02:03:06.039170+0000    57     128     44744     44616   12.2291   13.0781    0.178265    0.163221
2021-10-29T02:03:07.039286+0000    58     128     45827     45699   12.3099   16.9219    0.125689    0.162352
2021-10-29T02:03:08.039397+0000    59     128     46723     46595   12.3386        14    0.210325    0.161749
2021-10-29T02:03:09.039463+0000 min lat: 0.056184 max lat: 0.581525 avg lat: 0.160731
2021-10-29T02:03:09.039463+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T02:03:09.039463+0000    60     128     47875     47747   12.4329        18   0.0999666    0.160731
2021-10-29T02:03:10.039620+0000 Total time run:         60.0761
Total writes made:      47875
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     12.4517
Stddev Bandwidth:       4.68579
Max bandwidth (MB/sec): 19.125
Min bandwidth (MB/sec): 4
Average IOPS:           796
Stddev IOPS:            299.89
Max IOPS:               1224
Min IOPS:               256
Average Latency(s):     0.16057
Stddev Latency(s):      0.0874114
Max latency(s):         0.581525
Min latency(s):         0.056184

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:03:10,704770199-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:03:10,709731086-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 1025123

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:03:10,715154857-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 39821
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:03:10,722761951-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 39821
[1] 19:03:10 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:03:10,905535834-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16KB_write.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16KB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:03:11,074872418-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:03:35,065877249-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 47.88k objects, 748 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:03:35,072950868-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:03:44,105421251-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 47.88k objects, 748 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:03:44,112444264-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:03:53,338422385-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 47.88k objects, 748 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:03:53,345331864-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:04:02,346772905-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 47.88k objects, 748 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:04:02,353651356-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:04:11,445162324-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 47.88k objects, 748 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:04:11,452462099-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:04:11,457819114-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:04:11,461142085-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:04:11,467434211-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:04:11,472125411-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=1027026
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:04:11,478742911-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:04:11,487313280-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'40607\n'
[1] 19:04:11 [SUCCESS] ljishen@10.10.2.2
40607

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:04:11,677608035-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16KB_seq.log.2
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:04:11,697248544-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:04:11,700316926-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '786109a0-385b-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 786109a0-385b-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T02:04:14.703629+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T02:04:14.703629+0000     0       0         0         0         0         0           -           0
2021-10-29T02:04:15.703782+0000     1     128      2422      2294   35.8357   35.8438   0.0661629   0.0537785
2021-10-29T02:04:16.703886+0000     2     128      4798      4670   36.4784    37.125    0.194343    0.052827
2021-10-29T02:04:17.703986+0000     3     128      7838      7710   40.1505      47.5   0.0150468    0.049356
2021-10-29T02:04:18.704098+0000     4     128     10352     10224   39.9321   39.2812   0.0994759   0.0494307
2021-10-29T02:04:19.704211+0000     5     128     13425     13297   41.5477   48.0156   0.0461546   0.0479859
2021-10-29T02:04:20.704324+0000     6     128     15818     15690   40.8542   37.3906   0.0317535   0.0487671
2021-10-29T02:04:21.704423+0000     7     128     19125     18997   42.3988   51.6719   0.0762676   0.0470205
2021-10-29T02:04:22.704537+0000     8     128     21269     21141    41.286      33.5   0.0234443   0.0483331
2021-10-29T02:04:23.704651+0000     9     128     24284     24156   41.9324   47.1094   0.0327176   0.0475799
2021-10-29T02:04:24.704738+0000    10     128     26931     26803   41.8747   41.3594    0.072433   0.0475974
2021-10-29T02:04:25.704836+0000    11     128     29782     29654   42.1173   44.5469    0.059167   0.0472057
2021-10-29T02:04:26.704953+0000    12     128     33117     32989   42.9494   52.1094   0.0231101   0.0460535
2021-10-29T02:04:27.705072+0000    13     128     36812     36684   44.0862   57.7344   0.0207223   0.0453131
2021-10-29T02:04:28.705187+0000    14     128     40181     40053   44.6968   52.6406   0.0383571   0.0446724
2021-10-29T02:04:29.705282+0000    15     128     43100     42972   44.7574   45.6094   0.0191487    0.044585
2021-10-29T02:04:30.705396+0000    16     128     45572     45444   44.3738    38.625   0.0414573   0.0450043
2021-10-29T02:04:31.705537+0000 Total time run:       16.7392
Total reads made:     47875
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   44.6885
Average IOPS:         2860
Stddev IOPS:          453.273
Max IOPS:             3695
Min IOPS:             2144
Average Latency(s):   0.0446958
Max latency(s):       0.359796
Min latency(s):       0.000332807

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:04:32,388019560-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:04:32,393366324-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 1027026

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:04:32,398950296-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 40607
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:04:32,406773948-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 40607
[1] 19:04:32 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:04:32,588937652-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16KB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16KB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:04:32,741168152-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:04:56,611577901-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 47.88k objects, 748 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:04:56,618852128-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:05:05,983764740-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 47.88k objects, 748 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:05:05,990955290-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:05:15,075535816-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 47.88k objects, 748 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:05:15,083078429-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:05:24,115093103-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 47.88k objects, 748 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:05:24,122210244-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:05:33,198607628-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 47.88k objects, 748 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:05:33,205963438-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:05:33,211449286-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T19:05:33,213550315-07:00][RUNNING][ROUND 3/2/21] object_size=16KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:05:33,216778487-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:05:33,225829302-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:05:33,666216267-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/786109a0-385b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 786109a0-385b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:05:33,676386395-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:05:33,679585629-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '786109a0-385b-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 786109a0-385b-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:05:33,688290503-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 786109a0-385b-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 19:05:39 [SUCCESS] 10.10.2.1\n[2] 19:05:39 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:05:39,627979785-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:05:39,638532422-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:05:39,643295456-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:05:39,791739477-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:05:39,796078544-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:05:39,942046549-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:05:40,218358070-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:05:40,222898796-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--713f15bf--1532--45c8--8d7d--1b693f6304e6-osd--block--9951e896--f8f6--47f6--a085--6dfb2aa1c42f (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-713f15bf-1532-45c8-8d7d-1b693f6304e6" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-9951e896-f8f6-47f6-a085-6dfb2aa1c42f"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-713f15bf-1532-45c8-8d7d-1b693f6304e6" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-9951e896-f8f6-47f6-a085-6dfb2aa1c42f" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-713f15bf-1532-45c8-8d7d-1b693f6304e6"\n'
10.10.2.1: b'  Volume group "ceph-713f15bf-1532-45c8-8d7d-1b693f6304e6" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:05:40,536484505-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:05:40,546228853-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:05:40,549565145-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: b7742810-385c-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\nWaiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\n'
10.10.2.1: b'Waiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\nAdding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid b7742810-385c-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\nPlease consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:06:45,159976666-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:07:05,167206639-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:07:05,176693593-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:07:05,180304751-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid b7742810-385c-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/b7742810-385c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:07:14,806167443-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:07:14,815454120-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:07:14,818623989-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid b7742810-385c-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/b7742810-385c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:07:24,645796387-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:07:24,651470393-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:07:24,856705361-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:07:24,860380720-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid b7742810-385c-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/b7742810-385c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:07:34,575210399-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:07:54,580240352-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:07:54,586606799-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:07:54,596734147-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:07:54,600584535-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid b7742810-385c-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/b7742810-385c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:08:20,300230538-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:08:40,305965421-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:08:40,316397512-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:08:40,319929000-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid b7742810-385c-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/b7742810-385c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     b7742810-385c-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.jnvebj(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 41s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 19:08:48 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:05:33,666216267-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/786109a0-385b-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 786109a0-385b-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:05:33,676386395-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:05:33,679585629-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '786109a0-385b-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 786109a0-385b-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:05:33,688290503-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 786109a0-385b-11ec-b51d-53e6e728d2d3'
[1] 19:05:39 [SUCCESS] 10.10.2.1
[2] 19:05:39 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:05:39,627979785-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:05:39,638532422-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:05:39,643295456-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:05:39,791739477-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:05:39,796078544-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:05:39,942046549-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:05:40,218358070-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:05:40,222898796-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--713f15bf--1532--45c8--8d7d--1b693f6304e6-osd--block--9951e896--f8f6--47f6--a085--6dfb2aa1c42f (253:0)
  Archiving volume group "ceph-713f15bf-1532-45c8-8d7d-1b693f6304e6" metadata (seqno 5).
  Releasing logical volume "osd-block-9951e896-f8f6-47f6-a085-6dfb2aa1c42f"
  Creating volume group backup "/etc/lvm/backup/ceph-713f15bf-1532-45c8-8d7d-1b693f6304e6" (seqno 6).
  Logical volume "osd-block-9951e896-f8f6-47f6-a085-6dfb2aa1c42f" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-713f15bf-1532-45c8-8d7d-1b693f6304e6"
  Volume group "ceph-713f15bf-1532-45c8-8d7d-1b693f6304e6" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:05:40,536484505-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:05:40,546228853-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:05:40,549565145-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: b7742810-385c-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid b7742810-385c-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:06:45,159976666-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:07:05,167206639-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:07:05,176693593-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:07:05,180304751-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid b7742810-385c-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/b7742810-385c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:07:14,806167443-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:07:14,815454120-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:07:14,818623989-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid b7742810-385c-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/b7742810-385c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:07:24,645796387-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:07:24,651470393-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:07:24,856705361-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:07:24,860380720-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid b7742810-385c-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/b7742810-385c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:07:34,575210399-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:07:54,580240352-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:07:54,586606799-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:07:54,596734147-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:07:54,600584535-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid b7742810-385c-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/b7742810-385c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:08:20,300230538-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:08:40,305965421-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:08:40,316397512-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:08:40,319929000-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid b7742810-385c-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/b7742810-385c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     b7742810-385c-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.jnvebj(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 41s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:08:48,953118259-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:08:48,960903519-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 19:08:49 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:08:49,436570512-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:08:49,439679360-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:08:49,460991619-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:08:49,463773310-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b7742810-385c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b7742810-385c-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:08:53,383721870-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:08:53,386759163-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b7742810-385c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b7742810-385c-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:08:57,612983579-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:08:57,615966029-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b7742810-385c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b7742810-385c-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:09:01,644725100-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:09:01,647944486-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b7742810-385c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b7742810-385c-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:09:09,673283061-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:09:09,676375057-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b7742810-385c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b7742810-385c-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:09:14,828663462-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:09:14,831955966-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b7742810-385c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b7742810-385c-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:09:19,079480637-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:09:19,082486601-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b7742810-385c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b7742810-385c-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:09:23,845924090-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:09:23,849113539-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b7742810-385c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b7742810-385c-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:09:28,564523299-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:09:28,567489168-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b7742810-385c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b7742810-385c-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:09:32,803795649-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:09:32,806936377-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b7742810-385c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b7742810-385c-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:09:37,872387301-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:09:37,875274060-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b7742810-385c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b7742810-385c-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 18 lfor 0/0/15 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:09:41,871626554-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:09:41,874646224-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b7742810-385c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b7742810-385c-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default   
-3         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host node-1
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0  
                       TOTAL  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:09:45,791085056-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:10:09,815430521-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:10:18,905039556-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:10:27,954278399-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:10:27,961331659-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:10:37,144511046-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:10:37,151442427-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:10:46,151192705-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:10:46,158550299-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:10:55,184235546-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:10:55,191191954-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:11:04,341691446-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:11:04,348908706-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:11:04,354480595-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:11:04,357961023-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:11:04,363936743-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:11:04,368930223-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=1034513
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:11:04,375089348-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:11:04,383818016-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'46436\n'
[1] 19:11:04 [SUCCESS] ljishen@10.10.2.2
46436

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:11:04,569611157-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16KB_write.log.3
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:11:04,589715459-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:11:04,592634039-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b7742810-385c-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b7742810-385c-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T02:11:07.678799+0000 Maintaining 128 concurrent writes of 16384 bytes to objects of size 16384 for up to 60 seconds or 0 objects
2021-10-29T02:11:07.678811+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-29T02:11:07.679855+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T02:11:07.679855+0000     0       0         0         0         0         0           -           0
2021-10-29T02:11:08.680007+0000     1     128      1318      1190    18.592   18.5938   0.0917585    0.103622
2021-10-29T02:11:09.680094+0000     2     128      2342      2214   17.2953        16    0.108301    0.111065
2021-10-29T02:11:10.680217+0000     3     128      3366      3238   16.8629        16    0.117072    0.116553
2021-10-29T02:11:11.680290+0000     4     128      4518      4390   17.1468        18    0.108203    0.115547
2021-10-29T02:11:12.680401+0000     5     127      5614      5487   17.1452   17.1406    0.100007    0.115936
2021-10-29T02:11:13.680486+0000     6     128      6530      6402   16.6703   14.2969    0.108609    0.119274
2021-10-29T02:11:14.680600+0000     7     128      7462      7334   16.3689   14.5625     0.11674    0.121126
2021-10-29T02:11:15.680712+0000     8     128      8578      8450   16.5023   17.4375    0.116874    0.120286
2021-10-29T02:11:16.680778+0000     9     128      9730      9602   16.6685        18    0.108564    0.119296
2021-10-29T02:11:17.680861+0000    10     128     10882     10754   16.8015        18    0.108549    0.118421
2021-10-29T02:11:18.680974+0000    11     128     11686     11558    16.416   12.5625    0.116763    0.120881
2021-10-29T02:11:19.681085+0000    12     128     12710     12582   16.3812        16   0.0995565    0.121362
2021-10-29T02:11:20.681196+0000    13     128     13734     13606   16.3518        16   0.0998574    0.121477
2021-10-29T02:11:21.681262+0000    14     128     14886     14758   16.4694        18    0.147469    0.120768
2021-10-29T02:11:22.681343+0000    15     128     15910     15782    16.438        16    0.108779    0.121361
2021-10-29T02:11:23.681468+0000    16     128     16770     16642   16.2504   13.4375    0.250376    0.121968
2021-10-29T02:11:24.681580+0000    17     128     17702     17574    16.151   14.5625    0.107723    0.123372
2021-10-29T02:11:25.681648+0000    18     128     18854     18726   16.2536        18     0.10925    0.122792
2021-10-29T02:11:26.681717+0000    19     128     19842     19714   16.2106   15.4375    0.117008    0.122992
2021-10-29T02:11:27.681832+0000 min lat: 0.0482642 max lat: 0.350593 avg lat: 0.123299
2021-10-29T02:11:27.681832+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T02:11:27.681832+0000    20     128     20866     20738      16.2        16    0.108752    0.123299
2021-10-29T02:11:28.681919+0000    21     128     21890     21762   16.1904        16    0.117666    0.123334
2021-10-29T02:11:29.682003+0000    22     128     22786     22658   16.0908        14     0.10828    0.124164
2021-10-29T02:11:30.682083+0000    23     127     23731     23604   16.0338   14.7812    0.125175    0.124219
2021-10-29T02:11:31.682156+0000    24     128     24742     24614   16.0232   15.7812    0.108592    0.123807
2021-10-29T02:11:32.682232+0000    25     128     25986     25858   16.1598   19.4375    0.108464      0.1236
2021-10-29T02:11:33.682329+0000    26     128     26882     26754   16.0766        14    0.116872    0.124276
2021-10-29T02:11:34.682411+0000    27     128     27522     27394   15.8515        10    0.244799    0.124956
2021-10-29T02:11:35.682525+0000    28     128     28582     28454   15.8769   16.5625    0.109036    0.125661
2021-10-29T02:11:36.682596+0000    29     128     29606     29478   15.8811        16     0.10823    0.125568
2021-10-29T02:11:37.682696+0000    30     128     30758     30630   15.9516        18    0.125351    0.125081
2021-10-29T02:11:38.682791+0000    31     128     31362     31234   15.7415    9.4375    0.343706    0.126645
2021-10-29T02:11:39.682912+0000    32     128     31654     31526   15.3921    4.5625    0.420862    0.128842
2021-10-29T02:11:40.682991+0000    33     128     32002     31874   15.0904    5.4375    0.373251    0.131684
2021-10-29T02:11:41.683059+0000    34     128     32386     32258   14.8231         6    0.304655    0.134491
2021-10-29T02:11:42.683141+0000    35     128     32770     32642    14.571         6    0.322815    0.136751
2021-10-29T02:11:43.683263+0000    36     128     33062     32934   14.2929    4.5625    0.324961    0.138805
2021-10-29T02:11:44.683346+0000    37     128     33410     33282   14.0536    5.4375    0.606492    0.141662
2021-10-29T02:11:45.683434+0000    38     128     33702     33574   13.8038    4.5625    0.377034     0.14427
2021-10-29T02:11:46.683508+0000    39     128     34086     33958   13.6037         6    0.291792    0.146474
2021-10-29T02:11:47.683621+0000 min lat: 0.0482642 max lat: 0.60699 avg lat: 0.14853
2021-10-29T02:11:47.683621+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T02:11:47.683621+0000    40     128     34434     34306   13.3995    5.4375    0.382979     0.14853
2021-10-29T02:11:48.683705+0000    41     128     34818     34690   13.2191         6    0.377428    0.150997
2021-10-29T02:11:49.683789+0000    42     128     35202     35074   13.0472         6    0.401475    0.152866
2021-10-29T02:11:50.683871+0000    43     128     35586     35458   12.8833         6    0.265089    0.155033
2021-10-29T02:11:51.683943+0000    44     128     35878     35750   12.6941    4.5625    0.303415     0.15714
2021-10-29T02:11:52.684016+0000    45     128     36134     36006   12.5009         4    0.496694    0.158979
2021-10-29T02:11:53.684105+0000    46     128     36610     36482   12.3909    7.4375    0.319593    0.161172
2021-10-29T02:11:54.684187+0000    47     128     36902     36774   12.2243    4.5625    0.304157    0.162586
2021-10-29T02:11:55.684299+0000    48     128     37286     37158   12.0946         6    0.287466    0.164664
2021-10-29T02:11:56.684368+0000    49     128     37634     37506   11.9587    5.4375    0.565798    0.166736
2021-10-29T02:11:57.684456+0000    50     128     38018     37890   11.8395         6    0.303234    0.168702
2021-10-29T02:11:58.684572+0000    51     128     38822     38694   11.8537   12.5625    0.117236    0.168567
2021-10-29T02:11:59.684686+0000    52     128     39938     39810    11.961   17.4375      0.1256     0.16714
2021-10-29T02:12:00.684805+0000    53     128     40962     40834   12.0372        16    0.150674    0.165865
2021-10-29T02:12:01.684877+0000    54     128     41986     41858   12.1106        16    0.116996    0.164902
2021-10-29T02:12:02.684963+0000    55     128     42882     42754   12.1449        14    0.100444    0.164554
2021-10-29T02:12:03.685115+0000    56     128     44070     43942   12.2595   18.5625    0.108386    0.162933
2021-10-29T02:12:04.685230+0000    57     128     45222     45094   12.3601        18   0.0998388    0.161622
2021-10-29T02:12:05.685313+0000    58     128     46246     46118   12.4229        16    0.108514    0.160816
2021-10-29T02:12:06.685379+0000    59     128     47234     47106    12.474   15.4375    0.108172    0.160169
2021-10-29T02:12:07.685492+0000 min lat: 0.0482642 max lat: 0.60699 avg lat: 0.159511
2021-10-29T02:12:07.685492+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T02:12:07.685492+0000    60     128     48258     48130   12.5327        16     0.10022    0.159511
2021-10-29T02:12:08.685646+0000 Total time run:         60.0883
Total writes made:      48258
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     12.5487
Stddev Bandwidth:       5.18495
Max bandwidth (MB/sec): 19.4375
Min bandwidth (MB/sec): 4
Average IOPS:           803
Stddev IOPS:            331.837
Max IOPS:               1244
Min IOPS:               256
Average Latency(s):     0.159342
Stddev Latency(s):      0.0955661
Max latency(s):         0.60699
Min latency(s):         0.0482642

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:12:09,414027893-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:12:09,419153481-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 1034513

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:12:09,424584044-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 46436
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:12:09,432016790-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 46436
[1] 19:12:09 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:12:09,612511777-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16KB_write.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16KB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_write.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:12:09,783309733-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:12:33,678724486-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 48.26k objects, 754 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:12:33,685738332-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:12:42,766783935-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 48.26k objects, 754 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:12:42,773533844-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:12:51,700032522-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 48.26k objects, 754 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:12:51,707214385-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:13:00,605612541-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 48.26k objects, 754 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:13:00,612603324-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:13:09,687785574-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 48.26k objects, 754 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:13:09,694907153-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:13:09,700407417-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:13:09,703801443-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:13:09,709868014-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:13:09,715174573-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=1036402
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:13:09,721923670-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:13:09,730777103-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'47164\n'
[1] 19:13:09 [SUCCESS] ljishen@10.10.2.2
47164

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:13:09,913700035-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16KB_seq.log.3
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:13:09,933252067-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:13:09,936055850-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b7742810-385c-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b7742810-385c-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T02:13:12.943504+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T02:13:12.943504+0000     0       0         0         0         0         0           -           0
2021-10-29T02:13:13.943632+0000     1     128      3667      3539    55.286   55.2969   0.0111916   0.0339438
2021-10-29T02:13:14.943749+0000     2     128      6723      6595   51.5154     47.75    0.041748   0.0384361
2021-10-29T02:13:15.943868+0000     3     128      8974      8846   46.0663   35.1719   0.0510457   0.0431768
2021-10-29T02:13:16.943940+0000     4     128     11770     11642   45.4708   43.6875    0.110197   0.0435535
2021-10-29T02:13:17.944045+0000     5     128     14802     14674   45.8507    47.375     0.10116   0.0433282
2021-10-29T02:13:18.944156+0000     6     128     17868     17740   46.1924   47.9062   0.0196547   0.0431408
2021-10-29T02:13:19.944276+0000     7     128     20890     20762   46.3382   47.2188    0.026479     0.04294
2021-10-29T02:13:20.944390+0000     8     128     23207     23079   45.0708   36.2031   0.0525611   0.0442206
2021-10-29T02:13:21.944470+0000     9     128     26559     26431   45.8819    52.375    0.121477   0.0433465
2021-10-29T02:13:22.944554+0000    10     128     29053     28925   45.1903   38.9688  0.00587809   0.0439768
2021-10-29T02:13:23.944636+0000    11     128     32140     32012   45.4666   48.2344    0.020955    0.043938
2021-10-29T02:13:24.944712+0000    12     128     35983     35855   46.6812   60.0469   0.0286813   0.0428004
2021-10-29T02:13:25.944793+0000    13     128     39442     39314   47.2475   54.0469   0.0123557   0.0421832
2021-10-29T02:13:26.944867+0000    14     128     41765     41637   46.4651   36.2969    0.014031   0.0428933
2021-10-29T02:13:27.944941+0000    15     128     44902     44774   46.6349   49.0156   0.0129202   0.0428089
2021-10-29T02:13:28.945014+0000    16     128     47542     47414   46.2982     41.25   0.0056927   0.0430293
2021-10-29T02:13:29.945153+0000 Total time run:       16.2379
Total reads made:     48258
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   46.4366
Average IOPS:         2971
Stddev IOPS:          466.383
Max IOPS:             3843
Min IOPS:             2251
Average Latency(s):   0.0429967
Max latency(s):       0.44181
Min latency(s):       0.000267815

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:13:30,697507342-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:13:30,703048984-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 1036402

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:13:30,708234225-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 47164
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:13:30,715814529-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 47164
[1] 19:13:30 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:13:30,896993414-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16KB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16KB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_16KB_seq.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:13:31,049028925-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:13:55,083941507-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 48.26k objects, 754 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:13:55,091155801-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:14:04,198636882-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 48.26k objects, 754 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:14:04,205880151-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:14:13,371304964-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 48.26k objects, 754 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:14:13,378296618-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:14:22,460522107-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 48.26k objects, 754 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:14:22,467363087-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:14:31,409924556-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 48.26k objects, 754 MiB
    usage:   2.3 GiB used, 398 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:14:31,417470194-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:14:31,423030721-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T19:14:31,426660782-07:00][RUNNING][ROUND 1/3/21] object_size=64KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:14:31,429840604-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:14:31,438796429-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:14:31,915435349-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/b7742810-385c-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid b7742810-385c-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:14:31,925953672-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:14:31,929761972-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'b7742810-385c-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid b7742810-385c-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:14:31,937628911-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid b7742810-385c-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 19:14:37 [SUCCESS] 10.10.2.1\n[2] 19:14:38 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:14:38,238739855-07:00] INFO: > Deploy a new cluster\x1b[0m\n# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:14:38,250847847-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:14:38,255400546-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:14:38,411772322-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:14:38,416702069-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:14:38,562200715-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:14:38,838846416-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:14:38,843639106-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--788498ee--d546--4357--a52b--5256c5f389c6-osd--block--e289eada--4385--4ba6--9f9a--d6c2da17ed34 (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-788498ee-d546-4357-a52b-5256c5f389c6" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-e289eada-4385-4ba6-9f9a-d6c2da17ed34"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-788498ee-d546-4357-a52b-5256c5f389c6" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-e289eada-4385-4ba6-9f9a-d6c2da17ed34" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-788498ee-d546-4357-a52b-5256c5f389c6"\n'
10.10.2.1: b'  Volume group "ceph-788498ee-d546-4357-a52b-5256c5f389c6" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:14:39,164475082-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:14:39,173975091-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:14:39,177156902-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\n'
10.10.2.1: b'Verifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: f8807736-385d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\nWaiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\n'
10.10.2.1: b'Waiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid f8807736-385d-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:15:37,292621902-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:15:57,299821077-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:15:57,309955769-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:15:57,313247588-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid f8807736-385d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/f8807736-385d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:16:06,304398982-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:16:06,313312639-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:16:06,316754830-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid f8807736-385d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/f8807736-385d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:16:15,284787500-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:16:15,290758415-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:16:15,496412827-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:16:15,499871349-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid f8807736-385d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/f8807736-385d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:16:24,382708205-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:16:44,386959705-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:16:44,392579370-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:16:44,402376337-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:16:44,405632278-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid f8807736-385d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/f8807736-385d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:17:10,234531430-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:17:30,240120915-07:00] STAGE: Show Cluster Status\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:17:30,249530524-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:17:30,253005206-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid f8807736-385d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/f8807736-385d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     f8807736-385d-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.wkoamf(active, since 2m)\n    osd: 1 osds: 1 up (since 15s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 19:17:37 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:14:31,915435349-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/b7742810-385c-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid b7742810-385c-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:14:31,925953672-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:14:31,929761972-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'b7742810-385c-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid b7742810-385c-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:14:31,937628911-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid b7742810-385c-11ec-b51d-53e6e728d2d3'
[1] 19:14:37 [SUCCESS] 10.10.2.1
[2] 19:14:38 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:14:38,238739855-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:14:38,250847847-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:14:38,255400546-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:14:38,411772322-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:14:38,416702069-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:14:38,562200715-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:14:38,838846416-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:14:38,843639106-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--788498ee--d546--4357--a52b--5256c5f389c6-osd--block--e289eada--4385--4ba6--9f9a--d6c2da17ed34 (253:0)
  Archiving volume group "ceph-788498ee-d546-4357-a52b-5256c5f389c6" metadata (seqno 5).
  Releasing logical volume "osd-block-e289eada-4385-4ba6-9f9a-d6c2da17ed34"
  Creating volume group backup "/etc/lvm/backup/ceph-788498ee-d546-4357-a52b-5256c5f389c6" (seqno 6).
  Logical volume "osd-block-e289eada-4385-4ba6-9f9a-d6c2da17ed34" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-788498ee-d546-4357-a52b-5256c5f389c6"
  Volume group "ceph-788498ee-d546-4357-a52b-5256c5f389c6" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:14:39,164475082-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:14:39,173975091-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:14:39,177156902-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: f8807736-385d-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid f8807736-385d-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:15:37,292621902-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:15:57,299821077-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:15:57,309955769-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:15:57,313247588-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid f8807736-385d-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/f8807736-385d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:16:06,304398982-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:16:06,313312639-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:16:06,316754830-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid f8807736-385d-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/f8807736-385d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:16:15,284787500-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:16:15,290758415-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:16:15,496412827-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:16:15,499871349-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid f8807736-385d-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/f8807736-385d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:16:24,382708205-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:16:44,386959705-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:16:44,392579370-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:16:44,402376337-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:16:44,405632278-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid f8807736-385d-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/f8807736-385d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:17:10,234531430-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:17:30,240120915-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:17:30,249530524-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:17:30,253005206-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid f8807736-385d-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/f8807736-385d-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     f8807736-385d-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.wkoamf(active, since 2m)
    osd: 1 osds: 1 up (since 15s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:17:37,316002678-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:17:37,323603570-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 19:17:37 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:17:37,804659613-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:17:37,807902664-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:17:37,829426000-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:17:37,832278285-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f8807736-385d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f8807736-385d-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:17:42,073181218-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:17:42,076302619-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f8807736-385d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f8807736-385d-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:17:46,115151392-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:17:46,118150934-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f8807736-385d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f8807736-385d-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:17:50,165342156-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:17:50,168387635-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f8807736-385d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f8807736-385d-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:17:58,141134360-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:17:58,144300506-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f8807736-385d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f8807736-385d-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:18:02,480363477-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:18:02,483220140-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f8807736-385d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f8807736-385d-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:18:06,946754454-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:18:06,949654188-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f8807736-385d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f8807736-385d-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:18:11,355041987-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:18:11,358121971-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f8807736-385d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f8807736-385d-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:18:16,345166584-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:18:16,348238392-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f8807736-385d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f8807736-385d-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:18:20,701706719-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:18:20,704507587-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f8807736-385d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f8807736-385d-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:18:25,135917819-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:18:25,138935926-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f8807736-385d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f8807736-385d-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/17 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 19 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:18:29,417077413-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:18:29,420101482-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f8807736-385d-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f8807736-385d-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.39059         -  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default   
-3         0.39059         -  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host node-1
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0  
                       TOTAL  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:18:33,536116675-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:18:57,391933555-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:19:06,565209222-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:19:15,731063153-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:19:24,804516734-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:19:24,811450659-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:19:33,827360979-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:19:33,834439708-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:19:43,058862656-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:19:43,065948919-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:19:52,065840312-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:19:52,073250825-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:20:01,089609366-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:20:01,096940941-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:20:01,102534220-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:20:01,105745742-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:20:01,111806672-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:20:01,116909989-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=1044549
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:20:01,123301973-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:20:01,131698765-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'52828\n'
[1] 19:20:01 [SUCCESS] ljishen@10.10.2.2
52828

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:20:01,318612546-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_64KB_write.log.1
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:20:01,338970977-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:20:01,341875200-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f8807736-385d-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f8807736-385d-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T02:20:04.426982+0000 Maintaining 128 concurrent writes of 65536 bytes to objects of size 65536 for up to 60 seconds or 0 objects
2021-10-29T02:20:04.426993+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-29T02:20:04.430002+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T02:20:04.430002+0000     0       0         0         0         0         0           -           0
2021-10-29T02:20:05.430107+0000     1     128       570       442   27.6234    27.625    0.217247    0.242142
2021-10-29T02:20:06.430174+0000     2     128      1082       954   29.8107        32    0.233675    0.259737
2021-10-29T02:20:07.430245+0000     3     128      1594      1466   30.5397        32    0.233456     0.24944
2021-10-29T02:20:08.430318+0000     4     128      2106      1978   30.9042        32    0.209535    0.248199
2021-10-29T02:20:09.430387+0000     5     128      2689      2561   32.0103   36.4375    0.192011    0.245564
2021-10-29T02:20:10.430458+0000     6     128      3130      3002   31.2687   27.5625    0.283539    0.249642
2021-10-29T02:20:11.430528+0000     7     128      3642      3514   31.3729        32    0.199398    0.252037
2021-10-29T02:20:12.430595+0000     8     128      4154      4026    31.451        32    0.249097    0.251712
2021-10-29T02:20:13.430669+0000     9     128      4609      4481   31.1159   28.4375    0.191429    0.251962
2021-10-29T02:20:14.430755+0000    10     128      5121      4993   31.2041        32    0.225364    0.253608
2021-10-29T02:20:15.430831+0000    11     128      5633      5505   31.2762        32    0.250321    0.251583
2021-10-29T02:20:16.430904+0000    12     128      6202      6074   31.6332   35.5625    0.216459    0.251066
2021-10-29T02:20:17.430980+0000    13     128      6657      6529   31.3872   28.4375    0.207775    0.251988
2021-10-29T02:20:18.431053+0000    14     128      7169      7041   31.4308        32    0.224633    0.252615
2021-10-29T02:20:19.431121+0000    15     128      7681      7553   31.4686        32    0.209791    0.252873
2021-10-29T02:20:20.431193+0000    16     128      8193      8065   31.5017        32    0.375017    0.252956
2021-10-29T02:20:21.431269+0000    17     128      8321      8193   30.1192         8    0.735505    0.258265
2021-10-29T02:20:22.431339+0000    18     128      8506      8378   29.0882   11.5625    0.921734     0.27059
2021-10-29T02:20:23.431408+0000    19     128      8634      8506   27.9783         8     1.04216    0.282918
2021-10-29T02:20:24.431502+0000 min lat: 0.0956229 max lat: 1.12906 avg lat: 0.287861
2021-10-29T02:20:24.431502+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T02:20:24.431502+0000    20     128      8705      8577   26.8012    4.4375    0.879965    0.287861
2021-10-29T02:20:25.431584+0000    21     128      8833      8705   25.9058         8     1.12396    0.299974
2021-10-29T02:20:26.431668+0000    22     128      8961      8833   25.0919         8    0.980492    0.310467
2021-10-29T02:20:27.431752+0000    23     128      9089      8961   24.3487         8     1.00448    0.319679
2021-10-29T02:20:28.431841+0000    24     128      9217      9089   23.6675         8    0.993315    0.330118
2021-10-29T02:20:29.431923+0000    25     128      9402      9274   23.1833   11.5625    0.782542    0.339657
2021-10-29T02:20:30.432004+0000    26     128      9530      9402   22.5993         8    0.890743    0.346769
2021-10-29T02:20:31.432073+0000    27     128      9729      9601   22.2229   12.4375    0.795728    0.356709
2021-10-29T02:20:32.432140+0000    28     128      9857      9729   21.7149         8     1.03898    0.365505
2021-10-29T02:20:33.432211+0000    29     128      9985      9857    21.242         8    0.880519    0.371779
2021-10-29T02:20:34.432290+0000    30     128     10113      9985   20.8005         8      1.0261    0.381207
2021-10-29T02:20:35.432371+0000    31     128     10241     10113   20.3876         8    0.875641    0.387117
2021-10-29T02:20:36.432434+0000    32     127     10380     10253   20.0239      8.75    0.851343    0.392859
2021-10-29T02:20:37.432503+0000    33     128     10753     10625   20.1216     23.25    0.262968    0.396942
2021-10-29T02:20:38.432581+0000    34     128     11265     11137   20.4709        32    0.257432    0.388956
2021-10-29T02:20:39.432654+0000    35     128     11706     11578   20.6735   27.5625    0.258257    0.385593
2021-10-29T02:20:40.432725+0000    36     128     12218     12090    20.988        32    0.241866    0.380257
2021-10-29T02:20:41.432796+0000    37     128     12801     12673   21.4055   36.4375     0.23309    0.373168
2021-10-29T02:20:42.432866+0000    38     128     13313     13185   21.6842        32    0.209022    0.368173
2021-10-29T02:20:43.432949+0000    39     128     13754     13626   21.8349   27.5625     0.26717    0.364255
2021-10-29T02:20:44.433029+0000 min lat: 0.0956229 max lat: 1.20891 avg lat: 0.361021
2021-10-29T02:20:44.433029+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T02:20:44.433029+0000    40     128     14266     14138    22.089        32     0.21661    0.361021
2021-10-29T02:20:45.433104+0000    41     128     14778     14650   22.3307        32    0.225282    0.357474
2021-10-29T02:20:46.433173+0000    42     128     15290     15162   22.5608        32    0.208688    0.353677
2021-10-29T02:20:47.433244+0000    43     128     15873     15745   22.8835   36.4375    0.208406    0.348802
2021-10-29T02:20:48.433322+0000    44     128     16314     16186   22.9898   27.5625    0.308231    0.346223
2021-10-29T02:20:49.433389+0000    45     128     16442     16314   22.6567         8     1.09303    0.350709
2021-10-29T02:20:50.433457+0000    46     128     16570     16442    22.338         8      1.0169    0.356218
2021-10-29T02:20:51.433524+0000    47     128     16698     16570   22.0329         8    0.930178     0.36069
2021-10-29T02:20:52.433611+0000    48     128     16769     16641   21.6664    4.4375      1.0525    0.363642
2021-10-29T02:20:53.433692+0000    49     128     16769     16641   21.2242         0           -    0.363642
2021-10-29T02:20:54.433804+0000    50     128     16826     16698   20.8709   1.78125     2.29183    0.370225
2021-10-29T02:20:55.433920+0000    51     128     16826     16698   20.4617         0           -    0.370225
2021-10-29T02:20:56.434029+0000    52     128     16897     16769   20.1535   2.21875     3.46577    0.383331
2021-10-29T02:20:57.434104+0000    53     128     16954     16826   19.8405    3.5625     3.40448    0.393565
2021-10-29T02:20:58.434221+0000    54     128     16954     16826    19.473         0           -    0.393565
2021-10-29T02:20:59.434319+0000    55     128     17025     16897   19.1996   2.21875     3.33612     0.40593
2021-10-29T02:21:00.434404+0000    56     128     17082     16954   18.9204    3.5625     3.10927    0.415018
2021-10-29T02:21:01.434479+0000    57     128     17082     16954   18.5885         0           -    0.415018
2021-10-29T02:21:02.434576+0000    58     128     17153     17025   18.3445   2.21875     3.05476    0.426026
2021-10-29T02:21:03.434650+0000    59     128     17210     17082   18.0939    3.5625     3.15464    0.435131
2021-10-29T02:21:04.434772+0000 min lat: 0.0956229 max lat: 3.46614 avg lat: 0.435131
2021-10-29T02:21:04.434772+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T02:21:04.434772+0000    60     128     17210     17082   17.7923         0           -    0.435131
2021-10-29T02:21:05.434860+0000    61      57     17210     17153   17.5734   2.21875     3.05535    0.445979
2021-10-29T02:21:06.434960+0000    62      57     17210     17153     17.29         0           -    0.445979
2021-10-29T02:21:07.435100+0000 Total time run:         62.0948
Total writes made:      17210
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     17.3223
Stddev Bandwidth:       13.3772
Max bandwidth (MB/sec): 36.4375
Min bandwidth (MB/sec): 0
Average IOPS:           277
Stddev IOPS:            214.081
Max IOPS:               583
Min IOPS:               0
Average Latency(s):     0.455106
Stddev Latency(s):      0.559628
Max latency(s):         3.46614
Min latency(s):         0.0956229

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:21:08,560548645-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:21:08,565587009-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 1044549

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:21:08,570975362-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 52828
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:21:08,578158367-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 52828
[1] 19:21:08 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:21:08,761245196-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_64KB_write.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_64KB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:21:08,931563526-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:21:33,022876365-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.21k objects, 1.1 GiB
    usage:   4.6 GiB used, 395 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:21:33,029682450-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:21:42,137922647-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.21k objects, 1.1 GiB
    usage:   3.3 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:21:42,145450201-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:21:51,194590946-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.21k objects, 1.1 GiB
    usage:   3.2 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:21:51,201848130-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:22:00,384762523-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.21k objects, 1.1 GiB
    usage:   3.2 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:22:00,392404633-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:22:09,348505781-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.21k objects, 1.1 GiB
    usage:   3.2 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:22:09,355810415-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:22:09,361231279-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:22:09,364597562-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:22:09,370756127-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:22:09,375725901-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=1046565
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:22:09,382152561-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:22:09,390863234-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'53602\n'
[1] 19:22:09 [SUCCESS] ljishen@10.10.2.2
53602

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:22:09,578185725-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_64KB_seq.log.1
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:22:09,597877911-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:22:09,600660484-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f8807736-385d-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f8807736-385d-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T02:22:12.782495+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T02:22:12.782495+0000     0       0         0         0         0         0           -           0
2021-10-29T02:22:13.782623+0000     1     128      1422      1294   80.8579    80.875     0.10328   0.0898404
2021-10-29T02:22:14.782735+0000     2     128      2648      2520   78.7373    76.625    0.107835   0.0996121
2021-10-29T02:22:15.782846+0000     3     128      4018      3890   81.0299    85.625    0.155408   0.0956359
2021-10-29T02:22:16.782956+0000     4     128      5405      5277   82.4419   86.6875   0.0681379   0.0962603
2021-10-29T02:22:17.783042+0000     5     128      6613      6485   81.0523      75.5    0.125062   0.0967791
2021-10-29T02:22:18.783165+0000     6     127      7906      7779   81.0211    80.875    0.165076   0.0975466
2021-10-29T02:22:19.783235+0000     7     128      9055      8927    79.696     71.75   0.0646787   0.0996801
2021-10-29T02:22:20.783331+0000     8     128     10172     10044   78.4597   69.8125     0.16557    0.101204
2021-10-29T02:22:21.783421+0000     9     128     11440     11312   78.5467     79.25    0.082305     0.10076
2021-10-29T02:22:22.783531+0000    10     128     12874     12746   79.6536    89.625    0.100693    0.100032
2021-10-29T02:22:23.783642+0000    11     128     14126     13998   79.5252     78.25     0.10233    0.100037
2021-10-29T02:22:24.783760+0000    12     127     15327     15200   79.1578    75.125   0.0519792    0.100502
2021-10-29T02:22:25.783857+0000    13     128     16617     16489   79.2652   80.5625   0.0645794    0.100347
2021-10-29T02:22:26.783992+0000 Total time run:       13.7047
Total reads made:     17210
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   78.4859
Average IOPS:         1255
Stddev IOPS:          91.7828
Max IOPS:             1434
Min IOPS:             1117
Average Latency(s):   0.101476
Max latency(s):       0.549759
Min latency(s):       0.00286707

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:22:27,435201109-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:22:27,440322790-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 1046565

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:22:27,445717145-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 53602
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:22:27,453673637-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 53602
[1] 19:22:27 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:22:27,632963011-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_64KB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_64KB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:22:27,788944044-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:22:51,713101496-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.21k objects, 1.1 GiB
    usage:   3.2 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:22:51,720578215-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:23:00,695027248-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.21k objects, 1.1 GiB
    usage:   3.2 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:23:00,702208079-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:23:09,777859746-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.21k objects, 1.1 GiB
    usage:   3.2 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:23:09,784949676-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:23:18,790778916-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.21k objects, 1.1 GiB
    usage:   3.2 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:23:18,798080975-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:23:27,839544058-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.21k objects, 1.1 GiB
    usage:   3.2 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:23:27,846840176-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:23:27,852496264-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T19:23:27,854735694-07:00][RUNNING][ROUND 2/3/21] object_size=64KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:23:27,857986189-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:23:27,867090604-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:23:28,347121267-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/f8807736-385d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid f8807736-385d-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:23:28,357839296-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:23:28,361489598-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'f8807736-385d-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid f8807736-385d-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:23:28,369714840-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid f8807736-385d-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 19:23:33 [SUCCESS] 10.10.2.1\n[2] 19:23:34 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:23:34,943996259-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:23:34,956311741-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:23:34,961261636-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:23:35,111615075-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:23:35,116647685-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:23:35,262051649-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:23:35,543189099-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:23:35,548198396-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--12fb2f74--c994--419f--be34--8cc55c34d820-osd--block--7f816074--92fa--4635--8caa--d21526e75ae3 (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-12fb2f74-c994-419f-be34-8cc55c34d820" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-7f816074-92fa-4635-8caa-d21526e75ae3"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-12fb2f74-c994-419f-be34-8cc55c34d820" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-7f816074-92fa-4635-8caa-d21526e75ae3" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-12fb2f74-c994-419f-be34-8cc55c34d820"\n'
10.10.2.1: b'  Volume group "ceph-12fb2f74-c994-419f-be34-8cc55c34d820" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:23:35,884452067-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:23:35,894649848-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:23:35,898779241-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n"
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\npodman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 38699318-385f-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 38699318-385f-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\nPlease consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:24:33,792961324-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:24:53,799770024-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:24:53,809594654-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:24:53,813099773-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 38699318-385f-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/38699318-385f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:25:02,772655956-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:25:02,781898752-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:25:02,785534857-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 38699318-385f-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/38699318-385f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:25:11,608454044-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:25:11,614534344-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:25:11,825668564-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:25:11,829426068-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid 38699318-385f-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/38699318-385f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:25:20,763929758-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:25:40,768644505-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:25:40,775392470-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:25:40,784825123-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:25:40,788829611-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 38699318-385f-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/38699318-385f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:26:06,372697049-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:26:26,378586056-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:26:26,389081817-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:26:26,392901558-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 38699318-385f-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/38699318-385f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     38699318-385f-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.maadlw(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 41s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 19:26:34 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:23:28,347121267-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/f8807736-385d-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid f8807736-385d-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:23:28,357839296-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:23:28,361489598-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'f8807736-385d-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid f8807736-385d-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:23:28,369714840-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid f8807736-385d-11ec-b51d-53e6e728d2d3'
[1] 19:23:33 [SUCCESS] 10.10.2.1
[2] 19:23:34 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:23:34,943996259-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:23:34,956311741-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:23:34,961261636-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:23:35,111615075-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:23:35,116647685-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:23:35,262051649-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:23:35,543189099-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:23:35,548198396-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--12fb2f74--c994--419f--be34--8cc55c34d820-osd--block--7f816074--92fa--4635--8caa--d21526e75ae3 (253:0)
  Archiving volume group "ceph-12fb2f74-c994-419f-be34-8cc55c34d820" metadata (seqno 5).
  Releasing logical volume "osd-block-7f816074-92fa-4635-8caa-d21526e75ae3"
  Creating volume group backup "/etc/lvm/backup/ceph-12fb2f74-c994-419f-be34-8cc55c34d820" (seqno 6).
  Logical volume "osd-block-7f816074-92fa-4635-8caa-d21526e75ae3" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-12fb2f74-c994-419f-be34-8cc55c34d820"
  Volume group "ceph-12fb2f74-c994-419f-be34-8cc55c34d820" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:23:35,884452067-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:23:35,894649848-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:23:35,898779241-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 38699318-385f-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 38699318-385f-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:24:33,792961324-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:24:53,799770024-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:24:53,809594654-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:24:53,813099773-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 38699318-385f-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/38699318-385f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:25:02,772655956-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:25:02,781898752-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:25:02,785534857-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 38699318-385f-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/38699318-385f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:25:11,608454044-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:25:11,614534344-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:25:11,825668564-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:25:11,829426068-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 38699318-385f-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/38699318-385f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:25:20,763929758-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:25:40,768644505-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:25:40,775392470-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:25:40,784825123-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:25:40,788829611-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 38699318-385f-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/38699318-385f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:26:06,372697049-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:26:26,378586056-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:26:26,389081817-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:26:26,392901558-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 38699318-385f-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/38699318-385f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     38699318-385f-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.maadlw(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 41s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:26:34,925824658-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:26:34,933528177-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 19:26:35 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:26:35,408657868-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:26:35,412000649-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:26:35,433834698-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:26:35,436704266-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '38699318-385f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 38699318-385f-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:26:39,619199596-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:26:39,622140518-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '38699318-385f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 38699318-385f-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:26:43,876581458-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:26:43,879533311-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '38699318-385f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 38699318-385f-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:26:47,988260471-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:26:47,991207635-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '38699318-385f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 38699318-385f-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:26:55,851378358-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:26:55,854618675-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '38699318-385f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 38699318-385f-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:27:00,902159603-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:27:00,905348763-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '38699318-385f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 38699318-385f-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:27:05,232535567-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:27:05,235397211-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '38699318-385f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 38699318-385f-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:27:09,964776353-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:27:09,967757332-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '38699318-385f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 38699318-385f-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:27:14,956767916-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:27:14,959768551-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '38699318-385f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 38699318-385f-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:27:19,431500998-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:27:19,434445808-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '38699318-385f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 38699318-385f-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:27:24,079443938-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:27:24,082567435-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '38699318-385f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 38699318-385f-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 21 lfor 0/0/17 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:27:28,130594997-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:27:28,133865781-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '38699318-385f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 38699318-385f-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.39059         -  400 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default   
-3         0.39059         -  400 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host node-1
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0  
                       TOTAL  400 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  400 GiB  0.00                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:27:32,133024040-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:27:56,171037163-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:28:05,062204991-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:28:14,052633394-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:28:23,109314343-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:28:23,116199930-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:28:32,125681941-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:28:32,132808332-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:28:41,174841054-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:28:41,182007741-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:28:50,242507913-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:28:50,249553711-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:28:59,324795771-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:28:59,332058779-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:28:59,337791242-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:28:59,341272002-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:28:59,347466356-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:28:59,352772226-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=1054660
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:28:59,359224837-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:28:59,367804847-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'59241\n'
[1] 19:28:59 [SUCCESS] ljishen@10.10.2.2
59241

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:28:59,558736975-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_64KB_write.log.2
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:28:59,579101254-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:28:59,582051334-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '38699318-385f-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 38699318-385f-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T02:29:02.696851+0000 Maintaining 128 concurrent writes of 65536 bytes to objects of size 65536 for up to 60 seconds or 0 objects
2021-10-29T02:29:02.696864+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-29T02:29:02.699545+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T02:29:02.699545+0000     0       0         0         0         0         0           -           0
2021-10-29T02:29:03.699702+0000     1     128       641       513   32.0594   32.0625    0.185697    0.234653
2021-10-29T02:29:04.699823+0000     2     128      1026       898   28.0594   24.0625    0.276211     0.25542
2021-10-29T02:29:05.699898+0000     3     128      1793      1665   34.6841   47.9375     0.17453    0.229842
2021-10-29T02:29:06.699970+0000     4     128      2310      2182   34.0906   32.3125    0.216572    0.228913
2021-10-29T02:29:07.700044+0000     5     128      2950      2822   35.2719        40    0.166737    0.222703
2021-10-29T02:29:08.700129+0000     6     128      3458      3330   34.6845     31.75    0.207558    0.227888
2021-10-29T02:29:09.700194+0000     7     128      3974      3846   34.3364     32.25    0.216656    0.228609
2021-10-29T02:29:10.700257+0000     8     128      4486      4358   34.0441        32    0.208376    0.229378
2021-10-29T02:29:11.700320+0000     9     128      5126      4998   34.7056        40    0.193651    0.228581
2021-10-29T02:29:12.700382+0000    10     128      5612      5484   34.2723    30.375    0.358577    0.230064
2021-10-29T02:29:13.700448+0000    11     128      6150      6022   34.2133    33.625    0.225655    0.232069
2021-10-29T02:29:14.700523+0000    12     128      6636      6508   33.8932    30.375    0.316618    0.234881
2021-10-29T02:29:15.700587+0000    13     128      7148      7020   33.7475        32    0.191726    0.234859
2021-10-29T02:29:16.700651+0000    14     128      7660      7532   33.6225        32    0.243908    0.235882
2021-10-29T02:29:17.700714+0000    15     128      8172      8044   33.5142        32    0.225178    0.236515
2021-10-29T02:29:18.700779+0000    16     128      8300      8172   31.9195         8    0.798523    0.243561
2021-10-29T02:29:19.700845+0000    17     128      8454      8326   30.6081     9.625    0.938844    0.257661
2021-10-29T02:29:20.700911+0000    18     128      8582      8454    29.352         8    0.868986    0.266686
2021-10-29T02:29:21.700974+0000    19     128      8710      8582   28.2282         8    0.924601    0.277499
2021-10-29T02:29:22.701041+0000 min lat: 0.0911435 max lat: 1.02013 avg lat: 0.284022
2021-10-29T02:29:22.701041+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T02:29:22.701041+0000    20     128      8838      8710   27.2168         8    0.774504    0.284022
2021-10-29T02:29:23.701127+0000    21     128      8966      8838   26.3017         8    0.732971    0.291196
2021-10-29T02:29:24.701230+0000    22     128      9094      8966   25.4697         8    0.982109     0.30107
2021-10-29T02:29:25.701300+0000    23     128      9222      9094   24.7101         8     1.06813    0.310936
2021-10-29T02:29:26.701366+0000    24     128      9350      9222   24.0139         8     1.16005    0.321478
2021-10-29T02:29:27.701435+0000    25     128      9478      9350   23.3733         8     1.04799    0.332898
2021-10-29T02:29:28.701499+0000    26     128      9708      9580   23.0272    14.375     0.82344    0.343961
2021-10-29T02:29:29.701562+0000    27     128      9862      9734   22.5308     9.625    0.845438    0.351529
2021-10-29T02:29:30.701626+0000    28     128      9990      9862   22.0118         8    0.764113    0.357406
2021-10-29T02:29:31.701693+0000    29     128     10118      9990   21.5286         8     1.27407    0.368558
2021-10-29T02:29:32.701755+0000    30     128     10220     10092   21.0235     6.375    0.956693    0.374501
2021-10-29T02:29:33.701820+0000    31     128     10374     10246   20.6558     9.625    0.931958    0.383791
2021-10-29T02:29:34.701889+0000    32     128     10502     10374   20.2603         8    0.771666    0.389059
2021-10-29T02:29:35.701956+0000    33     128     10630     10502   19.8887         8    0.839404    0.394137
2021-10-29T02:29:36.702024+0000    34     128     10860     10732   19.7265    14.375    0.833115      0.4028
2021-10-29T02:29:37.702090+0000    35     128     11372     11244   20.0771        32    0.233048    0.397309
2021-10-29T02:29:38.702160+0000    36     128     11884     11756   20.4083        32    0.233329    0.390913
2021-10-29T02:29:39.702227+0000    37     128     12396     12268   20.7215        32    0.225201    0.385294
2021-10-29T02:29:40.702291+0000    38     128     12806     12678   20.8505    25.625    0.392052    0.381233
2021-10-29T02:29:41.702355+0000    39     128     13420     13292   21.2998    38.375    0.217285    0.374713
2021-10-29T02:29:42.702422+0000 min lat: 0.0911435 max lat: 1.27508 avg lat: 0.369836
2021-10-29T02:29:42.702422+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T02:29:42.702422+0000    40     128     13932     13804   21.5672        32    0.215377    0.369836
2021-10-29T02:29:43.702500+0000    41     128     14353     14225   21.6829   26.3125    0.207733    0.367397
2021-10-29T02:29:44.702568+0000    42     127     14896     14769   21.9761        34    0.208567    0.362988
2021-10-29T02:29:45.702634+0000    43     128     15366     15238   22.1467   29.3125     0.22471    0.359578
2021-10-29T02:29:46.702704+0000    44     128     15878     15750   22.3706        32    0.200734    0.356589
2021-10-29T02:29:47.702771+0000    45     128     16262     16134   22.4068        24    0.217782    0.352972
2021-10-29T02:29:48.702834+0000    46     128     16390     16262   22.0936         8    0.820694    0.355272
2021-10-29T02:29:49.702897+0000    47     128     16518     16390   21.7937         8     1.04659    0.360754
2021-10-29T02:29:50.702960+0000    48     128     16646     16518   21.5063         8     0.78379    0.364717
2021-10-29T02:29:51.703023+0000    49     128     16774     16646   21.2307         8     1.76091    0.372815
2021-10-29T02:29:52.703087+0000    50     128     16774     16646   20.8061         0           -    0.372815
2021-10-29T02:29:53.703155+0000    51     128     16774     16646   20.3981         0           -    0.372815
2021-10-29T02:29:54.703238+0000    52     128     16876     16748   20.1284     2.125     3.27346    0.390475
2021-10-29T02:29:55.703304+0000    53     128     16902     16774   19.7793     1.625     3.34822     0.39506
2021-10-29T02:29:56.703370+0000    54     128     16902     16774    19.413         0           -     0.39506
2021-10-29T02:29:57.703432+0000    55     128     17004     16876   19.1759    3.1875     3.10901    0.411464
2021-10-29T02:29:58.703495+0000    56     128     17030     16902   18.8625     1.625      3.1731    0.415712
2021-10-29T02:29:59.703559+0000    57     128     17030     16902   18.5316         0           -    0.415712
2021-10-29T02:30:00.703624+0000    58     128     17132     17004    18.322    3.1875      3.0227     0.43135
2021-10-29T02:30:01.703694+0000    59     128     17158     17030    18.039     1.625     3.10988    0.435439
2021-10-29T02:30:02.703757+0000 min lat: 0.0911435 max lat: 3.34925 avg lat: 0.435439
2021-10-29T02:30:02.703757+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T02:30:02.703757+0000    60     128     17158     17030   17.7384         0           -    0.435439
2021-10-29T02:30:03.703827+0000    61     128     17158     17030   17.4476         0           -    0.435439
2021-10-29T02:30:04.703898+0000    62      26     17158     17132    17.269     2.125     3.47388    0.453533
2021-10-29T02:30:05.703994+0000 Total time run:         62.0693
Total writes made:      17158
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     17.277
Stddev Bandwidth:       13.8359
Max bandwidth (MB/sec): 47.9375
Min bandwidth (MB/sec): 0
Average IOPS:           276
Stddev IOPS:            221.374
Max IOPS:               767
Min IOPS:               0
Average Latency(s):     0.458083
Stddev Latency(s):      0.560486
Max latency(s):         3.47552
Min latency(s):         0.0911435

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:30:06,737822499-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:30:06,743444495-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 1054660

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:30:06,748991138-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 59241
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:30:06,757000032-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 59241
[1] 19:30:06 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:30:06,941344343-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_64KB_write.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_64KB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:30:07,115922239-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:30:31,116577904-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.16k objects, 1.0 GiB
    usage:   4.5 GiB used, 395 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:30:31,123758427-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:30:40,233899699-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.16k objects, 1.0 GiB
    usage:   3.3 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:30:40,241403431-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:30:49,374021799-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.16k objects, 1.0 GiB
    usage:   3.2 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:30:49,381692305-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:30:58,398398046-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.16k objects, 1.0 GiB
    usage:   3.2 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:30:58,405379884-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:31:07,373916418-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.16k objects, 1.0 GiB
    usage:   3.2 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:31:07,381437843-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:31:07,387010336-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:31:07,390370338-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:31:07,396758117-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:31:07,402084915-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=1056595
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:31:07,408837171-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:31:07,417481632-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'59931\n'
[1] 19:31:07 [SUCCESS] ljishen@10.10.2.2
59931

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:31:07,606226527-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_64KB_seq.log.2
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:31:07,626184570-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:31:07,629112488-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '38699318-385f-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 38699318-385f-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T02:31:10.642211+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T02:31:10.642211+0000     0       0         0         0         0         0           -           0
2021-10-29T02:31:11.642355+0000     1     128      1334      1206   75.3592    75.375   0.0671234   0.0975042
2021-10-29T02:31:12.642466+0000     2     128      2501      2373   74.1443   72.9375    0.124284    0.103351
2021-10-29T02:31:13.642545+0000     3     128      3634      3506   73.0319   70.8125   0.0239387    0.105973
2021-10-29T02:31:14.642665+0000     4     128      4791      4663   72.8499   72.3125   0.0555388    0.107796
2021-10-29T02:31:15.642778+0000     5     128      5843      5715   71.4284     65.75    0.478375    0.106286
2021-10-29T02:31:16.642888+0000     6     128      7267      7139   74.3554        89  0.00366965    0.105448
2021-10-29T02:31:17.642967+0000     7     128      8260      8132   72.5986   62.0625    0.042646    0.109611
2021-10-29T02:31:18.643081+0000     8     128      9364      9236   72.1478        69  0.00222252    0.109016
2021-10-29T02:31:19.643194+0000     9     128     10768     10640   73.8803     87.75    0.401821    0.106246
2021-10-29T02:31:20.643303+0000    10     128     11855     11727   73.2853   67.9375   0.0860909    0.108639
2021-10-29T02:31:21.643380+0000    11     128     12691     12563   71.3727     52.25    0.342011    0.110074
2021-10-29T02:31:22.643491+0000    12     128     13941     13813   71.9346    78.125   0.0147499    0.110225
2021-10-29T02:31:23.643609+0000    13     128     15173     15045   72.3236        77     0.16804    0.110309
2021-10-29T02:31:24.643734+0000    14     128     16396     16268   72.6168   76.4375  0.00292256    0.109198
2021-10-29T02:31:25.643838+0000 Total time run:       14.6806
Total reads made:     17158
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   73.047
Average IOPS:         1168
Stddev IOPS:          152.499
Max IOPS:             1424
Min IOPS:             836
Average Latency(s):   0.108969
Max latency(s):       0.76111
Min latency(s):       0.000563352

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:31:26,283468133-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:31:26,288940927-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 1056595

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:31:26,294457153-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 59931
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:31:26,302040415-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 59931
[1] 19:31:26 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:31:26,485425367-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_64KB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_64KB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:31:26,642511236-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:31:50,696717904-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.16k objects, 1.0 GiB
    usage:   3.2 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:31:50,703693701-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:31:59,711847079-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.16k objects, 1.0 GiB
    usage:   3.2 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:31:59,719403320-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:32:08,903017360-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.16k objects, 1.0 GiB
    usage:   3.2 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:32:08,910380297-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:32:17,996864840-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.16k objects, 1.0 GiB
    usage:   3.2 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:32:18,004285745-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:32:27,067632677-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.16k objects, 1.0 GiB
    usage:   3.2 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:32:27,075063000-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:32:27,080536876-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T19:32:27,082670617-07:00][RUNNING][ROUND 3/3/21] object_size=64KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:32:27,086011544-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:32:27,095100082-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:32:27,488990256-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/38699318-385f-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 38699318-385f-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:32:27,499440452-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:32:27,502761886-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '38699318-385f-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 38699318-385f-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:32:27,510963264-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 38699318-385f-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 19:32:32 [SUCCESS] 10.10.2.1\n[2] 19:32:33 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:32:33,988417919-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:32:33,999563261-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:32:34,004469264-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:32:34,155619622-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:32:34,160231963-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:32:34,310967401-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:32:34,595310847-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:32:34,599835012-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--9c1b25ea--deae--4353--9aa3--41228225212f-osd--block--80072f6f--74dd--423d--9fde--592a0c58519e (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-9c1b25ea-deae-4353-9aa3-41228225212f" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-80072f6f-74dd-423d-9fde-592a0c58519e"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-9c1b25ea-deae-4353-9aa3-41228225212f" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-80072f6f-74dd-423d-9fde-592a0c58519e" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-9c1b25ea-deae-4353-9aa3-41228225212f"\n'
10.10.2.1: b'  Volume group "ceph-9c1b25ea-deae-4353-9aa3-41228225212f" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:32:34,968398329-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:32:34,978216808-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:32:34,981748026-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 79bb998c-3860-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\n'
10.10.2.1: b'Waiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 79bb998c-3860-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:33:32,758310926-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:33:52,765088465-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:33:52,775735160-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:33:52,779129722-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 79bb998c-3860-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/79bb998c-3860-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:34:01,431943157-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:34:01,441528217-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:34:01,444902480-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 79bb998c-3860-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/79bb998c-3860-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:34:10,129614700-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:34:10,134728645-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n"
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:34:10,340459300-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:34:10,343837572-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid 79bb998c-3860-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/79bb998c-3860-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:34:20,046293987-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:34:40,051463529-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:34:40,058079096-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:34:40,067621355-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:34:40,071063857-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 79bb998c-3860-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/79bb998c-3860-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:35:05,738221413-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:35:25,743833186-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:35:25,753915640-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:35:25,757398889-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 79bb998c-3860-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/79bb998c-3860-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     79bb998c-3860-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.eysucd(active, since 2m)\n    osd: 1 osds: 1 up (since 16s), 1 in (since 41s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 19:35:33 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:32:27,488990256-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/38699318-385f-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 38699318-385f-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:32:27,499440452-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:32:27,502761886-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '38699318-385f-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 38699318-385f-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:32:27,510963264-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 38699318-385f-11ec-b51d-53e6e728d2d3'
[1] 19:32:32 [SUCCESS] 10.10.2.1
[2] 19:32:33 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:32:33,988417919-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:32:33,999563261-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:32:34,004469264-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:32:34,155619622-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:32:34,160231963-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:32:34,310967401-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:32:34,595310847-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:32:34,599835012-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--9c1b25ea--deae--4353--9aa3--41228225212f-osd--block--80072f6f--74dd--423d--9fde--592a0c58519e (253:0)
  Archiving volume group "ceph-9c1b25ea-deae-4353-9aa3-41228225212f" metadata (seqno 5).
  Releasing logical volume "osd-block-80072f6f-74dd-423d-9fde-592a0c58519e"
  Creating volume group backup "/etc/lvm/backup/ceph-9c1b25ea-deae-4353-9aa3-41228225212f" (seqno 6).
  Logical volume "osd-block-80072f6f-74dd-423d-9fde-592a0c58519e" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-9c1b25ea-deae-4353-9aa3-41228225212f"
  Volume group "ceph-9c1b25ea-deae-4353-9aa3-41228225212f" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:32:34,968398329-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:32:34,978216808-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:32:34,981748026-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 79bb998c-3860-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 79bb998c-3860-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:33:32,758310926-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:33:52,765088465-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:33:52,775735160-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:33:52,779129722-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 79bb998c-3860-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/79bb998c-3860-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:34:01,431943157-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:34:01,441528217-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:34:01,444902480-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 79bb998c-3860-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/79bb998c-3860-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:34:10,129614700-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:34:10,134728645-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:34:10,340459300-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:34:10,343837572-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 79bb998c-3860-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/79bb998c-3860-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:34:20,046293987-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:34:40,051463529-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:34:40,058079096-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:34:40,067621355-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:34:40,071063857-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 79bb998c-3860-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/79bb998c-3860-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:35:05,738221413-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:35:25,743833186-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:35:25,753915640-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:35:25,757398889-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 79bb998c-3860-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/79bb998c-3860-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     79bb998c-3860-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.eysucd(active, since 2m)
    osd: 1 osds: 1 up (since 16s), 1 in (since 41s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:35:33,992119962-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:35:33,999590572-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 19:35:34 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:35:34,476429649-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:35:34,479782979-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:35:34,501510697-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:35:34,504403840-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '79bb998c-3860-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 79bb998c-3860-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:35:38,440484641-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:35:38,443540790-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '79bb998c-3860-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 79bb998c-3860-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:35:42,549323292-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:35:42,552631256-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '79bb998c-3860-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 79bb998c-3860-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:35:46,707094161-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:35:46,710112229-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '79bb998c-3860-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 79bb998c-3860-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:35:54,875399101-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:35:54,878380590-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '79bb998c-3860-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 79bb998c-3860-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:35:59,176810794-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:35:59,179937987-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '79bb998c-3860-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 79bb998c-3860-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:36:03,715994895-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:36:03,718998946-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '79bb998c-3860-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 79bb998c-3860-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:36:09,067876664-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:36:09,071009528-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '79bb998c-3860-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 79bb998c-3860-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:36:14,050565245-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:36:14,053690294-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '79bb998c-3860-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 79bb998c-3860-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:36:18,577297064-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:36:18,580671293-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '79bb998c-3860-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 79bb998c-3860-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:36:22,983014296-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:36:22,986168651-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '79bb998c-3860-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 79bb998c-3860-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:36:27,057017646-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:36:27,060035624-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '79bb998c-3860-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 79bb998c-3860-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default   
-3         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host node-1
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0  
                       TOTAL  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:36:31,183791357-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:36:55,488508740-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:37:04,646615396-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:37:13,685400794-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:37:22,791694082-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:37:22,799193666-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:37:31,814041812-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:37:31,821674286-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:37:40,862646631-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:37:40,869944294-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:37:49,985714358-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:37:49,992892096-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:37:59,053313295-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:37:59,061088518-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:37:59,066842652-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:37:59,070296982-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:37:59,076937537-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:37:59,081885322-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=1064679
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:37:59,088720583-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:37:59,097665451-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'65490\n'
[1] 19:37:59 [SUCCESS] ljishen@10.10.2.2
65490

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:37:59,286241763-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_64KB_write.log.3
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:37:59,306379014-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:37:59,309188308-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '79bb998c-3860-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 79bb998c-3860-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T02:38:02.453982+0000 Maintaining 128 concurrent writes of 65536 bytes to objects of size 65536 for up to 60 seconds or 0 objects
2021-10-29T02:38:02.453993+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-29T02:38:02.457137+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T02:38:02.457137+0000     0       0         0         0         0         0           -           0
2021-10-29T02:38:03.457291+0000     1     128       512       384   23.9979        24    0.331512    0.286642
2021-10-29T02:38:04.457416+0000     2     128      1153      1025   32.0279   40.0625    0.226321    0.243981
2021-10-29T02:38:05.457507+0000     3     128      1666      1538   32.0384   32.0625    0.183294    0.227587
2021-10-29T02:38:06.457640+0000     4     128      2306      2178   34.0275        40    0.165421    0.228689
2021-10-29T02:38:07.457770+0000     5     128      2818      2690   33.6212        32    0.183408    0.229962
2021-10-29T02:38:08.457885+0000     6     128      3458      3330   34.6836        40    0.266797     0.22707
2021-10-29T02:38:09.457976+0000     7     128      3970      3842   34.2998        32     0.27857    0.226385
2021-10-29T02:38:10.458094+0000     8     128      4610      4482   35.0117        40    0.182694    0.224685
2021-10-29T02:38:11.458213+0000     9     128      5122      4994   34.6767        32    0.158329    0.228941
2021-10-29T02:38:12.458329+0000    10     128      5634      5506   34.4086        32    0.204434    0.225921
2021-10-29T02:38:13.458420+0000    11     128      6274      6146   34.9166        40    0.216646    0.224252
2021-10-29T02:38:14.458537+0000    12     128      6914      6786   35.3398        40    0.249879    0.224765
2021-10-29T02:38:15.458636+0000    13     128      7298      7170   34.4674        24     0.19172    0.230419
2021-10-29T02:38:16.458750+0000    14     128      7932      7804   34.8354    39.625    0.191386    0.229046
2021-10-29T02:38:17.458842+0000    15     128      8194      8066   33.6047    16.375    0.258576    0.229196
2021-10-29T02:38:18.458957+0000    16     128      8322      8194   32.0043         8    0.965751    0.240816
2021-10-29T02:38:19.459065+0000    17     128      8450      8322   30.5922         8    0.810324    0.249349
2021-10-29T02:38:20.459185+0000    18     128      8578      8450   29.3371         8     1.12455    0.258961
2021-10-29T02:38:21.459279+0000    19     128      8706      8578    28.214         8    0.718259    0.269619
2021-10-29T02:38:22.459395+0000 min lat: 0.083044 max lat: 1.12458 avg lat: 0.279518
2021-10-29T02:38:22.459395+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T02:38:22.459395+0000    20     128      8834      8706   27.2033         8    0.964759    0.279518
2021-10-29T02:38:23.459520+0000    21     128      8962      8834   26.2888         8     1.12088    0.290282
2021-10-29T02:38:24.459635+0000    22     128      9090      8962   25.4574         8    0.920701    0.301014
2021-10-29T02:38:25.459720+0000    23     128      9218      9090   24.6984         8     1.07327    0.310191
2021-10-29T02:38:26.459843+0000    24     128      9346      9218   24.0026         8     1.06071    0.322004
2021-10-29T02:38:27.459973+0000    25     128      9602      9474   23.6824        16    0.738519    0.335188
2021-10-29T02:38:28.460090+0000    26     128      9730      9602   23.0792         8    0.791829    0.341288
2021-10-29T02:38:29.460179+0000    27     128      9858      9730   22.5207         8    0.853872    0.348503
2021-10-29T02:38:30.460289+0000    28     128      9986      9858    22.002         8    0.771035    0.353914
2021-10-29T02:38:31.460395+0000    29     128     10114      9986   21.5192         8     1.13305    0.362768
2021-10-29T02:38:32.460481+0000    30     128     10242     10114   21.0685         8      1.1259    0.373631
2021-10-29T02:38:33.460575+0000    31     128     10370     10242   20.6469         8    0.853605    0.378163
2021-10-29T02:38:34.460690+0000    32     128     10620     10492     20.49    15.625    0.798973    0.388694
2021-10-29T02:38:35.460791+0000    33     128     11004     10876   20.5962        24     0.24211    0.387305
2021-10-29T02:38:36.460905+0000    34     128     11522     11394   20.9426    32.375    0.316702    0.380415
2021-10-29T02:38:37.461005+0000    35     128     12034     11906   21.2584        32    0.200281    0.375247
2021-10-29T02:38:38.461119+0000    36     128     12674     12546   21.7789        40    0.200157    0.365889
2021-10-29T02:38:39.461233+0000    37     128     13308     13180   22.2611    39.625    0.182933    0.359284
2021-10-29T02:38:40.461345+0000    38     128     13826     13698   22.5272    32.375    0.249953    0.353271
2021-10-29T02:38:41.461433+0000    39     128     14466     14338   22.9751        40    0.200229    0.346346
2021-10-29T02:38:42.461553+0000 min lat: 0.083044 max lat: 1.22665 avg lat: 0.346725
2021-10-29T02:38:42.461553+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T02:38:42.461553+0000    40     128     14850     14722   23.0006        24     0.21634    0.346725
2021-10-29T02:38:43.461680+0000    41     128     15490     15362   23.4151        40    0.224828    0.340837
2021-10-29T02:38:44.461793+0000    42     128     16124     15996    23.801    39.625     0.25277    0.335916
2021-10-29T02:38:45.461881+0000    43     128     16380     16252   23.6195        16    0.673425    0.337199
2021-10-29T02:38:46.461995+0000    44     128     16514     16386    23.273     8.375    0.790457    0.341242
2021-10-29T02:38:47.462117+0000    45     128     16642     16514   22.9336         8    0.865899     0.34531
2021-10-29T02:38:48.462241+0000    46     128     16764     16636   22.6008     7.625     1.36075    0.352756
2021-10-29T02:38:49.462330+0000    47     128     16770     16642   22.1279     0.375     1.48424    0.353164
2021-10-29T02:38:50.462442+0000    48     128     16770     16642   21.6669         0           -    0.353164
2021-10-29T02:38:51.462556+0000    49     128     16892     16764   21.3803    3.8125      2.9029    0.371721
2021-10-29T02:38:52.462671+0000    50     128     16898     16770   20.9602     0.375     2.87321    0.372616
2021-10-29T02:38:53.462760+0000    51     128     16898     16770   20.5492         0           -    0.372616
2021-10-29T02:38:54.462869+0000    52     128     17026     16898   20.3079         4      2.8493    0.391829
2021-10-29T02:38:55.462984+0000    53     128     17026     16898   19.9247         0           -    0.391829
2021-10-29T02:38:56.463102+0000    54     128     17026     16898   19.5557         0           -    0.391829
2021-10-29T02:38:57.463193+0000    55     128     17148     17020   19.3388   2.54167     3.20108    0.411966
2021-10-29T02:38:58.463305+0000    56     128     17154     17026   19.0002     0.375     3.26842    0.412972
2021-10-29T02:38:59.463418+0000    57     128     17154     17026   18.6668         0           -    0.412972
2021-10-29T02:39:00.463529+0000    58     128     17154     17026    18.345         0           -    0.412972
2021-10-29T02:39:01.463616+0000    59     128     17282     17154   18.1696   2.66667     3.24676    0.434058
2021-10-29T02:39:02.463727+0000 min lat: 0.083044 max lat: 3.26843 avg lat: 0.434058
2021-10-29T02:39:02.463727+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T02:39:02.463727+0000    60     128     17282     17154   17.8668         0           -    0.434058
2021-10-29T02:39:03.463846+0000    61     128     17282     17154   17.5739         0           -    0.434058
2021-10-29T02:39:04.464007+0000 Total time run:         61.6156
Total writes made:      17282
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     17.5301
Stddev Bandwidth:       15.0978
Max bandwidth (MB/sec): 40.0625
Min bandwidth (MB/sec): 0
Average IOPS:           280
Stddev IOPS:            241.587
Max IOPS:               641
Min IOPS:               0
Average Latency(s):     0.455007
Stddev Latency(s):      0.588464
Max latency(s):         3.26843
Min latency(s):         0.083044

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:39:06,246726500-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:39:06,252120065-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 1064679

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:39:06,257614169-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 65490
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:39:06,265137016-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 65490
[1] 19:39:06 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:39:06,450405824-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_64KB_write.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_64KB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_write.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:39:06,623373600-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:39:30,575938329-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.28k objects, 1.1 GiB
    usage:   3.5 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:39:30,583212508-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:39:39,699088917-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.28k objects, 1.1 GiB
    usage:   3.2 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:39:39,706257707-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:39:48,730179152-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.28k objects, 1.1 GiB
    usage:   3.2 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:39:48,737586953-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:39:57,861406332-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.28k objects, 1.1 GiB
    usage:   3.2 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:39:57,868807670-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:40:06,984502433-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.28k objects, 1.1 GiB
    usage:   3.2 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:40:06,992585857-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:40:06,998037661-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:40:07,001461874-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:40:07,007750686-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:40:07,013399853-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=1066685
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:40:07,019988540-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:40:07,029396991-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'66208\n'
[1] 19:40:07 [SUCCESS] ljishen@10.10.2.2
66208

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:40:07,218479806-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_64KB_seq.log.3
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:40:07,238308124-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:40:07,241185217-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '79bb998c-3860-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 79bb998c-3860-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T02:40:10.214099+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T02:40:10.214099+0000     0       0         0         0         0         0           -           0
2021-10-29T02:40:11.214252+0000     1     128      1468      1340    83.731     83.75   0.0170976   0.0867944
2021-10-29T02:40:12.214376+0000     2     128      2688      2560    79.986     76.25    0.131167   0.0964984
2021-10-29T02:40:13.214460+0000     3     128      3797      3669   76.4264   69.3125   0.0506367    0.102898
2021-10-29T02:40:14.214590+0000     4     128      5090      4962   77.5203   80.8125   0.0283465    0.100486
2021-10-29T02:40:15.214690+0000     5     128      6153      6025   75.3025   66.4375   0.0565357    0.102844
2021-10-29T02:40:16.214806+0000     6     128      7359      7231   75.3131    75.375   0.0464487    0.104906
2021-10-29T02:40:17.214916+0000     7     128      8362      8234   73.5085   62.6875   0.0234655    0.106364
2021-10-29T02:40:18.215035+0000     8     128      9627      9499   74.2016   79.0625   0.0597014    0.107299
2021-10-29T02:40:19.215140+0000     9     128     10749     10621   73.7478    70.125   0.0482195     0.10666
2021-10-29T02:40:20.215256+0000    10     128     11893     11765   73.5222      71.5   0.0898402    0.108035
2021-10-29T02:40:21.215357+0000    11     128     12878     12750   72.4344   61.5625   0.0357446    0.109904
2021-10-29T02:40:22.215470+0000    12     128     14101     13973   72.7673   76.4375     0.14363    0.109101
2021-10-29T02:40:23.215559+0000    13     128     15278     15150   72.8279   73.5625   0.0808617    0.109282
2021-10-29T02:40:24.215683+0000    14     128     16421     16293    72.728   71.4375   0.0733199    0.108476
2021-10-29T02:40:25.215834+0000 Total time run:       14.7666
Total reads made:     17282
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   73.1463
Average IOPS:         1170
Stddev IOPS:          103.834
Max IOPS:             1340
Min IOPS:             985
Average Latency(s):   0.108914
Max latency(s):       0.815206
Min latency(s):       0.000533546

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:40:25,920903015-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:40:25,926678510-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 1066685

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:40:25,932098884-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 66208
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:40:25,939856164-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 66208
[1] 19:40:26 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:40:26,126010911-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_64KB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_64KB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_64KB_seq.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:40:26,283035539-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:40:50,292186275-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.28k objects, 1.1 GiB
    usage:   3.2 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:40:50,299866078-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:40:59,446785941-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.28k objects, 1.1 GiB
    usage:   3.2 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:40:59,454212637-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:41:08,479267779-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.28k objects, 1.1 GiB
    usage:   3.2 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:41:08,486519596-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:41:17,652358144-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.28k objects, 1.1 GiB
    usage:   3.2 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:41:17,659940743-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:41:26,794171459-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 17.28k objects, 1.1 GiB
    usage:   3.2 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:41:26,801568980-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:41:26,807219790-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T19:41:26,810854840-07:00][RUNNING][ROUND 1/4/21] object_size=256KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:41:26,814161081-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:41:26,823550987-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:41:27,271205493-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/79bb998c-3860-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 79bb998c-3860-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:41:27,281749625-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:41:27,285665236-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '79bb998c-3860-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 79bb998c-3860-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:41:27,294180875-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 79bb998c-3860-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 19:41:32 [SUCCESS] 10.10.2.1\n[2] 19:41:33 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:41:33,707325071-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:41:33,719764586-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:41:33,724785446-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:41:33,879747354-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:41:33,884760148-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:41:34,034715284-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:41:34,318998830-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:41:34,324143722-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--f176c866--57c4--4f91--a9dd--928d5878cef5-osd--block--a7a9980c--80f0--4b6e--875f--fb04a84cac55 (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-f176c866-57c4-4f91-a9dd-928d5878cef5" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-a7a9980c-80f0-4b6e-875f-fb04a84cac55"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-f176c866-57c4-4f91-a9dd-928d5878cef5" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-a7a9980c-80f0-4b6e-875f-fb04a84cac55" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-f176c866-57c4-4f91-a9dd-928d5878cef5"\n'
10.10.2.1: b'  Volume group "ceph-f176c866-57c4-4f91-a9dd-928d5878cef5" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:41:34,692858272-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:41:34,702528502-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:41:34,706395702-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\nRepeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: bb6e81b8-3861-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\nWaiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\nAdding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid bb6e81b8-3861-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\nPlease consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:42:33,944976956-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:42:53,951532695-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:42:53,961261104-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:42:53,964899263-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid bb6e81b8-3861-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/bb6e81b8-3861-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:43:02,585299176-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:43:02,595856634-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:43:02,599531001-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid bb6e81b8-3861-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/bb6e81b8-3861-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:43:11,438794460-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:43:11,444493714-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:43:11,658084481-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:43:11,661930351-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid bb6e81b8-3861-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/bb6e81b8-3861-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:43:20,506948994-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:43:40,511610998-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:43:40,518074799-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:43:40,528128199-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:43:40,531931890-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid bb6e81b8-3861-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/bb6e81b8-3861-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:44:06,593944484-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:44:26,599517405-07:00] STAGE: Show Cluster Status\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:44:26,609002847-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:44:26,612725946-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid bb6e81b8-3861-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/bb6e81b8-3861-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     bb6e81b8-3861-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.mdivzl(active, since 2m)\n    osd: 1 osds: 1 up (since 17s), 1 in (since 41s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 19:44:35 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:41:27,271205493-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/79bb998c-3860-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 79bb998c-3860-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:41:27,281749625-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:41:27,285665236-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '79bb998c-3860-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 79bb998c-3860-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:41:27,294180875-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 79bb998c-3860-11ec-b51d-53e6e728d2d3'
[1] 19:41:32 [SUCCESS] 10.10.2.1
[2] 19:41:33 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:41:33,707325071-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:41:33,719764586-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:41:33,724785446-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:41:33,879747354-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:41:33,884760148-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:41:34,034715284-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:41:34,318998830-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:41:34,324143722-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--f176c866--57c4--4f91--a9dd--928d5878cef5-osd--block--a7a9980c--80f0--4b6e--875f--fb04a84cac55 (253:0)
  Archiving volume group "ceph-f176c866-57c4-4f91-a9dd-928d5878cef5" metadata (seqno 5).
  Releasing logical volume "osd-block-a7a9980c-80f0-4b6e-875f-fb04a84cac55"
  Creating volume group backup "/etc/lvm/backup/ceph-f176c866-57c4-4f91-a9dd-928d5878cef5" (seqno 6).
  Logical volume "osd-block-a7a9980c-80f0-4b6e-875f-fb04a84cac55" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-f176c866-57c4-4f91-a9dd-928d5878cef5"
  Volume group "ceph-f176c866-57c4-4f91-a9dd-928d5878cef5" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:41:34,692858272-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:41:34,702528502-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:41:34,706395702-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: bb6e81b8-3861-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid bb6e81b8-3861-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:42:33,944976956-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:42:53,951532695-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:42:53,961261104-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:42:53,964899263-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid bb6e81b8-3861-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/bb6e81b8-3861-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:43:02,585299176-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:43:02,595856634-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:43:02,599531001-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid bb6e81b8-3861-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/bb6e81b8-3861-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:43:11,438794460-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:43:11,444493714-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:43:11,658084481-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:43:11,661930351-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid bb6e81b8-3861-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/bb6e81b8-3861-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:43:20,506948994-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:43:40,511610998-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:43:40,518074799-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:43:40,528128199-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:43:40,531931890-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid bb6e81b8-3861-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/bb6e81b8-3861-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:44:06,593944484-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:44:26,599517405-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:44:26,609002847-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:44:26,612725946-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid bb6e81b8-3861-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/bb6e81b8-3861-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     bb6e81b8-3861-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.mdivzl(active, since 2m)
    osd: 1 osds: 1 up (since 17s), 1 in (since 41s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:44:35,134464297-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:44:35,142188785-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 19:44:35 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:44:35,617107318-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:44:35,620762006-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:44:35,642648583-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:44:35,645531847-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb6e81b8-3861-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb6e81b8-3861-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:44:39,695839620-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:44:39,698666297-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb6e81b8-3861-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb6e81b8-3861-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:44:43,811441355-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:44:43,814371396-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb6e81b8-3861-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb6e81b8-3861-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:44:48,005045139-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:44:48,008100056-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb6e81b8-3861-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb6e81b8-3861-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:44:56,203317833-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:44:56,206358984-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb6e81b8-3861-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb6e81b8-3861-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:45:00,988457002-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:45:00,991276596-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb6e81b8-3861-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb6e81b8-3861-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:45:05,295304951-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:45:05,298341704-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb6e81b8-3861-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb6e81b8-3861-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:45:10,078789891-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:45:10,081935298-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb6e81b8-3861-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb6e81b8-3861-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:45:15,092684906-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:45:15,095728351-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb6e81b8-3861-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb6e81b8-3861-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:45:19,518156046-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:45:19,521104463-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb6e81b8-3861-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb6e81b8-3861-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:45:23,816346986-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:45:23,819256820-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb6e81b8-3861-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb6e81b8-3861-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:45:27,784368815-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:45:27,787220519-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb6e81b8-3861-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb6e81b8-3861-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default   
-3         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host node-1
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0  
                       TOTAL  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:45:31,860074472-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:45:55,872869718-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:46:04,944330966-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:46:13,900754262-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:46:22,923616062-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:46:22,931195697-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:46:31,972121896-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:46:31,979911386-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:46:40,989056769-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:46:40,997354937-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:46:50,004060848-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:46:50,011333003-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:46:59,116908867-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:46:59,124233191-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:46:59,130123371-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:46:59,133600955-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:46:59,140180505-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:46:59,145613083-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=1074929
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:46:59,152272673-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:46:59,161299936-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'72021\n'
[1] 19:46:59 [SUCCESS] ljishen@10.10.2.2
72021

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:46:59,350612850-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_256KB_write.log.1
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:46:59,370532841-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:46:59,373441803-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb6e81b8-3861-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb6e81b8-3861-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T02:47:02.282474+0000 Maintaining 128 concurrent writes of 262144 bytes to objects of size 262144 for up to 60 seconds or 0 objects
2021-10-29T02:47:02.282487+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-29T02:47:02.291224+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T02:47:02.291224+0000     0       0         0         0         0         0           -           0
2021-10-29T02:47:03.291337+0000     1     128       257       129   32.2481     32.25     0.77587    0.600458
2021-10-29T02:47:04.291468+0000     2     128       385       257   32.1219        32    0.859185    0.635957
2021-10-29T02:47:05.291548+0000     3     128       641       513   42.7461        64    0.657705    0.686734
2021-10-29T02:47:06.291662+0000     4     128       769       641   40.0586        32    0.800387    0.695643
2021-10-29T02:47:07.291727+0000     5     128       979       851   42.5462      52.5    0.687552    0.708556
2021-10-29T02:47:08.291823+0000     6     128      1153      1025   42.7044      43.5    0.825177    0.717937
2021-10-29T02:47:09.291904+0000     7     128      1363      1235   44.1032      52.5    0.599396    0.698311
2021-10-29T02:47:10.291982+0000     8     128      1537      1409   44.0274      43.5    0.608613    0.697843
2021-10-29T02:47:11.292098+0000     9     128      1747      1619   44.9681      52.5    0.592114     0.69718
2021-10-29T02:47:12.292170+0000    10     128      1875      1747   43.6711        32    0.949416    0.714851
2021-10-29T02:47:13.292242+0000    11     128      2049      1921   43.6553      43.5     0.60013    0.704184
2021-10-29T02:47:14.292354+0000    12     128      2177      2049   42.6837        32    0.833699    0.707318
2021-10-29T02:47:15.292452+0000    13     128      2177      2049   39.4003         0           -    0.707318
2021-10-29T02:47:16.292551+0000    14     128      2259      2131   38.0501     10.25     2.54139    0.777878
2021-10-29T02:47:17.292618+0000    15     128      2305      2177   36.2801      11.5     3.10567    0.827057
2021-10-29T02:47:18.292705+0000    16     128      2305      2177   34.0126         0           -    0.827057
2021-10-29T02:47:19.292826+0000    17     128      2387      2259   33.2176     10.25     3.24273    0.914736
2021-10-29T02:47:20.292901+0000    18     128      2387      2259   31.3722         0           -    0.914736
2021-10-29T02:47:21.292978+0000    19     128      2433      2305   30.3262      5.75     3.53248    0.966971
2021-10-29T02:47:22.293044+0000 min lat: 0.366636 max lat: 3.53268 avg lat: 1.0288
2021-10-29T02:47:22.293044+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T02:47:22.293044+0000    20     128      2515      2387   29.8349      20.5     2.76686      1.0288
2021-10-29T02:47:23.293123+0000    21     128      2515      2387   28.4142         0           -      1.0288
2021-10-29T02:47:24.293238+0000    22     128      2561      2433   27.6453      5.75      2.9809      1.0657
2021-10-29T02:47:25.293318+0000    23     128      2643      2515   27.3345      20.5     2.94811     1.12708
2021-10-29T02:47:26.293425+0000    24     128      2689      2561   26.6747      11.5      2.7811     1.15678
2021-10-29T02:47:27.293494+0000    25     128      2689      2561   25.6077         0           -     1.15678
2021-10-29T02:47:28.293579+0000    26     128      2771      2643   25.4112     10.25     2.98293     1.21344
2021-10-29T02:47:29.293653+0000    27     128      2817      2689    24.896      11.5     2.72909     1.23936
2021-10-29T02:47:30.293724+0000    28     128      2817      2689   24.0068         0           -     1.23936
2021-10-29T02:47:31.293817+0000    29     128      2899      2771   23.8858     10.25     2.80356     1.28564
2021-10-29T02:47:32.293884+0000    30     128      2945      2817    23.473      11.5     2.60278     1.30715
2021-10-29T02:47:33.293953+0000    31     128      2945      2817   22.7158         0           -     1.30715
2021-10-29T02:47:34.294047+0000    32     128      3027      2899   22.6465     10.25     2.87288     1.35144
2021-10-29T02:47:35.294157+0000    33     128      3073      2945   22.3087      11.5     2.86046     1.37501
2021-10-29T02:47:36.294258+0000    34     128      3155      3027   22.2554      20.5     2.42315      1.4034
2021-10-29T02:47:37.294326+0000    35     128      3201      3073   21.9481      11.5     2.75222     1.42359
2021-10-29T02:47:38.294436+0000    36     128      3201      3073   21.3384         0           -     1.42359
2021-10-29T02:47:39.294507+0000    37     128      3201      3073   20.7617         0           -     1.42359
2021-10-29T02:47:40.294617+0000    38     128      3283      3155   20.7548   6.83333     3.45713     1.47644
2021-10-29T02:47:41.294693+0000    39     128      3329      3201   20.5174      11.5     3.57277     1.50656
2021-10-29T02:47:42.294763+0000 min lat: 0.366636 max lat: 3.57377 avg lat: 1.53622
2021-10-29T02:47:42.294763+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T02:47:42.294763+0000    40     128      3411      3283    20.517      20.5     2.69401     1.53622
2021-10-29T02:47:43.294860+0000    41     128      3457      3329    20.297      11.5     2.35166     1.54748
2021-10-29T02:47:44.294950+0000    42     128      3585      3457   20.5756        32     1.04155     1.53489
2021-10-29T02:47:45.295028+0000    43     128      3795      3667   21.3179      52.5    0.600005     1.49092
2021-10-29T02:47:46.295146+0000    44     128      3969      3841   21.8219      43.5    0.566501     1.45291
2021-10-29T02:47:47.295217+0000    45     128      4179      4051   22.5036      52.5    0.670837     1.41229
2021-10-29T02:47:48.295291+0000    46     128      4225      4097   22.2644      11.5    0.758778     1.40495
2021-10-29T02:47:49.295382+0000    47     128      4307      4179   22.2268      20.5     2.04048      1.4174
2021-10-29T02:47:50.295492+0000    48     128      4307      4179   21.7637         0           -      1.4174
2021-10-29T02:47:51.295576+0000    49     128      4353      4225   21.5542      5.75     3.04786     1.43515
2021-10-29T02:47:52.295647+0000    50     128      4435      4307   21.5331      20.5     3.04555     1.46581
2021-10-29T02:47:53.295727+0000    51     128      4435      4307   21.1109         0           -     1.46581
2021-10-29T02:47:54.295805+0000    52     128      4435      4307   20.7049         0           -     1.46581
2021-10-29T02:47:55.295873+0000    53     128      4481      4353   20.5312   3.83333     3.88416     1.49136
2021-10-29T02:47:56.295987+0000    54     128      4481      4353    20.151         0           -     1.49136
2021-10-29T02:47:57.296051+0000    55     128      4481      4353   19.7846         0           -     1.49136
2021-10-29T02:47:58.296166+0000    56     128      4563      4435   19.7974   6.83333     5.59647     1.56726
2021-10-29T02:47:59.296257+0000    57     128      4563      4435   19.4501         0           -     1.56726
2021-10-29T02:48:00.296356+0000    58     128      4609      4481    19.313      5.75     5.17256     1.60427
2021-10-29T02:48:01.296436+0000    59     128      4609      4481   18.9856         0           -     1.60427
2021-10-29T02:48:02.296506+0000 min lat: 0.366636 max lat: 5.59724 avg lat: 1.60427
2021-10-29T02:48:02.296506+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T02:48:02.296506+0000    60     128      4609      4481   18.6692         0           -     1.60427
2021-10-29T02:48:03.296634+0000    61      46      4609      4563   18.6992   6.83333     5.20002      1.6689
2021-10-29T02:48:04.296739+0000 Total time run:         61.9898
Total writes made:      4609
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     18.5877
Stddev Bandwidth:       17.8639
Max bandwidth (MB/sec): 64
Min bandwidth (MB/sec): 0
Average IOPS:           74
Stddev IOPS:            71.4687
Max IOPS:               256
Min IOPS:               0
Average Latency(s):     1.69843
Stddev Latency(s):      1.3455
Max latency(s):         5.59724
Min latency(s):         0.366636

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:48:05,355405531-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:48:05,360897622-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 1074929

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:48:05,366246742-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 72021
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:48:05,373887622-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 72021
[1] 19:48:05 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:48:05,558115062-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_256KB_write.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_256KB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:48:05,731665099-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:48:30,257886380-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   4.6 GiB used, 395 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:48:30,265402214-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:48:39,415206996-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   4.8 GiB used, 395 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:48:39,422653560-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:48:48,329185483-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:48:48,336583926-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:48:57,402372035-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:48:57,409996343-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:49:06,525952303-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:49:06,533392384-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:49:06,539220187-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:49:06,542383949-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:49:06,548950995-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:49:06,554280639-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=1076969
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:49:06,560869397-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:49:06,570190222-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'72796\n'
[1] 19:49:06 [SUCCESS] ljishen@10.10.2.2
72796

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:49:06,758426467-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_256KB_seq.log.1
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:49:06,778208517-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:49:06,781079989-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bb6e81b8-3861-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bb6e81b8-3861-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T02:49:09.835907+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T02:49:09.835907+0000     0       0         0         0         0         0           -           0
2021-10-29T02:49:10.836054+0000     1     128       517       389   97.2277     97.25    0.195112    0.280184
2021-10-29T02:49:11.836170+0000     2     128       864       736   91.9841     86.75    0.359063    0.306897
2021-10-29T02:49:12.836284+0000     3     128      1301      1173    97.735    109.25    0.197736    0.313472
2021-10-29T02:49:13.836384+0000     4     128      1702      1574   98.3612    100.25    0.210317    0.313711
2021-10-29T02:49:14.836496+0000     5     128      2128      2000   99.9866     106.5    0.237434    0.305271
2021-10-29T02:49:15.836608+0000     6     128      2469      2341   97.5289     85.25    0.168515    0.318157
2021-10-29T02:49:16.836723+0000     7     128      2777      2649    94.595        77    0.342265    0.326978
2021-10-29T02:49:17.836828+0000     8     128      3226      3098   96.8004    112.25    0.265465     0.32118
2021-10-29T02:49:18.836942+0000     9     128      3646      3518   97.7101       105    0.140277     0.32237
2021-10-29T02:49:19.837053+0000    10     128      4023      3895    97.363     94.25    0.189032    0.322648
2021-10-29T02:49:20.837166+0000    11     128      4472      4344   98.7152    112.25    0.233803    0.319862
2021-10-29T02:49:21.837291+0000 Total time run:       11.6833
Total reads made:     4609
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   98.6235
Average IOPS:         394
Stddev IOPS:          47.2937
Max IOPS:             449
Min IOPS:             308
Average Latency(s):   0.318052
Max latency(s):       0.858656
Min latency(s):       0.00513171

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:49:22,520948367-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:49:22,526740072-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 1076969

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:49:22,532201154-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 72796
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:49:22,540008177-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 72796
[1] 19:49:22 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:49:22,726089320-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_256KB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_256KB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:49:22,884527367-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:49:46,864773708-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:49:46,871914255-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:49:55,889402003-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:49:55,896953334-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:50:05,061902809-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:50:05,069758113-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:50:14,184981212-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:50:14,192901228-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:50:22,569002729-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:50:22,576634111-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:50:22,582557934-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T19:50:22,584779551-07:00][RUNNING][ROUND 2/4/21] object_size=256KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:50:22,588057279-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:50:22,597319734-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:50:23,055917555-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/bb6e81b8-3861-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid bb6e81b8-3861-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:50:23,065746303-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:50:23,069183014-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'bb6e81b8-3861-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid bb6e81b8-3861-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:50:23,076787341-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid bb6e81b8-3861-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 19:50:28 [SUCCESS] 10.10.2.1\n[2] 19:50:29 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:50:29,515961514-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:50:29,527556753-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:50:29,531869171-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:50:29,683974962-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:50:29,688631196-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:50:29,838797572-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:50:30,118548909-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:50:30,123570490-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--a95f8349--3387--470b--86ae--47318e4ab933-osd--block--4cd967ee--bb3b--408e--a17c--364ef60fc801 (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-a95f8349-3387-470b-86ae-47318e4ab933" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-4cd967ee-bb3b-408e-a17c-364ef60fc801"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-a95f8349-3387-470b-86ae-47318e4ab933" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-4cd967ee-bb3b-408e-a17c-364ef60fc801" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-a95f8349-3387-470b-86ae-47318e4ab933"\n'
10.10.2.1: b'  Volume group "ceph-a95f8349-3387-470b-86ae-47318e4ab933" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:50:30,472255604-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:50:30,481831737-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:50:30,485081497-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: fac87c00-3862-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\n'
10.10.2.1: b'Waiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid fac87c00-3862-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:51:30,406342764-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:51:50,412991913-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:51:50,422604625-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:51:50,426504848-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid fac87c00-3862-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/fac87c00-3862-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:51:59,061523306-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:51:59,071481858-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:51:59,074941142-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid fac87c00-3862-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/fac87c00-3862-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:52:08,033112466-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:52:08,038946724-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:52:08,248730370-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:52:08,252305661-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid fac87c00-3862-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/fac87c00-3862-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:52:17,831687342-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:52:37,836611954-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:52:37,843145176-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:52:37,852923720-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:52:37,856821428-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid fac87c00-3862-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/fac87c00-3862-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:53:02,900837314-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:53:22,906769571-07:00] STAGE: Show Cluster Status\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:53:22,916536794-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:53:22,920380179-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid fac87c00-3862-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/fac87c00-3862-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     fac87c00-3862-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.obwfxm(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 41s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 19:53:31 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:50:23,055917555-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/bb6e81b8-3861-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid bb6e81b8-3861-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:50:23,065746303-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:50:23,069183014-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'bb6e81b8-3861-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid bb6e81b8-3861-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:50:23,076787341-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid bb6e81b8-3861-11ec-b51d-53e6e728d2d3'
[1] 19:50:28 [SUCCESS] 10.10.2.1
[2] 19:50:29 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:50:29,515961514-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:50:29,527556753-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:50:29,531869171-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:50:29,683974962-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:50:29,688631196-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:50:29,838797572-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:50:30,118548909-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:50:30,123570490-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--a95f8349--3387--470b--86ae--47318e4ab933-osd--block--4cd967ee--bb3b--408e--a17c--364ef60fc801 (253:0)
  Archiving volume group "ceph-a95f8349-3387-470b-86ae-47318e4ab933" metadata (seqno 5).
  Releasing logical volume "osd-block-4cd967ee-bb3b-408e-a17c-364ef60fc801"
  Creating volume group backup "/etc/lvm/backup/ceph-a95f8349-3387-470b-86ae-47318e4ab933" (seqno 6).
  Logical volume "osd-block-4cd967ee-bb3b-408e-a17c-364ef60fc801" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-a95f8349-3387-470b-86ae-47318e4ab933"
  Volume group "ceph-a95f8349-3387-470b-86ae-47318e4ab933" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:50:30,472255604-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:50:30,481831737-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:50:30,485081497-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: fac87c00-3862-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid fac87c00-3862-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:51:30,406342764-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:51:50,412991913-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:51:50,422604625-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:51:50,426504848-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid fac87c00-3862-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/fac87c00-3862-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:51:59,061523306-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:51:59,071481858-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:51:59,074941142-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid fac87c00-3862-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/fac87c00-3862-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:52:08,033112466-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:52:08,038946724-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:52:08,248730370-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:52:08,252305661-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid fac87c00-3862-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/fac87c00-3862-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:52:17,831687342-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:52:37,836611954-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:52:37,843145176-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:52:37,852923720-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:52:37,856821428-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid fac87c00-3862-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/fac87c00-3862-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:53:02,900837314-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:53:22,906769571-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:53:22,916536794-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:53:22,920380179-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid fac87c00-3862-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/fac87c00-3862-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     fac87c00-3862-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.obwfxm(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 41s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:53:31,797307999-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:53:31,805001879-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 19:53:31 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:53:32,281087690-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:53:32,284515169-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:53:32,306198793-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:53:32,309082188-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fac87c00-3862-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fac87c00-3862-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:53:36,472421119-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:53:36,475257364-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fac87c00-3862-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fac87c00-3862-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:53:40,557497189-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:53:40,560545553-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fac87c00-3862-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fac87c00-3862-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:53:44,582273870-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:53:44,585385104-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fac87c00-3862-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fac87c00-3862-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:53:52,592549100-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:53:52,595807891-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fac87c00-3862-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fac87c00-3862-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:53:56,908496486-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:53:56,911568927-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fac87c00-3862-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fac87c00-3862-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:54:01,227550507-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:54:01,230613890-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fac87c00-3862-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fac87c00-3862-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:54:05,909292683-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:54:05,912443271-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fac87c00-3862-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fac87c00-3862-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:54:10,934420007-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:54:10,937541770-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fac87c00-3862-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fac87c00-3862-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:54:15,445848645-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:54:15,449003381-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fac87c00-3862-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fac87c00-3862-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:54:19,702095930-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:54:19,705005944-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fac87c00-3862-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fac87c00-3862-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:54:23,784965754-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:54:23,787895836-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fac87c00-3862-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fac87c00-3862-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default   
-3         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host node-1
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0  
                       TOTAL  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:54:27,696111645-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:54:51,759943799-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:55:00,802178647-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:55:09,850828170-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:55:18,999402313-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:55:19,007039266-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:55:28,356781597-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:55:28,364396637-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:55:37,315653739-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:55:37,323238853-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:55:46,389310485-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:55:46,397076220-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:55:55,387214939-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:55:55,394680568-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:55:55,400770935-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:55:55,404088688-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:55:55,410701310-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:55:55,416255296-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=1084785
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:55:55,423220212-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:55:55,432152265-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'78546\n'
[1] 19:55:55 [SUCCESS] ljishen@10.10.2.2
78546

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:55:55,623080078-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_256KB_write.log.2
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:55:55,643954718-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:55:55,647011068-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fac87c00-3862-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fac87c00-3862-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T02:55:58.684940+0000 Maintaining 128 concurrent writes of 262144 bytes to objects of size 262144 for up to 60 seconds or 0 objects
2021-10-29T02:55:58.684953+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_8
2021-10-29T02:55:58.693516+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T02:55:58.693516+0000     0       0         0         0         0         0           -           0
2021-10-29T02:55:59.693627+0000     1     128       257       129   32.2485     32.25    0.938123    0.671339
2021-10-29T02:56:00.693700+0000     2     128       467       339   42.3724      52.5    0.658051    0.666894
2021-10-29T02:56:01.693770+0000     3     128       595       467   38.9142        32     0.74507    0.677645
2021-10-29T02:56:02.693837+0000     4     128       769       641   40.0599      43.5     0.67614    0.739323
2021-10-29T02:56:03.693903+0000     5     128       897       769   38.4475        32    0.615844    0.726578
2021-10-29T02:56:04.693969+0000     6     128      1107       979    40.789      52.5    0.642041      0.7312
2021-10-29T02:56:05.694035+0000     7     128      1281      1153   41.1759      43.5    0.799221    0.721678
2021-10-29T02:56:06.694103+0000     8     128      1491      1363    42.591      52.5    0.622284      0.7195
2021-10-29T02:56:07.694170+0000     9     128      1619      1491    41.414        32    0.595068    0.709201
2021-10-29T02:56:08.694215+0000    10     128      1793      1665   41.6224      43.5    0.624726    0.731496
2021-10-29T02:56:09.694282+0000    11     128      2049      1921   43.6563        64    0.633022    0.719157
2021-10-29T02:56:10.694348+0000    12     128      2177      2049   42.6848        32    0.802857    0.716912
2021-10-29T02:56:11.694415+0000    13     128      2177      2049   39.4013         0           -    0.716912
2021-10-29T02:56:12.694480+0000    14     128      2259      2131   38.0511     10.25     2.43528    0.783033
2021-10-29T02:56:13.694547+0000    15     128      2305      2177    36.281      11.5     3.12441     0.83252
2021-10-29T02:56:14.694613+0000    16     128      2305      2177   34.0134         0           -     0.83252
2021-10-29T02:56:15.694679+0000    17     128      2305      2177   32.0126         0           -     0.83252
2021-10-29T02:56:16.694746+0000    18     128      2387      2259    31.373   6.83333     3.28945    0.921701
2021-10-29T02:56:17.694813+0000    19     128      2433      2305    30.327      11.5     3.48095    0.972772
2021-10-29T02:56:18.694881+0000 min lat: 0.400208 max lat: 3.48131 avg lat: 0.972772
2021-10-29T02:56:18.694881+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T02:56:18.694881+0000    20     128      2433      2305   28.8106         0           -    0.972772
2021-10-29T02:56:19.694957+0000    21     128      2515      2387   28.4148     10.25      3.0066     1.04265
2021-10-29T02:56:20.695023+0000    22     128      2561      2433   27.6459      11.5     2.95881     1.07888
2021-10-29T02:56:21.695090+0000    23     128      2643      2515   27.3352      20.5     2.80182     1.13504
2021-10-29T02:56:22.695157+0000    24     128      2643      2515   26.1962         0           -     1.13504
2021-10-29T02:56:23.695215+0000    25     128      2689      2561   25.6083      5.75     2.77331     1.16446
2021-10-29T02:56:24.695287+0000    26     128      2771      2643   25.4118      20.5     3.01522     1.22187
2021-10-29T02:56:25.695354+0000    27     128      2817      2689   24.8965      11.5     2.75511      1.2481
2021-10-29T02:56:26.695423+0000    28     128      2817      2689   24.0073         0           -      1.2481
2021-10-29T02:56:27.695489+0000    29     128      2899      2771   23.8864     10.25     2.55806     1.28687
2021-10-29T02:56:28.695556+0000    30     128      2945      2817   23.4735      11.5       2.441     1.30572
2021-10-29T02:56:29.695644+0000    31     128      2945      2817   22.7162         0           -     1.30572
2021-10-29T02:56:30.695708+0000    32     128      3027      2899   22.6469     10.25     2.90659     1.35099
2021-10-29T02:56:31.695774+0000    33     128      3073      2945   22.3091      11.5     2.91905     1.37548
2021-10-29T02:56:32.695841+0000    34     128      3073      2945    21.653         0           -     1.37548
2021-10-29T02:56:33.695910+0000    35     128      3155      3027     21.62     10.25     2.70877      1.4116
2021-10-29T02:56:34.695975+0000    36     128      3201      3073   21.3389      11.5     2.86261     1.43332
2021-10-29T02:56:35.696040+0000    37     128      3201      3073   20.7621         0           -     1.43332
2021-10-29T02:56:36.696104+0000    38     128      3283      3155   20.7552     10.25     3.11269     1.47695
2021-10-29T02:56:37.696170+0000    39     128      3329      3201   20.5179      11.5     3.24688     1.50239
2021-10-29T02:56:38.696215+0000 min lat: 0.400208 max lat: 3.48131 avg lat: 1.53008
2021-10-29T02:56:38.696215+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T02:56:38.696215+0000    40     128      3411      3283   20.5174      20.5     2.61103     1.53008
2021-10-29T02:56:39.696288+0000    41     128      3539      3411   20.7974        32    0.767678      1.5162
2021-10-29T02:56:40.696353+0000    42     128      3713      3585   21.3379      43.5    0.608299     1.48735
2021-10-29T02:56:41.696422+0000    43     128      3841      3713   21.5858        32    0.883051     1.46155
2021-10-29T02:56:42.696489+0000    44     128      4097      3969   22.5496        64    0.641495     1.41067
2021-10-29T02:56:43.696557+0000    45     128      4225      4097   22.7596        32    0.891808     1.38903
2021-10-29T02:56:44.696624+0000    46     128      4225      4097   22.2648         0           -     1.38903
2021-10-29T02:56:45.696689+0000    47     128      4307      4179   22.2273     10.25     1.98376     1.40073
2021-10-29T02:56:46.696757+0000    48     128      4353      4225   22.0038      11.5     2.96269     1.41774
2021-10-29T02:56:47.696824+0000    49     128      4353      4225   21.5547         0           -     1.41774
2021-10-29T02:56:48.696888+0000    50     128      4435      4307   21.5336     10.25     3.24499     1.45253
2021-10-29T02:56:49.696954+0000    51     128      4435      4307   21.1113         0           -     1.45253
2021-10-29T02:56:50.697019+0000    52     128      4481      4353   20.9265      5.75     3.91051      1.4785
2021-10-29T02:56:51.697084+0000    53     128      4481      4353   20.5317         0           -      1.4785
2021-10-29T02:56:52.697148+0000    54     128      4481      4353   20.1514         0           -      1.4785
2021-10-29T02:56:53.697215+0000    55     128      4481      4353   19.7851         0           -      1.4785
2021-10-29T02:56:54.697283+0000    56     128      4563      4435   19.7978     5.125     5.60885     1.55486
2021-10-29T02:56:55.697347+0000    57     128      4563      4435   19.4505         0           -     1.55486
2021-10-29T02:56:56.697410+0000    58     128      4609      4481   19.3134      5.75     5.53603     1.59573
2021-10-29T02:56:57.697476+0000    59     128      4609      4481    18.986         0           -     1.59573
2021-10-29T02:56:58.697540+0000 min lat: 0.400208 max lat: 5.60912 avg lat: 1.59573
2021-10-29T02:56:58.697540+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T02:56:58.697540+0000    60     128      4609      4481   18.6696         0           -     1.59573
2021-10-29T02:56:59.697615+0000    61      46      4609      4563   18.6996   6.83333     5.69071     1.66933
2021-10-29T02:57:00.697678+0000    62      46      4609      4563    18.398         0           -     1.66933
2021-10-29T02:57:01.697779+0000 Total time run:         62.7416
Total writes made:      4609
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     18.365
Stddev Bandwidth:       17.9517
Max bandwidth (MB/sec): 64
Min bandwidth (MB/sec): 0
Average IOPS:           73
Stddev IOPS:            71.8175
Max IOPS:               256
Min IOPS:               0
Average Latency(s):     1.70769
Stddev Latency(s):      1.39264
Max latency(s):         5.69209
Min latency(s):         0.400208

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:57:02,959626709-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:57:02,965283790-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 1084785

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:57:02,970900164-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 78546
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:57:02,978329134-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 78546
[1] 19:57:03 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:57:03,166164469-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_256KB_write.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_256KB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:57:03,339962850-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:57:27,905644420-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   4.6 GiB used, 395 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:57:27,912712250-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:57:37,080823747-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   3.5 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:57:37,088526713-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:57:46,320769344-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:57:46,328642953-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:57:55,552769726-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:57:55,560270321-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:58:04,801872604-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:58:04,809417323-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:58:04,815049316-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:58:04,818713662-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:58:04,825501033-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:58:04,831002020-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=1086719
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:58:04,838038881-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:58:04,846908075-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'79316\n'
[1] 19:58:05 [SUCCESS] ljishen@10.10.2.2
79316

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:58:05,039084170-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_256KB_seq.log.2
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:58:05,058714474-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:58:05,061644656-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fac87c00-3862-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fac87c00-3862-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T02:58:08.204477+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T02:58:08.204477+0000     0       0         0         0         0         0           -           0
2021-10-29T02:58:09.204637+0000     1     128       648       520    129.97       130    0.326432     0.20333
2021-10-29T02:58:10.204748+0000     2     128       951       823   102.857     75.75    0.521641    0.280878
2021-10-29T02:58:11.204859+0000     3     128      1384      1256   104.651    108.25    0.279754    0.285108
2021-10-29T02:58:12.204975+0000     4     128      1773      1645   102.798     97.25    0.191803    0.300671
2021-10-29T02:58:13.205089+0000     5     128      2216      2088   104.386    110.75    0.349212    0.297238
2021-10-29T02:58:14.205203+0000     6     128      2595      2467   102.778     94.75     0.32399     0.29771
2021-10-29T02:58:15.205318+0000     7     128      3080      2952   105.415    121.25    0.183022    0.295091
2021-10-29T02:58:16.205430+0000     8     128      3415      3287   102.706     83.75    0.192183    0.304257
2021-10-29T02:58:17.205539+0000     9     128      3775      3647   101.293        90    0.319302    0.306951
2021-10-29T02:58:18.205650+0000    10     128      4161      4033   100.812      96.5     0.26365    0.311695
2021-10-29T02:58:19.205761+0000    11     128      4607      4479   101.783     111.5    0.311982    0.310193
2021-10-29T02:58:20.205880+0000 Total time run:       11.5192
Total reads made:     4609
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   100.028
Average IOPS:         400
Stddev IOPS:          64.8873
Max IOPS:             520
Min IOPS:             303
Average Latency(s):   0.309652
Max latency(s):       1.16133
Min latency(s):       0.00564908

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:58:20,849144968-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:58:20,855006444-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 1086719

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:58:20,860758594-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 79316
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:58:20,868366602-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 79316
[1] 19:58:21 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:58:21,053764733-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_256KB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_256KB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:58:21,208674417-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:58:45,311225175-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:58:45,319582374-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:58:54,442103995-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:58:54,449516034-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:59:03,590400707-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:59:03,597776647-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:59:12,714363566-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:59:12,722418697-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:59:21,803549848-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:59:21,811670243-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:59:21,817513958-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T19:59:21,819970969-07:00][RUNNING][ROUND 3/4/21] object_size=256KB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:59:21,823513997-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:59:21,832967466-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:59:22,281682803-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/fac87c00-3862-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid fac87c00-3862-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:59:22,292487747-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:59:22,296161203-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'fac87c00-3862-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid fac87c00-3862-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:59:22,304660752-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid fac87c00-3862-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 19:59:27 [SUCCESS] 10.10.2.1\n[2] 19:59:28 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:59:28,681997157-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:59:28,694501275-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:59:28,699552020-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:59:28,851528513-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:59:28,856775638-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:59:29,006778591-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:59:29,291359528-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:59:29,296381018-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--85a9255c--3b8c--4122--a5a3--f281da0d2b06-osd--block--5cf1b49e--906d--4c47--8fb4--1709c9dd7021 (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-85a9255c-3b8c-4122-a5a3-f281da0d2b06" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-5cf1b49e-906d-4c47-8fb4-1709c9dd7021"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-85a9255c-3b8c-4122-a5a3-f281da0d2b06" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-5cf1b49e-906d-4c47-8fb4-1709c9dd7021" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-85a9255c-3b8c-4122-a5a3-f281da0d2b06"\n'
10.10.2.1: b'  Volume group "ceph-85a9255c-3b8c-4122-a5a3-f281da0d2b06" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:59:29,648078053-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:59:29,658174454-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T19:59:29,661834395-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\n'
10.10.2.1: b'Verifying lvm2 is present...\nVerifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\npodman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 3c27bba6-3864-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 3c27bba6-3864-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:00:28,355392214-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:00:48,362204649-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:00:48,372000274-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:00:48,375526444-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 3c27bba6-3864-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/3c27bba6-3864-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:00:57,242841281-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:00:57,252931340-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:00:57,257036127-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 3c27bba6-3864-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/3c27bba6-3864-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:01:05,961751397-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:01:05,967364289-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:01:06,177526329-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:01:06,180956758-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid 3c27bba6-3864-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/3c27bba6-3864-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:01:15,196228171-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:01:35,200903103-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:01:35,207574986-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:01:35,216961813-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:01:35,220768980-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 3c27bba6-3864-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/3c27bba6-3864-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:02:00,503965674-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:02:20,509128671-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:02:20,518807798-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:02:20,522537579-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 3c27bba6-3864-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/3c27bba6-3864-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     3c27bba6-3864-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.gzrrcj(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 41s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 20:02:28 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:59:22,281682803-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/fac87c00-3862-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid fac87c00-3862-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:59:22,292487747-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:59:22,296161203-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'fac87c00-3862-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid fac87c00-3862-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:59:22,304660752-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid fac87c00-3862-11ec-b51d-53e6e728d2d3'
[1] 19:59:27 [SUCCESS] 10.10.2.1
[2] 19:59:28 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:59:28,681997157-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:59:28,694501275-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:59:28,699552020-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:59:28,851528513-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:59:28,856775638-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:59:29,006778591-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:59:29,291359528-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:59:29,296381018-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--85a9255c--3b8c--4122--a5a3--f281da0d2b06-osd--block--5cf1b49e--906d--4c47--8fb4--1709c9dd7021 (253:0)
  Archiving volume group "ceph-85a9255c-3b8c-4122-a5a3-f281da0d2b06" metadata (seqno 5).
  Releasing logical volume "osd-block-5cf1b49e-906d-4c47-8fb4-1709c9dd7021"
  Creating volume group backup "/etc/lvm/backup/ceph-85a9255c-3b8c-4122-a5a3-f281da0d2b06" (seqno 6).
  Logical volume "osd-block-5cf1b49e-906d-4c47-8fb4-1709c9dd7021" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-85a9255c-3b8c-4122-a5a3-f281da0d2b06"
  Volume group "ceph-85a9255c-3b8c-4122-a5a3-f281da0d2b06" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:59:29,648078053-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:59:29,658174454-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T19:59:29,661834395-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 3c27bba6-3864-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 3c27bba6-3864-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:00:28,355392214-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:00:48,362204649-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:00:48,372000274-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:00:48,375526444-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 3c27bba6-3864-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/3c27bba6-3864-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:00:57,242841281-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:00:57,252931340-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:00:57,257036127-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 3c27bba6-3864-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/3c27bba6-3864-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:01:05,961751397-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:01:05,967364289-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:01:06,177526329-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:01:06,180956758-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 3c27bba6-3864-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/3c27bba6-3864-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:01:15,196228171-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:01:35,200903103-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:01:35,207574986-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:01:35,216961813-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:01:35,220768980-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 3c27bba6-3864-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/3c27bba6-3864-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:02:00,503965674-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:02:20,509128671-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:02:20,518807798-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:02:20,522537579-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 3c27bba6-3864-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/3c27bba6-3864-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     3c27bba6-3864-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.gzrrcj(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 41s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:02:28,767865893-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:02:28,775957654-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 20:02:28 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:02:29,252972694-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:02:29,256746026-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:02:29,278744239-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:02:29,281537403-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3c27bba6-3864-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3c27bba6-3864-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:02:33,300866523-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:02:33,303776919-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3c27bba6-3864-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3c27bba6-3864-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:02:37,567292415-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:02:37,570211797-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3c27bba6-3864-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3c27bba6-3864-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:02:41,775820689-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:02:41,778879845-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3c27bba6-3864-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3c27bba6-3864-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:02:49,790774802-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:02:49,794024938-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3c27bba6-3864-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3c27bba6-3864-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:02:54,147160718-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:02:54,150045064-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3c27bba6-3864-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3c27bba6-3864-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:02:58,464702638-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:02:58,467694227-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3c27bba6-3864-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3c27bba6-3864-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:03:02,789191091-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:03:02,792328143-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3c27bba6-3864-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3c27bba6-3864-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:03:07,272566775-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:03:07,275537014-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3c27bba6-3864-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3c27bba6-3864-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:03:11,787978341-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:03:11,790884268-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3c27bba6-3864-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3c27bba6-3864-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:03:16,119749215-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:03:16,122621599-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3c27bba6-3864-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3c27bba6-3864-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 20 lfor 0/0/17 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 19 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:03:20,179249176-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:03:20,182105830-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3c27bba6-3864-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3c27bba6-3864-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default   
-3         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host node-1
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0  
                       TOTAL  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:03:24,153981073-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:03:48,251070003-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:03:57,325027817-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:04:06,372898858-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:04:15,303889513-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:04:15,311040532-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:04:24,301811968-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:04:24,309069007-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:04:33,260109031-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:04:33,268054667-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:04:42,464397737-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:04:42,472220541-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:04:51,496515095-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:04:51,503823921-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:04:51,510002546-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:04:51,513473959-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:04:51,520179778-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:04:51,525631484-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=1094519
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:04:51,532733310-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:04:51,541712073-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'85201\n'
[1] 20:04:51 [SUCCESS] ljishen@10.10.2.2
85201

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:04:51,730313872-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_256KB_write.log.3
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:04:51,750812939-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:04:51,753873097-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3c27bba6-3864-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3c27bba6-3864-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T03:04:54.859607+0000 Maintaining 128 concurrent writes of 262144 bytes to objects of size 262144 for up to 60 seconds or 0 objects
2021-10-29T03:04:54.859619+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-29T03:04:54.868386+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T03:04:54.868386+0000     0       0         0         0         0         0           -           0
2021-10-29T03:04:55.868526+0000     1     128       257       129   32.2467     32.25    0.834184    0.651681
2021-10-29T03:04:56.868645+0000     2     128       385       257   32.1214        32    0.838358    0.662475
2021-10-29T03:04:57.868754+0000     3     128       595       467   38.9124      52.5    0.807961    0.718584
2021-10-29T03:04:58.868826+0000     4     128       769       641   40.0585      43.5    0.633636    0.710092
2021-10-29T03:04:59.868938+0000     5     128       979       851   42.5456      52.5    0.766831     0.70579
2021-10-29T03:05:00.869052+0000     6     128      1153      1025   42.7039      43.5    0.797572    0.714371
2021-10-29T03:05:01.869155+0000     7     128      1363      1235   44.1025      52.5     0.66648    0.695827
2021-10-29T03:05:02.869273+0000     8     128      1491      1363   42.5892        32    0.801299    0.707517
2021-10-29T03:05:03.869345+0000     9     128      1665      1537   42.6901      43.5     0.82512     0.72502
2021-10-29T03:05:04.869458+0000    10     128      1875      1747   43.6705      52.5     0.71817    0.721738
2021-10-29T03:05:05.869558+0000    11     128      2049      1921   43.6546      43.5    0.608513    0.712963
2021-10-29T03:05:06.869671+0000    12     128      2177      2049   42.6831        32    0.850325     0.71563
2021-10-29T03:05:07.869790+0000    13     128      2177      2049   39.3997         0           -     0.71563
2021-10-29T03:05:08.869861+0000    14     128      2259      2131   38.0497     10.25     2.49086    0.783925
2021-10-29T03:05:09.869967+0000    15     128      2305      2177   36.2796      11.5     2.99809    0.830461
2021-10-29T03:05:10.870083+0000    16     128      2305      2177   34.0121         0           -    0.830461
2021-10-29T03:05:11.870199+0000    17     128      2387      2259   33.2171     10.25     2.99641     0.90908
2021-10-29T03:05:12.870313+0000    18     128      2387      2259   31.3717         0           -     0.90908
2021-10-29T03:05:13.870377+0000    19     128      2433      2305   30.3258      5.75     3.40521    0.959128
2021-10-29T03:05:14.870495+0000 min lat: 0.406403 max lat: 3.41993 avg lat: 1.0249
2021-10-29T03:05:14.870495+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T03:05:14.870495+0000    20     128      2515      2387   29.8344      20.5     2.87374      1.0249
2021-10-29T03:05:15.870616+0000    21     128      2515      2387   28.4137         0           -      1.0249
2021-10-29T03:05:16.870730+0000    22     128      2561      2433   27.6448      5.75     3.06308     1.06343
2021-10-29T03:05:17.870834+0000    23     128      2643      2515   27.3341      20.5     3.06684     1.12875
2021-10-29T03:05:18.870906+0000    24     128      2643      2515   26.1952         0           -     1.12875
2021-10-29T03:05:19.871023+0000    25     128      2689      2561   25.6073      5.75     2.80324     1.15883
2021-10-29T03:05:20.871139+0000    26     128      2771      2643   25.4108      20.5     2.88145     1.21226
2021-10-29T03:05:21.871244+0000    27     128      2817      2689   24.8955      11.5     2.74465     1.23847
2021-10-29T03:05:22.871367+0000    28     128      2817      2689   24.0064         0           -     1.23847
2021-10-29T03:05:23.871441+0000    29     128      2899      2771   23.8854     10.25     3.13048     1.29446
2021-10-29T03:05:24.871592+0000    30     128      2945      2817   23.4725      11.5     2.83399      1.3196
2021-10-29T03:05:25.871698+0000    31     128      2945      2817   22.7153         0           -      1.3196
2021-10-29T03:05:26.871822+0000    32     128      3027      2899    22.646     10.25     2.78831     1.36114
2021-10-29T03:05:27.871939+0000    33     128      3073      2945   22.3082      11.5     2.82249     1.38397
2021-10-29T03:05:28.872010+0000    34     128      3084      2956    21.733      2.75     2.47544     1.38803
2021-10-29T03:05:29.872114+0000    35     128      3155      3027   21.6191     17.75     2.47729     1.41357
2021-10-29T03:05:30.872235+0000    36     128      3201      3073    21.338      11.5     2.79744     1.43429
2021-10-29T03:05:31.872356+0000    37     128      3283      3155   21.3153      20.5     2.82573     1.47046
2021-10-29T03:05:32.872475+0000    38     128      3283      3155   20.7544         0           -     1.47046
2021-10-29T03:05:33.872541+0000    39     128      3329      3201   20.5171      5.75     2.89283      1.4909
2021-10-29T03:05:34.872661+0000 min lat: 0.406403 max lat: 3.41993 avg lat: 1.52039
2021-10-29T03:05:34.872661+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T03:05:34.872661+0000    40     128      3411      3283   20.5166      20.5     2.67163     1.52039
2021-10-29T03:05:35.872785+0000    41     128      3539      3411   20.7966        32     1.25758     1.52491
2021-10-29T03:05:36.872906+0000    42     128      3713      3585    21.337      43.5    0.778124     1.48524
2021-10-29T03:05:37.873017+0000    43     128      3841      3713   21.5849        32    0.981541     1.46603
2021-10-29T03:05:38.873102+0000    44     128      4051      3923   22.2874      52.5    0.623362     1.42318
2021-10-29T03:05:39.873217+0000    45     128      4179      4051   22.5032        32     0.76142     1.40288
2021-10-29T03:05:40.873342+0000    46     128      4225      4097   22.2639      11.5    0.824638     1.39639
2021-10-29T03:05:41.873453+0000    47     128      4307      4179   22.2263      20.5      1.9655     1.40755
2021-10-29T03:05:42.873574+0000    48     128      4353      4225   22.0028      11.5     2.82111     1.42294
2021-10-29T03:05:43.873648+0000    49     128      4353      4225   21.5538         0           -     1.42294
2021-10-29T03:05:44.873771+0000    50     128      4435      4307   21.5327     10.25     3.01625     1.45327
2021-10-29T03:05:45.873882+0000    51     128      4435      4307   21.1105         0           -     1.45327
2021-10-29T03:05:46.874000+0000    52     128      4435      4307   20.7045         0           -     1.45327
2021-10-29T03:05:47.874115+0000    53     128      4481      4353   20.5308   3.83333     4.16385     1.48191
2021-10-29T03:05:48.874189+0000    54     128      4481      4353   20.1506         0           -     1.48191
2021-10-29T03:05:49.874262+0000    55     128      4481      4353   19.7843         0           -     1.48191
2021-10-29T03:05:50.874418+0000    56     128      4563      4435    19.797   6.83333     6.40973     1.57285
2021-10-29T03:05:51.874579+0000    57     128      4563      4435   19.4497         0           -     1.57285
2021-10-29T03:05:52.874708+0000    58     128      4563      4435   19.1143         0           -     1.57285
2021-10-29T03:05:53.874808+0000    59     128      4609      4481   18.9852   3.83333     6.03009      1.6186
2021-10-29T03:05:54.874971+0000 min lat: 0.406403 max lat: 6.41021 avg lat: 1.6186
2021-10-29T03:05:54.874971+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T03:05:54.874971+0000    60     128      4609      4481   18.6688         0           -      1.6186
2021-10-29T03:05:55.875096+0000    61     128      4609      4481   18.3627         0           -      1.6186
2021-10-29T03:05:56.875217+0000    62      46      4609      4563   18.3972   6.83333     6.06397     1.69866
2021-10-29T03:05:57.875300+0000    63      46      4609      4563   18.1052         0           -     1.69866
2021-10-29T03:05:58.875448+0000 Total time run:         63.9251
Total writes made:      4609
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     18.025
Stddev Bandwidth:       17.1981
Max bandwidth (MB/sec): 52.5
Min bandwidth (MB/sec): 0
Average IOPS:           72
Stddev IOPS:            68.8063
Max IOPS:               210
Min IOPS:               0
Average Latency(s):     1.74043
Stddev Latency(s):      1.46521
Max latency(s):         6.41021
Min latency(s):         0.406403

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:05:59,967706565-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:05:59,973552734-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 1094519

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:05:59,979343308-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 85201
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:05:59,987053330-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 85201
[1] 20:06:00 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:06:00,169299479-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_256KB_write.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_256KB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_write.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:06:00,344901886-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:06:25,556482950-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   4.7 GiB used, 395 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:06:25,564262513-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:06:34,628629783-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   3.5 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:06:34,636232343-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:06:43,677021767-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:06:43,684578871-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:06:52,693955343-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:06:52,701959349-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:07:01,740180782-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:07:01,747999890-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:07:01,754112531-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:07:01,757884220-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:07:01,764524535-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:07:01,770043377-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=1096588
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:07:01,776762011-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:07:01,785756664-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'86023\n'
[1] 20:07:01 [SUCCESS] ljishen@10.10.2.2
86023

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:07:01,974540294-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_256KB_seq.log.3
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:07:01,994385939-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:07:01,997344436-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3c27bba6-3864-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3c27bba6-3864-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T03:07:05.093488+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T03:07:05.093488+0000     0       0         0         0         0         0           -           0
2021-10-29T03:07:06.093632+0000     1     128       568       440   109.976       110    0.661696    0.221103
2021-10-29T03:07:07.093711+0000     2     128       882       754    94.236      78.5   0.0996818    0.309628
2021-10-29T03:07:08.093784+0000     3     128      1304      1176   97.9879     105.5    0.167771     0.30132
2021-10-29T03:07:09.093862+0000     4     128      1650      1522   95.1144      86.5   0.0748355    0.316572
2021-10-29T03:07:10.093934+0000     5     128      1990      1862   93.0903        85    0.273511    0.323865
2021-10-29T03:07:11.094011+0000     6     128      2412      2284   95.1572     105.5    0.327284    0.325303
2021-10-29T03:07:12.094083+0000     7     128      2803      2675   95.5266     97.75    0.127208     0.32606
2021-10-29T03:07:13.094156+0000     8     128      3169      3041   95.0224      91.5    0.236538    0.325562
2021-10-29T03:07:14.094232+0000     9     128      3545      3417    94.908        94    0.188065    0.329754
2021-10-29T03:07:15.094317+0000    10     128      3852      3724   93.0916     76.75    0.806844    0.331339
2021-10-29T03:07:16.094388+0000    11     128      4251      4123   93.6962     99.75    0.122831    0.334261
2021-10-29T03:07:17.094454+0000    12     128      4609      4481   93.3461      89.5    0.190423    0.334072
2021-10-29T03:07:18.094558+0000 Total time run:       12.4563
Total reads made:     4609
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   92.5037
Average IOPS:         370
Stddev IOPS:          42.8814
Max IOPS:             440
Min IOPS:             307
Average Latency(s):   0.34008
Max latency(s):       1.02819
Min latency(s):       0.00556219

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:07:18,775904842-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:07:18,781590688-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 1096588

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:07:18,787634440-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 86023
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:07:18,795480478-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 86023
[1] 20:07:18 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:07:18,981880375-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_256KB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_256KB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_256KB_seq.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:07:19,136652771-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:07:43,221723963-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:07:43,229652947-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:07:52,410202180-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:07:52,418171010-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:08:01,385810174-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:08:01,393386694-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:08:10,574055662-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:08:10,582011327-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:08:19,605070195-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.61k objects, 1.1 GiB
    usage:   3.4 GiB used, 397 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:08:19,613204096-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:08:19,618763915-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T20:08:19,622363580-07:00][RUNNING][ROUND 1/5/21] object_size=1MB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:08:19,626082750-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:08:19,635396023-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:08:20,036234562-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/3c27bba6-3864-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 3c27bba6-3864-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:08:20,046702612-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:08:20,050536560-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '3c27bba6-3864-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 3c27bba6-3864-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:08:20,059829510-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 3c27bba6-3864-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 20:08:25 [SUCCESS] 10.10.2.1\n[2] 20:08:26 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:08:26,160887022-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:08:26,172567170-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:08:26,177553143-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:08:26,328185825-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:08:26,333119931-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:08:26,482892217-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:08:26,767495146-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:08:26,772487351-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--5cb78490--1579--41e2--93e6--98857d9345ea-osd--block--2e1a440b--000a--4ee8--906d--7f645961b3f6 (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-5cb78490-1579-41e2-93e6-98857d9345ea" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-2e1a440b-000a-4ee8-906d-7f645961b3f6"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-5cb78490-1579-41e2-93e6-98857d9345ea" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-2e1a440b-000a-4ee8-906d-7f645961b3f6" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-5cb78490-1579-41e2-93e6-98857d9345ea"\n'
10.10.2.1: b'  Volume group "ceph-5cb78490-1579-41e2-93e6-98857d9345ea" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:08:27,119810723-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:08:27,129810232-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:08:27,133497844-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 7c8390f2-3865-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\nWrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 7c8390f2-3865-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:09:27,255417363-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:09:47,262512211-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:09:47,272769274-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:09:47,276816012-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 7c8390f2-3865-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/7c8390f2-3865-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:09:56,032963878-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:09:56,042846787-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:09:56,046391881-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 7c8390f2-3865-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/7c8390f2-3865-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:10:05,279605481-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:10:05,285298494-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:10:05,497072518-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:10:05,500884885-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid 7c8390f2-3865-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/7c8390f2-3865-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:10:14,892946877-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:10:34,897250691-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:10:34,904222758-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:10:34,914197691-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:10:34,917945727-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 7c8390f2-3865-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/7c8390f2-3865-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:10:59,843267501-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:11:19,848518780-07:00] STAGE: Show Cluster Status\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:11:19,857895027-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:11:19,861718956-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 7c8390f2-3865-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/7c8390f2-3865-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     7c8390f2-3865-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.bfqabr(active, since 2m)\n    osd: 1 osds: 1 up (since 17s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 20:11:28 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:08:20,036234562-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/3c27bba6-3864-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 3c27bba6-3864-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:08:20,046702612-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:08:20,050536560-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '3c27bba6-3864-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 3c27bba6-3864-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:08:20,059829510-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 3c27bba6-3864-11ec-b51d-53e6e728d2d3'
[1] 20:08:25 [SUCCESS] 10.10.2.1
[2] 20:08:26 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:08:26,160887022-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:08:26,172567170-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:08:26,177553143-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:08:26,328185825-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:08:26,333119931-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:08:26,482892217-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:08:26,767495146-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:08:26,772487351-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--5cb78490--1579--41e2--93e6--98857d9345ea-osd--block--2e1a440b--000a--4ee8--906d--7f645961b3f6 (253:0)
  Archiving volume group "ceph-5cb78490-1579-41e2-93e6-98857d9345ea" metadata (seqno 5).
  Releasing logical volume "osd-block-2e1a440b-000a-4ee8-906d-7f645961b3f6"
  Creating volume group backup "/etc/lvm/backup/ceph-5cb78490-1579-41e2-93e6-98857d9345ea" (seqno 6).
  Logical volume "osd-block-2e1a440b-000a-4ee8-906d-7f645961b3f6" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-5cb78490-1579-41e2-93e6-98857d9345ea"
  Volume group "ceph-5cb78490-1579-41e2-93e6-98857d9345ea" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:08:27,119810723-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:08:27,129810232-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:08:27,133497844-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 7c8390f2-3865-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 7c8390f2-3865-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:09:27,255417363-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:09:47,262512211-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:09:47,272769274-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:09:47,276816012-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 7c8390f2-3865-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/7c8390f2-3865-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:09:56,032963878-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:09:56,042846787-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:09:56,046391881-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 7c8390f2-3865-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/7c8390f2-3865-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:10:05,279605481-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:10:05,285298494-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:10:05,497072518-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:10:05,500884885-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 7c8390f2-3865-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/7c8390f2-3865-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:10:14,892946877-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:10:34,897250691-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:10:34,904222758-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:10:34,914197691-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:10:34,917945727-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 7c8390f2-3865-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/7c8390f2-3865-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:10:59,843267501-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:11:19,848518780-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:11:19,857895027-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:11:19,861718956-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 7c8390f2-3865-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/7c8390f2-3865-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     7c8390f2-3865-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.bfqabr(active, since 2m)
    osd: 1 osds: 1 up (since 17s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:11:28,468057484-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:11:28,475740625-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 20:11:28 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:11:28,953181520-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:11:28,956984978-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:11:28,978535266-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:11:28,981357766-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7c8390f2-3865-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7c8390f2-3865-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:11:33,150242020-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:11:33,153241243-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7c8390f2-3865-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7c8390f2-3865-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:11:37,136044933-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:11:37,139340865-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7c8390f2-3865-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7c8390f2-3865-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:11:41,210265183-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:11:41,213202259-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7c8390f2-3865-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7c8390f2-3865-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:11:49,399834049-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:11:49,402657861-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7c8390f2-3865-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7c8390f2-3865-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:11:53,982168883-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:11:53,985141857-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7c8390f2-3865-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7c8390f2-3865-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:11:58,381520132-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:11:58,384756422-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7c8390f2-3865-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7c8390f2-3865-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:12:02,939446359-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:12:02,942727273-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7c8390f2-3865-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7c8390f2-3865-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:12:07,255382987-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:12:07,258446961-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7c8390f2-3865-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7c8390f2-3865-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:12:11,754853952-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:12:11,757923187-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7c8390f2-3865-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7c8390f2-3865-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:12:16,153336910-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:12:16,156370388-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7c8390f2-3865-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7c8390f2-3865-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:12:20,302699789-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:12:20,305748625-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7c8390f2-3865-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7c8390f2-3865-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default   
-3         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host node-1
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0  
                       TOTAL  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:12:24,295671331-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:12:48,276503484-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:12:57,423593915-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:13:06,598061512-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:13:15,554419535-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:13:15,562060607-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:13:24,645398103-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:13:24,652754488-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:13:33,763112687-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:13:33,771051380-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:13:42,693733025-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:13:42,701845034-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:13:51,767954685-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:13:51,775713619-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:13:51,781333471-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:13:51,785004129-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:13:51,792083181-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:13:51,797984544-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=1104596
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:13:51,805102981-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:13:51,814084599-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'91796\n'
[1] 20:13:51 [SUCCESS] ljishen@10.10.2.2
91796

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:13:52,002532973-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_1MB_write.log.1
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:13:52,022524483-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:13:52,025464684-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7c8390f2-3865-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7c8390f2-3865-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T03:13:55.091579+0000 Maintaining 128 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 60 seconds or 0 objects
2021-10-29T03:13:55.091593+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-29T03:13:55.123717+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T03:13:55.123717+0000     0       0         0         0         0         0           -           0
2021-10-29T03:13:56.123827+0000     1      67        67         0         0         0           -           0
2021-10-29T03:13:57.123897+0000     2     120       120         0         0         0           -           0
2021-10-29T03:13:58.123971+0000     3     127       166        39   12.9991        13     2.32129     2.52984
2021-10-29T03:13:59.124046+0000     4     127       219        92   22.9984        53      2.3533      2.5571
2021-10-29T03:14:00.124117+0000     5     127       265       138   27.5981        46     2.41391     2.59021
2021-10-29T03:14:01.124193+0000     6     127       298       171    28.498        33     2.53595     2.61631
2021-10-29T03:14:02.124277+0000     7     127       351       224   31.9977        53     2.55399     2.65718
2021-10-29T03:14:03.124361+0000     8     127       397       270   33.7475        46     2.39958     2.66327
2021-10-29T03:14:04.124433+0000     9     127       463       336   37.3306        66     2.25772     2.64559
2021-10-29T03:14:05.124508+0000    10     127       496       369   36.8973        33     2.29816     2.63864
2021-10-29T03:14:06.124598+0000    11     127       529       402   36.5427        33     2.51115     2.64833
2021-10-29T03:14:07.124703+0000    12     127       549       422   35.1639        20     2.83216     2.66783
2021-10-29T03:14:08.124796+0000    13     127       562       435   33.4589        13     3.69567     2.70572
2021-10-29T03:14:09.124860+0000    14     127       582       455   32.4975        20     4.90282     2.81217
2021-10-29T03:14:10.124929+0000    15     127       595       468   31.1976        13     5.57933     2.89569
2021-10-29T03:14:11.125044+0000    16     127       595       468   29.2477         0           -     2.89569
2021-10-29T03:14:12.125161+0000    17     127       595       468   27.5272         0           -     2.89569
2021-10-29T03:14:13.125224+0000    18     127       615       488   27.1089   6.66667     7.47141     3.09717
2021-10-29T03:14:14.125324+0000    19     127       628       501   26.3663        13     7.97951     3.23317
2021-10-29T03:14:15.125436+0000 min lat: 2.21465 max lat: 8.49815 avg lat: 3.23317
2021-10-29T03:14:15.125436+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T03:14:15.125436+0000    20     127       628       501   25.0479         0           -     3.23317
2021-10-29T03:14:16.125554+0000    21     127       648       521   24.8074        10     9.49996     3.48301
2021-10-29T03:14:17.125619+0000    22     127       661       534   24.2707        13     10.2482     3.65792
2021-10-29T03:14:18.125698+0000    23     127       681       554   24.0849        20     10.3818     3.93565
2021-10-29T03:14:19.125781+0000    24     127       694       567    23.623        13      9.8316     4.09445
2021-10-29T03:14:20.125894+0000    25     127       694       567   22.6781         0           -     4.09445
2021-10-29T03:14:21.125962+0000    26     127       714       587    22.575        10     10.6027     4.34407
2021-10-29T03:14:22.126055+0000    27     127       727       600   22.2203        13     9.19484     4.48394
2021-10-29T03:14:23.126165+0000    28     127       727       600   21.4267         0           -     4.48394
2021-10-29T03:14:24.126262+0000    29     127       747       620   21.3775        10     10.0743     4.69075
2021-10-29T03:14:25.126330+0000    30     127       760       633   21.0982        13     9.55072     4.81646
2021-10-29T03:14:26.126404+0000    31     127       780       653   21.0627        20     9.39372     4.98984
2021-10-29T03:14:27.126477+0000    32     127       826       699   21.8419        46     6.34391     5.22019
2021-10-29T03:14:28.126554+0000    33     127       859       732   22.1799        33      4.4064     5.27161
2021-10-29T03:14:29.126619+0000    34     127       912       785   23.0863        53     2.56956     5.15785
2021-10-29T03:14:30.126698+0000    35     127       958       831   23.7409        46     2.30917     5.02535
2021-10-29T03:14:31.126777+0000    36     127      1011       884   24.5535        53     2.39766     4.87946
2021-10-29T03:14:32.126855+0000    37     127      1057       930    25.133        46     2.42147     4.76886
2021-10-29T03:14:33.126927+0000    38     127      1057       930   24.4717         0           -     4.76886
2021-10-29T03:14:34.127018+0000    39     127      1077       950   24.3569        10     3.68772     4.75291
2021-10-29T03:14:35.127117+0000 min lat: 2.21465 max lat: 11.6261 avg lat: 4.75021
2021-10-29T03:14:35.127117+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T03:14:35.127117+0000    40     127      1090       963    24.073        13     4.33524     4.75021
2021-10-29T03:14:36.127236+0000    41     127      1090       963   23.4858         0           -     4.75021
2021-10-29T03:14:37.127303+0000    42     127      1090       963   22.9266         0           -     4.75021
2021-10-29T03:14:38.127410+0000    43     127      1110       983   22.8585   6.66667     6.83595     4.79702
2021-10-29T03:14:39.127522+0000    44     127      1110       983    22.339         0           -     4.79702
2021-10-29T03:14:40.127604+0000    45     127      1123       996   22.1314       6.5      8.8503     4.85364
2021-10-29T03:14:41.127669+0000    46     127      1123       996   21.6503         0           -     4.85364
2021-10-29T03:14:42.127780+0000    47     127      1123       996   21.1897         0           -     4.85364
2021-10-29T03:14:43.127876+0000    48     127      1143      1016   21.1649   6.66667      11.178     4.98383
2021-10-29T03:14:44.127993+0000    49     127      1143      1016   20.7329         0           -     4.98383
2021-10-29T03:14:45.128090+0000    50     127      1156      1029   20.5782       6.5     13.0294     5.08802
2021-10-29T03:14:46.128207+0000    51     127      1156      1029   20.1747         0           -     5.08802
2021-10-29T03:14:47.128322+0000    52     127      1156      1029   19.7867         0           -     5.08802
2021-10-29T03:14:48.128387+0000    53     127      1176      1049   19.7907   6.66667     16.2191     5.30569
2021-10-29T03:14:49.128455+0000    54     127      1176      1049   19.4242         0           -     5.30569
2021-10-29T03:14:50.128535+0000    55     127      1189      1062   19.3074       6.5     16.2815      5.4542
2021-10-29T03:14:51.128651+0000    56     127      1189      1062   18.9626         0           -      5.4542
2021-10-29T03:14:52.128726+0000    57     127      1189      1062     18.63         0           -      5.4542
2021-10-29T03:14:53.128794+0000    58     127      1189      1062   18.3088         0           -      5.4542
2021-10-29T03:14:54.128893+0000    59     127      1209      1082   18.3374         5       18.81     5.71529
2021-10-29T03:14:55.129040+0000 min lat: 2.21465 max lat: 19.7718 avg lat: 5.71529
2021-10-29T03:14:55.129040+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T03:14:55.129040+0000    60     127      1209      1082   18.0317         0           -     5.71529
2021-10-29T03:14:56.129141+0000    61      21      1210      1189   19.4901      53.5     5.94218     6.38504
2021-10-29T03:14:57.129220+0000    62      21      1210      1189   19.1757         0           -     6.38504
2021-10-29T03:14:58.129307+0000    63      21      1210      1189   18.8714         0           -     6.38504
2021-10-29T03:14:59.129432+0000 Total time run:         63.3401
Total writes made:      1210
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     19.1032
Stddev Bandwidth:       18.8134
Max bandwidth (MB/sec): 66
Min bandwidth (MB/sec): 0
Average IOPS:           19
Stddev IOPS:            18.8302
Max IOPS:               66
Min IOPS:               0
Average Latency(s):     6.36314
Stddev Latency(s):      4.79691
Max latency(s):         21.26
Min latency(s):         2.21465

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:14:59,980944592-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:14:59,986816279-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 1104596

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:14:59,992528265-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 91796
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:15:00,000553831-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 91796
[1] 20:15:00 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:15:00,185834013-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_1MB_write.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_1MB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:15:00,360969083-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:15:24,544747586-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:15:24,552233255-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:15:33,614128587-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:15:33,621709566-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:15:42,771007948-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:15:42,778342872-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:15:52,001874382-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:15:52,009285761-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:16:01,094106297-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:16:01,101839382-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:16:01,107854649-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:16:01,111842857-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:16:01,118635960-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:16:01,124150764-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=1106645
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:16:01,131150006-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:16:01,139949200-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'92587\n'
[1] 20:16:01 [SUCCESS] ljishen@10.10.2.2
92587

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:16:01,330604743-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_1MB_seq.log.1
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:16:01,350467580-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:16:01,353350453-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7c8390f2-3865-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7c8390f2-3865-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T03:16:04.448924+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T03:16:04.448924+0000     0       0         0         0         0         0           -           0
2021-10-29T03:16:05.449046+0000     1     127       248       121   120.977       121    0.813484     0.52428
2021-10-29T03:16:06.449163+0000     2     127       361       234   116.982       113    0.968598    0.740841
2021-10-29T03:16:07.449234+0000     3     127       489       362   120.651       128     1.33911    0.837903
2021-10-29T03:16:08.449298+0000     4     127       632       505   126.236       143      1.1669    0.884552
2021-10-29T03:16:09.449365+0000     5     127       753       626   125.187       121    0.422848      0.9005
2021-10-29T03:16:10.449429+0000     6     127       878       751   125.155       125        1.24    0.934136
2021-10-29T03:16:11.449496+0000     7     127       990       863   123.274       112     1.00331    0.942506
2021-10-29T03:16:12.449562+0000     8     127      1115       988   123.489       125    0.881748    0.963637
2021-10-29T03:16:13.449627+0000     9      64      1210      1146   127.322       158    0.907317    0.951714
2021-10-29T03:16:14.449726+0000 Total time run:       9.71041
Total reads made:     1210
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   124.609
Average IOPS:         124
Stddev IOPS:          14.6544
Max IOPS:             158
Min IOPS:             112
Average Latency(s):   0.957712
Max latency(s):       1.8825
Min latency(s):       0.125565

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:16:15,078508790-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:16:15,084311877-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 1106645

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:16:15,089845126-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 92587
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:16:15,097719126-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 92587
[1] 20:16:15 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:16:15,281925634-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_1MB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_1MB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:16:15,436232076-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:16:39,399399231-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:16:39,407037447-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:16:48,572263053-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:16:48,580172832-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:16:57,677086431-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:16:57,684804778-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:17:06,934488538-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:17:06,941935234-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:17:15,944412516-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:17:15,952676222-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:17:15,958663778-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T20:17:15,960837064-07:00][RUNNING][ROUND 2/5/21] object_size=1MB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:17:15,964483928-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:17:15,973911398-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:17:16,402531440-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/7c8390f2-3865-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 7c8390f2-3865-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:17:16,413673646-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:17:16,417426291-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '7c8390f2-3865-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 7c8390f2-3865-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:17:16,425690848-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 7c8390f2-3865-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 20:17:22 [SUCCESS] 10.10.2.1\n[2] 20:17:22 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:17:22,687119933-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:17:22,698446286-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:17:22,703780925-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:17:22,855589664-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:17:22,860281254-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:17:23,010484004-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:17:23,295062330-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:17:23,300159763-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--5c32996a--c8d0--4d71--ad25--4e62c00ec189-osd--block--4db950ab--0e86--444a--a881--30e436dba6d0 (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-5c32996a-c8d0-4d71-ad25-4e62c00ec189" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-4db950ab-0e86-444a-a881-30e436dba6d0"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-5c32996a-c8d0-4d71-ad25-4e62c00ec189" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-4db950ab-0e86-444a-a881-30e436dba6d0" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-5c32996a-c8d0-4d71-ad25-4e62c00ec189"\n'
10.10.2.1: b'  Volume group "ceph-5c32996a-c8d0-4d71-ad25-4e62c00ec189" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:17:23,672031745-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:17:23,682061520-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:17:23,686004643-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: bc5260fe-3866-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\nWaiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid bc5260fe-3866-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:18:23,669719216-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:18:43,676692445-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:18:43,686718594-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:18:43,690818431-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid bc5260fe-3866-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/bc5260fe-3866-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:18:52,778174144-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:18:52,788570037-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:18:52,792382565-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid bc5260fe-3866-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/bc5260fe-3866-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:19:01,508607737-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:19:01,514754553-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n"
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:19:01,730145711-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:19:01,733996741-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid bc5260fe-3866-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/bc5260fe-3866-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:19:11,129590840-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:19:31,134038840-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:19:31,140539301-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:19:31,150158193-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:19:31,153844583-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid bc5260fe-3866-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/bc5260fe-3866-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:19:56,332460152-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:20:16,337854119-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:20:16,348229564-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:20:16,352174450-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid bc5260fe-3866-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/bc5260fe-3866-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     bc5260fe-3866-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.mxqxid(active, since 2m)\n    osd: 1 osds: 1 up (since 17s), 1 in (since 41s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 20:20:24 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:17:16,402531440-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/7c8390f2-3865-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 7c8390f2-3865-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:17:16,413673646-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:17:16,417426291-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '7c8390f2-3865-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 7c8390f2-3865-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:17:16,425690848-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 7c8390f2-3865-11ec-b51d-53e6e728d2d3'
[1] 20:17:22 [SUCCESS] 10.10.2.1
[2] 20:17:22 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:17:22,687119933-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:17:22,698446286-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:17:22,703780925-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:17:22,855589664-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:17:22,860281254-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:17:23,010484004-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:17:23,295062330-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:17:23,300159763-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--5c32996a--c8d0--4d71--ad25--4e62c00ec189-osd--block--4db950ab--0e86--444a--a881--30e436dba6d0 (253:0)
  Archiving volume group "ceph-5c32996a-c8d0-4d71-ad25-4e62c00ec189" metadata (seqno 5).
  Releasing logical volume "osd-block-4db950ab-0e86-444a-a881-30e436dba6d0"
  Creating volume group backup "/etc/lvm/backup/ceph-5c32996a-c8d0-4d71-ad25-4e62c00ec189" (seqno 6).
  Logical volume "osd-block-4db950ab-0e86-444a-a881-30e436dba6d0" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-5c32996a-c8d0-4d71-ad25-4e62c00ec189"
  Volume group "ceph-5c32996a-c8d0-4d71-ad25-4e62c00ec189" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:17:23,672031745-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:17:23,682061520-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:17:23,686004643-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: bc5260fe-3866-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid bc5260fe-3866-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:18:23,669719216-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:18:43,676692445-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:18:43,686718594-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:18:43,690818431-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid bc5260fe-3866-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/bc5260fe-3866-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:18:52,778174144-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:18:52,788570037-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:18:52,792382565-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid bc5260fe-3866-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/bc5260fe-3866-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:19:01,508607737-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:19:01,514754553-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:19:01,730145711-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:19:01,733996741-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid bc5260fe-3866-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/bc5260fe-3866-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:19:11,129590840-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:19:31,134038840-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:19:31,140539301-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:19:31,150158193-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:19:31,153844583-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid bc5260fe-3866-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/bc5260fe-3866-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:19:56,332460152-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:20:16,337854119-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:20:16,348229564-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:20:16,352174450-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid bc5260fe-3866-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/bc5260fe-3866-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     bc5260fe-3866-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.mxqxid(active, since 2m)
    osd: 1 osds: 1 up (since 17s), 1 in (since 41s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:20:24,687097264-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:20:24,694858533-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 20:20:24 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:20:25,176777025-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:20:25,180505452-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:20:25,202430819-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:20:25,205285931-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bc5260fe-3866-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bc5260fe-3866-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:20:29,254014865-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:20:29,256959034-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bc5260fe-3866-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bc5260fe-3866-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:20:33,386555492-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:20:33,389553353-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bc5260fe-3866-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bc5260fe-3866-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:20:37,503174478-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:20:37,506092217-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bc5260fe-3866-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bc5260fe-3866-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:20:45,435891173-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:20:45,438859357-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bc5260fe-3866-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bc5260fe-3866-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:20:50,167180946-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:20:50,170250481-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bc5260fe-3866-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bc5260fe-3866-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:20:54,501067766-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:20:54,504120079-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bc5260fe-3866-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bc5260fe-3866-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:20:59,284160467-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:20:59,287059912-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bc5260fe-3866-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bc5260fe-3866-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:21:04,507188756-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:21:04,510131843-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bc5260fe-3866-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bc5260fe-3866-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:21:08,932402730-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:21:08,935329968-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bc5260fe-3866-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bc5260fe-3866-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:21:13,657295406-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:21:13,660189572-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bc5260fe-3866-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bc5260fe-3866-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:21:17,662791968-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:21:17,665792584-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bc5260fe-3866-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bc5260fe-3866-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default   
-3         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host node-1
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0  
                       TOTAL  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:21:21,722174496-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:21:45,718945515-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:21:54,843611495-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:22:03,809104068-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:22:12,959574552-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:22:12,967689097-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:22:21,898694975-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:22:21,906333021-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:22:30,994276568-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:22:31,001760354-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:22:39,893683053-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:22:39,901350195-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:22:49,008527442-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:22:49,016502693-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:22:49,022556464-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:22:49,026130461-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:22:49,033451841-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:22:49,039290986-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=1114762
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:22:49,046203455-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:22:49,055199742-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'98222\n'
[1] 20:22:49 [SUCCESS] ljishen@10.10.2.2
98222

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:22:49,242723006-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_1MB_write.log.2
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:22:49,263097990-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:22:49,266200808-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bc5260fe-3866-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bc5260fe-3866-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T03:22:52.242347+0000 Maintaining 128 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 60 seconds or 0 objects
2021-10-29T03:22:52.242360+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-29T03:22:52.274540+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T03:22:52.274540+0000     0       0         0         0         0         0           -           0
2021-10-29T03:22:53.274663+0000     1      67        67         0         0         0           -           0
2021-10-29T03:22:54.274776+0000     2     120       120         0         0         0           -           0
2021-10-29T03:22:55.274841+0000     3     127       166        39    12.999        13     2.30903     2.50534
2021-10-29T03:22:56.274911+0000     4     127       219        92   22.9982        53     2.49324     2.58128
2021-10-29T03:22:57.274982+0000     5     127       252       125   24.9981        33     2.62347     2.65137
2021-10-29T03:22:58.275053+0000     6     127       298       171   28.4979        46     2.72911     2.72607
2021-10-29T03:22:59.275119+0000     7     127       351       224   31.9976        53     2.53536     2.77768
2021-10-29T03:23:00.275188+0000     8     127       397       270   33.7475        46     2.42614     2.77904
2021-10-29T03:23:01.275253+0000     9     127       450       323   35.8863        53     2.38036      2.7529
2021-10-29T03:23:02.275319+0000    10     127       483       356   35.5974        33     2.46593     2.74355
2021-10-29T03:23:03.275386+0000    11     127       529       402   36.5428        46     2.34708     2.73081
2021-10-29T03:23:04.275451+0000    12     127       549       422   35.1642        20     2.61756     2.73521
2021-10-29T03:23:05.275519+0000    13     127       562       435   33.4592        13     3.59583     2.76685
2021-10-29T03:23:06.275584+0000    14     127       582       455   32.4977        20      4.6968     2.86176
2021-10-29T03:23:07.275653+0000    15     127       582       455   30.3312         0           -     2.86176
2021-10-29T03:23:08.275721+0000    16     127       595       468    29.248       6.5     5.22655     2.93941
2021-10-29T03:23:09.275786+0000    17     127       615       488   28.7039        20      6.7139     3.10295
2021-10-29T03:23:10.275851+0000    18     127       628       501   27.8314        13     7.55603     3.22414
2021-10-29T03:23:11.275916+0000    19     127       628       501   26.3666         0           -     3.22414
2021-10-29T03:23:12.275982+0000 min lat: 2.17445 max lat: 9.18458 avg lat: 3.45069
2021-10-29T03:23:12.275982+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T03:23:12.275982+0000    20     127       648       521   26.0482        10     8.89163     3.45069
2021-10-29T03:23:13.276065+0000    21     127       661       534   25.4268        13     9.76218     3.61322
2021-10-29T03:23:14.276134+0000    22     127       661       534    24.271         0           -     3.61322
2021-10-29T03:23:15.276199+0000    23     127       681       554   24.0853        10     9.81135     3.87312
2021-10-29T03:23:16.276265+0000    24     127       694       567   23.6234        13     9.25985     4.01874
2021-10-29T03:23:17.276331+0000    25     127       694       567   22.6784         0           -     4.01874
2021-10-29T03:23:18.276400+0000    26     127       714       587   22.5754        10     10.0822     4.25674
2021-10-29T03:23:19.276469+0000    27     127       727       600   22.2207        13     9.47605     4.39619
2021-10-29T03:23:20.276535+0000    28     127       747       620   22.1413        20     9.88865     4.60316
2021-10-29T03:23:21.276607+0000    29     127       747       620   21.3778         0           -     4.60316
2021-10-29T03:23:22.276676+0000    30     127       760       633   21.0985       6.5     9.59063     4.72874
2021-10-29T03:23:23.276741+0000    31     127       793       666   21.4824        33      8.6296     5.00337
2021-10-29T03:23:24.276810+0000    32     127       826       699   21.8422        33     6.67211      5.1704
2021-10-29T03:23:25.276880+0000    33     127       879       752   22.7863        53     3.55086     5.20545
2021-10-29T03:23:26.276950+0000    34     127       925       798    23.469        46     2.37526      5.0733
2021-10-29T03:23:27.277018+0000    35     127       978       851   24.3126        53      2.3725     4.92093
2021-10-29T03:23:28.277090+0000    36     127      1024       897   24.9149        46     2.51326     4.80598
2021-10-29T03:23:29.277161+0000    37     127      1057       930   25.1334        33     2.50064     4.73348
2021-10-29T03:23:30.277231+0000    38     127      1057       930    24.472         0           -     4.73348
2021-10-29T03:23:31.277300+0000    39     127      1077       950   24.3573        10     3.77544     4.71837
2021-10-29T03:23:32.277371+0000 min lat: 2.17445 max lat: 11.2354 avg lat: 4.71907
2021-10-29T03:23:32.277371+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T03:23:32.277371+0000    40     127      1090       963   24.0733        13     4.55159     4.71907
2021-10-29T03:23:33.277453+0000    41     127      1090       963   23.4862         0           -     4.71907
2021-10-29T03:23:34.277522+0000    42     127      1090       963    22.927         0           -     4.71907
2021-10-29T03:23:35.277592+0000    43     127      1110       983   22.8589   6.66667     7.28809     4.77596
2021-10-29T03:23:36.277660+0000    44     127      1110       983   22.3394         0           -     4.77596
2021-10-29T03:23:37.277727+0000    45     127      1123       996   22.1318       6.5     9.27733     4.83898
2021-10-29T03:23:38.277797+0000    46     127      1123       996   21.6507         0           -     4.83898
2021-10-29T03:23:39.277862+0000    47     127      1123       996     21.19         0           -     4.83898
2021-10-29T03:23:40.277935+0000    48     127      1143      1016   21.1652   6.66667      11.849     4.98348
2021-10-29T03:23:41.278006+0000    49     127      1143      1016   20.7333         0           -     4.98348
2021-10-29T03:23:42.278076+0000    50     127      1156      1029   20.5786       6.5     13.5466     5.09418
2021-10-29T03:23:43.278144+0000    51     127      1156      1029   20.1751         0           -     5.09418
2021-10-29T03:23:44.278214+0000    52     127      1156      1029   19.7871         0           -     5.09418
2021-10-29T03:23:45.278278+0000    53     127      1156      1029   19.4137         0           -     5.09418
2021-10-29T03:23:46.278344+0000    54     127      1176      1049   19.4246         5     16.6292     5.32041
2021-10-29T03:23:47.278415+0000    55     127      1176      1049   19.0714         0           -     5.32041
2021-10-29T03:23:48.278483+0000    56     127      1189      1062    18.963       6.5     16.8762     5.47521
2021-10-29T03:23:49.278552+0000    57     127      1189      1062   18.6303         0           -     5.47521
2021-10-29T03:23:50.278619+0000    58     127      1189      1062   18.3091         0           -     5.47521
2021-10-29T03:23:51.278685+0000    59     127      1209      1082   18.3377   6.66667     19.2024     5.74511
2021-10-29T03:23:52.278755+0000 min lat: 2.17445 max lat: 20.2956 avg lat: 5.74511
2021-10-29T03:23:52.278755+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T03:23:52.278755+0000    60     127      1209      1082   18.0321         0           -     5.74511
2021-10-29T03:23:53.278836+0000    61      21      1210      1189   19.4905      53.5     5.87172     6.41516
2021-10-29T03:23:54.278905+0000    62      21      1210      1189   19.1761         0           -     6.41516
2021-10-29T03:23:55.278970+0000    63      21      1210      1189   18.8717         0           -     6.41516
2021-10-29T03:23:56.279067+0000 Total time run:         63.5478
Total writes made:      1210
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     19.0408
Stddev Bandwidth:       18.4819
Max bandwidth (MB/sec): 53.5
Min bandwidth (MB/sec): 0
Average IOPS:           19
Stddev IOPS:            18.5019
Max IOPS:               53
Min IOPS:               0
Average Latency(s):     6.39133
Stddev Latency(s):      4.83922
Max latency(s):         21.6523
Min latency(s):         2.17445

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:23:57,178150885-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:23:57,184205156-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 1114762

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:23:57,190120906-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 98222
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:23:57,198012972-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 98222
[1] 20:23:57 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:23:57,381837033-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_1MB_write.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_1MB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:23:57,557355739-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:24:21,556916345-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:24:21,565138623-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:24:30,629846440-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:24:30,637676489-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:24:39,753505034-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:24:39,761280891-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:24:48,802395035-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:24:48,810758649-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:24:57,775929646-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:24:57,783731441-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:24:57,789727402-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:24:57,793318401-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:24:57,800196585-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:24:57,805939730-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=1116727
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:24:57,812919656-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:24:57,822015420-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'99012\n'
[1] 20:24:57 [SUCCESS] ljishen@10.10.2.2
99012

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:24:58,011324118-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_1MB_seq.log.2
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:24:58,031820561-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:24:58,034710047-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bc5260fe-3866-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bc5260fe-3866-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T03:25:01.121784+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T03:25:01.121784+0000     0       0         0         0         0         0           -           0
2021-10-29T03:25:02.121945+0000     1     127       216        89   88.9797        89    0.927638    0.593365
2021-10-29T03:25:03.122065+0000     2     127       345       218   108.981       129    0.383533    0.837347
2021-10-29T03:25:04.122184+0000     3     127       476       349   116.315       131    0.583473    0.867358
2021-10-29T03:25:05.122258+0000     4     127       598       471   117.734       122    0.940378    0.894878
2021-10-29T03:25:06.122378+0000     5     127       709       582   116.385       111     2.06446    0.919954
2021-10-29T03:25:07.122490+0000     6     127       816       689   114.819       107     0.58832    0.965575
2021-10-29T03:25:08.122603+0000     7     127       926       799   114.128       110     1.61701     1.00402
2021-10-29T03:25:09.122674+0000     8     127      1039       912   113.986       113      0.5591     1.01548
2021-10-29T03:25:10.122793+0000     9     127      1156      1029    114.32       117     1.01381     1.03163
2021-10-29T03:25:11.122921+0000    10      48      1210      1162   116.186       133    0.546847     1.02854
2021-10-29T03:25:12.123082+0000 Total time run:       10.582
Total reads made:     1210
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   114.345
Average IOPS:         114
Stddev IOPS:          13.3317
Max IOPS:             133
Min IOPS:             89
Average Latency(s):   1.04338
Max latency(s):       2.6851
Min latency(s):       0.17276

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:25:12,852747653-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:25:12,858806943-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 1116727

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:25:12,864970451-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 99012
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:25:12,873085105-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 99012
[1] 20:25:13 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:25:13,057689927-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_1MB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_1MB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:25:13,212044992-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:25:37,213622372-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:25:37,221829231-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:25:46,503913537-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:25:46,511545912-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:25:55,535934617-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:25:55,543848483-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:26:04,643123459-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:26:04,651081108-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:26:13,749302574-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:26:13,756991395-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:26:13,763270090-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T20:26:13,765634687-07:00][RUNNING][ROUND 3/5/21] object_size=1MB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:26:13,769300506-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:26:13,778815761-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:26:14,173970953-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/bc5260fe-3866-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid bc5260fe-3866-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:26:14,185108735-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:26:14,189101202-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'bc5260fe-3866-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid bc5260fe-3866-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:26:14,198429511-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid bc5260fe-3866-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 20:26:19 [SUCCESS] 10.10.2.1\n[2] 20:26:20 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:26:20,375222445-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:26:20,387275508-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:26:20,392183756-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:26:20,543953961-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:26:20,548755219-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:26:20,699194661-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:26:20,983212112-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:26:20,988257238-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--e99562a8--4ad9--4923--ab99--5c9c75d77b78-osd--block--76653418--57ae--423e--8f7b--da938f7bd3c2 (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-e99562a8-4ad9-4923-ab99-5c9c75d77b78" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-76653418-57ae-423e-8f7b-da938f7bd3c2"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-e99562a8-4ad9-4923-ab99-5c9c75d77b78" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-76653418-57ae-423e-8f7b-da938f7bd3c2" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-e99562a8-4ad9-4923-ab99-5c9c75d77b78"\n'
10.10.2.1: b'  Volume group "ceph-e99562a8-4ad9-4923-ab99-5c9c75d77b78" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:26:21,323465439-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:26:21,333025995-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:26:21,336628108-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\nVerifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\npodman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\nHost looks OK\n'
10.10.2.1: b'Cluster fsid: fcca3278-3867-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid fcca3278-3867-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\nPlease consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:27:21,545356384-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:27:41,552362884-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:27:41,562131311-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:27:41,565884979-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid fcca3278-3867-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/fcca3278-3867-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:27:50,435882341-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:27:50,445575897-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:27:50,449867967-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid fcca3278-3867-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/fcca3278-3867-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:27:59,500210014-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:27:59,506095400-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:27:59,722284216-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:27:59,725638442-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid fcca3278-3867-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/fcca3278-3867-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:28:09,235715841-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:28:29,240558217-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:28:29,247363703-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:28:29,257620969-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:28:29,261352095-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid fcca3278-3867-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/fcca3278-3867-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:28:54,526571509-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:29:14,532448771-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:29:14,542603774-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:29:14,546734521-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n"
10.10.2.1: b'Inferring fsid fcca3278-3867-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/fcca3278-3867-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     fcca3278-3867-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.nkjlyg(active, since 2m)\n    osd: 1 osds: 1 up (since 16s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 20:29:21 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:26:14,173970953-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/bc5260fe-3866-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid bc5260fe-3866-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:26:14,185108735-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:26:14,189101202-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'bc5260fe-3866-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid bc5260fe-3866-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:26:14,198429511-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid bc5260fe-3866-11ec-b51d-53e6e728d2d3'
[1] 20:26:19 [SUCCESS] 10.10.2.1
[2] 20:26:20 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:26:20,375222445-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:26:20,387275508-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:26:20,392183756-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:26:20,543953961-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:26:20,548755219-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:26:20,699194661-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:26:20,983212112-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:26:20,988257238-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--e99562a8--4ad9--4923--ab99--5c9c75d77b78-osd--block--76653418--57ae--423e--8f7b--da938f7bd3c2 (253:0)
  Archiving volume group "ceph-e99562a8-4ad9-4923-ab99-5c9c75d77b78" metadata (seqno 5).
  Releasing logical volume "osd-block-76653418-57ae-423e-8f7b-da938f7bd3c2"
  Creating volume group backup "/etc/lvm/backup/ceph-e99562a8-4ad9-4923-ab99-5c9c75d77b78" (seqno 6).
  Logical volume "osd-block-76653418-57ae-423e-8f7b-da938f7bd3c2" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-e99562a8-4ad9-4923-ab99-5c9c75d77b78"
  Volume group "ceph-e99562a8-4ad9-4923-ab99-5c9c75d77b78" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:26:21,323465439-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:26:21,333025995-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:26:21,336628108-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: fcca3278-3867-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid fcca3278-3867-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:27:21,545356384-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:27:41,552362884-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:27:41,562131311-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:27:41,565884979-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid fcca3278-3867-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/fcca3278-3867-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:27:50,435882341-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:27:50,445575897-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:27:50,449867967-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid fcca3278-3867-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/fcca3278-3867-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:27:59,500210014-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:27:59,506095400-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:27:59,722284216-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:27:59,725638442-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid fcca3278-3867-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/fcca3278-3867-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:28:09,235715841-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:28:29,240558217-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:28:29,247363703-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:28:29,257620969-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:28:29,261352095-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid fcca3278-3867-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/fcca3278-3867-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:28:54,526571509-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:29:14,532448771-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:29:14,542603774-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:29:14,546734521-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid fcca3278-3867-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/fcca3278-3867-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     fcca3278-3867-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.nkjlyg(active, since 2m)
    osd: 1 osds: 1 up (since 16s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:29:21,665662803-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:29:21,673645639-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 20:29:21 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:29:22,152789071-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:29:22,156417971-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:29:22,178384464-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:29:22,181281845-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fcca3278-3867-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fcca3278-3867-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:29:26,302670437-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:29:26,305934198-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fcca3278-3867-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fcca3278-3867-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:29:30,387650072-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:29:30,390463806-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fcca3278-3867-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fcca3278-3867-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:29:34,468813143-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:29:34,471917064-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fcca3278-3867-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fcca3278-3867-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:29:42,726972009-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:29:42,729992753-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fcca3278-3867-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fcca3278-3867-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:29:47,825643411-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:29:47,828741721-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fcca3278-3867-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fcca3278-3867-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:29:52,184360989-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:29:52,187298786-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fcca3278-3867-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fcca3278-3867-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:29:56,700361636-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:29:56,703403690-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fcca3278-3867-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fcca3278-3867-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:30:01,124457697-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:30:01,127348105-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fcca3278-3867-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fcca3278-3867-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:30:05,658073834-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:30:05,661088847-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fcca3278-3867-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fcca3278-3867-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:30:10,024971944-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:30:10,028055356-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fcca3278-3867-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fcca3278-3867-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/17 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:30:14,109344624-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:30:14,112436090-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fcca3278-3867-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fcca3278-3867-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default   
-3         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host node-1
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0  
                       TOTAL  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:30:18,184421410-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:30:42,215457487-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:30:51,141643956-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:31:00,123213046-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:31:09,039149505-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:31:09,047025600-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:31:18,196176530-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:31:18,204484198-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:31:27,220645902-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:31:27,228308995-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:31:36,227953141-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:31:36,236068607-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:31:45,094876607-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:31:45,102156308-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:31:45,108625531-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:31:45,112431815-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:31:45,119500238-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:31:45,124938047-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=1126238
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:31:45,132222487-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:31:45,141558895-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'104728\n'
[1] 20:31:45 [SUCCESS] ljishen@10.10.2.2
104728

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:31:45,329013142-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_1MB_write.log.3
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:31:45,349479557-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:31:45,352412174-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fcca3278-3867-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fcca3278-3867-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T03:31:48.406675+0000 Maintaining 128 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 60 seconds or 0 objects
2021-10-29T03:31:48.406691+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-29T03:31:48.438345+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T03:31:48.438345+0000     0       0         0         0         0         0           -           0
2021-10-29T03:31:49.438479+0000     1      67        67         0         0         0           -           0
2021-10-29T03:31:50.438553+0000     2     120       120         0         0         0           -           0
2021-10-29T03:31:51.438619+0000     3     127       166        39   12.9992        13     2.41293     2.50715
2021-10-29T03:31:52.438690+0000     4     127       219        92   22.9985        53     2.40808     2.55692
2021-10-29T03:31:53.438761+0000     5     127       252       125   24.9983        33     2.74029     2.63844
2021-10-29T03:31:54.438833+0000     6     127       298       171   28.4981        46     2.68466      2.7098
2021-10-29T03:31:55.438940+0000     7     127       351       224   31.9976        53     2.53928     2.75403
2021-10-29T03:31:56.439008+0000     8     127       397       270   33.7475        46     2.37004     2.74002
2021-10-29T03:31:57.439075+0000     9     127       450       323   35.8863        53     2.46759     2.71965
2021-10-29T03:31:58.439144+0000    10     127       496       369   36.8973        46     2.33165     2.70388
2021-10-29T03:31:59.439213+0000    11     127       529       402   36.5428        33     2.39177     2.69717
2021-10-29T03:32:00.439284+0000    12     127       549       422   35.1641        20     2.82581     2.71373
2021-10-29T03:32:01.439350+0000    13     127       562       435   33.4592        13      3.5793     2.74872
2021-10-29T03:32:02.439417+0000    14     127       582       455   32.4977        20     4.79508     2.84797
2021-10-29T03:32:03.439484+0000    15     127       595       468   31.1978        13     5.35157     2.92407
2021-10-29T03:32:04.439552+0000    16     127       595       468   29.2479         0           -     2.92407
2021-10-29T03:32:05.439627+0000    17     127       615       488   28.7038        10     7.09891     3.10705
2021-10-29T03:32:06.439696+0000    18     127       615       488   27.1092         0           -     3.10705
2021-10-29T03:32:07.439767+0000    19     127       628       501   26.3666       6.5     7.81705     3.23539
2021-10-29T03:32:08.439834+0000 min lat: 2.11668 max lat: 9.4354 avg lat: 3.47072
2021-10-29T03:32:08.439834+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T03:32:08.439834+0000    20     127       648       521   26.0482        20      9.0878     3.47072
2021-10-29T03:32:09.439910+0000    21     127       648       521   24.8078         0           -     3.47072
2021-10-29T03:32:10.439983+0000    22     127       661       534    24.271       6.5     9.87272     3.63853
2021-10-29T03:32:11.440058+0000    23     127       681       554   24.0852        20     10.4151     3.91767
2021-10-29T03:32:12.440128+0000    24     127       694       567   23.6233        13     9.81215     4.07632
2021-10-29T03:32:13.440199+0000    25     127       694       567   22.6784         0           -     4.07632
2021-10-29T03:32:14.440267+0000    26     127       714       587   22.5753        10     10.8637     4.33205
2021-10-29T03:32:15.440341+0000    27     127       727       600   22.2206        13     9.90079     4.48453
2021-10-29T03:32:16.440410+0000    28     127       727       600   21.4271         0           -     4.48453
2021-10-29T03:32:17.440478+0000    29     127       747       620   21.3778        10     10.5017     4.70598
2021-10-29T03:32:18.440546+0000    30     127       760       633   21.0985        13     10.0158     4.83801
2021-10-29T03:32:19.440614+0000    31     127       780       653    21.063        20     9.74908     5.02504
2021-10-29T03:32:20.440683+0000    32     127       826       699   21.8422        46     6.12542     5.25244
2021-10-29T03:32:21.440754+0000    33     127       859       732   22.1803        33     4.19021     5.28682
2021-10-29T03:32:22.440821+0000    34     127       912       785   23.0866        53     2.64997     5.16988
2021-10-29T03:32:23.440888+0000    35     127       958       831   23.7412        46     2.56172     5.04441
2021-10-29T03:32:24.440956+0000    36     127      1011       884   24.5538        53     2.33909     4.90459
2021-10-29T03:32:25.441023+0000    37     127      1057       930   25.1334        46     2.47574     4.79022
2021-10-29T03:32:26.441093+0000    38     127      1057       930    24.472         0           -     4.79022
2021-10-29T03:32:27.441163+0000    39     127      1077       950   24.3573        10     3.88037     4.77903
2021-10-29T03:32:28.441230+0000 min lat: 2.11668 max lat: 12.0226 avg lat: 4.77934
2021-10-29T03:32:28.441230+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T03:32:28.441230+0000    40     127      1090       963   24.0733        13     4.62005     4.77934
2021-10-29T03:32:29.441304+0000    41     127      1090       963   23.4862         0           -     4.77934
2021-10-29T03:32:30.441372+0000    42     127      1090       963    22.927         0           -     4.77934
2021-10-29T03:32:31.441442+0000    43     127      1110       983   22.8589   6.66667     7.19642     4.83278
2021-10-29T03:32:32.441512+0000    44     127      1110       983   22.3393         0           -     4.83278
2021-10-29T03:32:33.441580+0000    45     127      1110       983   21.8429         0           -     4.83278
2021-10-29T03:32:34.441650+0000    46     127      1123       996   21.6507   4.33333     9.38385     4.89529
2021-10-29T03:32:35.441735+0000    47     127      1123       996     21.19         0           -     4.89529
2021-10-29T03:32:36.441803+0000    48     127      1143      1016   21.1652        10     11.8206     5.03608
2021-10-29T03:32:37.441876+0000    49     127      1143      1016   20.7332         0           -     5.03608
2021-10-29T03:32:38.441949+0000    50     127      1143      1016   20.3186         0           -     5.03608
2021-10-29T03:32:39.442018+0000    51     127      1156      1029    20.175   4.33333     13.5629      5.1468
2021-10-29T03:32:40.442094+0000    52     127      1156      1029   19.7871         0           -      5.1468
2021-10-29T03:32:41.442168+0000    53     127      1156      1029   19.4137         0           -      5.1468
2021-10-29T03:32:42.442235+0000    54     127      1176      1049   19.4246   6.66667     16.2611     5.36633
2021-10-29T03:32:43.442302+0000    55     127      1176      1049   19.0714         0           -     5.36633
2021-10-29T03:32:44.442368+0000    56     127      1189      1062   18.9629       6.5      16.322     5.51634
2021-10-29T03:32:45.442442+0000    57     127      1189      1062   18.6303         0           -     5.51634
2021-10-29T03:32:46.442511+0000    58     127      1189      1062   18.3091         0           -     5.51634
2021-10-29T03:32:47.442583+0000    59     127      1209      1082   18.3377   6.66667     18.7111     5.77505
2021-10-29T03:32:48.442652+0000 min lat: 2.11668 max lat: 19.7128 avg lat: 5.77505
2021-10-29T03:32:48.442652+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T03:32:48.442652+0000    60     127      1209      1082   18.0321         0           -     5.77505
2021-10-29T03:32:49.442733+0000    61      21      1210      1189   19.4904      53.5     5.71764     6.41482
2021-10-29T03:32:50.442802+0000    62      21      1210      1189   19.1761         0           -     6.41482
2021-10-29T03:32:51.442870+0000    63      21      1210      1189   18.8717         0           -     6.41482
2021-10-29T03:32:52.442964+0000 Total time run:         63.9494
Total writes made:      1210
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     18.9212
Stddev Bandwidth:       18.632
Max bandwidth (MB/sec): 53.5
Min bandwidth (MB/sec): 0
Average IOPS:           18
Stddev IOPS:            18.6503
Max IOPS:               53
Min IOPS:               0
Average Latency(s):     6.39438
Stddev Latency(s):      4.78675
Max latency(s):         21.0355
Min latency(s):         2.11668

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:32:53,289722168-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:32:53,296108194-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 1126238

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:32:53,302121919-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 104728
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:32:53,309948230-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 104728
[1] 20:32:53 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:32:53,494626564-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_1MB_write.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_1MB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_write.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:32:53,673717787-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:33:17,719621170-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:33:17,727645544-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:33:26,812187919-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:33:26,820145558-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:33:35,925121261-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:33:35,932741364-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:33:44,941756985-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:33:44,949348894-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:33:53,991231878-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:33:53,999369306-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:33:54,005462691-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:33:54,009399852-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:33:54,016661027-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:33:54,022651558-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=1128138
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:33:54,030029495-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:33:54,039119508-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'105441\n'
[1] 20:33:54 [SUCCESS] ljishen@10.10.2.2
105441

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:33:54,226803736-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_1MB_seq.log.3
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:33:54,248468129-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:33:54,251375129-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fcca3278-3867-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fcca3278-3867-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T03:33:57.461356+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T03:33:57.461356+0000     0       0         0         0         0         0           -           0
2021-10-29T03:33:58.461504+0000     1     127       232       105   104.978       105    0.970608    0.504148
2021-10-29T03:33:59.461624+0000     2     127       345       218   108.982       113    0.465308    0.769681
2021-10-29T03:34:00.461741+0000     3     127       446       319   106.317       101    0.628316    0.906194
2021-10-29T03:34:01.461817+0000     4     127       582       455   113.735       136    0.764848      0.9761
2021-10-29T03:34:02.461934+0000     5     127       708       581   116.185       126    0.954054    0.958559
2021-10-29T03:34:03.462050+0000     6     127       848       721   120.151       140    0.703412    0.969073
2021-10-29T03:34:04.462165+0000     7     127       962       835   119.271       114     1.72625    0.969262
2021-10-29T03:34:05.462244+0000     8     127      1082       955   119.361       120     1.04291    0.992766
2021-10-29T03:34:06.462322+0000     9     127      1185      1058   117.542       103    0.505279       1.001
2021-10-29T03:34:07.462462+0000 Total time run:       9.99131
Total reads made:     1210
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   121.105
Average IOPS:         121
Stddev IOPS:          14.1343
Max IOPS:             140
Min IOPS:             101
Average Latency(s):   1.0037
Max latency(s):       2.43884
Min latency(s):       0.122946

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:34:08,155023096-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:34:08,160995653-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 1128138

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:34:08,166911323-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 105441
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:34:08,174587862-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 105441
[1] 20:34:08 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:34:08,358101630-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_1MB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_1MB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_1MB_seq.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:34:08,513793078-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:34:32,576434642-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:34:32,584418429-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:34:41,591380694-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:34:41,599240439-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:34:50,649400436-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:34:50,657237547-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:34:59,780891183-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:34:59,788943460-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:35:08,781653100-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 1.21k objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:35:08,789694566-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:35:08,795811406-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T20:35:08,799505078-07:00][RUNNING][ROUND 1/6/21] object_size=4MB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:35:08,803249406-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:35:08,813068252-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:35:09,238702068-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/fcca3278-3867-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid fcca3278-3867-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:35:09,250498067-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:35:09,253826435-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'fcca3278-3867-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid fcca3278-3867-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:35:09,261064463-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid fcca3278-3867-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 20:35:14 [SUCCESS] 10.10.2.1\n[2] 20:35:15 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:35:15,508051800-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:35:15,520233514-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:35:15,524878357-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:35:15,674804769-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:35:15,679741491-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:35:15,831227886-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:35:16,115467974-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:35:16,120576088-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--21cc50f3--bf67--4d59--8193--5c2b4342e245-osd--block--9e85f7e8--dd4e--4bdc--81cd--9c2156a9ba1c (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-21cc50f3-bf67-4d59-8193-5c2b4342e245" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-9e85f7e8-dd4e-4bdc-81cd-9c2156a9ba1c"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-21cc50f3-bf67-4d59-8193-5c2b4342e245" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-9e85f7e8-dd4e-4bdc-81cd-9c2156a9ba1c" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-21cc50f3-bf67-4d59-8193-5c2b4342e245"\n'
10.10.2.1: b'  Volume group "ceph-21cc50f3-bf67-4d59-8193-5c2b4342e245" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:35:16,436440617-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:35:16,446500311-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:35:16,450085732-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\n'
10.10.2.1: b'Verifying lvm2 is present...\nVerifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\npodman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\nlvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 3bbe01a2-3869-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\nCreating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 3bbe01a2-3869-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:36:17,671892099-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:36:37,678997995-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:36:37,689400174-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:36:37,693415634-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 3bbe01a2-3869-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/3bbe01a2-3869-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:36:46,460852891-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:36:46,470479711-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:36:46,473926291-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 3bbe01a2-3869-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/3bbe01a2-3869-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:36:55,357011018-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:36:55,363303420-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:36:55,577822947-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:36:55,581284005-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid 3bbe01a2-3869-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/3bbe01a2-3869-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:37:05,016208391-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:37:25,021086936-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:37:25,028046221-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:37:25,038047175-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:37:25,042005166-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 3bbe01a2-3869-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/3bbe01a2-3869-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:37:50,992461556-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:38:10,998157518-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:38:11,008534929-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:38:11,012551682-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 3bbe01a2-3869-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/3bbe01a2-3869-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     3bbe01a2-3869-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.ujqsko(active, since 2m)\n    osd: 1 osds: 1 up (since 16s), 1 in (since 41s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 20:38:19 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:35:09,238702068-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/fcca3278-3867-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid fcca3278-3867-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:35:09,250498067-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:35:09,253826435-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'fcca3278-3867-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid fcca3278-3867-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:35:09,261064463-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid fcca3278-3867-11ec-b51d-53e6e728d2d3'
[1] 20:35:14 [SUCCESS] 10.10.2.1
[2] 20:35:15 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:35:15,508051800-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:35:15,520233514-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:35:15,524878357-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:35:15,674804769-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:35:15,679741491-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:35:15,831227886-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:35:16,115467974-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:35:16,120576088-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--21cc50f3--bf67--4d59--8193--5c2b4342e245-osd--block--9e85f7e8--dd4e--4bdc--81cd--9c2156a9ba1c (253:0)
  Archiving volume group "ceph-21cc50f3-bf67-4d59-8193-5c2b4342e245" metadata (seqno 5).
  Releasing logical volume "osd-block-9e85f7e8-dd4e-4bdc-81cd-9c2156a9ba1c"
  Creating volume group backup "/etc/lvm/backup/ceph-21cc50f3-bf67-4d59-8193-5c2b4342e245" (seqno 6).
  Logical volume "osd-block-9e85f7e8-dd4e-4bdc-81cd-9c2156a9ba1c" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-21cc50f3-bf67-4d59-8193-5c2b4342e245"
  Volume group "ceph-21cc50f3-bf67-4d59-8193-5c2b4342e245" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:35:16,436440617-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:35:16,446500311-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:35:16,450085732-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 3bbe01a2-3869-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 3bbe01a2-3869-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:36:17,671892099-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:36:37,678997995-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:36:37,689400174-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:36:37,693415634-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 3bbe01a2-3869-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/3bbe01a2-3869-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:36:46,460852891-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:36:46,470479711-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:36:46,473926291-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 3bbe01a2-3869-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/3bbe01a2-3869-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:36:55,357011018-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:36:55,363303420-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:36:55,577822947-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:36:55,581284005-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 3bbe01a2-3869-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/3bbe01a2-3869-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:37:05,016208391-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:37:25,021086936-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:37:25,028046221-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:37:25,038047175-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:37:25,042005166-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 3bbe01a2-3869-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/3bbe01a2-3869-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:37:50,992461556-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:38:10,998157518-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:38:11,008534929-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:38:11,012551682-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 3bbe01a2-3869-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/3bbe01a2-3869-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     3bbe01a2-3869-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.ujqsko(active, since 2m)
    osd: 1 osds: 1 up (since 16s), 1 in (since 41s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:38:19,597777353-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:38:19,605770239-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 20:38:19 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:38:20,085478898-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:38:20,089242772-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:38:20,110949656-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:38:20,114015544-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3bbe01a2-3869-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3bbe01a2-3869-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:38:24,300537022-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:38:24,303404837-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3bbe01a2-3869-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3bbe01a2-3869-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:38:28,635276294-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:38:28,638225303-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3bbe01a2-3869-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3bbe01a2-3869-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:38:32,612903039-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:38:32,615745487-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3bbe01a2-3869-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3bbe01a2-3869-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:38:40,629270154-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:38:40,632208773-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3bbe01a2-3869-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3bbe01a2-3869-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:38:45,010175196-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:38:45,013171774-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3bbe01a2-3869-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3bbe01a2-3869-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:38:49,987035319-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:38:49,990204963-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3bbe01a2-3869-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3bbe01a2-3869-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:38:55,075616645-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:38:55,078510549-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3bbe01a2-3869-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3bbe01a2-3869-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:38:59,335921822-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:38:59,338974616-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3bbe01a2-3869-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3bbe01a2-3869-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:39:03,975643823-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:39:03,978692148-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3bbe01a2-3869-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3bbe01a2-3869-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:39:08,449740648-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:39:08,452617179-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3bbe01a2-3869-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3bbe01a2-3869-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:39:12,499601520-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:39:12,502642753-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3bbe01a2-3869-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3bbe01a2-3869-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default   
-3         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host node-1
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0  
                       TOTAL  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:39:16,458410685-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:39:40,405204232-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:39:49,472989557-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:39:58,609393593-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:40:07,690883508-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:40:07,698655907-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:40:16,758018560-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:40:16,766258872-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:40:25,896225244-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:40:25,904388160-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:40:34,811498052-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:40:34,818924360-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:40:43,773605460-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:40:43,781162483-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:40:43,786937218-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:40:43,791141332-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:40:43,798636799-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:40:43,804304882-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=1135960
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:40:43,811534138-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:40:43,820506991-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'110923\n'
[1] 20:40:43 [SUCCESS] ljishen@10.10.2.2
110923

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:40:44,011297206-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4MB_write.log.1
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:40:44,031671748-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:40:44,034511901-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3bbe01a2-3869-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3bbe01a2-3869-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T03:40:47.240938+0000 Maintaining 128 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 60 seconds or 0 objects
2021-10-29T03:40:47.240952+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-29T03:40:47.367120+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T03:40:47.367120+0000     0       0         0         0         0         0           -           0
2021-10-29T03:40:48.367240+0000     1      17        17         0         0         0           -           0
2021-10-29T03:40:49.367353+0000     2      25        25         0         0         0           -           0
2021-10-29T03:40:50.367462+0000     3      38        38         0         0         0           -           0
2021-10-29T03:40:51.367568+0000     4      54        54         0         0         0           -           0
2021-10-29T03:40:52.367684+0000     5      65        65         0         0         0           -           0
2021-10-29T03:40:53.367787+0000     6      78        78         0         0         0           -           0
2021-10-29T03:40:54.367925+0000     7      89        89         0         0         0           -           0
2021-10-29T03:40:55.368041+0000     8     102       102         0         0         0           -           0
2021-10-29T03:40:56.368172+0000     9     113       113         0         0         0           -           0
2021-10-29T03:40:57.368290+0000    10     126       126         0         0         0           -           0
2021-10-29T03:40:58.368407+0000    11     127       137        10   3.63596   3.63636     10.4361     10.3293
2021-10-29T03:40:59.368528+0000    12     127       137        10   3.33296         0           -     10.3293
2021-10-29T03:41:00.368607+0000    13     127       137        10   3.07659         0           -     10.3293
2021-10-29T03:41:01.368677+0000    14     127       142        15   4.28526   6.66667     12.8676     11.1753
2021-10-29T03:41:02.368781+0000    15     127       142        15   3.99957         0           -     11.1753
2021-10-29T03:41:03.368864+0000    16     127       145        18   4.49953         6     14.4802     11.7261
2021-10-29T03:41:04.368960+0000    17     127       145        18   4.23485         0           -     11.7261
2021-10-29T03:41:05.369031+0000    18     127       150        23   5.11059        10     16.7298     12.8141
2021-10-29T03:41:06.369126+0000    19     127       150        23   4.84161         0           -     12.8141
2021-10-29T03:41:07.369199+0000 min lat: 10.0155 max lat: 16.7309 avg lat: 12.8141
2021-10-29T03:41:07.369199+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T03:41:07.369199+0000    20     127       150        23   4.59954         0           -     12.8141
2021-10-29T03:41:08.369277+0000    21     127       153        26   4.95189         4     18.5992     13.4817
2021-10-29T03:41:09.369368+0000    22     127       161        34    6.1812        32     19.5702     14.9045
2021-10-29T03:41:10.369493+0000    23     127       169        42   7.30361        32      19.588     15.7976
2021-10-29T03:41:11.369592+0000    24     127       185        58    9.6657        64     19.7283     16.8715
2021-10-29T03:41:12.369725+0000    25     127       193        66   10.5589        32     19.8988      17.236
2021-10-29T03:41:13.369830+0000    26     127       206        79   12.1526        52     19.9617     17.6906
2021-10-29T03:41:14.369994+0000    27     127       222        95   14.0726        64     19.7556      18.065
2021-10-29T03:41:15.370153+0000    28     127       233       106   15.1413        44     19.6693     18.2257
2021-10-29T03:41:16.370304+0000    29     127       246       119    16.412        52     19.5816     18.3741
2021-10-29T03:41:17.370412+0000    30     127       257       130   17.3315        44     19.8927     18.4951
2021-10-29T03:41:18.370567+0000    31     127       265       138   17.8045        32      19.999     18.5795
2021-10-29T03:41:19.370702+0000    32     127       265       138   17.2481         0           -     18.5795
2021-10-29T03:41:20.370811+0000    33     127       265       138   16.7254         0           -     18.5795
2021-10-29T03:41:21.370882+0000    34     127       265       138   16.2335         0           -     18.5795
2021-10-29T03:41:22.371001+0000    35     127       270       143   16.3411         5     20.6294     18.6511
2021-10-29T03:41:23.371136+0000    36     127       270       143   15.8871         0           -     18.6511
2021-10-29T03:41:24.371248+0000    37     127       270       143   15.4578         0           -     18.6511
2021-10-29T03:41:25.371319+0000    38     127       273       146   15.3667         4     22.0447     18.7209
2021-10-29T03:41:26.371447+0000    39     127       273       146   14.9727         0           -     18.7209
2021-10-29T03:41:27.371563+0000 min lat: 10.0155 max lat: 22.0447 avg lat: 18.7209
2021-10-29T03:41:27.371563+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T03:41:27.371563+0000    40     127       273       146   14.5984         0           -     18.7209
2021-10-29T03:41:28.371685+0000    41     127       273       146   14.2423         0           -     18.7209
2021-10-29T03:41:29.371756+0000    42     127       278       151   14.3794         5     23.9892     18.8953
2021-10-29T03:41:30.371885+0000    43     127       278       151    14.045         0           -     18.8953
2021-10-29T03:41:31.371997+0000    44     127       278       151   13.7258         0           -     18.8953
2021-10-29T03:41:32.372109+0000    45     127       278       151   13.4208         0           -     18.8953
2021-10-29T03:41:33.372179+0000    46     127       281       154   13.3898         3     24.9241     19.0127
2021-10-29T03:41:34.372300+0000    47     127       281       154    13.105         0           -     19.0127
2021-10-29T03:41:35.372413+0000    48     127       281       154   12.8319         0           -     19.0127
2021-10-29T03:41:36.372527+0000    49     127       286       159   12.9782   6.66667     27.1227     19.2678
2021-10-29T03:41:37.372619+0000    50     127       286       159   12.7186         0           -     19.2678
2021-10-29T03:41:38.372740+0000    51     127       289       162   12.7045         6     28.1273     19.4319
2021-10-29T03:41:39.372868+0000    52     127       289       162   12.4602         0           -     19.4319
2021-10-29T03:41:40.372984+0000    53     127       294       167   12.6024        10     30.2776     19.7566
2021-10-29T03:41:41.373077+0000    54     127       294       167    12.369         0           -     19.7566
2021-10-29T03:41:42.373188+0000    55     127       297       170   12.3623         6     31.5613     19.9649
2021-10-29T03:41:43.373307+0000    56     127       297       170   12.1415         0           -     19.9649
2021-10-29T03:41:44.373421+0000    57     127       302       175   12.2794        10     33.9385     20.3642
2021-10-29T03:41:45.373518+0000    58     127       302       175   12.0676         0           -     20.3642
2021-10-29T03:41:46.373635+0000    59     127       305       178   12.0665         6      35.287     20.6157
2021-10-29T03:41:47.373755+0000 min lat: 10.0155 max lat: 35.287 avg lat: 20.6157
2021-10-29T03:41:47.373755+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T03:41:47.373755+0000    60     127       305       178   11.8654         0           -     20.6157
2021-10-29T03:41:48.373877+0000    61     127       305       178   11.6709         0           -     20.6157
2021-10-29T03:41:49.373975+0000 Total time run:         61.6443
Total writes made:      306
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     19.8559
Stddev Bandwidth:       17.0418
Max bandwidth (MB/sec): 64
Min bandwidth (MB/sec): 0
Average IOPS:           4
Stddev IOPS:            4.28449
Max IOPS:               16
Min IOPS:               0
Average Latency(s):     23.5686
Stddev Latency(s):      8.62472
Max latency(s):         37.4657
Min latency(s):         3.00612

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:41:50,219398436-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:41:50,225369751-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 1135960

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:41:50,231242961-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 110923
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:41:50,239132962-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 110923
[1] 20:41:50 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:41:50,421899152-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4MB_write.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4MB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:41:50,596083648-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:42:14,631324691-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 307 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:42:14,638966385-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:42:23,646836156-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 307 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:42:23,654990195-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:42:32,705100435-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 307 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:42:32,713290783-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:42:41,651909808-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 307 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:42:41,660091699-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:42:50,742134677-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 307 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:42:50,749712750-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:42:50,755941200-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:42:50,760066114-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:42:50,767343300-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:42:50,773449229-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=1137326
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:42:50,780774606-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:42:50,790343972-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'111640\n'
[1] 20:42:50 [SUCCESS] ljishen@10.10.2.2
111640

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:42:50,979368477-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4MB_seq.log.1
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:42:51,000435234-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:42:51,003275287-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '3bbe01a2-3869-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 3bbe01a2-3869-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T03:42:54.139548+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T03:42:54.139548+0000     0       0         0         0         0         0           -           0
2021-10-29T03:42:55.139682+0000     1      59        59         0         0         0           -           0
2021-10-29T03:42:56.139788+0000     2     103       103         0         0         0           -           0
2021-10-29T03:42:57.139873+0000     3     127       142        15   19.9973        20     2.98821     2.81564
2021-10-29T03:42:58.139989+0000     4     127       179        52   51.9933       148      3.2164     3.07163
2021-10-29T03:42:59.140090+0000     5     127       220        93   74.3908       164     3.23791     3.14076
2021-10-29T03:43:00.140193+0000     6     127       255       128   85.3231       140      3.3764     3.18968
2021-10-29T03:43:01.140287+0000     7     127       293       166   94.8461       152     3.39148     3.23378
2021-10-29T03:43:02.140427+0000 Total time run:       7.99592
Total reads made:     306
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   153.078
Average IOPS:         38
Stddev IOPS:          19.4398
Max IOPS:             41
Min IOPS:             0
Average Latency(s):   2.61105
Max latency(s):       3.44352
Min latency(s):       0.283409

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:43:02,812990955-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:43:02,819170823-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 1137326

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:43:02,825403740-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 111640
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:43:02,833560184-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 111640
[1] 20:43:03 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:43:03,017925568-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4MB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4MB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:43:03,171318715-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:43:27,214072914-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 307 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:43:27,221817060-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:43:36,237999797-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 307 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:43:36,246187439-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:43:45,403261523-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 307 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:43:45,411721378-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:43:54,517195200-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 307 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:43:54,525041599-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:44:03,649554080-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 307 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:44:03,657597179-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:44:03,664037569-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T20:44:03,666486394-07:00][RUNNING][ROUND 2/6/21] object_size=4MB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:44:03,670164857-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:44:03,679685522-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:44:04,113595526-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/3bbe01a2-3869-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 3bbe01a2-3869-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:44:04,124756783-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:44:04,128651566-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '3bbe01a2-3869-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 3bbe01a2-3869-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:44:04,137055908-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 3bbe01a2-3869-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 20:44:09 [SUCCESS] 10.10.2.1\n[2] 20:44:10 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:44:10,391402828-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:44:10,403130530-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:44:10,407847218-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:44:10,559627377-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:44:10,564421312-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:44:10,715198565-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:44:10,999520290-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:44:11,004842237-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--052c367a--2836--445d--a31a--ddd530f1c17c-osd--block--66da6110--64b6--4398--91c2--41572c3acc01 (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-052c367a-2836-445d-a31a-ddd530f1c17c" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-66da6110-64b6-4398-91c2-41572c3acc01"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-052c367a-2836-445d-a31a-ddd530f1c17c" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-66da6110-64b6-4398-91c2-41572c3acc01" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-052c367a-2836-445d-a31a-ddd530f1c17c"\n'
10.10.2.1: b'  Volume group "ceph-052c367a-2836-445d-a31a-ddd530f1c17c" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:44:11,360408020-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:44:11,370598231-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:44:11,374605515-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 7a949430-386a-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\n'
10.10.2.1: b'Waiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\nWaiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 7a949430-386a-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:45:11,306998652-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:45:31,314451768-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:45:31,325432685-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:45:31,329025971-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 7a949430-386a-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/7a949430-386a-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:45:39,056899997-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:45:39,066682491-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:45:39,070161412-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 7a949430-386a-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/7a949430-386a-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:45:47,761188163-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:45:47,767035648-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:45:47,981193673-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:45:47,984846822-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid 7a949430-386a-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/7a949430-386a-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:45:57,514426052-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:46:17,519665007-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:46:17,526571593-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:46:17,536774076-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:46:17,540485084-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 7a949430-386a-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/7a949430-386a-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:46:43,364450808-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:47:03,370261248-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:47:03,380933795-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:47:03,384853575-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 7a949430-386a-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/7a949430-386a-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     7a949430-386a-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.jyenwx(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 41s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 20:47:12 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:44:04,113595526-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/3bbe01a2-3869-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 3bbe01a2-3869-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:44:04,124756783-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:44:04,128651566-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '3bbe01a2-3869-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 3bbe01a2-3869-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:44:04,137055908-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 3bbe01a2-3869-11ec-b51d-53e6e728d2d3'
[1] 20:44:09 [SUCCESS] 10.10.2.1
[2] 20:44:10 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:44:10,391402828-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:44:10,403130530-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:44:10,407847218-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:44:10,559627377-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:44:10,564421312-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:44:10,715198565-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:44:10,999520290-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:44:11,004842237-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--052c367a--2836--445d--a31a--ddd530f1c17c-osd--block--66da6110--64b6--4398--91c2--41572c3acc01 (253:0)
  Archiving volume group "ceph-052c367a-2836-445d-a31a-ddd530f1c17c" metadata (seqno 5).
  Releasing logical volume "osd-block-66da6110-64b6-4398-91c2-41572c3acc01"
  Creating volume group backup "/etc/lvm/backup/ceph-052c367a-2836-445d-a31a-ddd530f1c17c" (seqno 6).
  Logical volume "osd-block-66da6110-64b6-4398-91c2-41572c3acc01" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-052c367a-2836-445d-a31a-ddd530f1c17c"
  Volume group "ceph-052c367a-2836-445d-a31a-ddd530f1c17c" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:44:11,360408020-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:44:11,370598231-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:44:11,374605515-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 7a949430-386a-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 7a949430-386a-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:45:11,306998652-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:45:31,314451768-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:45:31,325432685-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:45:31,329025971-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 7a949430-386a-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/7a949430-386a-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:45:39,056899997-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:45:39,066682491-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:45:39,070161412-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 7a949430-386a-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/7a949430-386a-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:45:47,761188163-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:45:47,767035648-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:45:47,981193673-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:45:47,984846822-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 7a949430-386a-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/7a949430-386a-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:45:57,514426052-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:46:17,519665007-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:46:17,526571593-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:46:17,536774076-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:46:17,540485084-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 7a949430-386a-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/7a949430-386a-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:46:43,364450808-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:47:03,370261248-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:47:03,380933795-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:47:03,384853575-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 7a949430-386a-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/7a949430-386a-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     7a949430-386a-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.jyenwx(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 41s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:47:12,255696629-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:47:12,263534522-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 20:47:12 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:47:12,741106577-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:47:12,744655916-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:47:12,766710785-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:47:12,769545979-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7a949430-386a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7a949430-386a-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:47:17,027285001-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:47:17,030164348-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7a949430-386a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7a949430-386a-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:47:21,204923829-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:47:21,207915047-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7a949430-386a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7a949430-386a-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:47:25,512902109-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:47:25,515954031-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7a949430-386a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7a949430-386a-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:47:33,877273480-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:47:33,880487387-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7a949430-386a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7a949430-386a-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:47:38,727088937-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:47:38,730177008-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7a949430-386a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7a949430-386a-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:47:43,183277339-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:47:43,186236676-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7a949430-386a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7a949430-386a-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:47:48,482709437-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:47:48,485804050-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7a949430-386a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7a949430-386a-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:47:53,564486199-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:47:53,567319620-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7a949430-386a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7a949430-386a-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:47:58,404858343-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:47:58,407934170-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7a949430-386a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7a949430-386a-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:48:03,663155226-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:48:03,666524115-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7a949430-386a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7a949430-386a-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:48:07,987982231-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:48:07,990866367-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7a949430-386a-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7a949430-386a-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default   
-3         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host node-1
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0  
                       TOTAL  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:48:12,146047334-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:48:36,219173440-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:48:45,516182280-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:48:54,698966566-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:48:54,707157424-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:49:03,904182355-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:49:03,912118984-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:49:13,051920746-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:49:13,060263350-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:49:22,268824955-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:49:22,276774748-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:49:31,325586484-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:49:31,333484881-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:49:31,339503786-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:49:31,342901830-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:49:31,350570063-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:49:31,356763706-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=1142898
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:49:31,363943799-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:49:31,373301868-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'117385\n'
[1] 20:49:31 [SUCCESS] ljishen@10.10.2.2
117385

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:49:31,567245169-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4MB_write.log.2
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:49:31,587413502-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:49:31,590384261-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7a949430-386a-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7a949430-386a-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T03:49:34.723164+0000 Maintaining 128 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 60 seconds or 0 objects
2021-10-29T03:49:34.723178+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-29T03:49:34.848612+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T03:49:34.848612+0000     0       0         0         0         0         0           -           0
2021-10-29T03:49:35.848722+0000     1      17        17         0         0         0           -           0
2021-10-29T03:49:36.848797+0000     2      33        33         0         0         0           -           0
2021-10-29T03:49:37.848893+0000     3      41        41         0         0         0           -           0
2021-10-29T03:49:38.848962+0000     4      54        54         0         0         0           -           0
2021-10-29T03:49:39.849077+0000     5      65        65         0         0         0           -           0
2021-10-29T03:49:40.849146+0000     6      81        81         0         0         0           -           0
2021-10-29T03:49:41.849241+0000     7      94        94         0         0         0           -           0
2021-10-29T03:49:42.849312+0000     8     105       105         0         0         0           -           0
2021-10-29T03:49:43.849428+0000     9     121       121         0         0         0           -           0
2021-10-29T03:49:44.849493+0000    10     127       134         7   2.79977       2.8     9.93302     9.82105
2021-10-29T03:49:45.849593+0000    11     127       137        10   3.63606        12     10.1059     9.96385
2021-10-29T03:49:46.849664+0000    12     127       137        10   3.33306         0           -     9.96385
2021-10-29T03:49:47.849782+0000    13     127       137        10   3.07666         0           -     9.96385
2021-10-29T03:49:48.849849+0000    14     127       142        15   4.28535   6.66667     12.9184     10.9489
2021-10-29T03:49:49.849945+0000    15     127       142        15   3.99966         0           -     10.9489
2021-10-29T03:49:50.850016+0000    16     127       145        18   4.49962         6     14.5772     11.5535
2021-10-29T03:49:51.850133+0000    17     127       145        18   4.23493         0           -     11.5535
2021-10-29T03:49:52.850197+0000    18     127       150        23   5.11068        10     16.7694     12.6875
2021-10-29T03:49:53.850294+0000    19     127       150        23   4.84169         0           -     12.6875
2021-10-29T03:49:54.850362+0000 min lat: 9.54288 max lat: 16.7707 avg lat: 12.6875
2021-10-29T03:49:54.850362+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T03:49:54.850362+0000    20     127       150        23   4.59961         0           -     12.6875
2021-10-29T03:49:55.850486+0000    21     127       153        26   4.95195         4     18.6822     13.3791
2021-10-29T03:49:56.850553+0000    22     127       161        34   6.18129        32     19.9867     14.9315
2021-10-29T03:49:57.850649+0000    23     127       174        47   8.17321        52      19.863     16.2724
2021-10-29T03:49:58.850719+0000    24     127       185        58   9.66584        44     19.7868     16.9576
2021-10-29T03:49:59.850831+0000    25     127       193        66   10.5591        32     20.0573     17.3305
2021-10-29T03:50:00.850898+0000    26     127       209        82   12.6143        64        20.1     17.8644
2021-10-29T03:50:01.850992+0000    27     127       222        95   14.0729        52     20.1555     18.1723
2021-10-29T03:50:02.851061+0000    28     127       233       106   15.1416        44     20.2615     18.3913
2021-10-29T03:50:03.851179+0000    29     127       246       119   16.4124        52       20.22     18.5906
2021-10-29T03:50:04.851255+0000    30     127       257       130   17.3318        44     20.1366     18.7257
2021-10-29T03:50:05.851355+0000    31     127       265       138   17.8049        32     20.2296     18.8113
2021-10-29T03:50:06.851421+0000    32     127       265       138   17.2485         0           -     18.8113
2021-10-29T03:50:07.851534+0000    33     127       265       138   16.7258         0           -     18.8113
2021-10-29T03:50:08.851602+0000    34     127       265       138   16.2339         0           -     18.8113
2021-10-29T03:50:09.851698+0000    35     127       270       143   16.3414         5     20.9239     18.8852
2021-10-29T03:50:10.851766+0000    36     127       270       143   15.8875         0           -     18.8852
2021-10-29T03:50:11.851878+0000    37     127       270       143   15.4581         0           -     18.8852
2021-10-29T03:50:12.851943+0000    38     127       273       146   15.3671         4       22.36     18.9566
2021-10-29T03:50:13.852043+0000    39     127       273       146   14.9731         0           -     18.9566
2021-10-29T03:50:14.852113+0000 min lat: 9.54288 max lat: 22.3611 avg lat: 18.9566
2021-10-29T03:50:14.852113+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T03:50:14.852113+0000    40     127       273       146   14.5987         0           -     18.9566
2021-10-29T03:50:15.852241+0000    41     127       273       146   14.2427         0           -     18.9566
2021-10-29T03:50:16.852309+0000    42     127       273       146   13.9036         0           -     18.9566
2021-10-29T03:50:17.852404+0000    43     127       278       151   14.0453         4     24.5236     19.1409
2021-10-29T03:50:18.852473+0000    44     127       278       151   13.7261         0           -     19.1409
2021-10-29T03:50:19.852588+0000    45     127       278       151   13.4211         0           -     19.1409
2021-10-29T03:50:20.852656+0000    46     127       281       154   13.3901         4     25.3801     19.2625
2021-10-29T03:50:21.852752+0000    47     127       281       154   13.1052         0           -     19.2625
2021-10-29T03:50:22.852821+0000    48     127       281       154   12.8322         0           -     19.2625
2021-10-29T03:50:23.852937+0000    49     127       281       154   12.5703         0           -     19.2625
2021-10-29T03:50:24.853004+0000    50     127       286       159   12.7189         5      27.452       19.52
2021-10-29T03:50:25.853101+0000    51     127       289       162   12.7048        12     28.5609     19.6875
2021-10-29T03:50:26.853175+0000    52     127       289       162   12.4605         0           -     19.6875
2021-10-29T03:50:27.853292+0000    53     127       289       162   12.2253         0           -     19.6875
2021-10-29T03:50:28.853356+0000    54     127       294       167   12.3693   6.66667     30.7937       20.02
2021-10-29T03:50:29.853427+0000    55     127       297       170   12.3626        12     31.8185     20.2282
2021-10-29T03:50:30.853494+0000    56     127       297       170   12.1418         0           -     20.2282
2021-10-29T03:50:31.853606+0000    57     127       297       170   11.9288         0           -     20.2282
2021-10-29T03:50:32.853675+0000    58     127       302       175   12.0679   6.66667     34.4177     20.6336
2021-10-29T03:50:33.853776+0000    59     127       302       175   11.8634         0           -     20.6336
2021-10-29T03:50:34.853846+0000 min lat: 9.54288 max lat: 35.7465 avg lat: 20.8883
2021-10-29T03:50:34.853846+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T03:50:34.853846+0000    60     127       305       178   11.8656         6     35.7465     20.8883
2021-10-29T03:50:35.853973+0000    61     127       305       178   11.6711         0           -     20.8883
2021-10-29T03:50:36.854084+0000 Total time run:         61.8716
Total writes made:      306
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     19.7829
Stddev Bandwidth:       16.7835
Max bandwidth (MB/sec): 64
Min bandwidth (MB/sec): 0
Average IOPS:           4
Stddev IOPS:            4.21434
Max IOPS:               16
Min IOPS:               0
Average Latency(s):     23.7815
Stddev Latency(s):      8.73773
Max latency(s):         37.8071
Min latency(s):         2.80366

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:50:37,585678504-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:50:37,591831030-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 1142898

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:50:37,597998214-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 117385
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:50:37,605948248-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 117385
[1] 20:50:37 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:50:37,789617194-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4MB_write.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4MB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:50:37,964643239-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:51:02,183964113-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 307 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:51:02,192485514-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:51:11,365264617-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 307 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:51:11,373123289-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:51:20,536528470-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 307 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:51:20,544885791-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:51:29,742751969-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 307 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:51:29,751134598-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:51:38,898837253-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 307 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:51:38,906888048-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:51:38,912993274-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:51:38,916827101-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:51:38,924438406-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:51:38,930374083-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=1144257
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:51:38,937816541-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:51:38,946959554-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'118156\n'
[1] 20:51:39 [SUCCESS] ljishen@10.10.2.2
118156

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:51:39,134980091-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4MB_seq.log.2
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:51:39,155234315-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:51:39,158006110-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7a949430-386a-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7a949430-386a-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T03:51:42.398763+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T03:51:42.398763+0000     0       0         0         0         0         0           -           0
2021-10-29T03:51:43.398914+0000     1      60        60         0         0         0           -           0
2021-10-29T03:51:44.399032+0000     2     102       102         0         0         0           -           0
2021-10-29T03:51:45.399108+0000     3     127       138        11   14.6646   14.6667     2.97963     2.83899
2021-10-29T03:51:46.399224+0000     4     127       176        49   48.9935       152     3.41256     3.16681
2021-10-29T03:51:47.399337+0000     5     127       215        88   70.3909       156      3.2647     3.24064
2021-10-29T03:51:48.399449+0000     6     127       252       125   83.3228       148     3.36095     3.26854
2021-10-29T03:51:49.399525+0000     7     127       292       165   94.2745       160     3.33162     3.29112
2021-10-29T03:51:50.399638+0000     8       3       306       303   151.482       552     1.32566     2.64317
2021-10-29T03:51:51.399788+0000 Total time run:       8.09577
Total reads made:     306
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   151.19
Average IOPS:         37
Stddev IOPS:          44.8949
Max IOPS:             138
Min IOPS:             0
Average Latency(s):   2.62616
Max latency(s):       3.43242
Min latency(s):       0.138948

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:51:52,062157851-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:51:52,068738965-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 1144257

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:51:52,075012049-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 118156
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:51:52,083051532-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 118156
[1] 20:51:52 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:51:52,269759034-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4MB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4MB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:51:52,427997077-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:52:16,557286487-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 307 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:52:16,564934030-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:52:25,678822589-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 307 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:52:25,687074321-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:52:34,753392256-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 307 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:52:34,761370152-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:52:44,077146424-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 307 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:52:44,085378250-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:52:53,442505474-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 307 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:52:53,450840784-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:52:53,457164212-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T20:52:53,459882696-07:00][RUNNING][ROUND 3/6/21] object_size=4MB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:52:53,463503420-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:52:53,473143800-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:52:53,932282614-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/7a949430-386a-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 7a949430-386a-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:52:53,943405742-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:52:53,946940720-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '7a949430-386a-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 7a949430-386a-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:52:53,956519754-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 7a949430-386a-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 20:52:59 [SUCCESS] 10.10.2.1\n[2] 20:52:59 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:52:59,723130895-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:52:59,734704861-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:52:59,739660621-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:52:59,890472326-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:52:59,895341403-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:53:00,058052765-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:53:00,343199199-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:53:00,348704953-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--d2d56b18--4e2e--4a12--abf7--26592448174a-osd--block--0feeadaf--fa0f--4c4a--b3e8--8afacbc86b6b (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-d2d56b18-4e2e-4a12-abf7-26592448174a" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-0feeadaf-fa0f-4c4a-b3e8-8afacbc86b6b"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-d2d56b18-4e2e-4a12-abf7-26592448174a" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-0feeadaf-fa0f-4c4a-b3e8-8afacbc86b6b" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-d2d56b18-4e2e-4a12-abf7-26592448174a"\n'
10.10.2.1: b'  Volume group "ceph-d2d56b18-4e2e-4a12-abf7-26592448174a" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:53:00,688283000-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:53:00,698197415-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:53:00,701756728-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\n'
10.10.2.1: b'Verifying lvm2 is present...\nVerifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\npodman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: b6159dc8-386b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid b6159dc8-386b-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:54:00,742601479-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:54:20,750259633-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:54:20,761192182-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:54:20,765022485-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid b6159dc8-386b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/b6159dc8-386b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:54:30,240131946-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:54:30,249919573-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:54:30,253475710-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid b6159dc8-386b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/b6159dc8-386b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:54:39,611826733-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:54:39,617767435-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:54:39,833177365-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:54:39,836947324-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid b6159dc8-386b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/b6159dc8-386b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:54:48,837024295-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:55:08,841585360-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:55:08,848585875-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:55:08,859919769-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:55:08,863418759-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid b6159dc8-386b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/b6159dc8-386b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:55:33,285623368-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:55:53,291480273-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:55:53,301384408-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T20:55:53,305099464-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid b6159dc8-386b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/b6159dc8-386b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     b6159dc8-386b-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.tfbuuz(active, since 2m)\n    osd: 1 osds: 1 up (since 19s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 20:56:02 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:52:53,932282614-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/7a949430-386a-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 7a949430-386a-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:52:53,943405742-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:52:53,946940720-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '7a949430-386a-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 7a949430-386a-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:52:53,956519754-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 7a949430-386a-11ec-b51d-53e6e728d2d3'
[1] 20:52:59 [SUCCESS] 10.10.2.1
[2] 20:52:59 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:52:59,723130895-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:52:59,734704861-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:52:59,739660621-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:52:59,890472326-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:52:59,895341403-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:53:00,058052765-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:53:00,343199199-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:53:00,348704953-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--d2d56b18--4e2e--4a12--abf7--26592448174a-osd--block--0feeadaf--fa0f--4c4a--b3e8--8afacbc86b6b (253:0)
  Archiving volume group "ceph-d2d56b18-4e2e-4a12-abf7-26592448174a" metadata (seqno 5).
  Releasing logical volume "osd-block-0feeadaf-fa0f-4c4a-b3e8-8afacbc86b6b"
  Creating volume group backup "/etc/lvm/backup/ceph-d2d56b18-4e2e-4a12-abf7-26592448174a" (seqno 6).
  Logical volume "osd-block-0feeadaf-fa0f-4c4a-b3e8-8afacbc86b6b" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-d2d56b18-4e2e-4a12-abf7-26592448174a"
  Volume group "ceph-d2d56b18-4e2e-4a12-abf7-26592448174a" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:53:00,688283000-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:53:00,698197415-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:53:00,701756728-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: b6159dc8-386b-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid b6159dc8-386b-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:54:00,742601479-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:54:20,750259633-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:54:20,761192182-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:54:20,765022485-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid b6159dc8-386b-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/b6159dc8-386b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:54:30,240131946-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:54:30,249919573-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:54:30,253475710-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid b6159dc8-386b-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/b6159dc8-386b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:54:39,611826733-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:54:39,617767435-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:54:39,833177365-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:54:39,836947324-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid b6159dc8-386b-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/b6159dc8-386b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:54:48,837024295-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:55:08,841585360-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:55:08,848585875-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:55:08,859919769-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:55:08,863418759-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid b6159dc8-386b-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/b6159dc8-386b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:55:33,285623368-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:55:53,291480273-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:55:53,301384408-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:55:53,305099464-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid b6159dc8-386b-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/b6159dc8-386b-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     b6159dc8-386b-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.tfbuuz(active, since 2m)
    osd: 1 osds: 1 up (since 19s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:56:02,112065589-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:56:02,119843188-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 20:56:02 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:56:02,596968246-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:56:02,600906889-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:56:02,622861418-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:56:02,625685541-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b6159dc8-386b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b6159dc8-386b-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:56:07,022580779-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:56:07,025605089-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b6159dc8-386b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b6159dc8-386b-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:56:11,347875386-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:56:11,350866614-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b6159dc8-386b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b6159dc8-386b-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:56:15,398251996-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:56:15,401229338-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b6159dc8-386b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b6159dc8-386b-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:56:23,740875002-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:56:23,743974834-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b6159dc8-386b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b6159dc8-386b-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:56:29,057392645-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:56:29,060695049-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b6159dc8-386b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b6159dc8-386b-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:56:33,351403273-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:56:33,354383791-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b6159dc8-386b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b6159dc8-386b-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:56:38,007826603-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:56:38,011001977-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b6159dc8-386b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b6159dc8-386b-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:56:43,050285727-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:56:43,053289778-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b6159dc8-386b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b6159dc8-386b-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:56:48,001939092-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:56:48,004958542-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b6159dc8-386b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b6159dc8-386b-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:56:53,032902716-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:56:53,035981679-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b6159dc8-386b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b6159dc8-386b-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:56:57,273129690-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:56:57,276249129-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b6159dc8-386b-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b6159dc8-386b-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default   
-3         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host node-1
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0  
                       TOTAL  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:57:01,351888214-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:57:25,466606939-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:57:34,582142567-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:57:43,709334510-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:57:43,717433855-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:57:52,989002178-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:57:52,997328140-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:58:02,128412090-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:58:02,136422177-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:58:11,257186482-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:58:11,265132058-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:58:20,519392977-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:58:20,527329395-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:58:20,533676158-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:58:20,537449549-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:58:20,545027311-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:58:20,551428296-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=1149824
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:58:20,559015135-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:58:20,568614388-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'123833\n'
[1] 20:58:20 [SUCCESS] ljishen@10.10.2.2
123833

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:58:20,758935777-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4MB_write.log.3
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:58:20,779848633-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:58:20,782796839-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b6159dc8-386b-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b6159dc8-386b-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T03:58:24.061417+0000 Maintaining 128 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 60 seconds or 0 objects
2021-10-29T03:58:24.061432+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-29T03:58:24.187471+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T03:58:24.187471+0000     0       0         0         0         0         0           -           0
2021-10-29T03:58:25.187587+0000     1      17        17         0         0         0           -           0
2021-10-29T03:58:26.187666+0000     2      30        30         0         0         0           -           0
2021-10-29T03:58:27.187742+0000     3      41        41         0         0         0           -           0
2021-10-29T03:58:28.187818+0000     4      57        57         0         0         0           -           0
2021-10-29T03:58:29.187890+0000     5      65        65         0         0         0           -           0
2021-10-29T03:58:30.187964+0000     6      78        78         0         0         0           -           0
2021-10-29T03:58:31.188039+0000     7      94        94         0         0         0           -           0
2021-10-29T03:58:32.188116+0000     8     105       105         0         0         0           -           0
2021-10-29T03:58:33.188187+0000     9     121       121         0         0         0           -           0
2021-10-29T03:58:34.188259+0000    10     127       129         2  0.799942       0.8     9.72882     9.72888
2021-10-29T03:58:35.188335+0000    11     127       137        10    3.6361        32      9.9692     10.0259
2021-10-29T03:58:36.188411+0000    12     127       137        10   3.33309         0           -     10.0259
2021-10-29T03:58:37.188483+0000    13     127       137        10    3.0767         0           -     10.0259
2021-10-29T03:58:38.188556+0000    14     127       142        15    4.2854   6.66667      12.639     10.8968
2021-10-29T03:58:39.188634+0000    15     127       142        15   3.99971         0           -     10.8968
2021-10-29T03:58:40.188713+0000    16     127       145        18   4.49967         6     14.3821     11.4776
2021-10-29T03:58:41.188790+0000    17     127       145        18   4.23498         0           -     11.4776
2021-10-29T03:58:42.188862+0000    18     127       150        23   5.11073        10     16.7305     12.6195
2021-10-29T03:58:43.188936+0000    19     127       150        23   4.84175         0           -     12.6195
2021-10-29T03:58:44.189011+0000 min lat: 9.72882 max lat: 18.3974 avg lat: 13.2862
2021-10-29T03:58:44.189011+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T03:58:44.189011+0000    20     127       153        26   5.19962         6     18.3974     13.2862
2021-10-29T03:58:45.189092+0000    21     127       153        26   4.95201         0           -     13.2862
2021-10-29T03:58:46.189167+0000    22     127       161        34   6.18136        16      19.538      14.784
2021-10-29T03:58:47.189242+0000    23     127       169        42    7.3038        32     19.9629     15.7567
2021-10-29T03:58:48.189320+0000    24     127       182        55   9.16598        52     20.0365     16.7735
2021-10-29T03:58:49.189397+0000    25     127       193        66   10.5592        44     19.9499     17.3224
2021-10-29T03:58:50.189478+0000    26     127       206        79   12.1529        52     19.9379     17.7522
2021-10-29T03:58:51.189555+0000    27     127       222        95    14.073        64     20.0048     18.1283
2021-10-29T03:58:52.189630+0000    28     127       233       106   15.1417        44     20.0034     18.3227
2021-10-29T03:58:53.189701+0000    29     127       246       119   16.4126        52     20.3284     18.5425
2021-10-29T03:58:54.189778+0000    30     127       257       130    17.332        44     20.1725     18.6827
2021-10-29T03:58:55.189852+0000    31     127       265       138   17.8051        32     20.4241     18.7824
2021-10-29T03:58:56.189927+0000    32     127       265       138   17.2487         0           -     18.7824
2021-10-29T03:58:57.190003+0000    33     127       265       138    16.726         0           -     18.7824
2021-10-29T03:58:58.190078+0000    34     127       265       138   16.2341         0           -     18.7824
2021-10-29T03:58:59.190149+0000    35     127       270       143   16.3416         5     21.4138     18.8743
2021-10-29T03:59:00.190224+0000    36     127       270       143   15.8877         0           -     18.8743
2021-10-29T03:59:01.190298+0000    37     127       270       143   15.4583         0           -     18.8743
2021-10-29T03:59:02.190370+0000    38     127       270       143   15.0515         0           -     18.8743
2021-10-29T03:59:03.190442+0000    39     127       273       146   14.9732         3     22.7218     18.9534
2021-10-29T03:59:04.190515+0000 min lat: 9.72882 max lat: 22.7218 avg lat: 18.9534
2021-10-29T03:59:04.190515+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T03:59:04.190515+0000    40     127       273       146   14.5989         0           -     18.9534
2021-10-29T03:59:05.190594+0000    41     127       273       146   14.2428         0           -     18.9534
2021-10-29T03:59:06.190675+0000    42     127       273       146   13.9037         0           -     18.9534
2021-10-29T03:59:07.190752+0000    43     127       278       151   14.0455         5      24.595     19.1402
2021-10-29T03:59:08.190823+0000    44     127       278       151   13.7262         0           -     19.1402
2021-10-29T03:59:09.190892+0000    45     127       278       151   13.4212         0           -     19.1402
2021-10-29T03:59:10.190963+0000    46     127       281       154   13.3903         4     25.8865     19.2716
2021-10-29T03:59:11.191035+0000    47     127       281       154   13.1054         0           -     19.2716
2021-10-29T03:59:12.191108+0000    48     127       281       154   12.8324         0           -     19.2716
2021-10-29T03:59:13.191184+0000    49     127       281       154   12.5705         0           -     19.2716
2021-10-29T03:59:14.191254+0000    50     127       286       159   12.7191         5     28.1004     19.5493
2021-10-29T03:59:15.191328+0000    51     127       286       159   12.4697         0           -     19.5493
2021-10-29T03:59:16.191398+0000    52     127       289       162   12.4606         6     29.3599     19.7309
2021-10-29T03:59:17.191468+0000    53     127       289       162   12.2255         0           -     19.7309
2021-10-29T03:59:18.191540+0000    54     127       294       167   12.3695        10     31.0352     20.0694
2021-10-29T03:59:19.191612+0000    55     127       297       170   12.3627        12     32.1429     20.2825
2021-10-29T03:59:20.191684+0000    56     127       297       170    12.142         0           -     20.2825
2021-10-29T03:59:21.191753+0000    57     127       297       170   11.9289         0           -     20.2825
2021-10-29T03:59:22.191824+0000    58     127       302       175   12.0681   6.66667     34.8007     20.6973
2021-10-29T03:59:23.191898+0000    59     127       302       175   11.8635         0           -     20.6973
2021-10-29T03:59:24.191969+0000 min lat: 9.72882 max lat: 35.9422 avg lat: 20.9542
2021-10-29T03:59:24.191969+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T03:59:24.191969+0000    60     127       305       178   11.8658         6     35.9422     20.9542
2021-10-29T03:59:25.192047+0000    61     127       305       178   11.6713         0           -     20.9542
2021-10-29T03:59:26.192121+0000    62     127       305       178    11.483         0           -     20.9542
2021-10-29T03:59:27.192224+0000 Total time run:         62.9573
Total writes made:      306
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     19.4417
Stddev Bandwidth:       16.7102
Max bandwidth (MB/sec): 64
Min bandwidth (MB/sec): 0
Average IOPS:           4
Stddev IOPS:            4.19932
Max IOPS:               16
Min IOPS:               0
Average Latency(s):     24.1103
Stddev Latency(s):      8.97728
Max latency(s):         38.6071
Min latency(s):         3.53967

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:59:27,866234558-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:59:27,872485539-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 1149824

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:59:27,878910989-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 123833
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:59:27,887130741-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 123833
[1] 20:59:28 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:59:28,074499405-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4MB_write.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4MB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_write.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:59:28,248339681-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:59:52,403770594-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 307 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T20:59:52,411812711-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:00:01,662351747-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 307 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:00:01,670444630-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:00:10,803082447-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 307 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:00:10,811201510-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:00:20,183935144-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 307 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:00:20,192327661-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:00:29,425852927-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 307 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:00:29,433708383-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:00:29,440288846-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:00:29,444374285-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:00:29,451618339-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:00:29,457455691-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=1151195
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:00:29,464928755-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:00:29,473999181-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'124685\n'
[1] 21:00:29 [SUCCESS] ljishen@10.10.2.2
124685

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:00:29,663205097-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_4MB_seq.log.3
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:00:29,683301504-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:00:29,686250612-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b6159dc8-386b-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b6159dc8-386b-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T04:00:32.907816+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T04:00:32.907816+0000     0       0         0         0         0         0           -           0
2021-10-29T04:00:33.907931+0000     1      63        63         0         0         0           -           0
2021-10-29T04:00:34.908043+0000     2     106       106         0         0         0           -           0
2021-10-29T04:00:35.908154+0000     3     127       148        21   27.9962        28     2.99966     2.76984
2021-10-29T04:00:36.908277+0000     4     127       185        58   57.9923       148     3.13901      2.9642
2021-10-29T04:00:37.908343+0000     5     127       223        96   76.7909       152     3.21998     3.05221
2021-10-29T04:00:38.908460+0000     6     127       264       137   91.3225       164     3.23531      3.1322
2021-10-29T04:00:39.908572+0000     7     127       301       174   99.4169       148     3.31135     3.16742
2021-10-29T04:00:40.908703+0000 Total time run:       7.96115
Total reads made:     306
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   153.747
Average IOPS:         38
Stddev IOPS:          19.3858
Max IOPS:             41
Min IOPS:             0
Average Latency(s):   2.5772
Max latency(s):       4.48559
Min latency(s):       0.109536

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:00:41,622852686-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:00:41,629321507-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 1151195

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:00:41,635784509-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 124685
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:00:41,644099069-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 124685
[1] 21:00:41 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:00:41,826169977-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_4MB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_4MB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_4MB_seq.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:00:41,979960604-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:01:06,093739344-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 307 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:01:06,101641768-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:01:15,273554730-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 307 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:01:15,281711743-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:01:24,397735744-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 307 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:01:24,406266953-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:01:33,571084463-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 307 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:01:33,579358968-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:01:42,784612045-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 307 objects, 1.2 GiB
    usage:   3.6 GiB used, 396 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:01:42,793092328-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:01:42,800036816-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T21:01:42,804095396-07:00][RUNNING][ROUND 1/7/21] object_size=16MB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:01:42,808113088-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:01:42,818077709-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:01:43,223879373-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/b6159dc8-386b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid b6159dc8-386b-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:01:43,234883526-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:01:43,238632406-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'b6159dc8-386b-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid b6159dc8-386b-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:01:43,247672195-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid b6159dc8-386b-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 21:01:48 [SUCCESS] 10.10.2.1\n[2] 21:01:49 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:01:49,324732606-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:01:49,336810559-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:01:49,341832582-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:01:49,491302299-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:01:49,496074944-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:01:49,647249927-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:01:49,931325879-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:01:49,936225042-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--b05321ab--b311--476d--b53a--5153c4b8ed77-osd--block--160e5e78--e36a--445c--977f--24df421b65ea (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-b05321ab-b311-476d-b53a-5153c4b8ed77" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-160e5e78-e36a-445c-977f-24df421b65ea"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-b05321ab-b311-476d-b53a-5153c4b8ed77" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-160e5e78-e36a-445c-977f-24df421b65ea" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-b05321ab-b311-476d-b53a-5153c4b8ed77"\n'
10.10.2.1: b'  Volume group "ceph-b05321ab-b311-476d-b53a-5153c4b8ed77" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:01:50,296072932-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:01:50,305540907-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:01:50,309226107-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: f1c0c8ce-386c-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\nWaiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\nAdding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid f1c0c8ce-386c-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\n'
10.10.2.1: b'Bootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:02:50,005547027-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:03:10,012731651-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:03:10,022508756-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:03:10,026116401-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid f1c0c8ce-386c-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/f1c0c8ce-386c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:03:19,212872394-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:03:19,222632898-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:03:19,226051026-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid f1c0c8ce-386c-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/f1c0c8ce-386c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:03:27,950273382-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:03:27,956799975-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:03:28,170109977-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:03:28,173575844-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid f1c0c8ce-386c-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/f1c0c8ce-386c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:03:37,155745629-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:03:57,160451891-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:03:57,167576108-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:03:57,177219933-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:03:57,180981827-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid f1c0c8ce-386c-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/f1c0c8ce-386c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:04:21,380366939-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:04:41,385905163-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:04:41,396177180-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:04:41,400146554-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid f1c0c8ce-386c-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/f1c0c8ce-386c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     f1c0c8ce-386c-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.otfwxu(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 21:04:49 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:01:43,223879373-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/b6159dc8-386b-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid b6159dc8-386b-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:01:43,234883526-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:01:43,238632406-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'b6159dc8-386b-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid b6159dc8-386b-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:01:43,247672195-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid b6159dc8-386b-11ec-b51d-53e6e728d2d3'
[1] 21:01:48 [SUCCESS] 10.10.2.1
[2] 21:01:49 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:01:49,324732606-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:01:49,336810559-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:01:49,341832582-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:01:49,491302299-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:01:49,496074944-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:01:49,647249927-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:01:49,931325879-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:01:49,936225042-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--b05321ab--b311--476d--b53a--5153c4b8ed77-osd--block--160e5e78--e36a--445c--977f--24df421b65ea (253:0)
  Archiving volume group "ceph-b05321ab-b311-476d-b53a-5153c4b8ed77" metadata (seqno 5).
  Releasing logical volume "osd-block-160e5e78-e36a-445c-977f-24df421b65ea"
  Creating volume group backup "/etc/lvm/backup/ceph-b05321ab-b311-476d-b53a-5153c4b8ed77" (seqno 6).
  Logical volume "osd-block-160e5e78-e36a-445c-977f-24df421b65ea" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-b05321ab-b311-476d-b53a-5153c4b8ed77"
  Volume group "ceph-b05321ab-b311-476d-b53a-5153c4b8ed77" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:01:50,296072932-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:01:50,305540907-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:01:50,309226107-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: f1c0c8ce-386c-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid f1c0c8ce-386c-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:02:50,005547027-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:03:10,012731651-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:03:10,022508756-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:03:10,026116401-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid f1c0c8ce-386c-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/f1c0c8ce-386c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:03:19,212872394-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:03:19,222632898-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:03:19,226051026-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid f1c0c8ce-386c-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/f1c0c8ce-386c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:03:27,950273382-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:03:27,956799975-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:03:28,170109977-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:03:28,173575844-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid f1c0c8ce-386c-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/f1c0c8ce-386c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:03:37,155745629-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:03:57,160451891-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:03:57,167576108-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:03:57,177219933-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:03:57,180981827-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid f1c0c8ce-386c-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/f1c0c8ce-386c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:04:21,380366939-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:04:41,385905163-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:04:41,396177180-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:04:41,400146554-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid f1c0c8ce-386c-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/f1c0c8ce-386c-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     f1c0c8ce-386c-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.otfwxu(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:04:49,676998291-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:04:49,685135738-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 21:04:49 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:04:50,160775253-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:04:50,165009844-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:04:50,187313320-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:04:50,190031784-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f1c0c8ce-386c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f1c0c8ce-386c-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:04:54,204762615-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:04:54,207731210-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f1c0c8ce-386c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f1c0c8ce-386c-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:04:58,548289940-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:04:58,551349036-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f1c0c8ce-386c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f1c0c8ce-386c-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:05:03,037289912-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:05:03,040473452-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f1c0c8ce-386c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f1c0c8ce-386c-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:05:11,512633518-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:05:11,515957653-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f1c0c8ce-386c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f1c0c8ce-386c-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:05:16,569473986-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:05:16,572717198-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f1c0c8ce-386c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f1c0c8ce-386c-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:05:20,742777640-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:05:20,745905986-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f1c0c8ce-386c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f1c0c8ce-386c-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:05:25,625937784-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:05:25,628974357-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f1c0c8ce-386c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f1c0c8ce-386c-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:05:30,601894494-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:05:30,604822142-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f1c0c8ce-386c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f1c0c8ce-386c-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:05:35,167373622-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:05:35,170244303-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f1c0c8ce-386c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f1c0c8ce-386c-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:05:39,515867118-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:05:39,519065798-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f1c0c8ce-386c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f1c0c8ce-386c-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 19 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:05:43,650271683-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:05:43,653322784-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f1c0c8ce-386c-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f1c0c8ce-386c-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.39059         -  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default   
-3         0.39059         -  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host node-1
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0  
                       TOTAL  400 GiB  5.7 MiB  160 KiB   0 B  5.6 MiB  400 GiB  0.00                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:05:47,691433811-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:06:11,905073411-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:06:21,237973835-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:06:30,409372227-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:06:39,650275713-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:06:39,658812323-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:06:49,030986645-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:06:49,039330942-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:06:58,186574676-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:06:58,194698358-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:07:07,337277577-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:07:07,345357616-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:07:16,500839770-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:07:16,508816474-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:07:16,515630647-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:07:16,519522452-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:07:16,526998613-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:07:16,533060147-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=1156961
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:07:16,540572866-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:07:16,549803003-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'130526\n'
[1] 21:07:16 [SUCCESS] ljishen@10.10.2.2
130526

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:07:16,740188358-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16MB_write.log.1
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:07:16,761217913-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:07:16,764296795-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f1c0c8ce-386c-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f1c0c8ce-386c-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T04:07:19.976052+0000 Maintaining 128 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 60 seconds or 0 objects
2021-10-29T04:07:19.976088+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-29T04:07:20.653249+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T04:07:20.653249+0000     0       0         0         0         0         0           -           0
2021-10-29T04:07:21.653370+0000     1       4         4         0         0         0           -           0
2021-10-29T04:07:22.653464+0000     2       7         7         0         0         0           -           0
2021-10-29T04:07:23.653543+0000     3      11        11         0         0         0           -           0
2021-10-29T04:07:24.653620+0000     4      14        14         0         0         0           -           0
2021-10-29T04:07:25.653695+0000     5      17        17         0         0         0           -           0
2021-10-29T04:07:26.653773+0000     6      21        21         0         0         0           -           0
2021-10-29T04:07:27.653856+0000     7      24        24         0         0         0           -           0
2021-10-29T04:07:28.653930+0000     8      28        28         0         0         0           -           0
2021-10-29T04:07:29.654002+0000     9      30        30         0         0         0           -           0
2021-10-29T04:07:30.654074+0000    10      34        34         0         0         0           -           0
2021-10-29T04:07:31.654153+0000    11      34        34         0         0         0           -           0
2021-10-29T04:07:32.654225+0000    12      34        34         0         0         0           -           0
2021-10-29T04:07:33.654301+0000    13      35        35         0         0         0           -           0
2021-10-29T04:07:34.654381+0000    14      35        35         0         0         0           -           0
2021-10-29T04:07:35.654454+0000    15      35        35         0         0         0           -           0
2021-10-29T04:07:36.654530+0000    16      35        35         0         0         0           -           0
2021-10-29T04:07:37.654603+0000    17      36        36         0         0         0           -           0
2021-10-29T04:07:38.654685+0000    18      36        36         0         0         0           -           0
2021-10-29T04:07:39.654756+0000    19      36        36         0         0         0           -           0
2021-10-29T04:07:40.654821+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-29T04:07:40.654821+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T04:07:40.654821+0000    20      39        39         0         0         0           -           0
2021-10-29T04:07:41.654908+0000    21      43        43         0         0         0           -           0
2021-10-29T04:07:42.654987+0000    22      46        46         0         0         0           -           0
2021-10-29T04:07:43.655069+0000    23      49        49         0         0         0           -           0
2021-10-29T04:07:44.655151+0000    24      53        53         0         0         0           -           0
2021-10-29T04:07:45.655237+0000    25      56        56         0         0         0           -           0
2021-10-29T04:07:46.655315+0000    26      59        59         0         0         0           -           0
2021-10-29T04:07:47.655394+0000    27      62        62         0         0         0           -           0
2021-10-29T04:07:48.655470+0000    28      66        66         0         0         0           -           0
2021-10-29T04:07:49.655552+0000    29      66        66         0         0         0           -           0
2021-10-29T04:07:50.655623+0000    30      66        66         0         0         0           -           0
2021-10-29T04:07:51.655704+0000    31      67        67         0         0         0           -           0
2021-10-29T04:07:52.655789+0000    32      67        67         0         0         0           -           0
2021-10-29T04:07:53.655869+0000    33      67        67         0         0         0           -           0
2021-10-29T04:07:54.655950+0000    34      67        67         0         0         0           -           0
2021-10-29T04:07:55.656025+0000    35      67        67         0         0         0           -           0
2021-10-29T04:07:56.656097+0000    36      67        67         0         0         0           -           0
2021-10-29T04:07:57.656165+0000    37      67        67         0         0         0           -           0
2021-10-29T04:07:58.656234+0000    38      67        67         0         0         0           -           0
2021-10-29T04:07:59.656308+0000    39      68        68         0         0         0           -           0
2021-10-29T04:08:00.656384+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-29T04:08:00.656384+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T04:08:00.656384+0000    40      68        68         0         0         0           -           0
2021-10-29T04:08:01.656455+0000    41      68        68         0         0         0           -           0
2021-10-29T04:08:02.656531+0000    42      68        68         0         0         0           -           0
2021-10-29T04:08:03.656608+0000    43      68        68         0         0         0           -           0
2021-10-29T04:08:04.656686+0000    44      69        69         0         0         0           -           0
2021-10-29T04:08:05.656763+0000    45      69        69         0         0         0           -           0
2021-10-29T04:08:06.656834+0000    46      70        70         0         0         0           -           0
2021-10-29T04:08:07.656909+0000    47      70        70         0         0         0           -           0
2021-10-29T04:08:08.656979+0000    48      71        71         0         0         0           -           0
2021-10-29T04:08:09.657049+0000    49      71        71         0         0         0           -           0
2021-10-29T04:08:10.657118+0000    50      72        72         0         0         0           -           0
2021-10-29T04:08:11.657196+0000    51      72        72         0         0         0           -           0
2021-10-29T04:08:12.657272+0000    52      72        72         0         0         0           -           0
2021-10-29T04:08:13.657347+0000    53      73        73         0         0         0           -           0
2021-10-29T04:08:14.657425+0000    54      73        73         0         0         0           -           0
2021-10-29T04:08:15.657500+0000    55      74        74         0         0         0           -           0
2021-10-29T04:08:16.657574+0000    56      77        77         0         0         0           -           0
2021-10-29T04:08:17.657648+0000    57      80        80         0         0         0           -           0
2021-10-29T04:08:18.657732+0000    58      84        84         0         0         0           -           0
2021-10-29T04:08:19.657820+0000    59      87        87         0         0         0           -           0
2021-10-29T04:08:20.657893+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-29T04:08:20.657893+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T04:08:20.657893+0000    60      90        90         0         0         0           -           0
2021-10-29T04:08:21.657974+0000    61      93        93         0         0         0           -           0
2021-10-29T04:08:22.658070+0000    62      97        97         0         0         0           -           0
2021-10-29T04:08:23.658158+0000    63      98        98         0         0         0           -           0
2021-10-29T04:08:24.658237+0000    64      98        98         0         0         0           -           0
2021-10-29T04:08:25.658311+0000    65      98        98         0         0         0           -           0
2021-10-29T04:08:26.658386+0000    66      98        98         0         0         0           -           0
2021-10-29T04:08:27.658457+0000    67      98        98         0         0         0           -           0
2021-10-29T04:08:28.658532+0000    68      99        99         0         0         0           -           0
2021-10-29T04:08:29.658613+0000    69      99        99         0         0         0           -           0
2021-10-29T04:08:30.658683+0000    70      99        99         0         0         0           -           0
2021-10-29T04:08:31.658750+0000    71      99        99         0         0         0           -           0
2021-10-29T04:08:32.658832+0000    72      99        99         0         0         0           -           0
2021-10-29T04:08:33.658916+0000    73      99        99         0         0         0           -           0
2021-10-29T04:08:34.658999+0000    74     100       100         0         0         0           -           0
2021-10-29T04:08:35.659086+0000    75     100       100         0         0         0           -           0
2021-10-29T04:08:36.659162+0000    76     102       102         0         0         0           -           0
2021-10-29T04:08:37.659247+0000    77     106       106         0         0         0           -           0
2021-10-29T04:08:38.659330+0000    78     109       109         0         0         0           -           0
2021-10-29T04:08:39.659406+0000    79     112       112         0         0         0           -           0
2021-10-29T04:08:40.659481+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-29T04:08:40.659481+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T04:08:40.659481+0000    80     116       116         0         0         0           -           0
2021-10-29T04:08:41.659567+0000    81     118       118         0         0         0           -           0
2021-10-29T04:08:42.659656+0000    82     122       122         0         0         0           -           0
2021-10-29T04:08:43.659740+0000    83     125       125         0         0         0           -           0
2021-10-29T04:08:44.659825+0000    84     125       125         0         0         0           -           0
2021-10-29T04:08:45.659898+0000    85     125       125         0         0         0           -           0
2021-10-29T04:08:46.659976+0000    86     126       126         0         0         0           -           0
2021-10-29T04:08:47.660060+0000    87     126       126         0         0         0           -           0
2021-10-29T04:08:48.660134+0000    88     126       126         0         0         0           -           0
2021-10-29T04:08:49.660211+0000    89     127       127         0         0         0           -           0
2021-10-29T04:08:50.660317+0000 Total time run:         89.7912
Total writes made:      128
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     22.8085
Stddev Bandwidth:       0
Max bandwidth (MB/sec): 0
Min bandwidth (MB/sec): 0
Average IOPS:           1
Stddev IOPS:            0
Max IOPS:               0
Min IOPS:               0
Average Latency(s):     50.5417
Stddev Latency(s):      29.0222
Max latency(s):         89.3581
Min latency(s):         1.42374

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:08:51,905420394-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:08:51,911438316-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 1156961

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:08:51,917746395-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 130526
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:08:51,925967710-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 130526
[1] 21:08:52 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:08:52,110189628-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16MB_write.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16MB_write.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:08:52,294839943-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:09:16,460512415-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.7 GiB used, 393 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:09:16,468702491-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:09:25,784997008-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:09:25,792879434-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:09:34,989869683-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:09:34,998326942-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:09:44,282126028-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:09:44,290992288-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:09:53,469520182-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:09:53,477584501-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:09:53,484299618-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:09:53,488488042-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:09:53,496087345-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:09:53,502218069-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.rados_bench_host.1
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=1158293
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.rados_bench_host.1 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:09:53,509875221-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:09:53,519033401-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'130947\n'
[1] 21:09:53 [SUCCESS] ljishen@10.10.2.2
130947

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:09:53,706997752-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16MB_seq.log.1
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:09:53,727023886-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:09:53,729937036-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f1c0c8ce-386c-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f1c0c8ce-386c-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T04:09:56.887016+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T04:09:56.887016+0000     0       0         0         0         0         0           -           0
2021-10-29T04:09:57.887171+0000     1      10        10         0         0         0           -           0
2021-10-29T04:09:58.887296+0000     2      22        22         0         0         0           -           0
2021-10-29T04:09:59.887405+0000     3      31        31         0         0         0           -           0
2021-10-29T04:10:00.887517+0000     4      42        42         0         0         0           -           0
2021-10-29T04:10:01.887607+0000     5      53        53         0         0         0           -           0
2021-10-29T04:10:02.887719+0000     6      64        64         0         0         0           -           0
2021-10-29T04:10:03.887838+0000     7      72        72         0         0         0           -           0
2021-10-29T04:10:04.887961+0000     8      83        83         0         0         0           -           0
2021-10-29T04:10:05.888053+0000     9      93        93         0         0         0           -           0
2021-10-29T04:10:06.888165+0000    10     104       104         0         0         0           -           0
2021-10-29T04:10:07.888238+0000    11     115       115         0         0         0           -           0
2021-10-29T04:10:08.888352+0000    12     125       125         0         0         0           -           0
2021-10-29T04:10:09.888437+0000    13       2       128       126   155.059   155.077    0.971567     6.43707
2021-10-29T04:10:10.888583+0000 Total time run:       13.1258
Total reads made:     128
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   156.028
Average IOPS:         9
Stddev IOPS:          2.49615
Max IOPS:             9
Min IOPS:             0
Average Latency(s):   6.35194
Max latency(s):       12.2954
Min latency(s):       0.742167

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:10:11,730092987-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:10:11,736654004-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 1158293

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:10:11,743245259-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 130947
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:10:11,751429876-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 130947
[1] 21:10:11 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:10:11,934189151-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16MB_seq.dat.osd_host.1
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16MB_seq.dat.osd_host.1 /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.osd_host.1


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:10:12,093755909-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:10:36,250003127-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:10:36,258703577-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:10:45,482449367-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:10:45,491133106-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:10:54,713420952-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:10:54,722119589-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:11:03,944741018-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:11:03,953235210-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:11:13,209146002-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:11:13,217467358-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:11:13,224703788-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T21:11:13,227462398-07:00][RUNNING][ROUND 2/7/21] object_size=16MB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:11:13,231230691-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:11:13,241211905-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:11:13,652011628-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/f1c0c8ce-386c-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid f1c0c8ce-386c-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:11:13,663765093-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:11:13,667382528-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'f1c0c8ce-386c-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid f1c0c8ce-386c-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:11:13,676246659-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid f1c0c8ce-386c-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 21:11:19 [SUCCESS] 10.10.2.1\n[2] 21:11:20 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:11:20,135879565-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:11:20,148125657-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:11:20,152780212-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:11:20,303336126-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:11:20,308182241-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:11:20,458863440-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:11:20,743310516-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:11:20,748322362-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--0fe81fa0--1eee--4383--aa30--30503060410e-osd--block--eda3552c--4995--4330--b856--680382406533 (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-0fe81fa0-1eee-4383-aa30-30503060410e" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-eda3552c-4995-4330-b856-680382406533"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-0fe81fa0-1eee-4383-aa30-30503060410e" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-eda3552c-4995-4330-b856-680382406533" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-0fe81fa0-1eee-4383-aa30-30503060410e"\n'
10.10.2.1: b'  Volume group "ceph-0fe81fa0-1eee-4383-aa30-30503060410e" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:11:21,073020752-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:11:21,083120487-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:11:21,087015182-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\n'
10.10.2.1: b'podman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\nlvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\nCluster fsid: 45f72b08-386e-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\n'
10.10.2.1: b'Setting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\n'
10.10.2.1: b'Waiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\n'
10.10.2.1: b'Setting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 45f72b08-386e-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:12:23,213843444-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:12:43,221448768-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:12:43,231408690-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:12:43,235293235-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 45f72b08-386e-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/45f72b08-386e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:12:52,380324267-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:12:52,390737911-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:12:52,394310370-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 45f72b08-386e-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/45f72b08-386e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:13:01,881597827-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:13:01,887783080-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:13:02,101251813-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:13:02,104931965-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid 45f72b08-386e-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/45f72b08-386e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:13:11,502296477-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:13:31,507157189-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:13:31,513726193-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:13:31,523642823-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:13:31,527231162-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 45f72b08-386e-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/45f72b08-386e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:13:55,922314688-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:14:15,927999548-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:14:15,938311672-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:14:15,941939554-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 45f72b08-386e-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/45f72b08-386e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     45f72b08-386e-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.dttkts(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 41s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 21:14:25 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:11:13,652011628-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/f1c0c8ce-386c-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid f1c0c8ce-386c-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:11:13,663765093-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:11:13,667382528-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', 'f1c0c8ce-386c-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid f1c0c8ce-386c-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:11:13,676246659-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid f1c0c8ce-386c-11ec-b51d-53e6e728d2d3'
[1] 21:11:19 [SUCCESS] 10.10.2.1
[2] 21:11:20 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:11:20,135879565-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:11:20,148125657-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:11:20,152780212-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:11:20,303336126-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:11:20,308182241-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:11:20,458863440-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:11:20,743310516-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:11:20,748322362-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--0fe81fa0--1eee--4383--aa30--30503060410e-osd--block--eda3552c--4995--4330--b856--680382406533 (253:0)
  Archiving volume group "ceph-0fe81fa0-1eee-4383-aa30-30503060410e" metadata (seqno 5).
  Releasing logical volume "osd-block-eda3552c-4995-4330-b856-680382406533"
  Creating volume group backup "/etc/lvm/backup/ceph-0fe81fa0-1eee-4383-aa30-30503060410e" (seqno 6).
  Logical volume "osd-block-eda3552c-4995-4330-b856-680382406533" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-0fe81fa0-1eee-4383-aa30-30503060410e"
  Volume group "ceph-0fe81fa0-1eee-4383-aa30-30503060410e" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:11:21,073020752-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:11:21,083120487-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:11:21,087015182-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 45f72b08-386e-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 45f72b08-386e-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:12:23,213843444-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:12:43,221448768-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:12:43,231408690-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:12:43,235293235-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 45f72b08-386e-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/45f72b08-386e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:12:52,380324267-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:12:52,390737911-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:12:52,394310370-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 45f72b08-386e-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/45f72b08-386e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:13:01,881597827-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:13:01,887783080-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:13:02,101251813-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:13:02,104931965-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 45f72b08-386e-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/45f72b08-386e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:13:11,502296477-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:13:31,507157189-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:13:31,513726193-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:13:31,523642823-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:13:31,527231162-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 45f72b08-386e-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/45f72b08-386e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:13:55,922314688-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:14:15,927999548-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:14:15,938311672-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:14:15,941939554-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 45f72b08-386e-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/45f72b08-386e-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     45f72b08-386e-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.dttkts(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 41s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:14:25,157863286-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:14:25,165924951-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 21:14:25 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:14:25,641747483-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:14:25,645731031-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:14:25,668311735-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:14:25,671096073-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '45f72b08-386e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 45f72b08-386e-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:14:29,770008174-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:14:29,773123767-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '45f72b08-386e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 45f72b08-386e-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:14:34,212638586-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:14:34,216003318-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '45f72b08-386e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 45f72b08-386e-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:14:38,385465050-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:14:38,388657878-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '45f72b08-386e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 45f72b08-386e-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:14:46,911801532-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:14:46,914719603-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '45f72b08-386e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 45f72b08-386e-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:14:51,442933110-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:14:51,445990713-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '45f72b08-386e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 45f72b08-386e-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:14:56,941493221-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:14:56,944627579-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '45f72b08-386e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 45f72b08-386e-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:15:01,514246780-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:15:01,517204284-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '45f72b08-386e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 45f72b08-386e-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:15:06,924363020-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:15:06,927415174-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '45f72b08-386e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 45f72b08-386e-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:15:11,630293709-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:15:11,633519409-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '45f72b08-386e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 45f72b08-386e-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:15:16,953738015-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:15:16,956735515-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '45f72b08-386e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 45f72b08-386e-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 19 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 21 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:15:21,020611642-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:15:21,023469719-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '45f72b08-386e-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 45f72b08-386e-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.39059         -  400 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default   
-3         0.39059         -  400 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host node-1
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0  
                       TOTAL  400 GiB  5.7 MiB  176 KiB   0 B  5.6 MiB  400 GiB  0.00                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:15:25,248361912-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:15:49,477598345-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:15:58,725530997-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:16:07,908890448-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:16:07,917389167-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:16:17,173328651-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:16:17,181499483-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:16:26,419609154-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:16:26,428179539-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:16:35,618607668-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:16:35,626759313-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:16:44,868256819-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:16:44,876693682-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:16:44,883046747-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:16:44,887007763-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:16:44,894395359-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:16:44,900727294-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=1163900
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:16:44,908533458-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:16:44,917866350-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'134585\n'
[1] 21:16:45 [SUCCESS] ljishen@10.10.2.2
134585

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:16:45,111993178-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16MB_write.log.2
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:16:45,132739704-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:16:45,135551995-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '45f72b08-386e-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 45f72b08-386e-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T04:16:48.431323+0000 Maintaining 128 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 60 seconds or 0 objects
2021-10-29T04:16:48.431363+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-29T04:16:49.109695+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T04:16:49.109695+0000     0       0         0         0         0         0           -           0
2021-10-29T04:16:50.109836+0000     1       3         3         0         0         0           -           0
2021-10-29T04:16:51.109949+0000     2       7         7         0         0         0           -           0
2021-10-29T04:16:52.110068+0000     3      10        10         0         0         0           -           0
2021-10-29T04:16:53.110182+0000     4      14        14         0         0         0           -           0
2021-10-29T04:16:54.110310+0000     5      17        17         0         0         0           -           0
2021-10-29T04:16:55.110419+0000     6      21        21         0         0         0           -           0
2021-10-29T04:16:56.110538+0000     7      23        23         0         0         0           -           0
2021-10-29T04:16:57.110662+0000     8      27        27         0         0         0           -           0
2021-10-29T04:16:58.110814+0000     9      30        30         0         0         0           -           0
2021-10-29T04:16:59.110926+0000    10      34        34         0         0         0           -           0
2021-10-29T04:17:00.111009+0000    11      34        34         0         0         0           -           0
2021-10-29T04:17:01.111116+0000    12      34        34         0         0         0           -           0
2021-10-29T04:17:02.111227+0000    13      34        34         0         0         0           -           0
2021-10-29T04:17:03.111338+0000    14      35        35         0         0         0           -           0
2021-10-29T04:17:04.111451+0000    15      35        35         0         0         0           -           0
2021-10-29T04:17:05.111564+0000    16      35        35         0         0         0           -           0
2021-10-29T04:17:06.111675+0000    17      35        35         0         0         0           -           0
2021-10-29T04:17:07.111789+0000    18      35        35         0         0         0           -           0
2021-10-29T04:17:08.111886+0000    19      36        36         0         0         0           -           0
2021-10-29T04:17:09.111996+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-29T04:17:09.111996+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T04:17:09.111996+0000    20      36        36         0         0         0           -           0
2021-10-29T04:17:10.112113+0000    21      39        39         0         0         0           -           0
2021-10-29T04:17:11.112228+0000    22      42        42         0         0         0           -           0
2021-10-29T04:17:12.112337+0000    23      46        46         0         0         0           -           0
2021-10-29T04:17:13.112447+0000    24      49        49         0         0         0           -           0
2021-10-29T04:17:14.112559+0000    25      52        52         0         0         0           -           0
2021-10-29T04:17:15.112672+0000    26      56        56         0         0         0           -           0
2021-10-29T04:17:16.112782+0000    27      59        59         0         0         0           -           0
2021-10-29T04:17:17.112896+0000    28      63        63         0         0         0           -           0
2021-10-29T04:17:18.113004+0000    29      66        66         0         0         0           -           0
2021-10-29T04:17:19.113114+0000    30      66        66         0         0         0           -           0
2021-10-29T04:17:20.113225+0000    31      66        66         0         0         0           -           0
2021-10-29T04:17:21.113339+0000    32      67        67         0         0         0           -           0
2021-10-29T04:17:22.113452+0000    33      67        67         0         0         0           -           0
2021-10-29T04:17:23.113567+0000    34      67        67         0         0         0           -           0
2021-10-29T04:17:24.113677+0000    35      67        67         0         0         0           -           0
2021-10-29T04:17:25.113789+0000    36      67        67         0         0         0           -           0
2021-10-29T04:17:26.113896+0000    37      67        67         0         0         0           -           0
2021-10-29T04:17:27.114008+0000    38      68        68         0         0         0           -           0
2021-10-29T04:17:28.114120+0000    39      68        68         0         0         0           -           0
2021-10-29T04:17:29.114234+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-29T04:17:29.114234+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T04:17:29.114234+0000    40      68        68         0         0         0           -           0
2021-10-29T04:17:30.114352+0000    41      68        68         0         0         0           -           0
2021-10-29T04:17:31.114474+0000    42      68        68         0         0         0           -           0
2021-10-29T04:17:32.114587+0000    43      68        68         0         0         0           -           0
2021-10-29T04:17:33.114696+0000    44      69        69         0         0         0           -           0
2021-10-29T04:17:34.114805+0000    45      69        69         0         0         0           -           0
2021-10-29T04:17:35.114920+0000    46      69        69         0         0         0           -           0
2021-10-29T04:17:36.115028+0000    47      70        70         0         0         0           -           0
2021-10-29T04:17:37.115138+0000    48      70        70         0         0         0           -           0
2021-10-29T04:17:38.115246+0000    49      70        70         0         0         0           -           0
2021-10-29T04:17:39.115361+0000    50      71        71         0         0         0           -           0
2021-10-29T04:17:40.115475+0000    51      71        71         0         0         0           -           0
2021-10-29T04:17:41.115583+0000    52      72        72         0         0         0           -           0
2021-10-29T04:17:42.115691+0000    53      72        72         0         0         0           -           0
2021-10-29T04:17:43.115801+0000    54      72        72         0         0         0           -           0
2021-10-29T04:17:44.115911+0000    55      73        73         0         0         0           -           0
2021-10-29T04:17:45.116025+0000    56      73        73         0         0         0           -           0
2021-10-29T04:17:46.116131+0000    57      76        76         0         0         0           -           0
2021-10-29T04:17:47.116241+0000    58      78        78         0         0         0           -           0
2021-10-29T04:17:48.116355+0000    59      81        81         0         0         0           -           0
2021-10-29T04:17:49.116468+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-29T04:17:49.116468+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T04:17:49.116468+0000    60      85        85         0         0         0           -           0
2021-10-29T04:17:50.116584+0000    61      89        89         0         0         0           -           0
2021-10-29T04:17:51.116695+0000    62      92        92         0         0         0           -           0
2021-10-29T04:17:52.116804+0000    63      94        94         0         0         0           -           0
2021-10-29T04:17:53.116914+0000    64      98        98         0         0         0           -           0
2021-10-29T04:17:54.117027+0000    65      98        98         0         0         0           -           0
2021-10-29T04:17:55.117140+0000    66      98        98         0         0         0           -           0
2021-10-29T04:17:56.117251+0000    67      98        98         0         0         0           -           0
2021-10-29T04:17:57.117364+0000    68      98        98         0         0         0           -           0
2021-10-29T04:17:58.117475+0000    69      99        99         0         0         0           -           0
2021-10-29T04:17:59.117590+0000    70      99        99         0         0         0           -           0
2021-10-29T04:18:00.117698+0000    71      99        99         0         0         0           -           0
2021-10-29T04:18:01.117808+0000    72      99        99         0         0         0           -           0
2021-10-29T04:18:02.117917+0000    73      99        99         0         0         0           -           0
2021-10-29T04:18:03.118027+0000    74      99        99         0         0         0           -           0
2021-10-29T04:18:04.118111+0000    75      99        99         0         0         0           -           0
2021-10-29T04:18:05.118216+0000    76     100       100         0         0         0           -           0
2021-10-29T04:18:06.118326+0000    77     101       101         0         0         0           -           0
2021-10-29T04:18:07.118437+0000    78     103       103         0         0         0           -           0
2021-10-29T04:18:08.118547+0000    79     107       107         0         0         0           -           0
2021-10-29T04:18:09.118658+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-29T04:18:09.118658+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T04:18:09.118658+0000    80     111       111         0         0         0           -           0
2021-10-29T04:18:10.118775+0000    81     113       113         0         0         0           -           0
2021-10-29T04:18:11.118892+0000    82     117       117         0         0         0           -           0
2021-10-29T04:18:12.119008+0000    83     120       120         0         0         0           -           0
2021-10-29T04:18:13.119122+0000    84     124       124         0         0         0           -           0
2021-10-29T04:18:14.119239+0000    85     125       125         0         0         0           -           0
2021-10-29T04:18:15.119350+0000    86     125       125         0         0         0           -           0
2021-10-29T04:18:16.119459+0000    87     125       125         0         0         0           -           0
2021-10-29T04:18:17.119569+0000    88     125       125         0         0         0           -           0
2021-10-29T04:18:18.119678+0000    89     126       126         0         0         0           -           0
2021-10-29T04:18:19.119789+0000    90     126       126         0         0         0           -           0
2021-10-29T04:18:20.119897+0000    91     126       126         0         0         0           -           0
2021-10-29T04:18:21.120007+0000    92       1       128       127   22.0845    22.087      3.7312     52.2637
2021-10-29T04:18:22.120147+0000 Total time run:         92.2392
Total writes made:      128
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     22.2031
Stddev Bandwidth:       2.30272
Max bandwidth (MB/sec): 22.087
Min bandwidth (MB/sec): 0
Average IOPS:           1
Stddev IOPS:            0.104257
Max IOPS:               1
Min IOPS:               0
Average Latency(s):     51.8636
Stddev Latency(s):      29.6039
Max latency(s):         91.7226
Min latency(s):         1.04144

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:18:23,439315327-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:18:23,446042928-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 1163900

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:18:23,452272941-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 134585
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:18:23,460836753-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 134585
[1] 21:18:23 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:18:23,646348962-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16MB_write.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16MB_write.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:18:23,838015501-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:18:48,168987812-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.7 GiB used, 393 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:18:48,177148304-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:18:57,409940987-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:18:57,418680941-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:19:06,785266079-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:19:06,793182231-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:19:16,183608683-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:19:16,192364517-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:19:25,399030888-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:19:25,407496936-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:19:25,413784268-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:19:25,417623044-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:19:25,424878970-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:19:25,431029484-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.rados_bench_host.2
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=1165261
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.rados_bench_host.2 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:19:25,438653885-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:19:25,448166516-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'134694\n'
[1] 21:19:25 [SUCCESS] ljishen@10.10.2.2
134694

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:19:25,639217033-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16MB_seq.log.2
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:19:25,659897614-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:19:25,662716588-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '45f72b08-386e-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 45f72b08-386e-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T04:19:29.091146+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T04:19:29.091146+0000     0       0         0         0         0         0           -           0
2021-10-29T04:19:30.091288+0000     1      11        11         0         0         0           -           0
2021-10-29T04:19:31.091402+0000     2      22        22         0         0         0           -           0
2021-10-29T04:19:32.091513+0000     3      34        34         0         0         0           -           0
2021-10-29T04:19:33.091624+0000     4      43        43         0         0         0           -           0
2021-10-29T04:19:34.091734+0000     5      53        53         0         0         0           -           0
2021-10-29T04:19:35.091847+0000     6      65        65         0         0         0           -           0
2021-10-29T04:19:36.091957+0000     7      76        76         0         0         0           -           0
2021-10-29T04:19:37.092068+0000     8      85        85         0         0         0           -           0
2021-10-29T04:19:38.092178+0000     9      96        96         0         0         0           -           0
2021-10-29T04:19:39.092289+0000    10     106       106         0         0         0           -           0
2021-10-29T04:19:40.092363+0000    11     115       115         0         0         0           -           0
2021-10-29T04:19:41.092481+0000    12     123       123         0         0         0           -           0
2021-10-29T04:19:42.092611+0000    13       3       128       125   153.827   153.846    0.780778     6.97142
2021-10-29T04:19:43.092758+0000 Total time run:       13.2656
Total reads made:     128
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   154.384
Average IOPS:         9
Stddev IOPS:          2.49615
Max IOPS:             9
Min IOPS:             0
Average Latency(s):   6.8297
Max latency(s):       12.6314
Min latency(s):       0.780778

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:19:43,886400972-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:19:43,893451281-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 1165261

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:19:43,900003332-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 134694
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:19:43,908105203-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 134694
[1] 21:19:44 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:19:44,094037652-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16MB_seq.dat.osd_host.2
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16MB_seq.dat.osd_host.2 /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.osd_host.2


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:19:44,249321762-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:20:08,654865839-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:20:08,663421867-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:20:17,870423361-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:20:17,878710943-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:20:27,078937111-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:20:27,087213532-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:20:36,427496833-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:20:36,435613201-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:20:45,695678823-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:20:45,704123892-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:20:45,710295925-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-10-28T21:20:45,712913078-07:00][RUNNING][ROUND 3/7/21] object_size=16MB[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:20:45,716884745-07:00] STAGE: Launch a Ceph cluster (CEPH_HOST: 10.10.2.1, OSD_HOST: 10.10.2.2) ...[0m
# ./benchmarks/bench-rados:183 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 bash -euo pipefail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:20:45,726575251-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.1 bash -euo pipefail
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:20:46,160170167-07:00] INFO: > Remove existing clusters\x1b[0m\n'
10.10.2.1: b'## bash:17 -  > basename -- /var/lib/ceph/45f72b08-386e-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 45f72b08-386e-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:20:46,170867422-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:20:46,173925393-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '45f72b08-386e-11ec-b51d-53e6e728d2d3']\x1b[0m\n"
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 45f72b08-386e-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:20:46,182925988-07:00] DEBUG: command from stdin\x1b[0m\n'
10.10.2.1: b"# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 45f72b08-386e-11ec-b51d-53e6e728d2d3'\n"
10.10.2.1: b'[1] 21:20:51 [SUCCESS] 10.10.2.1\n[2] 21:20:52 [SUCCESS] 10.10.2.2\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:20:52,717923092-07:00] INFO: > Deploy a new cluster\x1b[0m\n'
10.10.2.1: b'# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:20:52,730629125-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:20:52,735474757-07:00] INFO: > Check the host for the monitor\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:20:52,883135163-07:00] INFO: > Check hosts for OSDs\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:20:52,888073079-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:20:53,038895121-07:00] INFO: >>> Check device '/dev/nvme0n1'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:20:53,323425765-07:00] STAGE: Reset devices on hosts...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:20:53,328500728-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh\n'
10.10.2.1: b'  Removing ceph--b7e12174--5eae--4c89--a73b--e4aced54ad8f-osd--block--e71dec92--f14a--41ad--8b9e--f7030bb8bd04 (253:0)\n'
10.10.2.1: b'  Archiving volume group "ceph-b7e12174-5eae-4c89-a73b-e4aced54ad8f" metadata (seqno 5).\n'
10.10.2.1: b'  Releasing logical volume "osd-block-e71dec92-f14a-41ad-8b9e-f7030bb8bd04"\n'
10.10.2.1: b'  Creating volume group backup "/etc/lvm/backup/ceph-b7e12174-5eae-4c89-a73b-e4aced54ad8f" (seqno 6).\n'
10.10.2.1: b'  Logical volume "osd-block-e71dec92-f14a-41ad-8b9e-f7030bb8bd04" successfully removed\n'
10.10.2.1: b'  Removing physical volume "/dev/nvme0n1" from volume group "ceph-b7e12174-5eae-4c89-a73b-e4aced54ad8f"\n'
10.10.2.1: b'  Volume group "ceph-b7e12174-5eae-4c89-a73b-e4aced54ad8f" successfully removed\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:20:53,657061958-07:00] STAGE: Bootstrap a new cluster...\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:20:53,666481872-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:20:53,670122307-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults\n'
10.10.2.1: b'Verifying podman|docker is present...\nVerifying lvm2 is present...\n'
10.10.2.1: b'Verifying time synchronization is in place...\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Repeating the final host check...\npodman|docker (/usr/bin/docker) is present\n'
10.10.2.1: b'systemctl is present\n'
10.10.2.1: b'lvcreate is present\n'
10.10.2.1: b'Unit ntp.service is enabled and running\n'
10.10.2.1: b'Host looks OK\n'
10.10.2.1: b'Cluster fsid: 9b40480a-386f-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 3300 ...\n'
10.10.2.1: b'Verifying IP 10.10.2.1 port 6789 ...\n'
10.10.2.1: b'Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`\n'
10.10.2.1: b'- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network\n'
10.10.2.1: b'Adjusting default settings to suit single-host cluster...\n'
10.10.2.1: b'Pulling container image quay.io/ceph/ceph:v16...\n'
10.10.2.1: b'Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)\n'
10.10.2.1: b'Extracting ceph user uid/gid from container image...\n'
10.10.2.1: b'Creating initial keys...\n'
10.10.2.1: b'Creating initial monmap...\n'
10.10.2.1: b'Creating mon...\n'
10.10.2.1: b'Waiting for mon to start...\n'
10.10.2.1: b'Waiting for mon...\n'
10.10.2.1: b'mon is available\nSetting mon public_network to 10.10.2.0/24\n'
10.10.2.1: b'Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf\n'
10.10.2.1: b'Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n'
10.10.2.1: b'Creating mgr...\n'
10.10.2.1: b'Verifying port 9283 ...\n'
10.10.2.1: b'Waiting for mgr to start...\nWaiting for mgr...\n'
10.10.2.1: b'mgr not available, waiting (1/15)...\n'
10.10.2.1: b'mgr not available, waiting (2/15)...\n'
10.10.2.1: b'mgr is available\n'
10.10.2.1: b'Enabling cephadm module...\n'
10.10.2.1: b'Waiting for the mgr to restart...\n'
10.10.2.1: b'Waiting for mgr epoch 5...\n'
10.10.2.1: b'mgr epoch 5 is available\nSetting orchestrator backend to cephadm...\n'
10.10.2.1: b'Generating ssh key...\n'
10.10.2.1: b'Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub\n'
10.10.2.1: b'Adding key to ljishen@localhost authorized_keys...\n'
10.10.2.1: b'Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...\n'
10.10.2.1: b'Deploying unmanaged mon service...\n'
10.10.2.1: b'Deploying unmanaged mgr service...\n'
10.10.2.1: b'You can access the Ceph CLI with:\n\n\tsudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 9b40480a-386f-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring\n\n'
10.10.2.1: b'Please consider enabling telemetry to help improve Ceph:\n\n\tceph telemetry on\n\nFor more information see:\n\n\thttps://docs.ceph.com/docs/pacific/mgr/telemetry/\n\nBootstrap complete.\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:21:53,572002948-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:22:13,579248024-07:00] INFO: > Set the default number of monitor daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:22:13,589600942-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:22:13,593808594-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1\n'
10.10.2.1: b'Inferring fsid 9b40480a-386f-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/9b40480a-386f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mon update...\n'
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:22:22,728705348-07:00] INFO: > Set the default number of manager daemons to 1\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:22:22,738811442-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:22:22,742623450-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1\n'
10.10.2.1: b'Inferring fsid 9b40480a-386f-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/9b40480a-386f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'Scheduled mgr update...\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:22:32,128573501-07:00] STAGE: Add hosts to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:22:32,135013831-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\n'
10.10.2.1: b'/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"\n'
10.10.2.1: b'\nNumber of key(s) added: 1\n\nNow try logging into the machine, with:   "ssh -o \'GlobalKnownHostsFile=/dev/null\' -o \'LogLevel=ERROR\' -o \'PasswordAuthentication=no\' -o \'StrictHostKeyChecking=no\' -o \'UserKnownHostsFile=/dev/null\' \'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\'"\nand check to make sure that only the key(s) you wanted were added.\n\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:22:32,345129503-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:22:32,348939657-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2\n'
10.10.2.1: b'Inferring fsid 9b40480a-386f-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/9b40480a-386f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:22:41,938675589-07:00] INFO: Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:23:01,943688437-07:00] STAGE: Add OSDs to the cluster...\x1b[0m\n'
10.10.2.1: b"\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:23:01,951194872-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:23:01,961235572-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:23:01,964981776-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1\n'
10.10.2.1: b'Inferring fsid 9b40480a-386f-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/9b40480a-386f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b"Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'\n"
10.10.2.1: b'\n\x1b[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:23:26,124136963-07:00] INFO: > Sleep for 20 seconds\x1b[0m\n'
10.10.2.1: b'\n\n\x1b[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:23:46,129384762-07:00] STAGE: Show Cluster Status\x1b[0m\n'
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:23:46,139233631-07:00] DEBUG: HOST_PARAMS: []\x1b[0m\n'
10.10.2.1: b"\x1b[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us\t[2021-10-28T21:23:46,142777926-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status']\x1b[0m\n"
10.10.2.1: b'# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status\n'
10.10.2.1: b'Inferring fsid 9b40480a-386f-11ec-b51d-53e6e728d2d3\n'
10.10.2.1: b'Inferring config /var/lib/ceph/9b40480a-386f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config\n'
10.10.2.1: b'Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45\n'
10.10.2.1: b'  cluster:\n    id:     9b40480a-386f-11ec-b51d-53e6e728d2d3\n    health: HEALTH_WARN\n            1 pool(s) have no replicas configured\n \n  services:\n    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)\n    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.aaaahc(active, since 2m)\n    osd: 1 osds: 1 up (since 18s), 1 in (since 40s)\n \n  data:\n    pools:   1 pools, 1 pgs\n    objects: 0 objects, 0 B\n    usage:   4.7 MiB used, 400 GiB / 400 GiB avail\n    pgs:     1 active+clean\n \n'
[1] 21:23:54 [SUCCESS] ljishen@10.10.2.1

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:20:46,160170167-07:00] INFO: > Remove existing clusters[0m
## bash:17 -  > basename -- /var/lib/ceph/45f72b08-386e-11ec-b51d-53e6e728d2d3
# bash:17 -  > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm --host 10.10.2.1 --host 10.10.2.2 rm-cluster --force --fsid 45f72b08-386e-11ec-b51d-53e6e728d2d3
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:20:46,170867422-07:00] DEBUG: HOST_PARAMS: ['--host', '10.10.2.1', '--host', '10.10.2.2'][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:20:46,173925393-07:00] DEBUG: CEPHADM_PARAMS: ['rm-cluster', '--force', '--fsid', '45f72b08-386e-11ec-b51d-53e6e728d2d3'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:63 -  > /tmp/bench-rados/ceph-research/scripts/parallel-ssh --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 45f72b08-386e-11ec-b51d-53e6e728d2d3'
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:20:46,182925988-07:00] DEBUG: command from stdin[0m
# /tmp/bench-rados/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host 10.10.2.1 --host 10.10.2.2 'sudo python3 -u - rm-cluster --force --fsid 45f72b08-386e-11ec-b51d-53e6e728d2d3'
[1] 21:20:51 [SUCCESS] 10.10.2.1
[2] 21:20:52 [SUCCESS] 10.10.2.2

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:20:52,717923092-07:00] INFO: > Deploy a new cluster[0m
# bash:24 -  > /tmp/bench-rados/ceph-research/scripts/ceph-deploy -m 10.10.2.1 -o node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/10.10.2.2:/dev/nvme0n1


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:20:52,730629125-07:00] STAGE: Verify passwordless SSH to hosts, passwordless sudo via ljishen, and the existence of devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:20:52,735474757-07:00] INFO: > Check the host for the monitor[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:181 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@10.10.2.1 sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:20:52,883135163-07:00] INFO: > Check hosts for OSDs[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:20:52,888073079-07:00] INFO: >> Check host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:200 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo --non-interactive --validate

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:20:53,038895121-07:00] INFO: >>> Check device '/dev/nvme0n1'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:215 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us test -b /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:226 - verify_parameters() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us lsblk --nodeps --noheadings --output TYPE /dev/nvme0n1
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:227 - verify_parameters() > grep --word-regexp disk


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:20:53,323425765-07:00] STAGE: Reset devices on hosts...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:20:53,328500728-07:00] INFO: > Resetting device '/dev/nvme0n1' on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:288 - reset_host_devices() > ssh -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us sudo sh
  Removing ceph--b7e12174--5eae--4c89--a73b--e4aced54ad8f-osd--block--e71dec92--f14a--41ad--8b9e--f7030bb8bd04 (253:0)
  Archiving volume group "ceph-b7e12174-5eae-4c89-a73b-e4aced54ad8f" metadata (seqno 5).
  Releasing logical volume "osd-block-e71dec92-f14a-41ad-8b9e-f7030bb8bd04"
  Creating volume group backup "/etc/lvm/backup/ceph-b7e12174-5eae-4c89-a73b-e4aced54ad8f" (seqno 6).
  Logical volume "osd-block-e71dec92-f14a-41ad-8b9e-f7030bb8bd04" successfully removed
  Removing physical volume "/dev/nvme0n1" from volume group "ceph-b7e12174-5eae-4c89-a73b-e4aced54ad8f"
  Volume group "ceph-b7e12174-5eae-4c89-a73b-e4aced54ad8f" successfully removed


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:20:53,657061958-07:00] STAGE: Bootstrap a new cluster...[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:317 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:20:53,666481872-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:20:53,670122307-07:00] DEBUG: CEPHADM_PARAMS: ['bootstrap', '--config', '/tmp/bench-rados/ceph-research/scripts/ceph.conf', '--mon-ip', '10.10.2.1', '--output-dir', '/tmp/bench-rados/deployment_data_root/etc/ceph', '--ssh-user', 'ljishen', '--skip-dashboard', '--no-minimize-config', '--allow-overwrite', '--allow-fqdn-hostname', '--orphan-initial-daemons', '--skip-monitoring-stack', '--single-host-defaults'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm bootstrap --config /tmp/bench-rados/ceph-research/scripts/ceph.conf --mon-ip 10.10.2.1 --output-dir /tmp/bench-rados/deployment_data_root/etc/ceph --ssh-user ljishen --skip-dashboard --no-minimize-config --allow-overwrite --allow-fqdn-hostname --orphan-initial-daemons --skip-monitoring-stack --single-host-defaults
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit ntp.service is enabled and running
Repeating the final host check...
podman|docker (/usr/bin/docker) is present
systemctl is present
lvcreate is present
Unit ntp.service is enabled and running
Host looks OK
Cluster fsid: 9b40480a-386f-11ec-b51d-53e6e728d2d3
Verifying IP 10.10.2.1 port 3300 ...
Verifying IP 10.10.2.1 port 6789 ...
Mon IP `10.10.2.1` is in CIDR network `10.10.2.0/24`
- internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Adjusting default settings to suit single-host cluster...
Pulling container image quay.io/ceph/ceph:v16...
Ceph version: ceph version 16.2.6 (ee28fb57e47e9f88813e24bbf4c14496ca299d31) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
Waiting for mon to start...
Waiting for mon...
mon is available
Setting mon public_network to 10.10.2.0/24
Wrote config to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf
Wrote keyring to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub
Adding key to ljishen@localhost authorized_keys...
Adding host node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us...
Deploying unmanaged mon service...
Deploying unmanaged mgr service...
You can access the Ceph CLI with:

	sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --fsid 9b40480a-386f-11ec-b51d-53e6e728d2d3 -c /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf -k /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring

Please consider enabling telemetry to help improve Ceph:

	ceph telemetry on

For more information see:

	https://docs.ceph.com/docs/pacific/mgr/telemetry/

Bootstrap complete.

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:21:53,572002948-07:00] INFO: Sleep for 20 seconds[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:22:13,579248024-07:00] INFO: > Set the default number of monitor daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:337 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:22:13,589600942-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:22:13,593808594-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mon', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mon 1
Inferring fsid 9b40480a-386f-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/9b40480a-386f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mon update...

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:22:22,728705348-07:00] INFO: > Set the default number of manager daemons to 1[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:342 - bootstrap_cluster() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:22:22,738811442-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:22:22,742623450-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'apply', 'mgr', '1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch apply mgr 1
Inferring fsid 9b40480a-386f-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/9b40480a-386f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Scheduled mgr update...


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:22:32,128573501-07:00] STAGE: Add hosts to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:22:32,135013831-07:00] INFO: > Adding host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:361 - add_hosts() > ssh-copy-id -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -f -i /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.pub"

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh -o 'GlobalKnownHostsFile=/dev/null' -o 'LogLevel=ERROR' -o 'PasswordAuthentication=no' -o 'StrictHostKeyChecking=no' -o 'UserKnownHostsFile=/dev/null' 'ljishen@node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'"
and check to make sure that only the key(s) you wanted were added.

# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:367 - add_hosts() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:22:32,345129503-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:22:32,348939657-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'host', 'add', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us', '10.10.2.2'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch host add node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us 10.10.2.2
Inferring fsid 9b40480a-386f-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/9b40480a-386f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Added host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' with addr '10.10.2.2'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:22:41,938675589-07:00] INFO: Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:23:01,943688437-07:00] STAGE: Add OSDs to the cluster...[0m

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:23:01,951194872-07:00] INFO: > Adding devices ['/dev/nvme0n1'] on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us' as OSDs[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:388 - add_host_devices_as_osds() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:23:01,961235572-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:23:01,964981776-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', 'orch', 'daemon', 'add', 'osd', 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph orch daemon add osd node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us:/dev/nvme0n1
Inferring fsid 9b40480a-386f-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/9b40480a-386f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
Created osd(s) 0 on host 'node-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us'

[1;32mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:23:26,124136963-07:00] INFO: > Sleep for 20 seconds[0m


[1;33mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:23:46,129384762-07:00] STAGE: Show Cluster Status[0m
# /tmp/bench-rados/ceph-research/scripts/ceph-deploy:408 - show_status() > /tmp/bench-rados/ceph-research/scripts/parallel-cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:23:46,139233631-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:23:46,142777926-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--keyring', '/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--', 'ceph', '--status'][0m
# /tmp/bench-rados/ceph-research/scripts/parallel-cephadm:68 -  > sudo /tmp/bench-rados/deployment_data_root/usr/sbin/cephadm shell --keyring /tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring -- ceph --status
Inferring fsid 9b40480a-386f-11ec-b51d-53e6e728d2d3
Inferring config /var/lib/ceph/9b40480a-386f-11ec-b51d-53e6e728d2d3/mon.node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us/config
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
  cluster:
    id:     9b40480a-386f-11ec-b51d-53e6e728d2d3
    health: HEALTH_WARN
            1 pool(s) have no replicas configured
 
  services:
    mon: 1 daemons, quorum node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us (age 2m)
    mgr: node-0.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us.aaaahc(active, since 2m)
    osd: 1 osds: 1 up (since 18s), 1 in (since 40s)
 
  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   4.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
 

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:23:54,696110057-07:00] INFO: > Copy configuration files to local[0m
# ./benchmarks/bench-rados:220 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:23:54,704301678-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.1 sudo chown --changes --recursive ljishen /tmp/bench-rados/deployment_data_root/etc/ceph
[1] 21:23:54 [SUCCESS] ljishen@10.10.2.1
# ./benchmarks/bench-rados:223 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.conf /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf
# ./benchmarks/bench-rados:226 - launch_ceph_cluster() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.1:/tmp/bench-rados/deployment_data_root/etc/ceph/ceph.client.admin.keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:23:55,181360573-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:23:55,185484998-07:00] STAGE: Configure Ceph pool...[0m
# ./benchmarks/bench-rados:257 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:23:55,208249287-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:23:55,211023065-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b40480a-386f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b40480a-386f-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_max_backfills 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:258 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:23:59,484253572-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:23:59,487532863-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b40480a-386f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b40480a-386f-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_active 32
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:259 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:24:03,815390786-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:24:03,818353331-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b40480a-386f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b40480a-386f-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_max_single_start 8
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:260 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:24:08,148760038-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:24:08,152020032-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b40480a-386f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b40480a-386f-11ec-b51d-53e6e728d2d3 -- ceph config set osd osd_recovery_op_priority 63
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
# ./benchmarks/bench-rados:262 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./benchmarks/bench-rados:263 - configure_ceph_pool() > column -t -s ' '
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
osd_max_backfills                        32         mon
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  32         mon
osd_recovery_max_active_hdd              3          default
osd_recovery_max_active_ssd              10         default
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   default
osd_recovery_sleep_hdd                   0.100000   default
osd_recovery_sleep_hybrid                0.025000   default
osd_recovery_sleep_ssd                   0.000000   default
# ./benchmarks/bench-rados:265 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:24:16,724791462-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:24:16,727852001-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b40480a-386f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '1', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b40480a-386f-11ec-b51d-53e6e728d2d3 -- ceph osd pool create bench_rados 128 128 replicated --size 1 --pg-num-min 128
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 'bench_rados' created
# ./benchmarks/bench-rados:267 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:24:21,691407075-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:24:21,694617266-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b40480a-386f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b40480a-386f-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados min_size 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 min_size to 1
# ./benchmarks/bench-rados:268 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:24:26,966736976-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:24:26,970112609-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b40480a-386f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b40480a-386f-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados pg_autoscale_mode off
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 pg_autoscale_mode to off
# ./benchmarks/bench-rados:269 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:24:31,665828787-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:24:31,668735696-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b40480a-386f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b40480a-386f-11ec-b51d-53e6e728d2d3 -- ceph osd pool application enable bench_rados benchmark
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
enabled application 'benchmark' on pool 'bench_rados'
# ./benchmarks/bench-rados:270 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:24:36,116779743-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:24:36,119891558-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b40480a-386f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b40480a-386f-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados noscrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 noscrub to 1
# ./benchmarks/bench-rados:271 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:24:41,168474006-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:24:41,171378872-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b40480a-386f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b40480a-386f-11ec-b51d-53e6e728d2d3 -- ceph osd pool set bench_rados nodeep-scrub 1
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
set pool 2 nodeep-scrub to 1
# ./benchmarks/bench-rados:272 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:24:46,616039340-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:24:46,619062930-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b40480a-386f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b40480a-386f-11ec-b51d-53e6e728d2d3 -- ceph osd pool ls detail
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
pool 1 'device_health_metrics' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 64 pgp_num 64 autoscale_mode on last_change 18 lfor 0/0/16 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 1 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 20 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./benchmarks/bench-rados:273 - configure_ceph_pool() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:24:50,815016354-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:24:50,817963449-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b40480a-386f-11ec-b51d-53e6e728d2d3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b40480a-386f-11ec-b51d-53e6e728d2d3 -- ceph osd df tree
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP  META     AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -          root default   
-3         0.39059         -  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00    -              host node-1
 0    ssd  0.39059   1.00000  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00  1.00  192      up          osd.0  
                       TOTAL  400 GiB  5.7 MiB  168 KiB   0 B  5.6 MiB  400 GiB  0.00                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:24:54,986236306-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:25:19,425536910-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:25:28,841444521-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:25:38,156175530-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:25:38,164054951-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:25:47,681417727-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:25:47,690065227-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:25:56,837106806-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:25:56,845860496-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:26:05,969554683-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:26:05,977944547-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:26:15,133263219-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   5.7 MiB used, 400 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:26:15,142194143-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:26:15,148529715-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:26:15,152689536-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:26:15,160672453-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:26:15,166754637-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=1170898
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:26:15,174548308-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:26:15,183774779-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'138292\n'
[1] 21:26:15 [SUCCESS] ljishen@10.10.2.2
138292

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:26:15,375687587-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:370 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16MB_write.log.3
## ./benchmarks/bench-rados:361 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:361 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 60 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:26:15,397044152-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:26:15,399934380-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b40480a-386f-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '60', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '128', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b40480a-386f-11ec-b51d-53e6e728d2d3 -- rados bench 60 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 128 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T04:26:18.768860+0000 Maintaining 128 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 60 seconds or 0 objects
2021-10-29T04:26:18.768894+0000 Object prefix: benchmark_data_node-2.bluefield2.ucsc-cmps10_7
2021-10-29T04:26:19.452611+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T04:26:19.452611+0000     0       0         0         0         0         0           -           0
2021-10-29T04:26:20.452749+0000     1       4         4         0         0         0           -           0
2021-10-29T04:26:21.452825+0000     2       6         6         0         0         0           -           0
2021-10-29T04:26:22.452898+0000     3      10        10         0         0         0           -           0
2021-10-29T04:26:23.452967+0000     4      13        13         0         0         0           -           0
2021-10-29T04:26:24.453038+0000     5      16        16         0         0         0           -           0
2021-10-29T04:26:25.453106+0000     6      20        20         0         0         0           -           0
2021-10-29T04:26:26.453197+0000     7      22        22         0         0         0           -           0
2021-10-29T04:26:27.453290+0000     8      25        25         0         0         0           -           0
2021-10-29T04:26:28.453382+0000     9      29        29         0         0         0           -           0
2021-10-29T04:26:29.453457+0000    10      32        32         0         0         0           -           0
2021-10-29T04:26:30.453543+0000    11      34        34         0         0         0           -           0
2021-10-29T04:26:31.453626+0000    12      34        34         0         0         0           -           0
2021-10-29T04:26:32.453709+0000    13      34        34         0         0         0           -           0
2021-10-29T04:26:33.453785+0000    14      35        35         0         0         0           -           0
2021-10-29T04:26:34.453869+0000    15      35        35         0         0         0           -           0
2021-10-29T04:26:35.453950+0000    16      36        36         0         0         0           -           0
2021-10-29T04:26:36.454030+0000    17      36        36         0         0         0           -           0
2021-10-29T04:26:37.454100+0000    18      36        36         0         0         0           -           0
2021-10-29T04:26:38.454176+0000    19      37        37         0         0         0           -           0
2021-10-29T04:26:39.454249+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-29T04:26:39.454249+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T04:26:39.454249+0000    20      40        40         0         0         0           -           0
2021-10-29T04:26:40.454330+0000    21      44        44         0         0         0           -           0
2021-10-29T04:26:41.454401+0000    22      46        46         0         0         0           -           0
2021-10-29T04:26:42.454471+0000    23      50        50         0         0         0           -           0
2021-10-29T04:26:43.454540+0000    24      53        53         0         0         0           -           0
2021-10-29T04:26:44.454611+0000    25      56        56         0         0         0           -           0
2021-10-29T04:26:45.454682+0000    26      60        60         0         0         0           -           0
2021-10-29T04:26:46.454750+0000    27      63        63         0         0         0           -           0
2021-10-29T04:26:47.454815+0000    28      66        66         0         0         0           -           0
2021-10-29T04:26:48.454907+0000    29      66        66         0         0         0           -           0
2021-10-29T04:26:49.454979+0000    30      66        66         0         0         0           -           0
2021-10-29T04:26:50.455071+0000    31      67        67         0         0         0           -           0
2021-10-29T04:26:51.455154+0000    32      67        67         0         0         0           -           0
2021-10-29T04:26:52.455238+0000    33      67        67         0         0         0           -           0
2021-10-29T04:26:53.455309+0000    34      67        67         0         0         0           -           0
2021-10-29T04:26:54.455391+0000    35      67        67         0         0         0           -           0
2021-10-29T04:26:55.455469+0000    36      67        67         0         0         0           -           0
2021-10-29T04:26:56.455549+0000    37      68        68         0         0         0           -           0
2021-10-29T04:26:57.455621+0000    38      68        68         0         0         0           -           0
2021-10-29T04:26:58.455697+0000    39      68        68         0         0         0           -           0
2021-10-29T04:26:59.455771+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-29T04:26:59.455771+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T04:26:59.455771+0000    40      68        68         0         0         0           -           0
2021-10-29T04:27:00.455852+0000    41      68        68         0         0         0           -           0
2021-10-29T04:27:01.455925+0000    42      69        69         0         0         0           -           0
2021-10-29T04:27:02.456000+0000    43      69        69         0         0         0           -           0
2021-10-29T04:27:03.456071+0000    44      70        70         0         0         0           -           0
2021-10-29T04:27:04.456140+0000    45      70        70         0         0         0           -           0
2021-10-29T04:27:05.456206+0000    46      71        71         0         0         0           -           0
2021-10-29T04:27:06.456276+0000    47      71        71         0         0         0           -           0
2021-10-29T04:27:07.456345+0000    48      72        72         0         0         0           -           0
2021-10-29T04:27:08.456411+0000    49      72        72         0         0         0           -           0
2021-10-29T04:27:09.456483+0000    50      73        73         0         0         0           -           0
2021-10-29T04:27:10.456572+0000    51      73        73         0         0         0           -           0
2021-10-29T04:27:11.456657+0000    52      74        74         0         0         0           -           0
2021-10-29T04:27:12.456741+0000    53      74        74         0         0         0           -           0
2021-10-29T04:27:13.456812+0000    54      76        76         0         0         0           -           0
2021-10-29T04:27:14.456895+0000    55      79        79         0         0         0           -           0
2021-10-29T04:27:15.456976+0000    56      83        83         0         0         0           -           0
2021-10-29T04:27:16.457056+0000    57      86        86         0         0         0           -           0
2021-10-29T04:27:17.457126+0000    58      90        90         0         0         0           -           0
2021-10-29T04:27:18.457209+0000    59      93        93         0         0         0           -           0
2021-10-29T04:27:19.457295+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-29T04:27:19.457295+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T04:27:19.457295+0000    60      96        96         0         0         0           -           0
2021-10-29T04:27:20.457383+0000    61      98        98         0         0         0           -           0
2021-10-29T04:27:21.457456+0000    62      98        98         0         0         0           -           0
2021-10-29T04:27:22.457534+0000    63      98        98         0         0         0           -           0
2021-10-29T04:27:23.457610+0000    64      99        99         0         0         0           -           0
2021-10-29T04:27:24.457688+0000    65      99        99         0         0         0           -           0
2021-10-29T04:27:25.457759+0000    66      99        99         0         0         0           -           0
2021-10-29T04:27:26.457829+0000    67      99        99         0         0         0           -           0
2021-10-29T04:27:27.457901+0000    68      99        99         0         0         0           -           0
2021-10-29T04:27:28.457975+0000    69      99        99         0         0         0           -           0
2021-10-29T04:27:29.458042+0000    70     100       100         0         0         0           -           0
2021-10-29T04:27:30.458131+0000    71     100       100         0         0         0           -           0
2021-10-29T04:27:31.458219+0000    72     100       100         0         0         0           -           0
2021-10-29T04:27:32.458307+0000    73     102       102         0         0         0           -           0
2021-10-29T04:27:33.458380+0000    74     105       105         0         0         0           -           0
2021-10-29T04:27:34.458464+0000    75     109       109         0         0         0           -           0
2021-10-29T04:27:35.458549+0000    76     111       111         0         0         0           -           0
2021-10-29T04:27:36.458633+0000    77     114       114         0         0         0           -           0
2021-10-29T04:27:37.458707+0000    78     117       117         0         0         0           -           0
2021-10-29T04:27:38.458791+0000    79     120       120         0         0         0           -           0
2021-10-29T04:27:39.458880+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-10-29T04:27:39.458880+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T04:27:39.458880+0000    80     123       123         0         0         0           -           0
2021-10-29T04:27:40.458983+0000    81     125       125         0         0         0           -           0
2021-10-29T04:27:41.459058+0000    82     125       125         0         0         0           -           0
2021-10-29T04:27:42.459135+0000    83     125       125         0         0         0           -           0
2021-10-29T04:27:43.459212+0000    84     126       126         0         0         0           -           0
2021-10-29T04:27:44.459288+0000    85     126       126         0         0         0           -           0
2021-10-29T04:27:45.459362+0000    86     126       126         0         0         0           -           0
2021-10-29T04:27:46.459440+0000    87       2       128       126   23.1706   23.1724     6.64834     49.9745
2021-10-29T04:27:47.459546+0000 Total time run:         87.4269
Total writes made:      128
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     23.4253
Stddev Bandwidth:       2.48434
Max bandwidth (MB/sec): 23.1724
Min bandwidth (MB/sec): 0
Average IOPS:           1
Stddev IOPS:            0.107211
Max IOPS:               1
Min IOPS:               0
Average Latency(s):     49.2293
Stddev Latency(s):      27.8434
Max latency(s):         86.9939
Min latency(s):         1.13228

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:27:48,579035809-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:27:48,585826038-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 1170898

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:27:48,592440887-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 138292
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:27:48,600672212-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 138292
[1] 21:27:48 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:27:48,781949515-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16MB_write.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16MB_write.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_write.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:27:48,966297016-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:28:13,124001066-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.4 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:28:13,132671639-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:28:22,541360408-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:28:22,550219216-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:28:31,779494321-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:28:31,788232361-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:28:41,044125996-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:28:41,052965147-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:28:50,243091741-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:28:50,251661995-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:28:50,258873708-07:00] INFO: > The cluster is idle now.[0m


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:28:50,262671277-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:28:50,270377522-07:00] INFO: > Start system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:28:50,276786052-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:341 - rados_bench() > rm --force /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.rados_bench_host.3
# ./benchmarks/bench-rados:342 - rados_bench() > sar_pid_rados_bench_host=1172235
# ./benchmarks/bench-rados:342 - rados_bench() > S_TIME_FORMAT=ISO
# ./benchmarks/bench-rados:342 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.rados_bench_host.3 2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:28:50,284811529-07:00] INFO: >> for OSD host[0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:28:50,293978388-07:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.2.2 bash
10.10.2.2: b'138396\n'
[1] 21:28:50 [SUCCESS] ljishen@10.10.2.2
138396

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:28:50,483439186-07:00] INFO: > Run rados bench[0m
# ./benchmarks/bench-rados:379 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/output/rados_bench_16MB_seq.log.3
## ./benchmarks/bench-rados:376 - rados_bench() > count_cpus 32-63,96-127
# ./benchmarks/bench-rados:376 - rados_bench() > taskset --cpu-list 32-63,96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:28:50,503570231-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:28:50,506402520-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b40480a-386f-11ec-b51d-53e6e728d2d3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '128', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b40480a-386f-11ec-b51d-53e6e728d2d3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 128 --show-time
Using recent ceph image quay.io/ceph/ceph@sha256:5755c3a5c197ef186b8186212e023565f15b799f1ed411207f2c3fcd4a80ab45
hints = 1
2021-10-29T04:28:53.622338+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-10-29T04:28:53.622338+0000     0       0         0         0         0         0           -           0
2021-10-29T04:28:54.622474+0000     1      11        11         0         0         0           -           0
2021-10-29T04:28:55.622586+0000     2      21        21         0         0         0           -           0
2021-10-29T04:28:56.622652+0000     3      31        31         0         0         0           -           0
2021-10-29T04:28:57.622716+0000     4      41        41         0         0         0           -           0
2021-10-29T04:28:58.622780+0000     5      53        53         0         0         0           -           0
2021-10-29T04:28:59.622848+0000     6      61        61         0         0         0           -           0
2021-10-29T04:29:00.622917+0000     7      73        73         0         0         0           -           0
2021-10-29T04:29:01.622984+0000     8      82        82         0         0         0           -           0
2021-10-29T04:29:02.623048+0000     9      93        93         0         0         0           -           0
2021-10-29T04:29:03.623114+0000    10     105       105         0         0         0           -           0
2021-10-29T04:29:04.623181+0000    11     113       113         0         0         0           -           0
2021-10-29T04:29:05.623243+0000    12     122       122         0         0         0           -           0
2021-10-29T04:29:06.623308+0000    13       4       128       124   152.603   152.615    0.667865     6.98899
2021-10-29T04:29:07.623404+0000 Total time run:       13.5374
Total reads made:     128
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   151.285
Average IOPS:         9
Stddev IOPS:          2.49615
Max IOPS:             9
Min IOPS:             0
Average Latency(s):   6.80773
Max latency(s):       12.7883
Min latency(s):       0.667865

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:29:08,384331393-07:00] INFO: > Stop system activity collection services[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:29:08,391060797-07:00] INFO: >> for rados-bench host[0m
# ./benchmarks/bench-rados:391 - rados_bench() > kill -INT 1172235

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:29:08,397608890-07:00] INFO: >> for OSD host[0m
# ./benchmarks/bench-rados:396 - rados_bench() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.2.2 kill -INT 138396
[1;30mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:29:08,405750657-07:00] DEBUG: command from cmdline[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --print --host ljishen@10.10.2.2 kill -INT 138396
[1] 21:29:08 [SUCCESS] ljishen@10.10.2.2

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:29:08,590553685-07:00] INFO: >>> Copy system activity file from OSD host[0m
## ./benchmarks/bench-rados:403 - rados_bench() > basename -- /tmp/bench-rados/sys_activity_16MB_seq.dat.osd_host.3
# ./benchmarks/bench-rados:403 - rados_bench() > scp -C -o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null -p ljishen@10.10.2.2:/tmp/bench-rados/sys_activity_16MB_seq.dat.osd_host.3 /mnt/sda8/ceph-research/scripts/output/sys_activity_16MB_seq.dat.osd_host.3


[1;33mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:29:08,749119468-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:29:32,947266352-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:29:32,955951994-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:29:42,403319969-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:29:42,411847222-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:29:51,616954812-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:29:51,625356329-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:30:00,973811451-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:30:00,982819020-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:30:10,238668405-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 129 objects, 2.0 GiB
    usage:   6.1 GiB used, 394 GiB / 400 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:30:10,247442894-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-2.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-10-28T21:30:10,253952153-07:00] INFO: > The cluster is idle now.[0m
