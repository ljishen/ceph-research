

[1;7;39;49m[2021-05-17T11:57:50,256305662-04:00][RUNNING][ROUND 1/1/40] object_size=4KB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T11:57:50,259300267-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T11:57:50,268234229-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40776\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.357416\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid fa0499f5-c5b5-4311-a9b1-ae2ed87eb519\nsetting min_mon_release = octopus\nepoch 0\nfsid fa0499f5-c5b5-4311-a9b1-ae2ed87eb519\nlast_changed 2021-05-17T08:57:52.858415-0700\ncreated 2021-05-17T08:57:52.858415-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40776/0,v1:10.10.1.2:40777/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.357416 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 d3768e59-8c34-4518-984c-4901ae6818d7\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 6e6b60aa-0e69-42db-ac1b-b9a9c917d2df\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 d08d4e0b-8497-459e-b112-27b3d27e5733\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42776\n  w/ user/pass: admin / c284b998-b392-48ec-ba7c-224fe393ba56\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 11:58:06 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40776
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.357416
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid fa0499f5-c5b5-4311-a9b1-ae2ed87eb519
setting min_mon_release = octopus
epoch 0
fsid fa0499f5-c5b5-4311-a9b1-ae2ed87eb519
last_changed 2021-05-17T08:57:52.858415-0700
created 2021-05-17T08:57:52.858415-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40776/0,v1:10.10.1.2:40777/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.357416 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 d3768e59-8c34-4518-984c-4901ae6818d7
0
start osd.0
add osd1 6e6b60aa-0e69-42db-ac1b-b9a9c917d2df
1
start osd.1
add osd2 d08d4e0b-8497-459e-b112-27b3d27e5733
2
start osd.2


restful urls: https://10.10.1.2:42776
  w/ user/pass: admin / c284b998-b392-48ec-ba7c-224fe393ba56


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T08:57:51.174-0700 7f376ba671c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T08:57:51.174-0700 7f376ba671c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T08:57:51.190-0700 7f38833ff1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T08:57:51.190-0700 7f38833ff1c0 -1 WARNING: all dangerous and experimental features are enabled.
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40776,v1:10.10.1.2:40777] --print /tmp/ceph_monmap.357416 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.357416 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.357416 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42776 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.acufOamVG0 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new d3768e59-8c34-4518-984c-4901ae6818d7 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCKkqJgJJfqABAAePw/Mhqn31G14pryLWMqMg== --osd-uuid d3768e59-8c34-4518-984c-4901ae6818d7 
2021-05-17T08:58:02.366-0700 7f800c7b5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T08:58:02.366-0700 7f800c7b5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T08:58:02.366-0700 7f800c7b5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T08:58:02.446-0700 7f800c7b5f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 6e6b60aa-0e69-42db-ac1b-b9a9c917d2df -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T08:58:02.738-0700 7f4aeeeccf00 -1 Falling back to public interface
2021-05-17T08:58:02.750-0700 7f4aeeeccf00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCKkqJgFNQOLBAA4qD4OGcFGvOErHOoEI0iBg== --osd-uuid 6e6b60aa-0e69-42db-ac1b-b9a9c917d2df 
2021-05-17T08:58:03.070-0700 7f2aa41bef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T08:58:03.070-0700 7f2aa41bef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T08:58:03.070-0700 7f2aa41bef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T08:58:03.138-0700 7f2aa41bef00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new d08d4e0b-8497-459e-b112-27b3d27e5733 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T08:58:03.454-0700 7f28e19e8f00 -1 Falling back to public interface
2021-05-17T08:58:03.466-0700 7f28e19e8f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCLkqJgyw/XGRAA+IIvB6p8jKMBGVjtdz70gw== --osd-uuid d08d4e0b-8497-459e-b112-27b3d27e5733 
2021-05-17T08:58:03.798-0700 7fc3da9e2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T08:58:03.798-0700 7fc3da9e2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T08:58:03.798-0700 7fc3da9e2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T08:58:03.858-0700 7fc3da9e2f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T08:58:04.218-0700 7f10ec0b4f00 -1 Falling back to public interface
2021-05-17T08:58:04.234-0700 7f10ec0b4f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T11:58:06,240099378-04:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T11:58:06,243114962-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T11:58:06,283784618-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T11:58:06,286858552-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ef89850b-c4c2-4154-a93c-bd7020416b20', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LHod8y:/tmp/ceph-asok.LHod8y', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ef89850b-c4c2-4154-a93c-bd7020416b20 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LHod8y:/tmp/ceph-asok.LHod8y -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T11:58:09,623107914-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T11:58:09,626621214-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ef89850b-c4c2-4154-a93c-bd7020416b20', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LHod8y:/tmp/ceph-asok.LHod8y', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ef89850b-c4c2-4154-a93c-bd7020416b20 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LHod8y:/tmp/ceph-asok.LHod8y -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T11:58:12,901449801-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T11:58:12,905077676-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ef89850b-c4c2-4154-a93c-bd7020416b20', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LHod8y:/tmp/ceph-asok.LHod8y', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ef89850b-c4c2-4154-a93c-bd7020416b20 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LHod8y:/tmp/ceph-asok.LHod8y -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T11:58:16,148792379-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T11:58:16,152294759-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ef89850b-c4c2-4154-a93c-bd7020416b20', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LHod8y:/tmp/ceph-asok.LHod8y', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ef89850b-c4c2-4154-a93c-bd7020416b20 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LHod8y:/tmp/ceph-asok.LHod8y -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T11:58:22,306139458-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T11:58:22,310483087-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ef89850b-c4c2-4154-a93c-bd7020416b20', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LHod8y:/tmp/ceph-asok.LHod8y', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ef89850b-c4c2-4154-a93c-bd7020416b20 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LHod8y:/tmp/ceph-asok.LHod8y -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T11:58:26,307173442-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T11:58:26,310860338-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ef89850b-c4c2-4154-a93c-bd7020416b20', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LHod8y:/tmp/ceph-asok.LHod8y', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ef89850b-c4c2-4154-a93c-bd7020416b20 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LHod8y:/tmp/ceph-asok.LHod8y -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T11:58:29,579814295-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T11:58:29,583599355-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ef89850b-c4c2-4154-a93c-bd7020416b20', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LHod8y:/tmp/ceph-asok.LHod8y', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ef89850b-c4c2-4154-a93c-bd7020416b20 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LHod8y:/tmp/ceph-asok.LHod8y -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T11:58:34,061837271-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T11:58:34,065403210-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ef89850b-c4c2-4154-a93c-bd7020416b20', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LHod8y:/tmp/ceph-asok.LHod8y', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ef89850b-c4c2-4154-a93c-bd7020416b20 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LHod8y:/tmp/ceph-asok.LHod8y -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T11:58:37,416921871-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T11:58:37,420472040-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ef89850b-c4c2-4154-a93c-bd7020416b20', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LHod8y:/tmp/ceph-asok.LHod8y', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ef89850b-c4c2-4154-a93c-bd7020416b20 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LHod8y:/tmp/ceph-asok.LHod8y -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T11:58:41,721382770-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T11:58:41,724747050-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ef89850b-c4c2-4154-a93c-bd7020416b20', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LHod8y:/tmp/ceph-asok.LHod8y', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ef89850b-c4c2-4154-a93c-bd7020416b20 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LHod8y:/tmp/ceph-asok.LHod8y -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T11:58:45,986013251-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T11:58:45,989797450-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ef89850b-c4c2-4154-a93c-bd7020416b20', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LHod8y:/tmp/ceph-asok.LHod8y', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ef89850b-c4c2-4154-a93c-bd7020416b20 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LHod8y:/tmp/ceph-asok.LHod8y -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T11:58:49,168539082-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T11:58:49,171845914-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ef89850b-c4c2-4154-a93c-bd7020416b20', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LHod8y:/tmp/ceph-asok.LHod8y', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ef89850b-c4c2-4154-a93c-bd7020416b20 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LHod8y:/tmp/ceph-asok.LHod8y -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T11:58:52,283037819-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T11:59:15,419982401-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (4s)
      [=============...............] (remaining: 5s)
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T11:59:23,539361635-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T11:59:31,730589477-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T11:59:39,831599485-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T11:59:48,070537468-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T11:59:56,240011021-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:00:04,547564123-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   214 KiB used, 300 GiB / 300 GiB avail
    pgs:     1.042% pgs not active
             190 active+clean
             2   activating
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:00:04,554542392-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:00:12,784703853-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:00:12,791162756-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:00:21,152233428-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:00:21,159003906-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:00:29,363480569-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:00:29,370303616-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:00:37,611869004-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:00:37,618993697-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:00:37,623442043-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:00:37,626430567-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:00:37,632686839-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=329472
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_4KB_write.dat.1 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:00:37,637738408-04:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_4KB_write.log.1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:00:37,656292191-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:00:37,659188341-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ef89850b-c4c2-4154-a93c-bd7020416b20', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LHod8y:/tmp/ceph-asok.LHod8y', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ef89850b-c4c2-4154-a93c-bd7020416b20 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LHod8y:/tmp/ceph-asok.LHod8y -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T16:00:39.885+0000 7f2ab92eed00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:00:39.889+0000 7f2ab92eed00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:00:39.889+0000 7f2ab92eed00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T16:00:39.901949+0000 Maintaining 256 concurrent writes of 4096 bytes to objects of size 4096 for up to 20 seconds or 0 objects
2021-05-17T16:00:39.901960+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_8
2021-05-17T16:00:39.902927+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T16:00:39.902927+0000     0       0         0         0         0         0           -           0
2021-05-17T16:00:40.903012+0000     1     255     18721     18466   72.1264   72.1328  0.00138507   0.0134548
2021-05-17T16:00:41.903089+0000     2     255     37290     37035    72.328   72.5352  0.00169422   0.0135634
2021-05-17T16:00:42.903191+0000     3     256     55653     55397   72.1251   71.7266  0.00224633   0.0137209
2021-05-17T16:00:43.903258+0000     4     255     74260     74005   72.2645   72.6875 0.000844256   0.0137002
2021-05-17T16:00:44.903334+0000     5     256     92440     92184   72.0128   71.0117  0.00172138   0.0137751
2021-05-17T16:00:45.903408+0000     6     255    110532    110277   71.7891   70.6758  0.00119008    0.013844
2021-05-17T16:00:46.903502+0000     7     256    129033    128777   71.8562   72.2656  0.00154772   0.0138116
2021-05-17T16:00:47.903570+0000     8     256    146535    146279   71.4195   68.3672  0.00512128   0.0139076
2021-05-17T16:00:48.903638+0000     9     256    164452    164196     71.26   69.9883    0.121295   0.0139469
2021-05-17T16:00:49.903706+0000    10     256    182103    181847   71.0284   68.9492  0.00188132   0.0139865
2021-05-17T16:00:50.903800+0000    11     255    198861    198606   70.5221   65.4648  0.00276721   0.0141332
2021-05-17T16:00:51.903871+0000    12     255    217074    216819   70.5735   71.1445   0.0736959   0.0141379
2021-05-17T16:00:52.903945+0000    13     256    232819    232563   69.8752      61.5  0.00237537   0.0142766
2021-05-17T16:00:53.904012+0000    14     255    248749    248494   69.3289   62.2305  0.00314609   0.0143764
2021-05-17T16:00:54.904107+0000    15     255    265052    264797   68.9521   63.6836  0.00210617   0.0144536
2021-05-17T16:00:55.904184+0000    16     255    281287    281032   68.6059    63.418  0.00101115   0.0145323
2021-05-17T16:00:56.904257+0000    17     256    296448    296192   68.0535   59.2188  0.00142385   0.0146429
2021-05-17T16:00:57.904327+0000    18     256    312992    312736   67.8628    64.625  0.00274186   0.0146909
2021-05-17T16:00:58.904420+0000    19     255    329890    329635    67.765   66.0117   0.0162429    0.014723
2021-05-17T16:00:59.904493+0000 min lat: 0.000605627 max lat: 0.274181 avg lat: 0.0147404
2021-05-17T16:00:59.904493+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T16:00:59.904493+0000    20     227    346677    346450   67.6607   65.6836    0.112703   0.0147404
2021-05-17T16:01:00.904636+0000 Total time run:         20.0841
Total writes made:      346677
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     67.4267
Stddev Bandwidth:       4.23016
Max bandwidth (MB/sec): 72.6875
Min bandwidth (MB/sec): 59.2188
Average IOPS:           17261
Stddev IOPS:            1082.92
Max IOPS:               18608
Min IOPS:               15160
Average Latency(s):     0.0147938
Stddev Latency(s):      0.0330135
Max latency(s):         0.274181
Min latency(s):         0.000605627

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:01:01,426103868-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 329472


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:01:01,429953359-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:01:24,588644044-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 346.68k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:01:24,595457292-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:01:32,802274228-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 346.68k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:01:32,809003840-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:01:41,016453659-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 346.68k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:01:41,023422320-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:01:49,728245437-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 346.68k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:01:49,735496439-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:01:58,211017989-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 346.68k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:01:58,217987831-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:01:58,223405518-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:01:58,226779787-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:01:58,233451789-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=330830
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_4KB_seq.dat.1 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:01:58,238872683-04:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_4KB_seq.log.1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:01:58,259716497-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:01:58,263450651-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ef89850b-c4c2-4154-a93c-bd7020416b20', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LHod8y:/tmp/ceph-asok.LHod8y', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ef89850b-c4c2-4154-a93c-bd7020416b20 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LHod8y:/tmp/ceph-asok.LHod8y -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T16:02:00.653+0000 7f91f0afcd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:02:00.773+0000 7f91f0afcd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:02:00.773+0000 7f91f0afcd00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T16:02:00.786089+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T16:02:00.786089+0000     0       0         0         0         0         0           -           0
2021-05-17T16:02:01.786179+0000     1     255     53865     53610   209.382   209.414   0.0017832  0.00471838
2021-05-17T16:02:02.786293+0000     2     256     97953     97697   190.789   172.215 0.000266271  0.00505744
2021-05-17T16:02:03.786381+0000     3     255    145223    144968   188.738   184.652  0.00517295  0.00528901
2021-05-17T16:02:04.786449+0000     4     255    188098    187843   183.421    167.48  0.00507223  0.00544414
2021-05-17T16:02:05.786515+0000     5     255    236376    236121   184.451   188.586  0.00504212  0.00541466
2021-05-17T16:02:06.786615+0000     6     256    273740    273484   178.032   145.949 0.000273313  0.00554136
2021-05-17T16:02:07.786682+0000     7     255    316491    316236   176.454       167  0.00517163  0.00566135
2021-05-17T16:02:08.786819+0000 Total time run:       7.64308
Total reads made:     346677
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   177.181
Average IOPS:         45358
Stddev IOPS:          5140.41
Max IOPS:             53610
Min IOPS:             37363
Average Latency(s):   0.00564009
Max latency(s):       0.155228
Min latency(s):       0.000211206

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:02:09,405625549-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 330830


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:02:09,409704521-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:02:32,697797492-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 346.68k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:02:32,705533885-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:02:40,998991712-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 346.68k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:02:41,006532677-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:02:49,552880311-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 346.68k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:02:49,560110933-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:02:57,793848978-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 346.68k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:02:57,801991374-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:03:06,083359805-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 346.68k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:03:06,090982324-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:03:06,097158686-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T12:03:06,099276624-04:00][RUNNING][ROUND 2/1/40] object_size=4KB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:03:06,102936058-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:03:06,111876642-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40256\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.359117\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid ca9f3ca9-1f84-4e4d-bb79-6ab7e0eda166\nsetting min_mon_release = octopus\nepoch 0\nfsid ca9f3ca9-1f84-4e4d-bb79-6ab7e0eda166\nlast_changed 2021-05-17T09:03:23.358515-0700\ncreated 2021-05-17T09:03:23.358515-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40256/0,v1:10.10.1.2:40257/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.359117 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 0d8de9a3-921b-44e5-80b8-1724b47b9426\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 b5ae253c-8410-47d5-bba1-1fe426d4a595\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 046c87d2-d12a-4d3e-a589-f04b9d6c4111\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42256\n  w/ user/pass: admin / 1df09703-9262-4c43-9b56-6dd60ee2b97d\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 12:03:36 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40256
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.359117
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid ca9f3ca9-1f84-4e4d-bb79-6ab7e0eda166
setting min_mon_release = octopus
epoch 0
fsid ca9f3ca9-1f84-4e4d-bb79-6ab7e0eda166
last_changed 2021-05-17T09:03:23.358515-0700
created 2021-05-17T09:03:23.358515-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40256/0,v1:10.10.1.2:40257/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.359117 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 0d8de9a3-921b-44e5-80b8-1724b47b9426
0
start osd.0
add osd1 b5ae253c-8410-47d5-bba1-1fe426d4a595
1
start osd.1
add osd2 046c87d2-d12a-4d3e-a589-f04b9d6c4111
2
start osd.2


restful urls: https://10.10.1.2:42256
  w/ user/pass: admin / 1df09703-9262-4c43-9b56-6dd60ee2b97d


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T09:03:07.321-0700 7fa0c45941c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:03:07.321-0700 7fa0c45941c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:03:07.337-0700 7fc751b741c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:03:07.337-0700 7fc751b741c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd ceph-mon still alive after 1 seconds
WARNING:  ceph-osd ceph-mon still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40256,v1:10.10.1.2:40257] --print /tmp/ceph_monmap.359117 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.359117 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.359117 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42256 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.thlK896t8I 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 0d8de9a3-921b-44e5-80b8-1724b47b9426 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDUk6JgMLaxGxAAZl8ZTydgzaLrN58/Th9Ayg== --osd-uuid 0d8de9a3-921b-44e5-80b8-1724b47b9426 
2021-05-17T09:03:32.821-0700 7ff2428c5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T09:03:32.821-0700 7ff2428c5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T09:03:32.821-0700 7ff2428c5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T09:03:32.873-0700 7ff2428c5f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new b5ae253c-8410-47d5-bba1-1fe426d4a595 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T09:03:33.217-0700 7f49d367ef00 -1 Falling back to public interface
2021-05-17T09:03:33.233-0700 7f49d367ef00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDVk6Jgm34nDRAAeRa0j9CGfqPsx7vWo8UX6A== --osd-uuid b5ae253c-8410-47d5-bba1-1fe426d4a595 
2021-05-17T09:03:33.549-0700 7f51efeb2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T09:03:33.549-0700 7f51efeb2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T09:03:33.549-0700 7f51efeb2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T09:03:33.593-0700 7f51efeb2f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 046c87d2-d12a-4d3e-a589-f04b9d6c4111 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T09:03:33.989-0700 7f1041c30f00 -1 Falling back to public interface
2021-05-17T09:03:34.001-0700 7f1041c30f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDVk6JgA3LyOhAADg21XYVtiJdxXnVMKS/kWw== --osd-uuid 046c87d2-d12a-4d3e-a589-f04b9d6c4111 
2021-05-17T09:03:34.341-0700 7f456149df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T09:03:34.341-0700 7f456149df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T09:03:34.341-0700 7f456149df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T09:03:34.421-0700 7f456149df00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T09:03:34.705-0700 7ff7f0844f00 -1 Falling back to public interface
2021-05-17T09:03:34.721-0700 7ff7f0844f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:03:36,802649751-04:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:03:36,806396368-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:03:36,848264947-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:03:36,851638725-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '05c7a529-b971-4920-aeb7-a9c768e064df', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rqE4Uo:/tmp/ceph-asok.rqE4Uo', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 05c7a529-b971-4920-aeb7-a9c768e064df --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rqE4Uo:/tmp/ceph-asok.rqE4Uo -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:03:40,073633772-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:03:40,077431085-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '05c7a529-b971-4920-aeb7-a9c768e064df', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rqE4Uo:/tmp/ceph-asok.rqE4Uo', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 05c7a529-b971-4920-aeb7-a9c768e064df --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rqE4Uo:/tmp/ceph-asok.rqE4Uo -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:03:43,335837080-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:03:43,339967088-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '05c7a529-b971-4920-aeb7-a9c768e064df', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rqE4Uo:/tmp/ceph-asok.rqE4Uo', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 05c7a529-b971-4920-aeb7-a9c768e064df --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rqE4Uo:/tmp/ceph-asok.rqE4Uo -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:03:46,691700626-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:03:46,695675032-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '05c7a529-b971-4920-aeb7-a9c768e064df', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rqE4Uo:/tmp/ceph-asok.rqE4Uo', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 05c7a529-b971-4920-aeb7-a9c768e064df --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rqE4Uo:/tmp/ceph-asok.rqE4Uo -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:03:53,315513687-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:03:53,319596917-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '05c7a529-b971-4920-aeb7-a9c768e064df', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rqE4Uo:/tmp/ceph-asok.rqE4Uo', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 05c7a529-b971-4920-aeb7-a9c768e064df --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rqE4Uo:/tmp/ceph-asok.rqE4Uo -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:03:57,610605512-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:03:57,614099275-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '05c7a529-b971-4920-aeb7-a9c768e064df', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rqE4Uo:/tmp/ceph-asok.rqE4Uo', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 05c7a529-b971-4920-aeb7-a9c768e064df --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rqE4Uo:/tmp/ceph-asok.rqE4Uo -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:04:01,135464904-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:04:01,139395117-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '05c7a529-b971-4920-aeb7-a9c768e064df', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rqE4Uo:/tmp/ceph-asok.rqE4Uo', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 05c7a529-b971-4920-aeb7-a9c768e064df --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rqE4Uo:/tmp/ceph-asok.rqE4Uo -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:04:05,415590929-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:04:05,418973012-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '05c7a529-b971-4920-aeb7-a9c768e064df', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rqE4Uo:/tmp/ceph-asok.rqE4Uo', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 05c7a529-b971-4920-aeb7-a9c768e064df --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rqE4Uo:/tmp/ceph-asok.rqE4Uo -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:04:09,174096559-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:04:09,178377460-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '05c7a529-b971-4920-aeb7-a9c768e064df', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rqE4Uo:/tmp/ceph-asok.rqE4Uo', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 05c7a529-b971-4920-aeb7-a9c768e064df --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rqE4Uo:/tmp/ceph-asok.rqE4Uo -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:04:13,395856096-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:04:13,399868924-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '05c7a529-b971-4920-aeb7-a9c768e064df', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rqE4Uo:/tmp/ceph-asok.rqE4Uo', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 05c7a529-b971-4920-aeb7-a9c768e064df --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rqE4Uo:/tmp/ceph-asok.rqE4Uo -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:04:17,820488615-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:04:17,824068950-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '05c7a529-b971-4920-aeb7-a9c768e064df', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rqE4Uo:/tmp/ceph-asok.rqE4Uo', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 05c7a529-b971-4920-aeb7-a9c768e064df --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rqE4Uo:/tmp/ceph-asok.rqE4Uo -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:04:21,075008318-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:04:21,078186277-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '05c7a529-b971-4920-aeb7-a9c768e064df', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rqE4Uo:/tmp/ceph-asok.rqE4Uo', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 05c7a529-b971-4920-aeb7-a9c768e064df --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rqE4Uo:/tmp/ceph-asok.rqE4Uo -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  155 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:04:24,265033350-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:04:47,478792782-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (9s)
      [=========================...] 
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:04:55,782286536-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:05:04,161868203-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:05:12,517380803-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:05:20,971811301-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:05:29,245717585-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:05:37,438768619-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:05:37,446322419-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:05:45,635957881-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:05:45,643877017-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:05:53,990658154-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:05:53,998574514-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:06:02,365020787-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:06:02,372509413-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:06:10,850437997-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:06:10,858394873-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:06:10,863944146-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:06:10,867320178-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:06:10,874180745-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=337425
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_4KB_write.dat.2 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:06:10,879828002-04:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_4KB_write.log.2
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:06:10,898856945-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:06:10,902595347-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '05c7a529-b971-4920-aeb7-a9c768e064df', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rqE4Uo:/tmp/ceph-asok.rqE4Uo', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 05c7a529-b971-4920-aeb7-a9c768e064df --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rqE4Uo:/tmp/ceph-asok.rqE4Uo -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T16:06:13.394+0000 7f305c4d5d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:06:13.398+0000 7f305c4d5d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:06:13.398+0000 7f305c4d5d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T16:06:13.409754+0000 Maintaining 256 concurrent writes of 4096 bytes to objects of size 4096 for up to 20 seconds or 0 objects
2021-05-17T16:06:13.409763+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-17T16:06:13.410681+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T16:06:13.410681+0000     0       0         0         0         0         0           -           0
2021-05-17T16:06:14.410788+0000     1     255     19294     19039   74.3629   74.3711   0.0921445   0.0129639
2021-05-17T16:06:15.410889+0000     2     256     37287     37031   72.3185   70.2812  0.00200561   0.0134333
2021-05-17T16:06:16.410960+0000     3     256     54927     54671   71.1795   68.9062    0.104444    0.013832
2021-05-17T16:06:17.411066+0000     4     256     73081     72825   71.1112   70.9141  0.00235763   0.0139179
2021-05-17T16:06:18.411135+0000     5     256     90289     90033   70.3318   67.2188  0.00194082   0.0140904
2021-05-17T16:06:19.411229+0000     6     255    107566    107311   69.8575   67.4922      0.0338   0.0142431
2021-05-17T16:06:20.411328+0000     7     256    124214    123958   69.1666   65.0273 0.000980312   0.0143415
2021-05-17T16:06:21.411437+0000     8     256    140079    139823   68.2665   61.9727  0.00449112   0.0146026
2021-05-17T16:06:22.411512+0000     9     255    156354    156099    67.745   63.5781  0.00120528   0.0147111
2021-05-17T16:06:23.411617+0000    10     255    171335    171080   66.8218   58.5195   0.0284366    0.014903
2021-05-17T16:06:24.411712+0000    11     256    188707    188451   66.9152   67.8555 0.000842332   0.0148968
2021-05-17T16:06:25.411794+0000    12     256    205380    205124   66.7659   65.1289   0.0225706   0.0149315
2021-05-17T16:06:26.411837+0000    13     256    223149    222893   66.9691   69.4102 0.000669879   0.0148695
2021-05-17T16:06:27.411878+0000    14     256    238660    238404   66.5133   60.5898   0.0411987   0.0149528
2021-05-17T16:06:28.411923+0000    15     255    254275    254020   66.1455        61  0.00140575   0.0150975
2021-05-17T16:06:29.411962+0000    16     256    270204    269948      65.9   62.2188  0.00305312   0.0151409
2021-05-17T16:06:30.412034+0000    17     255    285306    285051   65.4936   58.9961   0.0314162   0.0152505
2021-05-17T16:06:31.412079+0000    18     256    301972    301716   65.4715   65.0977  0.00133802   0.0152479
2021-05-17T16:06:32.412136+0000    19     255    319093    318838   65.5455   66.8828  0.00162794   0.0152292
2021-05-17T16:06:33.412212+0000 min lat: 0.000578697 max lat: 0.241264 avg lat: 0.015286
2021-05-17T16:06:33.412212+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T16:06:33.412212+0000    20     226    334826    334600   65.3465   61.5703  0.00175406    0.015286
2021-05-17T16:06:34.412336+0000 Total time run:         20.0425
Total writes made:      334826
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     65.257
Stddev Bandwidth:       4.29172
Max bandwidth (MB/sec): 74.3711
Min bandwidth (MB/sec): 58.5195
Average IOPS:           16705
Stddev IOPS:            1098.68
Max IOPS:               19039
Min IOPS:               14981
Average Latency(s):     0.0152994
Stddev Latency(s):      0.0302045
Max latency(s):         0.241264
Min latency(s):         0.000578697

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:06:34,912461686-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 337425


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:06:34,916661184-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:06:57,965312937-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 334.83k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:06:57,973426918-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:07:06,373215059-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 334.83k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:07:06,381350229-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:07:14,708798787-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 334.83k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:07:14,716416375-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:07:22,980748484-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 334.83k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:07:22,988111925-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:07:31,335785745-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 334.83k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:07:31,344002409-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:07:31,350303965-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:07:31,354010808-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:07:31,361987000-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=338784
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_4KB_seq.dat.2 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:07:31,367757229-04:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_4KB_seq.log.2
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:07:31,386183540-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:07:31,389811174-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '05c7a529-b971-4920-aeb7-a9c768e064df', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rqE4Uo:/tmp/ceph-asok.rqE4Uo', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 05c7a529-b971-4920-aeb7-a9c768e064df --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rqE4Uo:/tmp/ceph-asok.rqE4Uo -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T16:07:33.870+0000 7fc9d1d5bd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:07:34.122+0000 7fc9d1d5bd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:07:34.122+0000 7fc9d1d5bd00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T16:07:34.134507+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T16:07:34.134507+0000     0       0         0         0         0         0           -           0
2021-05-17T16:07:35.134613+0000     1     255     49623     49368   192.808   192.844  0.00305612  0.00514795
2021-05-17T16:07:36.134691+0000     2     255     87894     87639   171.148   149.496  0.00521549  0.00582938
2021-05-17T16:07:37.134777+0000     3     256    137361    137105   178.501   193.227 0.000263445  0.00557877
2021-05-17T16:07:38.134846+0000     4     255    182786    182531   178.234   177.445  0.00482138  0.00560315
2021-05-17T16:07:39.134934+0000     5     255    234128    233873   182.695   200.555  0.00491073  0.00546722
2021-05-17T16:07:40.135001+0000     6     255    281621    281366   183.164    185.52  0.00523537  0.00545354
2021-05-17T16:07:41.135073+0000     7     255    330479    330224    184.26   190.852  0.00476017  0.00542193
2021-05-17T16:07:42.135404+0000 Total time run:       7.08612
Total reads made:     334826
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   184.574
Average IOPS:         47250
Stddev IOPS:          4333.89
Max IOPS:             51342
Min IOPS:             38271
Average Latency(s):   0.00541381
Max latency(s):       0.134068
Min latency(s):       0.000223479

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:07:42,688051062-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 338784


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:07:42,692802336-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:08:05,881746431-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 334.83k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:08:05,889747470-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:08:14,115359576-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 334.83k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:08:14,123290794-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:08:22,488849807-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 334.83k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:08:22,496696095-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:08:31,096081878-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 334.83k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:08:31,104032092-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:08:39,306507376-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 334.83k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:08:39,314161574-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:08:39,319839349-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T12:08:39,322098432-04:00][RUNNING][ROUND 3/1/40] object_size=4KB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:08:39,325583348-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:08:39,333859273-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40138\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.360229\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 51ed4d06-9253-4c0b-b316-451b6838cf5f\nsetting min_mon_release = octopus\nepoch 0\nfsid 51ed4d06-9253-4c0b-b316-451b6838cf5f\nlast_changed 2021-05-17T09:09:04.285815-0700\ncreated 2021-05-17T09:09:04.285815-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40138/0,v1:10.10.1.2:40139/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.360229 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 d5aa7507-3021-4f6e-9532-bba8f6949436\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 ee190782-42f5-409c-8190-74eb22e2979c\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 32b731da-c6eb-4dde-9111-c4624ac396d6\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42138\n  w/ user/pass: admin / a7ca4e4b-604b-4c36-adfb-81ae04f723b2\n'
10.10.1.2: b'\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\nCEPH_DEV=1\n'
[1] 12:09:18 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40138
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.360229
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 51ed4d06-9253-4c0b-b316-451b6838cf5f
setting min_mon_release = octopus
epoch 0
fsid 51ed4d06-9253-4c0b-b316-451b6838cf5f
last_changed 2021-05-17T09:09:04.285815-0700
created 2021-05-17T09:09:04.285815-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40138/0,v1:10.10.1.2:40139/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.360229 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 d5aa7507-3021-4f6e-9532-bba8f6949436
0
start osd.0
add osd1 ee190782-42f5-409c-8190-74eb22e2979c
1
start osd.1
add osd2 32b731da-c6eb-4dde-9111-c4624ac396d6
2
start osd.2


restful urls: https://10.10.1.2:42138
  w/ user/pass: admin / a7ca4e4b-604b-4c36-adfb-81ae04f723b2


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T09:08:40.684-0700 7f5a03ccd1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:08:40.684-0700 7f5a03ccd1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:08:40.700-0700 7f9d835a01c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:08:40.700-0700 7f9d835a01c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40138,v1:10.10.1.2:40139] --print /tmp/ceph_monmap.360229 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.360229 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.360229 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42138 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.cjStCPCKj6 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new d5aa7507-3021-4f6e-9532-bba8f6949436 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAqlaJgLwe3BRAAg3s7IndzLWr3RZ3B+n0jlw== --osd-uuid d5aa7507-3021-4f6e-9532-bba8f6949436 
2021-05-17T09:09:14.452-0700 7f28e387ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T09:09:14.452-0700 7f28e387ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T09:09:14.452-0700 7f28e387ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T09:09:14.504-0700 7f28e387ef00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new ee190782-42f5-409c-8190-74eb22e2979c -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T09:09:14.784-0700 7ffbdc66cf00 -1 Falling back to public interface
2021-05-17T09:09:14.796-0700 7ffbdc66cf00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAqlaJgKX7JLhAAyUXFDqG8femGBIHaqkjc1A== --osd-uuid ee190782-42f5-409c-8190-74eb22e2979c 
2021-05-17T09:09:15.172-0700 7fa2ec1c2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T09:09:15.172-0700 7fa2ec1c2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T09:09:15.172-0700 7fa2ec1c2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T09:09:15.216-0700 7fa2ec1c2f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 32b731da-c6eb-4dde-9111-c4624ac396d6 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T09:09:15.664-0700 7fed54236f00 -1 Falling back to public interface
2021-05-17T09:09:15.680-0700 7fed54236f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQArlaJg7PStJxAAHwPQbDBL+L71bHyrBsbPhg== --osd-uuid 32b731da-c6eb-4dde-9111-c4624ac396d6 
2021-05-17T09:09:16.052-0700 7f0854d21f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T09:09:16.052-0700 7f0854d21f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T09:09:16.052-0700 7f0854d21f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T09:09:16.116-0700 7f0854d21f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T09:09:16.400-0700 7f2e544e8f00 -1 Falling back to public interface
2021-05-17T09:09:16.416-0700 7f2e544e8f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:09:18,573708337-04:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:09:18,577538491-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:09:18,617464407-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:09:18,620613041-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '33220127-ab1f-4ef2-a5e2-47e9278a3c25', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.osVztx:/tmp/ceph-asok.osVztx', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 33220127-ab1f-4ef2-a5e2-47e9278a3c25 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.osVztx:/tmp/ceph-asok.osVztx -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:09:21,839607477-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:09:21,843555292-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '33220127-ab1f-4ef2-a5e2-47e9278a3c25', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.osVztx:/tmp/ceph-asok.osVztx', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 33220127-ab1f-4ef2-a5e2-47e9278a3c25 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.osVztx:/tmp/ceph-asok.osVztx -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:09:25,238164369-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:09:25,242022757-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '33220127-ab1f-4ef2-a5e2-47e9278a3c25', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.osVztx:/tmp/ceph-asok.osVztx', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 33220127-ab1f-4ef2-a5e2-47e9278a3c25 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.osVztx:/tmp/ceph-asok.osVztx -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:09:28,411157299-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:09:28,414847150-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '33220127-ab1f-4ef2-a5e2-47e9278a3c25', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.osVztx:/tmp/ceph-asok.osVztx', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 33220127-ab1f-4ef2-a5e2-47e9278a3c25 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.osVztx:/tmp/ceph-asok.osVztx -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:09:35,106265423-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:09:35,109597442-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '33220127-ab1f-4ef2-a5e2-47e9278a3c25', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.osVztx:/tmp/ceph-asok.osVztx', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 33220127-ab1f-4ef2-a5e2-47e9278a3c25 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.osVztx:/tmp/ceph-asok.osVztx -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:09:39,210443211-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:09:39,213922917-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '33220127-ab1f-4ef2-a5e2-47e9278a3c25', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.osVztx:/tmp/ceph-asok.osVztx', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 33220127-ab1f-4ef2-a5e2-47e9278a3c25 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.osVztx:/tmp/ceph-asok.osVztx -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:09:43,694106286-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:09:43,697917705-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '33220127-ab1f-4ef2-a5e2-47e9278a3c25', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.osVztx:/tmp/ceph-asok.osVztx', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 33220127-ab1f-4ef2-a5e2-47e9278a3c25 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.osVztx:/tmp/ceph-asok.osVztx -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:09:47,080480370-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:09:47,084249731-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '33220127-ab1f-4ef2-a5e2-47e9278a3c25', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.osVztx:/tmp/ceph-asok.osVztx', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 33220127-ab1f-4ef2-a5e2-47e9278a3c25 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.osVztx:/tmp/ceph-asok.osVztx -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:09:51,409302158-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:09:51,412571480-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '33220127-ab1f-4ef2-a5e2-47e9278a3c25', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.osVztx:/tmp/ceph-asok.osVztx', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 33220127-ab1f-4ef2-a5e2-47e9278a3c25 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.osVztx:/tmp/ceph-asok.osVztx -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:09:54,857476269-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:09:54,861235160-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '33220127-ab1f-4ef2-a5e2-47e9278a3c25', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.osVztx:/tmp/ceph-asok.osVztx', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 33220127-ab1f-4ef2-a5e2-47e9278a3c25 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.osVztx:/tmp/ceph-asok.osVztx -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:09:58,254219224-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:09:58,258022458-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '33220127-ab1f-4ef2-a5e2-47e9278a3c25', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.osVztx:/tmp/ceph-asok.osVztx', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 33220127-ab1f-4ef2-a5e2-47e9278a3c25 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.osVztx:/tmp/ceph-asok.osVztx -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:10:01,510619554-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:10:01,514092006-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '33220127-ab1f-4ef2-a5e2-47e9278a3c25', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.osVztx:/tmp/ceph-asok.osVztx', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 33220127-ab1f-4ef2-a5e2-47e9278a3c25 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.osVztx:/tmp/ceph-asok.osVztx -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:10:04,749920394-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:10:27,933806590-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   209 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (5s)
      [=========...................] (remaining: 10s)
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:10:36,066091715-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:10:44,281029589-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:10:52,519295177-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:11:00,867182933-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:11:09,309079620-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:11:17,588092405-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   227 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:11:17,596098423-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:11:26,028506740-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:11:26,036385098-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:11:34,483338452-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:11:34,491044146-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:11:43,022675534-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:11:43,030245774-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:11:51,496515073-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:11:51,504102074-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:11:51,510310957-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:11:51,513722014-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:11:51,521441584-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=345368
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_4KB_write.dat.3 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:11:51,527202135-04:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_4KB_write.log.3
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:11:51,546574263-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:11:51,550105767-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '33220127-ab1f-4ef2-a5e2-47e9278a3c25', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.osVztx:/tmp/ceph-asok.osVztx', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 33220127-ab1f-4ef2-a5e2-47e9278a3c25 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.osVztx:/tmp/ceph-asok.osVztx -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T16:11:54.075+0000 7fe559ce2d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:11:54.079+0000 7fe559ce2d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:11:54.079+0000 7fe559ce2d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T16:11:54.089256+0000 Maintaining 256 concurrent writes of 4096 bytes to objects of size 4096 for up to 20 seconds or 0 objects
2021-05-17T16:11:54.089264+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-17T16:11:54.090211+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T16:11:54.090211+0000     0       0         0         0         0         0           -           0
2021-05-17T16:11:55.090292+0000     1     256     19141     18885   73.7655   73.7695  0.00181697   0.0128605
2021-05-17T16:11:56.090365+0000     2     256     36150     35894    70.101   66.4414    0.128909   0.0138222
2021-05-17T16:11:57.090438+0000     3     256     53574     53318   69.4198   68.0625  0.00348315   0.0141547
2021-05-17T16:11:58.090505+0000     4     255     71111     70856   69.1907   68.5078  0.00223917   0.0142587
2021-05-17T16:11:59.090600+0000     5     255     88125     87870   68.6435   66.4609     0.13049   0.0143997
2021-05-17T16:12:00.090672+0000     6     256    105057    104801   68.2249   66.1367 0.000823336   0.0145047
2021-05-17T16:12:01.090784+0000     7     255    121328    121073   67.5578   63.5625   0.0088472   0.0147452
2021-05-17T16:12:02.090851+0000     8     256    137591    137335    67.053   63.5234  0.00148753   0.0148384
2021-05-17T16:12:03.090937+0000     9     255    155102    154847   67.2027   68.4062  0.00154787   0.0147814
2021-05-17T16:12:04.091004+0000    10     255    169851    169596   66.2434   57.6133    0.010214   0.0150763
2021-05-17T16:12:05.091071+0000    11     256    186192    185936   66.0234   63.8281 0.000916832   0.0150684
2021-05-17T16:12:06.091143+0000    12     256    203980    203724   66.3114   69.4844  0.00164109   0.0150074
2021-05-17T16:12:07.091236+0000    13     255    221387    221132   66.4408        68  0.00106157   0.0149938
2021-05-17T16:12:08.091315+0000    14     255    236886    236631   66.0192    60.543  0.00207777   0.0151228
2021-05-17T16:12:09.091388+0000    15     255    253528    253273   65.9514   65.0078  0.00319574   0.0151205
2021-05-17T16:12:10.091490+0000    16     255    271073    270818   66.1125   68.5352   0.0922998   0.0150891
2021-05-17T16:12:11.091579+0000    17     256    286778    286522   65.8317   61.3438   0.0892497   0.0151589
2021-05-17T16:12:12.091646+0000    18     256    301891    301635   65.4539   59.0352   0.0734519   0.0152553
2021-05-17T16:12:13.091722+0000    19     256    316464    316208   65.0048   56.9258  0.00084652   0.0152859
2021-05-17T16:12:14.091794+0000 min lat: 0.000594326 max lat: 0.289321 avg lat: 0.0153392
2021-05-17T16:12:14.091794+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T16:12:14.091794+0000    20     227    333268    333041    65.042   65.7539  0.00921126   0.0153392
2021-05-17T16:12:15.091948+0000 Total time run:         20.076
Total writes made:      333268
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     64.8449
Stddev Bandwidth:       4.30431
Max bandwidth (MB/sec): 73.7695
Min bandwidth (MB/sec): 56.9258
Average IOPS:           16600
Stddev IOPS:            1101.9
Max IOPS:               18885
Min IOPS:               14573
Average Latency(s):     0.015383
Stddev Latency(s):      0.0350938
Max latency(s):         0.289321
Min latency(s):         0.000594326

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:12:15,567438812-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 345368


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:12:15,571576193-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:12:39,312322128-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 333.27k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:12:39,319676172-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:12:47,609812545-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 333.27k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:12:47,617545159-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:12:55,940545779-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 333.27k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:12:55,947988690-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:13:04,530598955-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 333.27k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:13:04,538557374-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:13:12,785152947-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 333.27k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:13:12,793340556-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:13:12,798752131-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:13:12,802153871-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:13:12,810023994-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=346749
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_4KB_seq.dat.3 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:13:12,815817927-04:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_4KB_seq.log.3
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:13:12,834694906-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:13:12,838097608-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '33220127-ab1f-4ef2-a5e2-47e9278a3c25', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.osVztx:/tmp/ceph-asok.osVztx', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 33220127-ab1f-4ef2-a5e2-47e9278a3c25 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.osVztx:/tmp/ceph-asok.osVztx -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T16:13:15.259+0000 7fe0a1578d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:13:15.499+0000 7fe0a1578d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:13:15.499+0000 7fe0a1578d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T16:13:15.511333+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T16:13:15.511333+0000     0       0         0         0         0         0           -           0
2021-05-17T16:13:16.511413+0000     1     255     50715     50460    197.08   197.109  0.00235254  0.00503713
2021-05-17T16:13:17.511489+0000     2     255     94509     94254   184.069    171.07  0.00575729  0.00541977
2021-05-17T16:13:18.511584+0000     3     255    133481    133226   173.453   152.234  0.00646641  0.00575273
2021-05-17T16:13:19.511653+0000     4     255    169152    168897   164.922    139.34  0.00638413  0.00605346
2021-05-17T16:13:20.511710+0000     5     255    208441    208186   162.631   153.473  0.00597128  0.00614002
2021-05-17T16:13:21.511789+0000     6     255    246713    246458    160.44     149.5  0.00709102   0.0062238
2021-05-17T16:13:22.511878+0000     7     256    277262    277006   154.566   119.328 0.000254027  0.00641272
2021-05-17T16:13:23.511967+0000     8     255    312812    312557   152.602   138.871  0.00708909  0.00654447
2021-05-17T16:13:24.512108+0000 Total time run:       8.56907
Total reads made:     333268
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   151.922
Average IOPS:         38891
Stddev IOPS:          5967.81
Max IOPS:             50460
Min IOPS:             30548
Average Latency(s):   0.00657585
Max latency(s):       0.114172
Min latency(s):       0.000216016

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:13:25,102934119-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 346749


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:13:25,107360354-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:13:48,293018080-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 333.27k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:13:48,300410105-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:13:57,136097867-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 333.27k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:13:57,143709245-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:14:05,459656213-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 333.27k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:14:05,467735620-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:14:13,966660038-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 333.27k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:14:13,974611875-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:14:22,488288650-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 333.27k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:14:22,496331899-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:14:22,502523699-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T12:14:22,504509991-04:00][RUNNING][ROUND 4/1/40] object_size=4KB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:14:22,507821671-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:14:22,516675703-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40511\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.361344\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid a56984eb-52e7-46fc-a2c8-a7e5f6935d15\nsetting min_mon_release = octopus\nepoch 0\nfsid a56984eb-52e7-46fc-a2c8-a7e5f6935d15\nlast_changed 2021-05-17T09:14:47.107414-0700\ncreated 2021-05-17T09:14:47.107414-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40511/0,v1:10.10.1.2:40512/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.361344 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 2d0bcd66-1e8f-42e7-8c96-d2990cb49168\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 3c46c80a-ef13-4852-9b45-cc8250214a42\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 6bb64c63-1ccc-4920-901e-47bc701576b0\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42511\n  w/ user/pass: admin / 2163bf7e-e1e9-4c8a-ad32-12b0449f52d6\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 12:15:01 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40511
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.361344
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid a56984eb-52e7-46fc-a2c8-a7e5f6935d15
setting min_mon_release = octopus
epoch 0
fsid a56984eb-52e7-46fc-a2c8-a7e5f6935d15
last_changed 2021-05-17T09:14:47.107414-0700
created 2021-05-17T09:14:47.107414-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40511/0,v1:10.10.1.2:40512/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.361344 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 2d0bcd66-1e8f-42e7-8c96-d2990cb49168
0
start osd.0
add osd1 3c46c80a-ef13-4852-9b45-cc8250214a42
1
start osd.1
add osd2 6bb64c63-1ccc-4920-901e-47bc701576b0
2
start osd.2


restful urls: https://10.10.1.2:42511
  w/ user/pass: admin / 2163bf7e-e1e9-4c8a-ad32-12b0449f52d6


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T09:14:23.863-0700 7f5b241b51c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:14:23.863-0700 7f5b241b51c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:14:23.879-0700 7fa25c9fd1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:14:23.879-0700 7fa25c9fd1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40511,v1:10.10.1.2:40512] --print /tmp/ceph_monmap.361344 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.361344 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.361344 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42511 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.S96sOj9oiu 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 2d0bcd66-1e8f-42e7-8c96-d2990cb49168 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCAlqJg4ysXMhAAL6mKBFq6dOwutSuc3I+icw== --osd-uuid 2d0bcd66-1e8f-42e7-8c96-d2990cb49168 
2021-05-17T09:14:57.196-0700 7fc7d7be5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T09:14:57.196-0700 7fc7d7be5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T09:14:57.196-0700 7fc7d7be5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T09:14:57.548-0700 7fc7d7be5f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 3c46c80a-ef13-4852-9b45-cc8250214a42 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T09:14:57.952-0700 7ffbab33df00 -1 Falling back to public interface
2021-05-17T09:14:57.964-0700 7ffbab33df00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCBlqJgeGvQOBAAYZUjx1TWv5KQcPM0qLjA/g== --osd-uuid 3c46c80a-ef13-4852-9b45-cc8250214a42 
2021-05-17T09:14:58.276-0700 7fe928ca0f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T09:14:58.276-0700 7fe928ca0f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T09:14:58.276-0700 7fe928ca0f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T09:14:58.328-0700 7fe928ca0f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 6bb64c63-1ccc-4920-901e-47bc701576b0 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T09:14:58.700-0700 7fb73a1e2f00 -1 Falling back to public interface
2021-05-17T09:14:58.712-0700 7fb73a1e2f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCClqJg6mDEKRAASuxP/pn1i6+Xk09m0w0k9Q== --osd-uuid 6bb64c63-1ccc-4920-901e-47bc701576b0 
2021-05-17T09:14:59.088-0700 7fa6ddd47f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T09:14:59.088-0700 7fa6ddd47f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T09:14:59.088-0700 7fa6ddd47f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T09:14:59.140-0700 7fa6ddd47f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T09:14:59.416-0700 7f14f9265f00 -1 Falling back to public interface
2021-05-17T09:14:59.432-0700 7f14f9265f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:15:01,518259470-04:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:15:01,522643565-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:15:01,566144267-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:15:01,569944656-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '03634540-d1f6-431f-9913-0b984df6a241', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8sYEts:/tmp/ceph-asok.8sYEts', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 03634540-d1f6-431f-9913-0b984df6a241 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8sYEts:/tmp/ceph-asok.8sYEts -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:15:04,832217038-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:15:04,835700742-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '03634540-d1f6-431f-9913-0b984df6a241', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8sYEts:/tmp/ceph-asok.8sYEts', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 03634540-d1f6-431f-9913-0b984df6a241 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8sYEts:/tmp/ceph-asok.8sYEts -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:15:08,263725094-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:15:08,267806661-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '03634540-d1f6-431f-9913-0b984df6a241', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8sYEts:/tmp/ceph-asok.8sYEts', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 03634540-d1f6-431f-9913-0b984df6a241 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8sYEts:/tmp/ceph-asok.8sYEts -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:15:11,350785330-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:15:11,354454331-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '03634540-d1f6-431f-9913-0b984df6a241', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8sYEts:/tmp/ceph-asok.8sYEts', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 03634540-d1f6-431f-9913-0b984df6a241 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8sYEts:/tmp/ceph-asok.8sYEts -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:15:17,743321004-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:15:17,747146390-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '03634540-d1f6-431f-9913-0b984df6a241', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8sYEts:/tmp/ceph-asok.8sYEts', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 03634540-d1f6-431f-9913-0b984df6a241 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8sYEts:/tmp/ceph-asok.8sYEts -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:15:22,214308527-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:15:22,218130336-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '03634540-d1f6-431f-9913-0b984df6a241', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8sYEts:/tmp/ceph-asok.8sYEts', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 03634540-d1f6-431f-9913-0b984df6a241 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8sYEts:/tmp/ceph-asok.8sYEts -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:15:25,713772316-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:15:25,717416271-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '03634540-d1f6-431f-9913-0b984df6a241', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8sYEts:/tmp/ceph-asok.8sYEts', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 03634540-d1f6-431f-9913-0b984df6a241 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8sYEts:/tmp/ceph-asok.8sYEts -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:15:30,011793681-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:15:30,015818070-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '03634540-d1f6-431f-9913-0b984df6a241', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8sYEts:/tmp/ceph-asok.8sYEts', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 03634540-d1f6-431f-9913-0b984df6a241 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8sYEts:/tmp/ceph-asok.8sYEts -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:15:33,633580305-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:15:33,637301304-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '03634540-d1f6-431f-9913-0b984df6a241', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8sYEts:/tmp/ceph-asok.8sYEts', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 03634540-d1f6-431f-9913-0b984df6a241 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8sYEts:/tmp/ceph-asok.8sYEts -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:15:37,954327120-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:15:37,957813980-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '03634540-d1f6-431f-9913-0b984df6a241', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8sYEts:/tmp/ceph-asok.8sYEts', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 03634540-d1f6-431f-9913-0b984df6a241 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8sYEts:/tmp/ceph-asok.8sYEts -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:15:41,704992332-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:15:41,708506924-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '03634540-d1f6-431f-9913-0b984df6a241', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8sYEts:/tmp/ceph-asok.8sYEts', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 03634540-d1f6-431f-9913-0b984df6a241 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8sYEts:/tmp/ceph-asok.8sYEts -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:15:44,987933489-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:15:44,991602040-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '03634540-d1f6-431f-9913-0b984df6a241', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8sYEts:/tmp/ceph-asok.8sYEts', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 03634540-d1f6-431f-9913-0b984df6a241 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8sYEts:/tmp/ceph-asok.8sYEts -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  147 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  147 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.98   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.04   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.98   70      up          osd.2  
                       TOTAL  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.98/1.04  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:15:48,093503633-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:16:11,498484177-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   217 KiB used, 300 GiB / 300 GiB avail
    pgs:     10.938% pgs not active
             171 active+clean
             21  peering
 
  progress:
    Global Recovery Event (5s)
      [============................] (remaining: 6s)
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:16:19,777376171-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:16:28,009800299-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:16:36,239044276-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:16:44,526234374-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:16:52,722067250-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:17:01,011766973-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:17:01,019814810-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:17:09,390876089-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:17:09,398709483-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:17:17,735936593-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:17:17,743729852-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:17:26,056841370-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:17:26,064393697-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:17:34,296668072-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:17:34,304628986-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:17:34,310671897-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:17:34,314069339-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:17:34,321632916-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=353355
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_4KB_write.dat.4 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:17:34,326837703-04:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_4KB_write.log.4
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:17:34,345692631-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:17:34,348672929-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '03634540-d1f6-431f-9913-0b984df6a241', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8sYEts:/tmp/ceph-asok.8sYEts', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 03634540-d1f6-431f-9913-0b984df6a241 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8sYEts:/tmp/ceph-asok.8sYEts -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T16:17:36.844+0000 7f107f07ad00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:17:36.848+0000 7f107f07ad00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:17:36.848+0000 7f107f07ad00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T16:17:36.861420+0000 Maintaining 256 concurrent writes of 4096 bytes to objects of size 4096 for up to 20 seconds or 0 objects
2021-05-17T16:17:36.861427+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-17T16:17:36.862351+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T16:17:36.862351+0000     0       0         0         0         0         0           -           0
2021-05-17T16:17:37.862440+0000     1     256     19954     19698   76.9414   76.9453  0.00148636   0.0127261
2021-05-17T16:17:38.862516+0000     2     256     39365     39109     76.38   75.8242  0.00495945   0.0129101
2021-05-17T16:17:39.862594+0000     3     256     58070     57814   75.2735   73.0664   0.0010466   0.0131759
2021-05-17T16:17:40.862670+0000     4     255     76079     75824   74.0417   70.3516  0.00184852   0.0133036
2021-05-17T16:17:41.862721+0000     5     256     93477     93221   72.8241    67.957 0.000834808   0.0135771
2021-05-17T16:17:42.862793+0000     6     256    109540    109284   71.1437   62.7461  0.00266615   0.0139218
2021-05-17T16:17:43.862828+0000     7     255    127079    126824   70.7679   68.5156  0.00144632   0.0140152
2021-05-17T16:17:44.862868+0000     8     256    143893    143637   70.1311   65.6758  0.00224112    0.014156
2021-05-17T16:17:45.862912+0000     9     256    160800    160544   69.6765    66.043  0.00180164   0.0142609
2021-05-17T16:17:46.862984+0000    10     256    178186    177930   69.4998   67.9141  0.00570319   0.0143197
2021-05-17T16:17:47.863027+0000    11     255    195546    195291   69.3465   67.8164 0.000980662   0.0143458
2021-05-17T16:17:48.863093+0000    12     256    212569    212313   69.1083   66.4922   0.0216731   0.0144264
2021-05-17T16:17:49.863180+0000    13     256    228484    228228    68.574    62.168  0.00180768   0.0145166
2021-05-17T16:17:50.863279+0000    14     256    244505    244249   68.1455    62.582 0.000983858   0.0146379
2021-05-17T16:17:51.863376+0000    15     255    259467    259212   67.4987   58.4492  0.00085197   0.0147794
2021-05-17T16:17:52.863434+0000    16     256    274153    273897    66.865   57.3633  0.00219928    0.014936
2021-05-17T16:17:53.863525+0000    17     256    289371    289115   66.4282   59.4453   0.0401497   0.0150294
2021-05-17T16:17:54.863616+0000    18     256    306054    305798   66.3579    65.168  0.00116488   0.0150299
2021-05-17T16:17:55.863678+0000    19     256    321863    321607   66.1154   61.7539  0.00165058   0.0150781
2021-05-17T16:17:56.863718+0000 min lat: 0.000594928 max lat: 0.269365 avg lat: 0.0151806
2021-05-17T16:17:56.863718+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T16:17:56.863718+0000    20     231    336982    336751   65.7673   59.1562   0.0312267   0.0151806
2021-05-17T16:17:57.863872+0000 Total time run:         20.0595
Total writes made:      336982
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     65.6216
Stddev Bandwidth:       5.51886
Max bandwidth (MB/sec): 76.9453
Min bandwidth (MB/sec): 57.3633
Average IOPS:           16799
Stddev IOPS:            1412.83
Max IOPS:               19698
Min IOPS:               14685
Average Latency(s):     0.0152075
Stddev Latency(s):      0.0334048
Max latency(s):         0.269365
Min latency(s):         0.000594928

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:17:58,408632983-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 353355


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:17:58,412884068-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:18:21,561449687-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 336.98k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:18:21,569119214-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:18:30,116673923-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 336.98k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:18:30,124341677-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:18:38,629649535-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 336.98k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:18:38,637561136-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:18:47,092288729-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 336.98k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:18:47,100506866-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:18:55,314099551-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 336.98k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:18:55,321621711-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:18:55,327819332-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:18:55,331368459-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:18:55,338809547-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=354728
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_4KB_seq.dat.4 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:18:55,344774421-04:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_4KB_seq.log.4
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:18:55,362433302-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:18:55,365910324-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '03634540-d1f6-431f-9913-0b984df6a241', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8sYEts:/tmp/ceph-asok.8sYEts', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 03634540-d1f6-431f-9913-0b984df6a241 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8sYEts:/tmp/ceph-asok.8sYEts -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T16:18:57.836+0000 7f18f9ef0d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:18:57.980+0000 7f18f9ef0d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:18:57.980+0000 7f18f9ef0d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T16:18:57.993613+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T16:18:57.993613+0000     0       0         0         0         0         0           -           0
2021-05-17T16:18:58.993723+0000     1     255     55597     55342    216.14    216.18   0.0184716  0.00459283
2021-05-17T16:18:59.993839+0000     2     255    105424    105169   205.378   194.637  0.00532925  0.00485715
2021-05-17T16:19:00.993921+0000     3     255    153848    153593   199.965   189.156  0.00512392  0.00499201
2021-05-17T16:19:01.993993+0000     4     255    202248    201993   197.236   189.062  0.00518718  0.00506259
2021-05-17T16:19:02.994095+0000     5     255    250361    250106   195.374   187.941  0.00536163  0.00511169
2021-05-17T16:19:03.994172+0000     6     255    298459    298204   194.123   187.883  0.00540633  0.00514536
2021-05-17T16:19:04.994276+0000 Total time run:       6.89379
Total reads made:     336982
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   190.945
Average IOPS:         48881
Stddev IOPS:          2837.3
Max IOPS:             55342
Min IOPS:             48098
Average Latency(s):   0.00523306
Max latency(s):       0.0937099
Min latency(s):       0.000222749

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:19:05,615428822-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 354728


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:19:05,620069198-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:19:29,141895198-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 336.98k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:19:29,149744923-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:19:37,588818183-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 336.98k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:19:37,596460077-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:19:45,658886840-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 336.98k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:19:45,666599167-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:19:54,387543952-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 336.98k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:19:54,395664576-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:20:02,868448846-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 336.98k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:20:02,876489930-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:20:02,882519346-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T12:20:02,884792967-04:00][RUNNING][ROUND 5/1/40] object_size=4KB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:20:02,888093196-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:20:02,897722574-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40004\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.362463\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid cb815546-769b-4430-a671-b77d220ca81f\nsetting min_mon_release = octopus\nepoch 0\nfsid cb815546-769b-4430-a671-b77d220ca81f\nlast_changed 2021-05-17T09:20:27.638547-0700\ncreated 2021-05-17T09:20:27.638547-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40004/0,v1:10.10.1.2:40005/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.362463 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 b7d61676-f268-4e12-9682-63cf1f105043\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 b77f9702-5b76-4971-9e2a-e3258e260314\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 50839b59-6cfe-4c0a-bb14-a94f96623be6\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42004\n  w/ user/pass: admin / 3fbef36b-67a7-40a7-814d-3273d73945f5\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\nCEPH_DEV=1\n'
[1] 12:20:40 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40004
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.362463
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid cb815546-769b-4430-a671-b77d220ca81f
setting min_mon_release = octopus
epoch 0
fsid cb815546-769b-4430-a671-b77d220ca81f
last_changed 2021-05-17T09:20:27.638547-0700
created 2021-05-17T09:20:27.638547-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40004/0,v1:10.10.1.2:40005/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.362463 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 b7d61676-f268-4e12-9682-63cf1f105043
0
start osd.0
add osd1 b77f9702-5b76-4971-9e2a-e3258e260314
1
start osd.1
add osd2 50839b59-6cfe-4c0a-bb14-a94f96623be6
2
start osd.2


restful urls: https://10.10.1.2:42004
  w/ user/pass: admin / 3fbef36b-67a7-40a7-814d-3273d73945f5


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T09:20:04.214-0700 7f682e4981c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:20:04.214-0700 7f682e4981c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:20:04.230-0700 7f7d1f27a1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:20:04.230-0700 7f7d1f27a1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40004,v1:10.10.1.2:40005] --print /tmp/ceph_monmap.362463 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.362463 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.362463 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42004 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.qmItGlYqDF 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new b7d61676-f268-4e12-9682-63cf1f105043 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDUl6JgT9gSIRAAKYZKZeuINoVOgiYfkGLSwA== --osd-uuid b7d61676-f268-4e12-9682-63cf1f105043 
2021-05-17T09:20:36.947-0700 7f530abc5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T09:20:36.947-0700 7f530abc5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T09:20:36.947-0700 7f530abc5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T09:20:36.995-0700 7f530abc5f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new b77f9702-5b76-4971-9e2a-e3258e260314 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T09:20:37.331-0700 7f3932d9ef00 -1 Falling back to public interface
2021-05-17T09:20:37.343-0700 7f3932d9ef00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDVl6Jg6bObExAAu/md/fW6yNzvXZgI1os9/Q== --osd-uuid b77f9702-5b76-4971-9e2a-e3258e260314 
2021-05-17T09:20:37.675-0700 7fe39edb2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T09:20:37.679-0700 7fe39edb2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T09:20:37.679-0700 7fe39edb2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T09:20:37.731-0700 7fe39edb2f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 50839b59-6cfe-4c0a-bb14-a94f96623be6 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T09:20:38.083-0700 7f92b82d3f00 -1 Falling back to public interface
2021-05-17T09:20:38.095-0700 7f92b82d3f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDWl6Jgu1LkBBAA08EjH8DhMMOxEe7Kf6tb3w== --osd-uuid 50839b59-6cfe-4c0a-bb14-a94f96623be6 
2021-05-17T09:20:38.439-0700 7f1f85667f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T09:20:38.439-0700 7f1f85667f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T09:20:38.439-0700 7f1f85667f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T09:20:38.527-0700 7f1f85667f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T09:20:38.811-0700 7fe08aef7f00 -1 Falling back to public interface
2021-05-17T09:20:38.827-0700 7fe08aef7f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:20:40,894197001-04:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:20:40,898093460-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:20:40,939538562-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:20:40,942884167-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1da17601-713a-4888-ad14-4460d7c50e5d', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3q9GNI:/tmp/ceph-asok.3q9GNI', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1da17601-713a-4888-ad14-4460d7c50e5d --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3q9GNI:/tmp/ceph-asok.3q9GNI -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:20:44,206051296-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:20:44,209838229-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1da17601-713a-4888-ad14-4460d7c50e5d', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3q9GNI:/tmp/ceph-asok.3q9GNI', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1da17601-713a-4888-ad14-4460d7c50e5d --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3q9GNI:/tmp/ceph-asok.3q9GNI -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:20:47,426373134-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:20:47,430146322-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1da17601-713a-4888-ad14-4460d7c50e5d', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3q9GNI:/tmp/ceph-asok.3q9GNI', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1da17601-713a-4888-ad14-4460d7c50e5d --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3q9GNI:/tmp/ceph-asok.3q9GNI -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:20:50,648260654-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:20:50,652287999-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1da17601-713a-4888-ad14-4460d7c50e5d', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3q9GNI:/tmp/ceph-asok.3q9GNI', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1da17601-713a-4888-ad14-4460d7c50e5d --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3q9GNI:/tmp/ceph-asok.3q9GNI -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:20:57,079014657-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:20:57,082615361-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1da17601-713a-4888-ad14-4460d7c50e5d', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3q9GNI:/tmp/ceph-asok.3q9GNI', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1da17601-713a-4888-ad14-4460d7c50e5d --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3q9GNI:/tmp/ceph-asok.3q9GNI -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:21:00,705881174-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:21:00,709827767-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1da17601-713a-4888-ad14-4460d7c50e5d', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3q9GNI:/tmp/ceph-asok.3q9GNI', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1da17601-713a-4888-ad14-4460d7c50e5d --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3q9GNI:/tmp/ceph-asok.3q9GNI -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:21:04,170960426-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:21:04,174998922-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1da17601-713a-4888-ad14-4460d7c50e5d', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3q9GNI:/tmp/ceph-asok.3q9GNI', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1da17601-713a-4888-ad14-4460d7c50e5d --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3q9GNI:/tmp/ceph-asok.3q9GNI -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:21:08,660281436-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:21:08,664106291-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1da17601-713a-4888-ad14-4460d7c50e5d', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3q9GNI:/tmp/ceph-asok.3q9GNI', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1da17601-713a-4888-ad14-4460d7c50e5d --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3q9GNI:/tmp/ceph-asok.3q9GNI -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:21:12,140450606-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:21:12,144465137-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1da17601-713a-4888-ad14-4460d7c50e5d', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3q9GNI:/tmp/ceph-asok.3q9GNI', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1da17601-713a-4888-ad14-4460d7c50e5d --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3q9GNI:/tmp/ceph-asok.3q9GNI -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:21:16,578652716-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:21:16,582405195-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1da17601-713a-4888-ad14-4460d7c50e5d', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3q9GNI:/tmp/ceph-asok.3q9GNI', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1da17601-713a-4888-ad14-4460d7c50e5d --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3q9GNI:/tmp/ceph-asok.3q9GNI -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:21:20,907278265-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:21:20,910803677-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1da17601-713a-4888-ad14-4460d7c50e5d', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3q9GNI:/tmp/ceph-asok.3q9GNI', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1da17601-713a-4888-ad14-4460d7c50e5d --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3q9GNI:/tmp/ceph-asok.3q9GNI -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:21:24,267038678-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:21:24,270652907-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1da17601-713a-4888-ad14-4460d7c50e5d', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3q9GNI:/tmp/ceph-asok.3q9GNI', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1da17601-713a-4888-ad14-4460d7c50e5d --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3q9GNI:/tmp/ceph-asok.3q9GNI -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  155 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:21:27,486658817-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:21:50,605362999-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (5s)
      [=============...............] (remaining: 5s)
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:21:58,761479030-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:22:06,912420467-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:22:15,198350275-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:22:23,419555918-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:22:31,635294016-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:22:39,705817572-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   227 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:22:39,712953647-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:22:47,975483740-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:22:47,983191488-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:22:56,185813954-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:22:56,193477249-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:23:04,307139793-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:23:04,314913676-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:23:12,423153779-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:23:12,430979520-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:23:12,437661491-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:23:12,441667126-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:23:12,449667003-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=361355
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_4KB_write.dat.5 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:23:12,455976184-04:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_4KB_write.log.5
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:23:12,476005779-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:23:12,479898111-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1da17601-713a-4888-ad14-4460d7c50e5d', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3q9GNI:/tmp/ceph-asok.3q9GNI', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1da17601-713a-4888-ad14-4460d7c50e5d --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3q9GNI:/tmp/ceph-asok.3q9GNI -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T16:23:14.917+0000 7fcac69d8d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:23:14.921+0000 7fcac69d8d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:23:14.921+0000 7fcac69d8d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T16:23:14.933646+0000 Maintaining 256 concurrent writes of 4096 bytes to objects of size 4096 for up to 20 seconds or 0 objects
2021-05-17T16:23:14.933656+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-17T16:23:14.934575+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T16:23:14.934575+0000     0       0         0         0         0         0           -           0
2021-05-17T16:23:15.934652+0000     1     256     20684     20428   79.7905   79.7969 0.000962357   0.0123151
2021-05-17T16:23:16.934726+0000     2     255     40796     40541   79.1755   78.5664  0.00202549   0.0124404
2021-05-17T16:23:17.934799+0000     3     255     60144     59889   77.9746   75.5781  0.00224978    0.012635
2021-05-17T16:23:18.934904+0000     4     256     79637     79381   77.5141   76.1406     0.12713   0.0127394
2021-05-17T16:23:19.934969+0000     5     255     99063     98808   77.1876   75.8867  0.00187667   0.0128027
2021-05-17T16:23:20.935038+0000     6     255    117894    117639   76.5819   73.5586  0.00219195   0.0129841
2021-05-17T16:23:21.935130+0000     7     255    136813    136558   76.1982   73.9023  0.00216376   0.0130644
2021-05-17T16:23:22.935244+0000     8     255    155347    155092   75.7222   72.3984  0.00113975   0.0131478
2021-05-17T16:23:23.935296+0000     9     255    171204    170949   74.1906   61.9414  0.00256174   0.0134261
2021-05-17T16:23:24.935332+0000    10     256    184876    184620   72.1117   53.4023  0.00215212   0.0138051
2021-05-17T16:23:25.935366+0000    11     256    203005    202749   71.9937   70.8164  0.00351535   0.0138171
2021-05-17T16:23:26.935434+0000    12     256    218947    218691   71.1834   62.2734  0.00130733   0.0139902
2021-05-17T16:23:27.935469+0000    13     256    235938    235682    70.813   66.3711 0.000621167   0.0140708
2021-05-17T16:23:28.935499+0000    14     255    252861    252606   70.4769   66.1094  0.00210796   0.0141154
2021-05-17T16:23:29.935528+0000    15     255    270228    269973    70.301   67.8398  0.00132816   0.0141886
2021-05-17T16:23:30.935603+0000    16     255    287171    286916   70.0433   66.1836    0.102504   0.0142372
2021-05-17T16:23:31.935659+0000    17     255    303565    303310   69.6899   64.0391   0.0027891   0.0143224
2021-05-17T16:23:32.935687+0000    18     255    320896    320641   69.5792   67.6992   0.0032002   0.0143312
2021-05-17T16:23:33.935715+0000    19     256    337384    337128   69.3067   64.4023   0.0631348   0.0143738
2021-05-17T16:23:34.935781+0000 min lat: 0.000602141 max lat: 0.275762 avg lat: 0.0144373
2021-05-17T16:23:34.935781+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T16:23:34.935781+0000    20     232    354419    354187    69.173   66.6367   0.0591053   0.0144373
2021-05-17T16:23:35.936036+0000 Total time run:         20.1046
Total writes made:      354419
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     68.8622
Stddev Bandwidth:       6.58842
Max bandwidth (MB/sec): 79.7969
Min bandwidth (MB/sec): 53.4023
Average IOPS:           17628
Stddev IOPS:            1686.64
Max IOPS:               20428
Min IOPS:               13671
Average Latency(s):     0.0144583
Stddev Latency(s):      0.0306422
Max latency(s):         0.275762
Min latency(s):         0.000602141

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:23:36,485580405-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 361355


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:23:36,489613171-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:23:59,806402325-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 354.42k objects, 1.4 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:23:59,814165708-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:24:08,188943909-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 354.42k objects, 1.4 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:24:08,196472501-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:24:16,713933437-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 354.42k objects, 1.4 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:24:16,721658228-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:24:25,032290877-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 354.42k objects, 1.4 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:24:25,040763723-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:24:33,246658666-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 354.42k objects, 1.4 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:24:33,254823754-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:24:33,261057654-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:24:33,264772572-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:24:33,272034633-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=362749
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_4KB_seq.dat.5 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:24:33,277996702-04:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_4KB_seq.log.5
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:24:33,297316213-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:24:33,300603958-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1da17601-713a-4888-ad14-4460d7c50e5d', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3q9GNI:/tmp/ceph-asok.3q9GNI', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1da17601-713a-4888-ad14-4460d7c50e5d --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.3q9GNI:/tmp/ceph-asok.3q9GNI -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T16:24:35.765+0000 7f2c96caed00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:24:35.769+0000 7f2c96caed00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:24:35.769+0000 7f2c96caed00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T16:24:35.783417+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T16:24:35.783417+0000     0       0         0         0         0         0           -           0
2021-05-17T16:24:36.783485+0000     1     255     51833     51578   201.451   201.477  0.00522352  0.00494326
2021-05-17T16:24:37.783566+0000     2     255     99320     99065   193.466   185.496  0.00528573  0.00515663
2021-05-17T16:24:38.783684+0000     3     255    139542    139287   181.344   157.117  0.00524353  0.00550487
2021-05-17T16:24:39.783753+0000     4     255    184626    184371   180.032   176.109  0.00538617  0.00554668
2021-05-17T16:24:40.783823+0000     5     255    229624    229369   179.178   175.773  0.00523132  0.00557429
2021-05-17T16:24:41.783893+0000     6     255    276931    276676   180.112   184.793  0.00567347  0.00554568
2021-05-17T16:24:42.784000+0000     7     256    320027    319771   178.427    168.34   0.0052538   0.0055988
2021-05-17T16:24:43.784142+0000 Total time run:       7.82819
Total reads made:     354419
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   176.854
Average IOPS:         45274
Stddev IOPS:          3605.35
Max IOPS:             51578
Min IOPS:             40222
Average Latency(s):   0.00564792
Max latency(s):       0.196122
Min latency(s):       0.000221977

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:24:44,332093816-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 362749


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:24:44,336329402-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:25:07,611222266-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 354.42k objects, 1.4 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:25:07,618949461-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:25:16,126116307-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 354.42k objects, 1.4 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:25:16,134098200-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:25:24,514728182-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 354.42k objects, 1.4 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:25:24,522791799-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:25:32,811552015-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 354.42k objects, 1.4 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:25:32,819449820-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:25:41,017160140-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 354.42k objects, 1.4 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:25:41,025338012-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:25:41,030719451-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T12:25:41,034002107-04:00][RUNNING][ROUND 1/2/40] object_size=16KB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:25:41,037740579-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:25:41,046391640-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40889\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.363597\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 202a06e5-7fc0-4521-a486-46d8a2c1716a\nsetting min_mon_release = octopus\nepoch 0\nfsid 202a06e5-7fc0-4521-a486-46d8a2c1716a\nlast_changed 2021-05-17T09:26:05.964480-0700\ncreated 2021-05-17T09:26:05.964480-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40889/0,v1:10.10.1.2:40890/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.363597 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 4ea17efa-9099-48c0-a169-e35401376e59\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 723115b9-1b63-46a5-ad25-5c19241a97ed\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 1f3736ea-5219-499c-a95d-1cce5c7189e9\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42889\n  w/ user/pass: admin / a58c5f05-4c45-4e64-b3cc-67ac93e9bd67\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 12:26:19 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40889
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.363597
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 202a06e5-7fc0-4521-a486-46d8a2c1716a
setting min_mon_release = octopus
epoch 0
fsid 202a06e5-7fc0-4521-a486-46d8a2c1716a
last_changed 2021-05-17T09:26:05.964480-0700
created 2021-05-17T09:26:05.964480-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40889/0,v1:10.10.1.2:40890/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.363597 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 4ea17efa-9099-48c0-a169-e35401376e59
0
start osd.0
add osd1 723115b9-1b63-46a5-ad25-5c19241a97ed
1
start osd.1
add osd2 1f3736ea-5219-499c-a95d-1cce5c7189e9
2
start osd.2


restful urls: https://10.10.1.2:42889
  w/ user/pass: admin / a58c5f05-4c45-4e64-b3cc-67ac93e9bd67


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T09:25:41.977-0700 7f53b3ca21c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:25:41.977-0700 7f53b3ca21c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:25:41.993-0700 7f66036ad1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:25:41.993-0700 7f66036ad1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40889,v1:10.10.1.2:40890] --print /tmp/ceph_monmap.363597 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.363597 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.363597 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42889 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.jNv1DI5o9k 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 4ea17efa-9099-48c0-a169-e35401376e59 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAnmaJg0avoDBAA8kXBgdw0PBpVB4M/jbJMYA== --osd-uuid 4ea17efa-9099-48c0-a169-e35401376e59 
2021-05-17T09:26:15.566-0700 7ff938df8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T09:26:15.570-0700 7ff938df8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T09:26:15.570-0700 7ff938df8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T09:26:15.630-0700 7ff938df8f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 723115b9-1b63-46a5-ad25-5c19241a97ed -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T09:26:15.906-0700 7fa9fb755f00 -1 Falling back to public interface
2021-05-17T09:26:15.918-0700 7fa9fb755f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAnmaJgigMdNhAAVWDNTRToYgUqhVCUJXkrhA== --osd-uuid 723115b9-1b63-46a5-ad25-5c19241a97ed 
2021-05-17T09:26:16.274-0700 7f65c4af6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T09:26:16.274-0700 7f65c4af6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T09:26:16.274-0700 7f65c4af6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T09:26:16.358-0700 7f65c4af6f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 1f3736ea-5219-499c-a95d-1cce5c7189e9 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T09:26:16.670-0700 7fe1f5290f00 -1 Falling back to public interface
2021-05-17T09:26:16.682-0700 7fe1f5290f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAomaJgLb3hJxAA1ykVbxeVd6HQcjhII9SobA== --osd-uuid 1f3736ea-5219-499c-a95d-1cce5c7189e9 
2021-05-17T09:26:17.006-0700 7f18f13f1f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T09:26:17.006-0700 7f18f13f1f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T09:26:17.006-0700 7f18f13f1f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T09:26:17.070-0700 7f18f13f1f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T09:26:17.346-0700 7f5ecac07f00 -1 Falling back to public interface
2021-05-17T09:26:17.362-0700 7f5ecac07f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:26:19,506762207-04:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:26:19,510326201-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:26:19,550466472-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:26:19,554080701-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '61f83acb-7723-44af-ad36-06270bb8184f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tqlqqs:/tmp/ceph-asok.tqlqqs', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 61f83acb-7723-44af-ad36-06270bb8184f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tqlqqs:/tmp/ceph-asok.tqlqqs -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:26:22,827202390-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:26:22,831063592-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '61f83acb-7723-44af-ad36-06270bb8184f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tqlqqs:/tmp/ceph-asok.tqlqqs', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 61f83acb-7723-44af-ad36-06270bb8184f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tqlqqs:/tmp/ceph-asok.tqlqqs -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:26:26,066795788-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:26:26,070347951-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '61f83acb-7723-44af-ad36-06270bb8184f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tqlqqs:/tmp/ceph-asok.tqlqqs', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 61f83acb-7723-44af-ad36-06270bb8184f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tqlqqs:/tmp/ceph-asok.tqlqqs -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:26:29,273386374-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:26:29,277339349-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '61f83acb-7723-44af-ad36-06270bb8184f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tqlqqs:/tmp/ceph-asok.tqlqqs', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 61f83acb-7723-44af-ad36-06270bb8184f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tqlqqs:/tmp/ceph-asok.tqlqqs -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:26:35,595746477-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:26:35,599303649-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '61f83acb-7723-44af-ad36-06270bb8184f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tqlqqs:/tmp/ceph-asok.tqlqqs', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 61f83acb-7723-44af-ad36-06270bb8184f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tqlqqs:/tmp/ceph-asok.tqlqqs -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:26:39,093420175-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:26:39,097406353-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '61f83acb-7723-44af-ad36-06270bb8184f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tqlqqs:/tmp/ceph-asok.tqlqqs', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 61f83acb-7723-44af-ad36-06270bb8184f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tqlqqs:/tmp/ceph-asok.tqlqqs -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:26:42,858521730-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:26:42,862449418-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '61f83acb-7723-44af-ad36-06270bb8184f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tqlqqs:/tmp/ceph-asok.tqlqqs', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 61f83acb-7723-44af-ad36-06270bb8184f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tqlqqs:/tmp/ceph-asok.tqlqqs -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:26:47,197553457-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:26:47,201279135-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '61f83acb-7723-44af-ad36-06270bb8184f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tqlqqs:/tmp/ceph-asok.tqlqqs', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 61f83acb-7723-44af-ad36-06270bb8184f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tqlqqs:/tmp/ceph-asok.tqlqqs -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:26:50,587284906-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:26:50,591260995-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '61f83acb-7723-44af-ad36-06270bb8184f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tqlqqs:/tmp/ceph-asok.tqlqqs', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 61f83acb-7723-44af-ad36-06270bb8184f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tqlqqs:/tmp/ceph-asok.tqlqqs -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:26:53,877432554-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:26:53,881433790-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '61f83acb-7723-44af-ad36-06270bb8184f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tqlqqs:/tmp/ceph-asok.tqlqqs', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 61f83acb-7723-44af-ad36-06270bb8184f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tqlqqs:/tmp/ceph-asok.tqlqqs -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:26:57,254822161-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:26:57,258461136-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '61f83acb-7723-44af-ad36-06270bb8184f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tqlqqs:/tmp/ceph-asok.tqlqqs', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 61f83acb-7723-44af-ad36-06270bb8184f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tqlqqs:/tmp/ceph-asok.tqlqqs -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:27:00,391184455-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:27:00,394953264-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '61f83acb-7723-44af-ad36-06270bb8184f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tqlqqs:/tmp/ceph-asok.tqlqqs', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 61f83acb-7723-44af-ad36-06270bb8184f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tqlqqs:/tmp/ceph-asok.tqlqqs -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  156 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:27:03,621149706-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:27:26,742336448-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   199 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (4s)
      [............................] 
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:27:35,040244461-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:27:43,319780758-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:27:51,618723254-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:27:59,989674919-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:28:08,513774569-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:28:16,835129971-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:28:16,843474707-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:28:25,070864815-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:28:25,078409688-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:28:33,293652853-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:28:33,301873135-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:28:41,613199452-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:28:41,621365372-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:28:49,843478972-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:28:49,851672674-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:28:49,857670691-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:28:49,861496507-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:28:49,869034377-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=369325
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_16KB_write.dat.1 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:28:49,873936625-04:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_16KB_write.log.1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:28:49,894599229-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:28:49,898151211-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '61f83acb-7723-44af-ad36-06270bb8184f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tqlqqs:/tmp/ceph-asok.tqlqqs', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 61f83acb-7723-44af-ad36-06270bb8184f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tqlqqs:/tmp/ceph-asok.tqlqqs -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T16:28:52.502+0000 7fb5e4f9bd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:28:52.506+0000 7fb5e4f9bd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:28:52.506+0000 7fb5e4f9bd00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T16:28:52.516645+0000 Maintaining 256 concurrent writes of 16384 bytes to objects of size 16384 for up to 20 seconds or 0 objects
2021-05-17T16:28:52.516657+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-17T16:28:52.518624+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T16:28:52.518624+0000     0       0         0         0         0         0           -           0
2021-05-17T16:28:53.518700+0000     1     256     18819     18563   290.037   290.047  0.00146897   0.0131943
2021-05-17T16:28:54.518772+0000     2     256     36290     36034   281.501   272.984  0.00283988   0.0137897
2021-05-17T16:28:55.518858+0000     3     255     53573     53318    277.68   270.062  0.00089932   0.0141525
2021-05-17T16:28:56.518923+0000     4     256     71918     71662   279.912   286.625  0.00162522   0.0141155
2021-05-17T16:28:57.518991+0000     5     256     90165     89909   280.947   285.109  0.00671822   0.0141347
2021-05-17T16:28:58.519057+0000     6     256    107912    107656   280.336   277.297   0.0013366   0.0141568
2021-05-17T16:28:59.519163+0000     7     255    126172    125917   281.045   285.328  0.00669005   0.0141483
2021-05-17T16:29:00.519278+0000     8     255    142377    142122   277.561   253.203  0.00130714   0.0143669
2021-05-17T16:29:01.519394+0000     9     256    159161    158905   275.854   262.234  0.00162627   0.0144255
2021-05-17T16:29:02.519502+0000    10     256    176238    175982   274.949   266.828  0.00212078   0.0144618
2021-05-17T16:29:03.519608+0000    11     256    193297    193041   274.182   266.547  0.00142984   0.0145375
2021-05-17T16:29:04.519717+0000    12     255    210519    210264   273.757   269.109   0.0011816   0.0145759
2021-05-17T16:29:05.519830+0000    13     255    228231    227976   273.985    276.75  0.00180861    0.014545
2021-05-17T16:29:06.519902+0000    14     255    243303    243048   271.235     235.5   0.0884863   0.0147105
2021-05-17T16:29:07.519970+0000    15     256    259891    259635    270.43   259.172    0.115042    0.014741
2021-05-17T16:29:08.520063+0000    16     255    276221    275966   269.475   255.172  0.00170312   0.0147819
2021-05-17T16:29:09.520129+0000    17     255    291803    291548   267.944   243.469  0.00122606   0.0148842
2021-05-17T16:29:10.520196+0000    18     255    306730    306475   266.015   233.234  0.00438404   0.0150139
2021-05-17T16:29:11.520295+0000    19     256    320755    320499   263.546   219.125  0.00177975   0.0150866
2021-05-17T16:29:12.520373+0000 min lat: 0.000651243 max lat: 0.253048 avg lat: 0.0152346
2021-05-17T16:29:12.520373+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T16:29:12.520373+0000    20     225    335960    335735   262.271   238.062   0.0024609   0.0152346
2021-05-17T16:29:13.520517+0000 Total time run:         20.043
Total writes made:      335960
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     261.906
Stddev Bandwidth:       20.0233
Max bandwidth (MB/sec): 290.047
Min bandwidth (MB/sec): 219.125
Average IOPS:           16761
Stddev IOPS:            1281.49
Max IOPS:               18563
Min IOPS:               14024
Average Latency(s):     0.0152473
Stddev Latency(s):      0.0324268
Max latency(s):         0.253048
Min latency(s):         0.000651243

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:29:14,048208152-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 369325


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:29:14,052425294-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:29:37,652009993-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 335.96k objects, 5.1 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:29:37,660023696-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:29:46,138194419-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 335.96k objects, 5.1 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:29:46,146058962-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:29:54,277456946-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 335.96k objects, 5.1 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:29:54,285538787-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:30:02,814590557-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 335.96k objects, 5.1 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:30:02,822670104-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:30:11,120210777-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 335.96k objects, 5.1 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:30:11,128008324-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:30:11,133888490-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:30:11,137741959-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:30:11,145829741-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=370696
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_16KB_seq.dat.1 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:30:11,151674049-04:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_16KB_seq.log.1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:30:11,170864969-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:30:11,174458780-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '61f83acb-7723-44af-ad36-06270bb8184f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tqlqqs:/tmp/ceph-asok.tqlqqs', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 61f83acb-7723-44af-ad36-06270bb8184f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.tqlqqs:/tmp/ceph-asok.tqlqqs -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T16:30:13.663+0000 7f5bc6488d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:30:13.667+0000 7f5bc6488d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:30:13.667+0000 7f5bc6488d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T16:30:13.678648+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T16:30:13.678648+0000     0       0         0         0         0         0           -           0
2021-05-17T16:30:14.678761+0000     1     255     47028     46773   730.695   730.828  0.00362446  0.00543969
2021-05-17T16:30:15.678843+0000     2     255     94732     94477   738.004   745.375  0.00118132  0.00540588
2021-05-17T16:30:16.678952+0000     3     256    139748    139492    726.43   703.359 0.000526429  0.00548947
2021-05-17T16:30:17.679022+0000     4     256    186975    186719    729.29   737.922  0.00256367  0.00547469
2021-05-17T16:30:18.679124+0000     5     255    222308    222053    693.84   552.094  0.00514808  0.00575705
2021-05-17T16:30:19.679213+0000     6     255    264669    264414   688.505   661.891  0.00423035  0.00580193
2021-05-17T16:30:20.679328+0000     7     255    312652    312397    697.24   749.734  0.00363401  0.00572967
2021-05-17T16:30:21.679467+0000 Total time run:       7.51249
Total reads made:     335960
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   698.753
Average IOPS:         44720
Stddev IOPS:          4541.31
Max IOPS:             47983
Min IOPS:             35334
Average Latency(s):   0.00571813
Max latency(s):       0.182523
Min latency(s):       0.000234952

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:30:22,236802370-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 370696


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:30:22,241017198-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:30:45,829477070-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 335.96k objects, 5.1 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:30:45,837503788-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:30:54,252570362-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 335.96k objects, 5.1 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:30:54,260619522-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:31:02,825708912-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 335.96k objects, 5.1 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:31:02,833570700-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:31:11,293615433-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 335.96k objects, 5.1 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:31:11,301812802-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:31:19,779986935-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 335.96k objects, 5.1 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:31:19,788583975-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:31:19,794511719-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T12:31:19,796811309-04:00][RUNNING][ROUND 2/2/40] object_size=16KB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:31:19,800208100-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:31:19,809113258-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40341\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.364707\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid fcf7fcd2-4325-4a12-a787-f358ce9f5b5a\nsetting min_mon_release = octopus\nepoch 0\nfsid fcf7fcd2-4325-4a12-a787-f358ce9f5b5a\nlast_changed 2021-05-17T09:31:45.950418-0700\ncreated 2021-05-17T09:31:45.950418-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40341/0,v1:10.10.1.2:40342/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.364707 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 39334f94-8cd3-4290-934a-b2d66820b1d0\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 e54b06eb-089d-465b-a3f7-e2f64a285d1f\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 6a2cc706-4a5e-48b3-b717-a6d44d24cb52\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42341\n  w/ user/pass: admin / d17f87b6-63b8-46ba-95b1-6f03cf3b8c89\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\nCEPH_DEV=1\n'
[1] 12:31:59 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40341
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.364707
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid fcf7fcd2-4325-4a12-a787-f358ce9f5b5a
setting min_mon_release = octopus
epoch 0
fsid fcf7fcd2-4325-4a12-a787-f358ce9f5b5a
last_changed 2021-05-17T09:31:45.950418-0700
created 2021-05-17T09:31:45.950418-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40341/0,v1:10.10.1.2:40342/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.364707 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 39334f94-8cd3-4290-934a-b2d66820b1d0
0
start osd.0
add osd1 e54b06eb-089d-465b-a3f7-e2f64a285d1f
1
start osd.1
add osd2 6a2cc706-4a5e-48b3-b717-a6d44d24cb52
2
start osd.2


restful urls: https://10.10.1.2:42341
  w/ user/pass: admin / d17f87b6-63b8-46ba-95b1-6f03cf3b8c89


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T09:31:20.736-0700 7fe442c941c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:31:20.736-0700 7fe442c941c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:31:20.756-0700 7fa014f451c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:31:20.756-0700 7fa014f451c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40341,v1:10.10.1.2:40342] --print /tmp/ceph_monmap.364707 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.364707 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.364707 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42341 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.0GfRLfPNjV 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 39334f94-8cd3-4290-934a-b2d66820b1d0 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQB7mqJg1Yq7JBAA/HBrROXiCcBiWSTxtXwwVA== --osd-uuid 39334f94-8cd3-4290-934a-b2d66820b1d0 
2021-05-17T09:31:55.957-0700 7f902accdf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T09:31:55.957-0700 7f902accdf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T09:31:55.957-0700 7f902accdf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T09:31:56.045-0700 7f902accdf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new e54b06eb-089d-465b-a3f7-e2f64a285d1f -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T09:31:56.357-0700 7f48a04f7f00 -1 Falling back to public interface
2021-05-17T09:31:56.369-0700 7f48a04f7f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQB8mqJg+o9cFRAA6IQFnrveQnJ1CaYw3MTXTg== --osd-uuid e54b06eb-089d-465b-a3f7-e2f64a285d1f 
2021-05-17T09:31:56.721-0700 7f646bbfcf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T09:31:56.721-0700 7f646bbfcf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T09:31:56.721-0700 7f646bbfcf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T09:31:56.809-0700 7f646bbfcf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 6a2cc706-4a5e-48b3-b717-a6d44d24cb52 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T09:31:57.141-0700 7fd348760f00 -1 Falling back to public interface
2021-05-17T09:31:57.153-0700 7fd348760f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQB9mqJgidiVBhAA4GajXjKY5s9arICXd8PJ+g== --osd-uuid 6a2cc706-4a5e-48b3-b717-a6d44d24cb52 
2021-05-17T09:31:57.433-0700 7f1a414e5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T09:31:57.433-0700 7f1a414e5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T09:31:57.433-0700 7f1a414e5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T09:31:57.493-0700 7f1a414e5f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T09:31:57.757-0700 7f55c2bdaf00 -1 Falling back to public interface
2021-05-17T09:31:57.769-0700 7f55c2bdaf00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:31:59,890637251-04:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:31:59,894663895-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:31:59,935383545-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:31:59,938530887-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd1462dfa-64ee-4e2c-ae3c-93dad59aac10', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.50T8Zq:/tmp/ceph-asok.50T8Zq', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d1462dfa-64ee-4e2c-ae3c-93dad59aac10 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.50T8Zq:/tmp/ceph-asok.50T8Zq -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:32:03,306377911-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:32:03,309901780-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd1462dfa-64ee-4e2c-ae3c-93dad59aac10', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.50T8Zq:/tmp/ceph-asok.50T8Zq', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d1462dfa-64ee-4e2c-ae3c-93dad59aac10 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.50T8Zq:/tmp/ceph-asok.50T8Zq -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:32:06,697818284-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:32:06,701203483-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd1462dfa-64ee-4e2c-ae3c-93dad59aac10', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.50T8Zq:/tmp/ceph-asok.50T8Zq', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d1462dfa-64ee-4e2c-ae3c-93dad59aac10 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.50T8Zq:/tmp/ceph-asok.50T8Zq -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:32:10,143416421-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:32:10,146920864-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd1462dfa-64ee-4e2c-ae3c-93dad59aac10', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.50T8Zq:/tmp/ceph-asok.50T8Zq', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d1462dfa-64ee-4e2c-ae3c-93dad59aac10 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.50T8Zq:/tmp/ceph-asok.50T8Zq -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:32:16,864296586-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:32:16,868182566-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd1462dfa-64ee-4e2c-ae3c-93dad59aac10', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.50T8Zq:/tmp/ceph-asok.50T8Zq', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d1462dfa-64ee-4e2c-ae3c-93dad59aac10 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.50T8Zq:/tmp/ceph-asok.50T8Zq -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:32:20,634874957-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:32:20,638650910-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd1462dfa-64ee-4e2c-ae3c-93dad59aac10', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.50T8Zq:/tmp/ceph-asok.50T8Zq', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d1462dfa-64ee-4e2c-ae3c-93dad59aac10 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.50T8Zq:/tmp/ceph-asok.50T8Zq -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:32:24,315666298-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:32:24,319229221-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd1462dfa-64ee-4e2c-ae3c-93dad59aac10', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.50T8Zq:/tmp/ceph-asok.50T8Zq', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d1462dfa-64ee-4e2c-ae3c-93dad59aac10 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.50T8Zq:/tmp/ceph-asok.50T8Zq -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:32:28,703004959-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:32:28,705848841-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd1462dfa-64ee-4e2c-ae3c-93dad59aac10', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.50T8Zq:/tmp/ceph-asok.50T8Zq', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d1462dfa-64ee-4e2c-ae3c-93dad59aac10 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.50T8Zq:/tmp/ceph-asok.50T8Zq -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:32:33,083903485-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:32:33,087748417-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd1462dfa-64ee-4e2c-ae3c-93dad59aac10', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.50T8Zq:/tmp/ceph-asok.50T8Zq', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d1462dfa-64ee-4e2c-ae3c-93dad59aac10 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.50T8Zq:/tmp/ceph-asok.50T8Zq -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:32:36,527164247-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:32:36,530706230-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd1462dfa-64ee-4e2c-ae3c-93dad59aac10', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.50T8Zq:/tmp/ceph-asok.50T8Zq', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d1462dfa-64ee-4e2c-ae3c-93dad59aac10 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.50T8Zq:/tmp/ceph-asok.50T8Zq -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:32:39,945890924-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:32:39,949663431-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd1462dfa-64ee-4e2c-ae3c-93dad59aac10', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.50T8Zq:/tmp/ceph-asok.50T8Zq', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d1462dfa-64ee-4e2c-ae3c-93dad59aac10 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.50T8Zq:/tmp/ceph-asok.50T8Zq -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:32:43,166737873-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:32:43,170372070-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd1462dfa-64ee-4e2c-ae3c-93dad59aac10', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.50T8Zq:/tmp/ceph-asok.50T8Zq', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d1462dfa-64ee-4e2c-ae3c-93dad59aac10 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.50T8Zq:/tmp/ceph-asok.50T8Zq -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  156 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:32:46,532032983-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:33:09,884766790-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (5s)
      [============................] (remaining: 5s)
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:33:18,081220490-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:33:26,360134062-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:33:34,612677853-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:33:42,802617389-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:33:50,983902843-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:33:59,414172469-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   235 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:33:59,422243420-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:34:07,794275464-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:34:07,802184441-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:34:16,151887154-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:34:16,159692747-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:34:24,545675786-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:34:24,553507337-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:34:33,018793639-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:34:33,026712865-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:34:33,032520124-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:34:33,036445578-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:34:33,044317825-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=377291
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_16KB_write.dat.2 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:34:33,050464923-04:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_16KB_write.log.2
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:34:33,068573849-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:34:33,071729838-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd1462dfa-64ee-4e2c-ae3c-93dad59aac10', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.50T8Zq:/tmp/ceph-asok.50T8Zq', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d1462dfa-64ee-4e2c-ae3c-93dad59aac10 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.50T8Zq:/tmp/ceph-asok.50T8Zq -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T16:34:35.575+0000 7fb23e738d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:34:35.579+0000 7fb23e738d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:34:35.579+0000 7fb23e738d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T16:34:35.587652+0000 Maintaining 256 concurrent writes of 16384 bytes to objects of size 16384 for up to 20 seconds or 0 objects
2021-05-17T16:34:35.587660+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-17T16:34:35.589517+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T16:34:35.589517+0000     0       0         0         0         0         0           -           0
2021-05-17T16:34:36.589619+0000     1     256     18813     18557   289.932   289.953  0.00162977   0.0130146
2021-05-17T16:34:37.589703+0000     2     256     35367     35111   274.283   258.656  0.00129866   0.0142697
2021-05-17T16:34:38.589817+0000     3     256     52390     52134   271.507   265.984  0.00176781   0.0144691
2021-05-17T16:34:39.589895+0000     4     256     69571     69315   270.738   268.453  0.00123189   0.0145671
2021-05-17T16:34:40.590000+0000     5     255     86573     86318   269.719   265.672  0.00139196   0.0146494
2021-05-17T16:34:41.590078+0000     6     255    104422    104167   271.244   278.891 0.000876427   0.0146111
2021-05-17T16:34:42.590184+0000     7     256    121515    121259   270.643   267.062  0.00333861   0.0146673
2021-05-17T16:34:43.590264+0000     8     256    138075    137819   269.154    258.75   0.0010351   0.0147475
2021-05-17T16:34:44.590380+0000     9     256    153634    153378   266.257   243.109  0.00348781   0.0149684
2021-05-17T16:34:45.590456+0000    10     256    170523    170267   266.018   263.891  0.00335066   0.0149769
2021-05-17T16:34:46.590563+0000    11     255    187161    186906   265.467   259.984  0.00130975    0.015017
2021-05-17T16:34:47.590644+0000    12     255    205622    205367   267.381   288.453  0.00141453   0.0148926
2021-05-17T16:34:48.590754+0000    13     256    222025    221769   266.525   256.281    0.001326   0.0149704
2021-05-17T16:34:49.590836+0000    14     255    237701    237446   264.982   244.953  0.00110124   0.0150012
2021-05-17T16:34:50.590955+0000    15     255    254434    254179   264.745   261.453  0.00277796   0.0150697
2021-05-17T16:34:51.591047+0000    16     255    270673    270418   264.055   253.734  0.00117243    0.015063
2021-05-17T16:34:52.591152+0000    17     256    285555    285299   262.199   232.516  0.00169024   0.0152028
2021-05-17T16:34:53.591242+0000    18     255    302577    302322   262.408   265.984  0.00749526   0.0151994
2021-05-17T16:34:54.591348+0000    19     256    318882    318626   262.003    254.75  0.00269018   0.0152199
2021-05-17T16:34:55.591444+0000 min lat: 0.000629182 max lat: 0.219213 avg lat: 0.0152716
2021-05-17T16:34:55.591444+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T16:34:55.591444+0000    20     226    334504    334278    261.13   244.562  0.00285058   0.0152716
2021-05-17T16:34:56.591630+0000 Total time run:         20.0916
Total writes made:      334504
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     260.14
Stddev Bandwidth:       14.2695
Max bandwidth (MB/sec): 289.953
Min bandwidth (MB/sec): 232.516
Average IOPS:           16648
Stddev IOPS:            913.249
Max IOPS:               18557
Min IOPS:               14881
Average Latency(s):     0.0153421
Stddev Latency(s):      0.0373314
Max latency(s):         0.219213
Min latency(s):         0.000629182

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:34:57,144645115-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 377291


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:34:57,148832862-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:35:20,157599866-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 334.50k objects, 5.1 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:35:20,165484377-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:35:28,596704848-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 334.50k objects, 5.1 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:35:28,604941009-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:35:37,067718834-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 334.50k objects, 5.1 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:35:37,075808240-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:35:45,507845073-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 334.50k objects, 5.1 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:35:45,516487898-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:35:53,928422214-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 334.50k objects, 5.1 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:35:53,936138348-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:35:53,942673494-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:35:53,946618495-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:35:53,955515367-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=378680
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_16KB_seq.dat.2 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:35:53,961369444-04:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_16KB_seq.log.2
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:35:53,979721808-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:35:53,983406781-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd1462dfa-64ee-4e2c-ae3c-93dad59aac10', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.50T8Zq:/tmp/ceph-asok.50T8Zq', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d1462dfa-64ee-4e2c-ae3c-93dad59aac10 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.50T8Zq:/tmp/ceph-asok.50T8Zq -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T16:35:56.392+0000 7f27ee3d6d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:35:56.696+0000 7f27ee3d6d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:35:56.696+0000 7f27ee3d6d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T16:35:56.708032+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T16:35:56.708032+0000     0       0         0         0         0         0           -           0
2021-05-17T16:35:57.708120+0000     1     255     52804     52549   820.948   821.078  0.00787942  0.00484838
2021-05-17T16:35:58.708198+0000     2     255     97691     97436   761.129   701.359  0.00536836  0.00524204
2021-05-17T16:35:59.708287+0000     3     255    144777    144522   752.637   735.719   0.0053779   0.0053045
2021-05-17T16:36:00.708422+0000     4     255    187804    187549   732.529   672.297  0.00324055  0.00545063
2021-05-17T16:36:01.708541+0000     5     255    226464    226209   706.821   604.062  0.00536127  0.00565117
2021-05-17T16:36:02.708614+0000     6     255    272639    272384   709.256   721.484  0.00540776  0.00563235
2021-05-17T16:36:03.708693+0000     7     256    315096    314840   702.694   663.375  0.00375785  0.00568408
2021-05-17T16:36:04.708834+0000 Total time run:       7.40405
Total reads made:     334504
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   705.914
Average IOPS:         45178
Stddev IOPS:          4345.28
Max IOPS:             52549
Min IOPS:             38660
Average Latency(s):   0.00565324
Max latency(s):       0.131316
Min latency(s):       0.000216206

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:36:05,274499048-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 378680


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:36:05,279033867-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:36:28,636006054-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 334.50k objects, 5.1 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:36:28,644024778-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:36:36,841864084-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 334.50k objects, 5.1 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:36:36,850823915-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:36:45,114348043-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 334.50k objects, 5.1 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:36:45,122279862-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:36:53,370615634-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 334.50k objects, 5.1 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:36:53,378648834-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:37:01,809711838-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 334.50k objects, 5.1 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:37:01,817661812-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:37:01,823839828-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T12:37:01,826075908-04:00][RUNNING][ROUND 3/2/40] object_size=16KB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:37:01,829749719-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:37:01,838331359-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40930\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.365827\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid b9a69f02-eae9-4c82-865e-0eafb4408c96\nsetting min_mon_release = octopus\nepoch 0\nfsid b9a69f02-eae9-4c82-865e-0eafb4408c96\nlast_changed 2021-05-17T09:37:31.101018-0700\ncreated 2021-05-17T09:37:31.101018-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40930/0,v1:10.10.1.2:40931/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.365827 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 644528b4-f805-4392-800c-b2ca9b09f6f5\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 c297ef6a-da50-4d9a-a807-69ed89d37d3d\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 fa21a1d1-8b2e-4ce8-8f11-9bd1c684cc20\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42930\n  w/ user/pass: admin / b61514ee-a9d5-4fea-95ad-a96d12e2ba15\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 12:37:44 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40930
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.365827
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid b9a69f02-eae9-4c82-865e-0eafb4408c96
setting min_mon_release = octopus
epoch 0
fsid b9a69f02-eae9-4c82-865e-0eafb4408c96
last_changed 2021-05-17T09:37:31.101018-0700
created 2021-05-17T09:37:31.101018-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40930/0,v1:10.10.1.2:40931/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.365827 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 644528b4-f805-4392-800c-b2ca9b09f6f5
0
start osd.0
add osd1 c297ef6a-da50-4d9a-a807-69ed89d37d3d
1
start osd.1
add osd2 fa21a1d1-8b2e-4ce8-8f11-9bd1c684cc20
2
start osd.2


restful urls: https://10.10.1.2:42930
  w/ user/pass: admin / b61514ee-a9d5-4fea-95ad-a96d12e2ba15


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T09:37:02.759-0700 7f8c778851c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:37:02.759-0700 7f8c778851c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:37:02.775-0700 7f84d186a1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:37:02.775-0700 7f84d186a1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40930,v1:10.10.1.2:40931] --print /tmp/ceph_monmap.365827 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.365827 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.365827 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42930 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.F7Ysp1tKFs 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 644528b4-f805-4392-800c-b2ca9b09f6f5 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDTm6Jg+hjrOBAA7tEnOxrEoNzP/s9uDMrXgA== --osd-uuid 644528b4-f805-4392-800c-b2ca9b09f6f5 
2021-05-17T09:37:40.300-0700 7f6e11f75f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T09:37:40.300-0700 7f6e11f75f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T09:37:40.300-0700 7f6e11f75f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T09:37:40.380-0700 7f6e11f75f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new c297ef6a-da50-4d9a-a807-69ed89d37d3d -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T09:37:40.692-0700 7f38095c1f00 -1 Falling back to public interface
2021-05-17T09:37:40.704-0700 7f38095c1f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDUm6JgYu1qKRAAlTgWS0ZXtGaOpTPvDiudgg== --osd-uuid c297ef6a-da50-4d9a-a807-69ed89d37d3d 
2021-05-17T09:37:41.020-0700 7fbc97b18f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T09:37:41.024-0700 7fbc97b18f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T09:37:41.024-0700 7fbc97b18f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T09:37:41.092-0700 7fbc97b18f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new fa21a1d1-8b2e-4ce8-8f11-9bd1c684cc20 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T09:37:41.412-0700 7f70fa256f00 -1 Falling back to public interface
2021-05-17T09:37:41.424-0700 7f70fa256f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDVm6JgIMu7GBAALNdh0KR4czSHAHmR3FxF9Q== --osd-uuid fa21a1d1-8b2e-4ce8-8f11-9bd1c684cc20 
2021-05-17T09:37:41.760-0700 7f64bded8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T09:37:41.760-0700 7f64bded8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T09:37:41.760-0700 7f64bded8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T09:37:41.880-0700 7f64bded8f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T09:37:42.140-0700 7f558c530f00 -1 Falling back to public interface
2021-05-17T09:37:42.156-0700 7f558c530f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:37:44,210017893-04:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:37:44,213875570-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:37:44,257100107-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:37:44,259935212-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fda5045c-a305-4fd0-a65e-e26d1cbfa6cb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vYwqnk:/tmp/ceph-asok.vYwqnk', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fda5045c-a305-4fd0-a65e-e26d1cbfa6cb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vYwqnk:/tmp/ceph-asok.vYwqnk -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:37:47,518560238-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:37:47,522198182-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fda5045c-a305-4fd0-a65e-e26d1cbfa6cb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vYwqnk:/tmp/ceph-asok.vYwqnk', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fda5045c-a305-4fd0-a65e-e26d1cbfa6cb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vYwqnk:/tmp/ceph-asok.vYwqnk -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:37:50,891194007-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:37:50,895197357-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fda5045c-a305-4fd0-a65e-e26d1cbfa6cb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vYwqnk:/tmp/ceph-asok.vYwqnk', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fda5045c-a305-4fd0-a65e-e26d1cbfa6cb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vYwqnk:/tmp/ceph-asok.vYwqnk -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:37:54,006341766-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:37:54,009823526-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fda5045c-a305-4fd0-a65e-e26d1cbfa6cb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vYwqnk:/tmp/ceph-asok.vYwqnk', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fda5045c-a305-4fd0-a65e-e26d1cbfa6cb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vYwqnk:/tmp/ceph-asok.vYwqnk -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:38:00,678840525-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:38:00,682315102-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fda5045c-a305-4fd0-a65e-e26d1cbfa6cb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vYwqnk:/tmp/ceph-asok.vYwqnk', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fda5045c-a305-4fd0-a65e-e26d1cbfa6cb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vYwqnk:/tmp/ceph-asok.vYwqnk -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:38:05,123806532-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:38:05,127766751-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fda5045c-a305-4fd0-a65e-e26d1cbfa6cb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vYwqnk:/tmp/ceph-asok.vYwqnk', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fda5045c-a305-4fd0-a65e-e26d1cbfa6cb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vYwqnk:/tmp/ceph-asok.vYwqnk -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:38:09,621896172-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:38:09,625405294-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fda5045c-a305-4fd0-a65e-e26d1cbfa6cb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vYwqnk:/tmp/ceph-asok.vYwqnk', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fda5045c-a305-4fd0-a65e-e26d1cbfa6cb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vYwqnk:/tmp/ceph-asok.vYwqnk -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:38:13,961312539-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:38:13,965078713-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fda5045c-a305-4fd0-a65e-e26d1cbfa6cb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vYwqnk:/tmp/ceph-asok.vYwqnk', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fda5045c-a305-4fd0-a65e-e26d1cbfa6cb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vYwqnk:/tmp/ceph-asok.vYwqnk -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:38:17,502672653-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:38:17,506275501-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fda5045c-a305-4fd0-a65e-e26d1cbfa6cb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vYwqnk:/tmp/ceph-asok.vYwqnk', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fda5045c-a305-4fd0-a65e-e26d1cbfa6cb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vYwqnk:/tmp/ceph-asok.vYwqnk -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:38:21,050053749-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:38:21,054139905-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fda5045c-a305-4fd0-a65e-e26d1cbfa6cb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vYwqnk:/tmp/ceph-asok.vYwqnk', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fda5045c-a305-4fd0-a65e-e26d1cbfa6cb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vYwqnk:/tmp/ceph-asok.vYwqnk -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:38:25,433442890-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:38:25,437462571-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fda5045c-a305-4fd0-a65e-e26d1cbfa6cb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vYwqnk:/tmp/ceph-asok.vYwqnk', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fda5045c-a305-4fd0-a65e-e26d1cbfa6cb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vYwqnk:/tmp/ceph-asok.vYwqnk -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:38:28,508000468-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:38:28,511971537-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fda5045c-a305-4fd0-a65e-e26d1cbfa6cb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vYwqnk:/tmp/ceph-asok.vYwqnk', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fda5045c-a305-4fd0-a65e-e26d1cbfa6cb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vYwqnk:/tmp/ceph-asok.vYwqnk -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  155 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:38:31,869875877-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:38:55,148062779-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (9s)
      [=========================...] 
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:39:03,348165561-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:39:11,868193782-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:39:20,261461409-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:39:28,596976844-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:39:36,796483843-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:39:45,074920345-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   234 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:39:45,082926785-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:39:53,282762309-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:39:53,290613648-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:40:01,728451581-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:40:01,736451208-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:40:10,008429109-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:40:10,016147728-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:40:18,327911321-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:40:18,335584705-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:40:18,341881313-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:40:18,345521962-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:40:18,353073848-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=385270
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_16KB_write.dat.3 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:40:18,358748438-04:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_16KB_write.log.3
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:40:18,378399172-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:40:18,381920446-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fda5045c-a305-4fd0-a65e-e26d1cbfa6cb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vYwqnk:/tmp/ceph-asok.vYwqnk', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fda5045c-a305-4fd0-a65e-e26d1cbfa6cb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vYwqnk:/tmp/ceph-asok.vYwqnk -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T16:40:20.976+0000 7f081b9d4d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:40:20.980+0000 7f081b9d4d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:40:20.980+0000 7f081b9d4d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T16:40:20.993675+0000 Maintaining 256 concurrent writes of 16384 bytes to objects of size 16384 for up to 20 seconds or 0 objects
2021-05-17T16:40:20.993683+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-17T16:40:20.995667+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T16:40:20.995667+0000     0       0         0         0         0         0           -           0
2021-05-17T16:40:21.995752+0000     1     255     19889     19634   306.754   306.781  0.00128318   0.0123748
2021-05-17T16:40:22.995873+0000     2     255     38315     38060   297.313   287.906  0.00214043   0.0131093
2021-05-17T16:40:23.995952+0000     3     255     56014     55759   290.384   276.547  0.00166584   0.0135245
2021-05-17T16:40:24.996069+0000     4     255     73893     73638   287.619   279.359  0.00330963   0.0137079
2021-05-17T16:40:25.996138+0000     5     255     91537     91282   285.229   275.688  0.00132473   0.0138804
2021-05-17T16:40:26.996245+0000     6     256    108670    108414   282.301   267.688     0.12101    0.014032
2021-05-17T16:40:27.996315+0000     7     255    126669    126414   282.148    281.25   0.0012488   0.0140696
2021-05-17T16:40:28.996419+0000     8     255    144766    144511   282.221   282.766  0.00170615   0.0140822
2021-05-17T16:40:29.996509+0000     9     255    162756    162501   282.093   281.094    0.114577   0.0141023
2021-05-17T16:40:30.996626+0000    10     255    181077    180822   282.507   286.266  0.00122388   0.0140852
2021-05-17T16:40:31.996697+0000    11     256    198844    198588   282.059   277.594   0.0134454   0.0141417
2021-05-17T16:40:32.996768+0000    12     256    216927    216671   282.098   282.547  0.00135085   0.0141488
2021-05-17T16:40:33.996838+0000    13     255    233997    233742   280.915   266.734  0.00841501   0.0142188
2021-05-17T16:40:34.996944+0000    14     255    252181    251926   281.142   284.125  0.00161764   0.0141872
2021-05-17T16:40:35.997015+0000    15     256    269705    269449   280.651   273.797  0.00125953   0.0142037
2021-05-17T16:40:36.997085+0000    16     255    286820    286565   279.824   267.438  0.00179129   0.0142473
2021-05-17T16:40:37.997161+0000    17     256    303879    303623   279.041   266.531  0.00118033   0.0142904
2021-05-17T16:40:38.997278+0000    18     256    321243    320987    278.61   271.312  0.00154041   0.0143132
2021-05-17T16:40:39.997347+0000    19     256    335783    335527   275.902   227.188  0.00130007   0.0144286
2021-05-17T16:40:40.997418+0000 min lat: 0.000613091 max lat: 0.202727 avg lat: 0.014546
2021-05-17T16:40:40.997418+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T16:40:40.997418+0000    20     254    350935    350681   273.945   236.781   0.0909323    0.014546
2021-05-17T16:40:41.997551+0000 Total time run:         20.0512
Total writes made:      350935
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     273.468
Stddev Bandwidth:       17.168
Max bandwidth (MB/sec): 306.781
Min bandwidth (MB/sec): 227.188
Average IOPS:           17501
Stddev IOPS:            1098.75
Max IOPS:               19634
Min IOPS:               14540
Average Latency(s):     0.0146071
Stddev Latency(s):      0.0346482
Max latency(s):         0.202727
Min latency(s):         0.000613091

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:40:42,542018677-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 385270


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:40:42,546703337-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:41:05,878659845-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 350.94k objects, 5.4 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:41:05,886833740-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:41:14,125322275-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 350.94k objects, 5.4 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:41:14,133403165-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:41:22,495691302-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 350.94k objects, 5.4 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:41:22,503919668-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:41:30,935008739-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 350.94k objects, 5.4 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:41:30,942780137-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:41:39,254785249-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 350.94k objects, 5.4 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:41:39,262991344-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:41:39,269474613-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:41:39,273425655-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:41:39,281104389-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=386623
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_16KB_seq.dat.3 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:41:39,287049266-04:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_16KB_seq.log.3
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:41:39,306663099-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:41:39,310063186-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fda5045c-a305-4fd0-a65e-e26d1cbfa6cb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vYwqnk:/tmp/ceph-asok.vYwqnk', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fda5045c-a305-4fd0-a65e-e26d1cbfa6cb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vYwqnk:/tmp/ceph-asok.vYwqnk -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T16:41:41.585+0000 7f08ebc4cd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:41:41.589+0000 7f08ebc4cd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:41:41.589+0000 7f08ebc4cd00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T16:41:41.604126+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T16:41:41.604126+0000     0       0         0         0         0         0           -           0
2021-05-17T16:41:42.604220+0000     1     256     51392     51136   798.879       799 0.000257755  0.00489971
2021-05-17T16:41:43.604276+0000     2     255     96017     95762   748.063   697.281   0.0054282  0.00533369
2021-05-17T16:41:44.604325+0000     3     256    140368    140112   729.688   692.969 0.000319921  0.00540481
2021-05-17T16:41:45.604382+0000     4     255    183876    183621   717.213   679.828  0.00781146  0.00556788
2021-05-17T16:41:46.604464+0000     5     255    228326    228071   712.665   694.531  0.00945936   0.0056034
2021-05-17T16:41:47.604506+0000     6     256    272469    272213   708.836   689.719 0.000386215  0.00561306
2021-05-17T16:41:48.604543+0000     7     256    315400    315144   703.399   670.797   0.0002931  0.00564408
2021-05-17T16:41:49.604616+0000 Total time run:       7.78601
Total reads made:     350935
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   704.258
Average IOPS:         45072
Stddev IOPS:          2761.41
Max IOPS:             51136
Min IOPS:             42931
Average Latency(s):   0.00567375
Max latency(s):       0.0807332
Min latency(s):       0.000242065

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:41:50,173663191-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 386623


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:41:50,177699162-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:42:13,675907030-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 350.94k objects, 5.4 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:42:13,683566147-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:42:22,181725024-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 350.94k objects, 5.4 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:42:22,189719371-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:42:30,492671777-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 350.94k objects, 5.4 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:42:30,500546309-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:42:38,770582297-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 350.94k objects, 5.4 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:42:38,778883870-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:42:46,974684585-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 350.94k objects, 5.4 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:42:46,982664405-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:42:46,989687147-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T12:42:46,991797521-04:00][RUNNING][ROUND 4/2/40] object_size=16KB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:42:46,995736320-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:42:47,005003428-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40013\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.366937\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 0f5cc8c2-fa4e-4f46-acc7-cf3da4e5af5e\nsetting min_mon_release = octopus\nepoch 0\nfsid 0f5cc8c2-fa4e-4f46-acc7-cf3da4e5af5e\nlast_changed 2021-05-17T09:43:16.015227-0700\ncreated 2021-05-17T09:43:16.015227-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40013/0,v1:10.10.1.2:40014/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.366937 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 f4fd867e-785a-4ef5-9411-b0bed34465fb\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 7ff64d3a-1dfe-4b4e-acb3-543d332be61e\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 99398cb4-3a4d-44d8-ab44-1f69eb8bd9df\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\n'
10.10.1.2: b'restful urls: https://10.10.1.2:42013\n  w/ user/pass: admin / f61a6393-5177-4239-b35a-b4b3c5119040\n'
10.10.1.2: b'\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\nCEPH_DEV=1\n'
[1] 12:43:29 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40013
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.366937
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 0f5cc8c2-fa4e-4f46-acc7-cf3da4e5af5e
setting min_mon_release = octopus
epoch 0
fsid 0f5cc8c2-fa4e-4f46-acc7-cf3da4e5af5e
last_changed 2021-05-17T09:43:16.015227-0700
created 2021-05-17T09:43:16.015227-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40013/0,v1:10.10.1.2:40014/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.366937 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 f4fd867e-785a-4ef5-9411-b0bed34465fb
0
start osd.0
add osd1 7ff64d3a-1dfe-4b4e-acb3-543d332be61e
1
start osd.1
add osd2 99398cb4-3a4d-44d8-ab44-1f69eb8bd9df
2
start osd.2


restful urls: https://10.10.1.2:42013
  w/ user/pass: admin / f61a6393-5177-4239-b35a-b4b3c5119040


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T09:42:47.942-0700 7f79ae10f1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:42:47.942-0700 7f79ae10f1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:42:47.958-0700 7f71a9aac1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:42:47.958-0700 7f71a9aac1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40013,v1:10.10.1.2:40014] --print /tmp/ceph_monmap.366937 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.366937 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.366937 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42013 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.n24MHHvydo 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new f4fd867e-785a-4ef5-9411-b0bed34465fb -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAsnaJgXghMMhAANhS4/qagqC9+hvOt0TfAwQ== --osd-uuid f4fd867e-785a-4ef5-9411-b0bed34465fb 
2021-05-17T09:43:25.195-0700 7f8a9103ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T09:43:25.195-0700 7f8a9103ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T09:43:25.195-0700 7f8a9103ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T09:43:25.275-0700 7f8a9103ff00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 7ff64d3a-1dfe-4b4e-acb3-543d332be61e -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T09:43:25.687-0700 7f7f9db60f00 -1 Falling back to public interface
2021-05-17T09:43:25.699-0700 7f7f9db60f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAtnaJgphfTKBAAdrm8O6gAN9z9mBge9KOCGw== --osd-uuid 7ff64d3a-1dfe-4b4e-acb3-543d332be61e 
2021-05-17T09:43:26.023-0700 7f8547a25f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T09:43:26.023-0700 7f8547a25f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T09:43:26.023-0700 7f8547a25f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T09:43:26.067-0700 7f8547a25f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 99398cb4-3a4d-44d8-ab44-1f69eb8bd9df -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T09:43:26.475-0700 7ff2dd507f00 -1 Falling back to public interface
2021-05-17T09:43:26.491-0700 7ff2dd507f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAunaJggttuHBAA8Ok3ndC53Jmifc+lKEMJmQ== --osd-uuid 99398cb4-3a4d-44d8-ab44-1f69eb8bd9df 
2021-05-17T09:43:26.875-0700 7f5d4a2e8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T09:43:26.875-0700 7f5d4a2e8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T09:43:26.875-0700 7f5d4a2e8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T09:43:26.939-0700 7f5d4a2e8f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T09:43:27.199-0700 7fd56d7b1f00 -1 Falling back to public interface
2021-05-17T09:43:27.215-0700 7fd56d7b1f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:43:29,298674827-04:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:43:29,302451351-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:43:29,344081041-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:43:29,347742989-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2e0c0aad-df51-4fc3-adf9-92b706d95f01', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fp3M1V:/tmp/ceph-asok.Fp3M1V', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2e0c0aad-df51-4fc3-adf9-92b706d95f01 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fp3M1V:/tmp/ceph-asok.Fp3M1V -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:43:32,676304884-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:43:32,680031615-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2e0c0aad-df51-4fc3-adf9-92b706d95f01', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fp3M1V:/tmp/ceph-asok.Fp3M1V', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2e0c0aad-df51-4fc3-adf9-92b706d95f01 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fp3M1V:/tmp/ceph-asok.Fp3M1V -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:43:35,914949684-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:43:35,918763779-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2e0c0aad-df51-4fc3-adf9-92b706d95f01', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fp3M1V:/tmp/ceph-asok.Fp3M1V', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2e0c0aad-df51-4fc3-adf9-92b706d95f01 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fp3M1V:/tmp/ceph-asok.Fp3M1V -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:43:39,169757727-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:43:39,173797566-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2e0c0aad-df51-4fc3-adf9-92b706d95f01', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fp3M1V:/tmp/ceph-asok.Fp3M1V', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2e0c0aad-df51-4fc3-adf9-92b706d95f01 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fp3M1V:/tmp/ceph-asok.Fp3M1V -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:43:45,581962853-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:43:45,586144938-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2e0c0aad-df51-4fc3-adf9-92b706d95f01', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fp3M1V:/tmp/ceph-asok.Fp3M1V', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2e0c0aad-df51-4fc3-adf9-92b706d95f01 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fp3M1V:/tmp/ceph-asok.Fp3M1V -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:43:49,037418456-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:43:49,040941474-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2e0c0aad-df51-4fc3-adf9-92b706d95f01', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fp3M1V:/tmp/ceph-asok.Fp3M1V', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2e0c0aad-df51-4fc3-adf9-92b706d95f01 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fp3M1V:/tmp/ceph-asok.Fp3M1V -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:43:52,483871318-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:43:52,487338040-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2e0c0aad-df51-4fc3-adf9-92b706d95f01', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fp3M1V:/tmp/ceph-asok.Fp3M1V', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2e0c0aad-df51-4fc3-adf9-92b706d95f01 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fp3M1V:/tmp/ceph-asok.Fp3M1V -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:43:56,898126159-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:43:56,902106185-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2e0c0aad-df51-4fc3-adf9-92b706d95f01', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fp3M1V:/tmp/ceph-asok.Fp3M1V', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2e0c0aad-df51-4fc3-adf9-92b706d95f01 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fp3M1V:/tmp/ceph-asok.Fp3M1V -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:44:00,322226816-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:44:00,325797814-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2e0c0aad-df51-4fc3-adf9-92b706d95f01', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fp3M1V:/tmp/ceph-asok.Fp3M1V', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2e0c0aad-df51-4fc3-adf9-92b706d95f01 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fp3M1V:/tmp/ceph-asok.Fp3M1V -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:44:03,844484878-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:44:03,848170662-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2e0c0aad-df51-4fc3-adf9-92b706d95f01', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fp3M1V:/tmp/ceph-asok.Fp3M1V', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2e0c0aad-df51-4fc3-adf9-92b706d95f01 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fp3M1V:/tmp/ceph-asok.Fp3M1V -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:44:08,379726231-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:44:08,382969193-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2e0c0aad-df51-4fc3-adf9-92b706d95f01', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fp3M1V:/tmp/ceph-asok.Fp3M1V', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2e0c0aad-df51-4fc3-adf9-92b706d95f01 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fp3M1V:/tmp/ceph-asok.Fp3M1V -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:44:11,654237803-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:44:11,658269977-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2e0c0aad-df51-4fc3-adf9-92b706d95f01', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fp3M1V:/tmp/ceph-asok.Fp3M1V', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2e0c0aad-df51-4fc3-adf9-92b706d95f01 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fp3M1V:/tmp/ceph-asok.Fp3M1V -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:44:14,868557878-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:44:38,112451658-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   203 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
    Global Recovery Event (5s)
      [=============...............] (remaining: 5s)
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:44:46,516240555-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:44:54,704700138-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:45:03,088627804-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:45:11,402095990-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:45:19,634523718-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:45:27,939422305-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   213 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:45:27,948414837-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:45:36,210127084-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:45:36,218289176-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:45:44,492843831-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:45:44,500873164-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:45:52,804613017-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:45:52,812751575-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:46:01,052489607-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:46:01,060909153-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:46:01,067804576-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:46:01,071770816-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:46:01,079288217-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=393196
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_16KB_write.dat.4 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:46:01,085492292-04:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_16KB_write.log.4
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:46:01,105568735-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:46:01,109248908-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2e0c0aad-df51-4fc3-adf9-92b706d95f01', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fp3M1V:/tmp/ceph-asok.Fp3M1V', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2e0c0aad-df51-4fc3-adf9-92b706d95f01 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fp3M1V:/tmp/ceph-asok.Fp3M1V -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T16:46:03.562+0000 7f5f7905cd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:46:03.566+0000 7f5f7905cd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:46:03.566+0000 7f5f7905cd00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T16:46:03.576866+0000 Maintaining 256 concurrent writes of 16384 bytes to objects of size 16384 for up to 20 seconds or 0 objects
2021-05-17T16:46:03.576874+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-17T16:46:03.578818+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T16:46:03.578818+0000     0       0         0         0         0         0           -           0
2021-05-17T16:46:04.578918+0000     1     255     20876     20621    322.17   322.203   0.0724694   0.0120901
2021-05-17T16:46:05.578993+0000     2     256     41082     40826   318.925   315.703   0.0234131   0.0123482
2021-05-17T16:46:06.579064+0000     3     256     61285     61029   317.833   315.672  0.00175815   0.0124784
2021-05-17T16:46:07.579144+0000     4     256     81521     81265   317.415   316.188  0.00219841   0.0125162
2021-05-17T16:46:08.579211+0000     5     255    101787    101532   317.262   316.672  0.00287411   0.0125512
2021-05-17T16:46:09.579278+0000     6     255    122024    121769   317.082   316.203  0.00273262   0.0125534
2021-05-17T16:46:10.579344+0000     7     255    141824    141569   315.978   309.375  0.00168406   0.0125945
2021-05-17T16:46:11.579434+0000     8     255    157929    157674   307.933   251.641  0.00163528   0.0129678
2021-05-17T16:46:12.579508+0000     9     256    177882    177626   308.355    311.75  0.00567531   0.0129349
2021-05-17T16:46:13.579576+0000    10     255    195685    195430   305.336   278.188  0.00882812   0.0130499
2021-05-17T16:46:14.579642+0000    11     256    214449    214193   304.229   293.172   0.0113365   0.0131179
2021-05-17T16:46:15.579722+0000    12     255    230701    230446   300.037   253.953  0.00121032   0.0133003
2021-05-17T16:46:16.579795+0000    13     255    249004    248749   298.955   285.984   0.0009475    0.013325
2021-05-17T16:46:17.579899+0000    14     255    266946    266691   297.623   280.344  0.00111013   0.0133923
2021-05-17T16:46:18.579991+0000    15     255    284061    283806   295.608   267.422  0.00178042   0.0135135
2021-05-17T16:46:19.580056+0000    16     256    301267    301011   293.933   268.828   0.0949966   0.0135776
2021-05-17T16:46:20.580101+0000    17     255    318530    318275    292.51    269.75  0.00151107   0.0136515
2021-05-17T16:46:21.580144+0000    18     256    336945    336689   292.243   287.719   0.0607075   0.0136462
2021-05-17T16:46:22.580177+0000    19     256    356174    355918   292.675   300.453   0.0435268   0.0136443
2021-05-17T16:46:23.580224+0000 min lat: 0.000637397 max lat: 0.20556 avg lat: 0.0137719
2021-05-17T16:46:23.580224+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T16:46:23.580224+0000    20     225    371445    371220   289.995   239.094   0.0811833   0.0137719
2021-05-17T16:46:24.580327+0000 Total time run:         20.051
Total writes made:      371445
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     289.453
Stddev Bandwidth:       25.5392
Max bandwidth (MB/sec): 322.203
Min bandwidth (MB/sec): 239.094
Average IOPS:           18524
Stddev IOPS:            1634.51
Max IOPS:               20621
Min IOPS:               15302
Average Latency(s):     0.0137936
Stddev Latency(s):      0.0255098
Max latency(s):         0.20556
Min latency(s):         0.000637397

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:46:25,041548094-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 393196


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:46:25,045959490-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:46:48,170045236-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 371.45k objects, 5.7 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:46:48,178692290-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:46:56,467768901-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 371.45k objects, 5.7 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:46:56,475914543-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:47:04,760200369-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 371.45k objects, 5.7 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:47:04,768240012-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:47:13,039848828-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 371.45k objects, 5.7 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:47:13,048151385-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:47:21,557178891-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 371.45k objects, 5.7 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:47:21,565039286-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:47:21,571454036-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:47:21,574820119-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:47:21,582931487-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=394568
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_16KB_seq.dat.4 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:47:21,588902583-04:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_16KB_seq.log.4
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:47:21,607609573-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:47:21,611141728-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2e0c0aad-df51-4fc3-adf9-92b706d95f01', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fp3M1V:/tmp/ceph-asok.Fp3M1V', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2e0c0aad-df51-4fc3-adf9-92b706d95f01 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fp3M1V:/tmp/ceph-asok.Fp3M1V -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T16:47:24.070+0000 7faf79d6fd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:47:24.078+0000 7faf79d6fd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:47:24.078+0000 7faf79d6fd00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T16:47:24.090434+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T16:47:24.090434+0000     0       0         0         0         0         0           -           0
2021-05-17T16:47:25.090568+0000     1     256     44802     44546   695.889   696.031      0.1326  0.00504757
2021-05-17T16:47:26.090650+0000     2     255     88669     88414   690.635   685.438  0.00811769  0.00577326
2021-05-17T16:47:27.090766+0000     3     255    133263    133008   692.657   696.781  0.00304981  0.00576236
2021-05-17T16:47:28.090879+0000     4     255    171312    171057   668.105   594.516  0.00613877  0.00597636
2021-05-17T16:47:29.090953+0000     5     255    207492    207237   647.539   565.312  0.00292231  0.00616597
2021-05-17T16:47:30.091028+0000     6     255    251255    251000   653.574   683.797  0.00624504    0.006111
2021-05-17T16:47:31.091130+0000     7     255    293720    293465   654.984   663.516  0.00401587   0.0060993
2021-05-17T16:47:32.091222+0000     8     255    332859    332604   649.547   611.547  0.00352338  0.00615101
2021-05-17T16:47:33.091337+0000 Total time run:       8.89955
Total reads made:     371445
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   652.149
Average IOPS:         41737
Stddev IOPS:          3300.46
Max IOPS:             44594
Min IOPS:             36180
Average Latency(s):   0.00612778
Max latency(s):       0.156675
Min latency(s):       0.000238688

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:47:33,682184090-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 394568


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:47:33,686318987-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:47:56,752559140-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 371.45k objects, 5.7 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:47:56,760720811-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:48:05,315772866-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 371.45k objects, 5.7 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:48:05,323878462-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:48:13,663340691-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 371.45k objects, 5.7 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:48:13,671765928-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:48:21,842354526-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 371.45k objects, 5.7 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:48:21,850074167-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:48:30,140901645-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 371.45k objects, 5.7 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:48:30,149582211-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:48:30,155872037-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T12:48:30,158036783-04:00][RUNNING][ROUND 5/2/40] object_size=16KB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:48:30,161802878-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:48:30,170573282-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40981\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.368052\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid e26ca041-99f0-44d5-95fc-bee8e140bb30\nsetting min_mon_release = octopus\nepoch 0\nfsid e26ca041-99f0-44d5-95fc-bee8e140bb30\nlast_changed 2021-05-17T09:48:56.832094-0700\ncreated 2021-05-17T09:48:56.832094-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40981/0,v1:10.10.1.2:40982/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.368052 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 800f5228-5007-4fee-ac03-557b1f1fefd5\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 51056c96-5ef6-4652-8a78-7f08f5058b3e\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 37f82c2d-93d1-4b72-a43e-764b721052ee\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42981\n  w/ user/pass: admin / 57adb6fb-1a85-4a51-beaa-f4081f6dad5a\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\nCEPH_DEV=1\n'
[1] 12:49:09 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40981
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.368052
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid e26ca041-99f0-44d5-95fc-bee8e140bb30
setting min_mon_release = octopus
epoch 0
fsid e26ca041-99f0-44d5-95fc-bee8e140bb30
last_changed 2021-05-17T09:48:56.832094-0700
created 2021-05-17T09:48:56.832094-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40981/0,v1:10.10.1.2:40982/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.368052 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 800f5228-5007-4fee-ac03-557b1f1fefd5
0
start osd.0
add osd1 51056c96-5ef6-4652-8a78-7f08f5058b3e
1
start osd.1
add osd2 37f82c2d-93d1-4b72-a43e-764b721052ee
2
start osd.2


restful urls: https://10.10.1.2:42981
  w/ user/pass: admin / 57adb6fb-1a85-4a51-beaa-f4081f6dad5a


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T09:48:31.089-0700 7f021f3ea1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:48:31.089-0700 7f021f3ea1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:48:31.105-0700 7fbb9b2011c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:48:31.105-0700 7fbb9b2011c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40981,v1:10.10.1.2:40982] --print /tmp/ceph_monmap.368052 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.368052 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.368052 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42981 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.dFO6hU4oTh 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 800f5228-5007-4fee-ac03-557b1f1fefd5 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCBnqJgYSHcKRAA8ooBfUnyMC5MFop6B6jeng== --osd-uuid 800f5228-5007-4fee-ac03-557b1f1fefd5 
2021-05-17T09:49:06.094-0700 7fdd1f138f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T09:49:06.094-0700 7fdd1f138f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T09:49:06.094-0700 7fdd1f138f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T09:49:06.150-0700 7fdd1f138f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 51056c96-5ef6-4652-8a78-7f08f5058b3e -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T09:49:06.442-0700 7f0359adbf00 -1 Falling back to public interface
2021-05-17T09:49:06.454-0700 7f0359adbf00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCCnqJgddZmGhAAPnRl87MMoN2Vnj9EdoHuLw== --osd-uuid 51056c96-5ef6-4652-8a78-7f08f5058b3e 
2021-05-17T09:49:06.774-0700 7fddcd01ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T09:49:06.774-0700 7fddcd01ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T09:49:06.774-0700 7fddcd01ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T09:49:06.818-0700 7fddcd01ff00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 37f82c2d-93d1-4b72-a43e-764b721052ee -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T09:49:07.206-0700 7f5068b20f00 -1 Falling back to public interface
2021-05-17T09:49:07.226-0700 7f5068b20f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCDnqJg9h4fDBAAvymOd0Sl1oebbT5Ras2lnQ== --osd-uuid 37f82c2d-93d1-4b72-a43e-764b721052ee 
2021-05-17T09:49:07.534-0700 7fd59bcc9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T09:49:07.534-0700 7fd59bcc9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T09:49:07.534-0700 7fd59bcc9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T09:49:07.622-0700 7fd59bcc9f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T09:49:07.898-0700 7f637e3e8f00 -1 Falling back to public interface
2021-05-17T09:49:07.914-0700 7f637e3e8f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:49:09,981941276-04:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:49:09,985907016-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:49:10,026005079-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:49:10,029478574-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd1afecab-f2ab-4ac9-8130-53bf4107a70e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lcboGe:/tmp/ceph-asok.lcboGe', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d1afecab-f2ab-4ac9-8130-53bf4107a70e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lcboGe:/tmp/ceph-asok.lcboGe -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:49:13,458612683-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:49:13,462407672-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd1afecab-f2ab-4ac9-8130-53bf4107a70e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lcboGe:/tmp/ceph-asok.lcboGe', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d1afecab-f2ab-4ac9-8130-53bf4107a70e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lcboGe:/tmp/ceph-asok.lcboGe -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:49:16,840101582-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:49:16,843882434-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd1afecab-f2ab-4ac9-8130-53bf4107a70e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lcboGe:/tmp/ceph-asok.lcboGe', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d1afecab-f2ab-4ac9-8130-53bf4107a70e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lcboGe:/tmp/ceph-asok.lcboGe -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:49:20,271414855-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:49:20,275158408-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd1afecab-f2ab-4ac9-8130-53bf4107a70e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lcboGe:/tmp/ceph-asok.lcboGe', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d1afecab-f2ab-4ac9-8130-53bf4107a70e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lcboGe:/tmp/ceph-asok.lcboGe -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:49:26,918956412-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:49:26,922866517-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd1afecab-f2ab-4ac9-8130-53bf4107a70e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lcboGe:/tmp/ceph-asok.lcboGe', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d1afecab-f2ab-4ac9-8130-53bf4107a70e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lcboGe:/tmp/ceph-asok.lcboGe -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:49:30,568217047-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:49:30,572027916-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd1afecab-f2ab-4ac9-8130-53bf4107a70e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lcboGe:/tmp/ceph-asok.lcboGe', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d1afecab-f2ab-4ac9-8130-53bf4107a70e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lcboGe:/tmp/ceph-asok.lcboGe -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:49:35,006171561-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:49:35,009576016-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd1afecab-f2ab-4ac9-8130-53bf4107a70e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lcboGe:/tmp/ceph-asok.lcboGe', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d1afecab-f2ab-4ac9-8130-53bf4107a70e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lcboGe:/tmp/ceph-asok.lcboGe -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:49:38,803311096-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:49:38,807184712-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd1afecab-f2ab-4ac9-8130-53bf4107a70e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lcboGe:/tmp/ceph-asok.lcboGe', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d1afecab-f2ab-4ac9-8130-53bf4107a70e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lcboGe:/tmp/ceph-asok.lcboGe -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:49:42,759664402-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:49:42,763597180-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd1afecab-f2ab-4ac9-8130-53bf4107a70e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lcboGe:/tmp/ceph-asok.lcboGe', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d1afecab-f2ab-4ac9-8130-53bf4107a70e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lcboGe:/tmp/ceph-asok.lcboGe -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:49:46,551331348-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:49:46,554787711-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd1afecab-f2ab-4ac9-8130-53bf4107a70e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lcboGe:/tmp/ceph-asok.lcboGe', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d1afecab-f2ab-4ac9-8130-53bf4107a70e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lcboGe:/tmp/ceph-asok.lcboGe -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:49:51,133937031-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:49:51,138006595-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd1afecab-f2ab-4ac9-8130-53bf4107a70e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lcboGe:/tmp/ceph-asok.lcboGe', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d1afecab-f2ab-4ac9-8130-53bf4107a70e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lcboGe:/tmp/ceph-asok.lcboGe -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:49:54,623597986-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:49:54,627258793-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd1afecab-f2ab-4ac9-8130-53bf4107a70e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lcboGe:/tmp/ceph-asok.lcboGe', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d1afecab-f2ab-4ac9-8130-53bf4107a70e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lcboGe:/tmp/ceph-asok.lcboGe -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  154 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:49:57,894780134-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:50:20,865694488-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (9s)
      [=========================...] 
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:50:29,280672340-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:50:37,568724478-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:50:45,806971411-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:50:54,074238553-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:51:02,352276461-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:51:10,941216448-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   237 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:51:10,949493827-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:51:19,271554730-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:51:19,279871813-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:51:27,704757375-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:51:27,712900272-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:51:36,074762389-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:51:36,082937906-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:51:44,313406787-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:51:44,321683124-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:51:44,327845149-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:51:44,331416178-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:51:44,340040177-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=401187
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_16KB_write.dat.5 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:51:44,346233411-04:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_16KB_write.log.5
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:51:44,365722371-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:51:44,369509325-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd1afecab-f2ab-4ac9-8130-53bf4107a70e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lcboGe:/tmp/ceph-asok.lcboGe', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d1afecab-f2ab-4ac9-8130-53bf4107a70e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lcboGe:/tmp/ceph-asok.lcboGe -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T16:51:46.903+0000 7fa66d8ccd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:51:46.911+0000 7fa66d8ccd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:51:46.911+0000 7fa66d8ccd00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T16:51:46.919456+0000 Maintaining 256 concurrent writes of 16384 bytes to objects of size 16384 for up to 20 seconds or 0 objects
2021-05-17T16:51:46.919464+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-17T16:51:46.921574+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T16:51:46.921574+0000     0       0         0         0         0         0           -           0
2021-05-17T16:51:47.921668+0000     1     255     16986     16731   261.396   261.422  0.00114828   0.0146633
2021-05-17T16:51:48.921753+0000     2     255     35298     35043   273.748   286.125    0.111676   0.0142425
2021-05-17T16:51:49.921839+0000     3     256     54442     54186   282.193   299.109  0.00178808    0.013943
2021-05-17T16:51:50.921963+0000     4     256     73634     73378   286.605   299.875  0.00224661   0.0137781
2021-05-17T16:51:51.922045+0000     5     256     92160     91904   287.173   289.469  0.00168255    0.013787
2021-05-17T16:51:52.922124+0000     6     256    110324    110068   286.609   283.812  0.00248642   0.0138308
2021-05-17T16:51:53.922196+0000     7     256    126795    126539   282.428   257.359   0.0934839   0.0140855
2021-05-17T16:51:54.922306+0000     8     256    143388    143132   279.529   259.266  0.00632716   0.0142339
2021-05-17T16:51:55.922376+0000     9     256    159651    159395   276.703   254.109   0.0011246   0.0143642
2021-05-17T16:51:56.922451+0000    10     255    177915    177660   277.569   285.391  0.00429278   0.0143362
2021-05-17T16:51:57.922521+0000    11     255    195464    195209   277.261   274.203    0.110879   0.0143612
2021-05-17T16:51:58.922640+0000    12     256    214120    213864   278.444   291.484    0.148476   0.0142824
2021-05-17T16:51:59.922720+0000    13     255    231754    231499   278.219   275.547  0.00131785   0.0143242
2021-05-17T16:52:00.922790+0000    14     255    248748    248493   277.312   265.531  0.00235157   0.0143447
2021-05-17T16:52:01.922873+0000    15     255    266182    265927   276.983   272.406  0.00106416   0.0143837
2021-05-17T16:52:02.922992+0000    16     255    283497    283242   276.579   270.547  0.00199822   0.0144261
2021-05-17T16:52:03.923064+0000    17     255    301399    301144   276.762   279.719   0.0112184   0.0144243
2021-05-17T16:52:04.923135+0000    18     256    317080    316824   274.997       245  0.00116693   0.0145298
2021-05-17T16:52:05.923210+0000    19     255    335422    335167   275.607   286.609  0.00282351   0.0144958
2021-05-17T16:52:06.923320+0000 min lat: 0.000630264 max lat: 0.199926 avg lat: 0.0144772
2021-05-17T16:52:06.923320+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T16:52:06.923320+0000    20     221    353436    353215   275.925       282   0.0609526   0.0144772
2021-05-17T16:52:07.923748+0000 Total time run:         20.0428
Total writes made:      353436
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     275.532
Stddev Bandwidth:       15.1748
Max bandwidth (MB/sec): 299.875
Min bandwidth (MB/sec): 245
Average IOPS:           17634
Stddev IOPS:            971.187
Max IOPS:               19192
Min IOPS:               15680
Average Latency(s):     0.0144956
Stddev Latency(s):      0.0334233
Max latency(s):         0.199926
Min latency(s):         0.000630264

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:52:08,551728576-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 401187


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:52:08,556371628-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:52:31,690833550-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 353.44k objects, 5.4 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:52:31,698521171-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:52:39,994669536-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 353.44k objects, 5.4 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:52:40,002883155-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:52:48,556617057-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 353.44k objects, 5.4 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:52:48,564555108-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:52:56,786021532-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 353.44k objects, 5.4 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:52:56,793876498-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:53:05,524707944-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 353.44k objects, 5.4 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:53:05,532854647-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:53:05,538956880-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:53:05,542725730-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:53:05,551170533-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=402588
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_16KB_seq.dat.5 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:53:05,557257458-04:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_16KB_seq.log.5
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:53:05,575782918-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:53:05,579366599-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd1afecab-f2ab-4ac9-8130-53bf4107a70e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lcboGe:/tmp/ceph-asok.lcboGe', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d1afecab-f2ab-4ac9-8130-53bf4107a70e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.lcboGe:/tmp/ceph-asok.lcboGe -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T16:53:08.099+0000 7f3502987d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:53:08.371+0000 7f3502987d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:53:08.371+0000 7f3502987d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T16:53:08.384729+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T16:53:08.384729+0000     0       0         0         0         0         0           -           0
2021-05-17T16:53:09.384850+0000     1     255     50135     49880    779.22   779.375  0.00404042  0.00510611
2021-05-17T16:53:10.384932+0000     2     255     92947     92692   724.054   668.938   0.0056139  0.00551049
2021-05-17T16:53:11.385026+0000     3     256    128325    128069   666.942   552.766 0.000358343  0.00593791
2021-05-17T16:53:12.385135+0000     4     255    163077    162822   635.946   543.016  0.00537467  0.00627904
2021-05-17T16:53:13.385207+0000     5     255    204420    204165   637.945   645.984  0.00721845  0.00625907
2021-05-17T16:53:14.385278+0000     6     256    242068    241812   629.653   588.234 0.000515238  0.00634248
2021-05-17T16:53:15.385359+0000     7     256    278532    278276   621.089    569.75 0.000273433  0.00642146
2021-05-17T16:53:16.385441+0000     8     256    316197    315941   617.011   588.516 0.000252704  0.00644111
2021-05-17T16:53:17.385551+0000 Total time run:       8.97925
Total reads made:     353436
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   615.022
Average IOPS:         39361
Stddev IOPS:          5037.87
Max IOPS:             49880
Min IOPS:             34753
Average Latency(s):   0.00649632
Max latency(s):       0.129161
Min latency(s):       0.000239329

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:53:17,958275170-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 402588


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:53:17,962367358-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:53:41,186152343-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 353.44k objects, 5.4 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:53:41,194658412-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:53:49,573931149-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 353.44k objects, 5.4 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:53:49,582608690-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:53:57,850527141-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 353.44k objects, 5.4 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:53:57,858564810-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:54:06,248835093-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 353.44k objects, 5.4 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:54:06,256628543-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:54:14,553202792-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 353.44k objects, 5.4 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:54:14,561318247-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:54:14,568019646-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T12:54:14,571503961-04:00][RUNNING][ROUND 1/3/40] object_size=64KB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:54:14,575271869-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:54:14,583962985-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40353\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.369165\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid e8134bdd-c5dd-4dc1-8f3c-2ae04a3d3d2b\nsetting min_mon_release = octopus\nepoch 0\nfsid e8134bdd-c5dd-4dc1-8f3c-2ae04a3d3d2b\nlast_changed 2021-05-17T09:54:43.136346-0700\ncreated 2021-05-17T09:54:43.136346-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40353/0,v1:10.10.1.2:40354/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.369165 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 abc68b1a-1c10-40b8-9c11-845d786738d6\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 7f1ff0ea-3764-4450-b6ef-d4b6396f483a\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 5732c87a-1d47-4ce3-9238-6e358e6d2bdb\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42353\n  w/ user/pass: admin / f41b422d-0bf1-4065-8c53-7b935133f06c\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\nCEPH_DEV=1\n'
[1] 12:54:56 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40353
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.369165
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid e8134bdd-c5dd-4dc1-8f3c-2ae04a3d3d2b
setting min_mon_release = octopus
epoch 0
fsid e8134bdd-c5dd-4dc1-8f3c-2ae04a3d3d2b
last_changed 2021-05-17T09:54:43.136346-0700
created 2021-05-17T09:54:43.136346-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40353/0,v1:10.10.1.2:40354/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.369165 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 abc68b1a-1c10-40b8-9c11-845d786738d6
0
start osd.0
add osd1 7f1ff0ea-3764-4450-b6ef-d4b6396f483a
1
start osd.1
add osd2 5732c87a-1d47-4ce3-9238-6e358e6d2bdb
2
start osd.2


restful urls: https://10.10.1.2:42353
  w/ user/pass: admin / f41b422d-0bf1-4065-8c53-7b935133f06c


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T09:54:15.548-0700 7fe1889631c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:54:15.548-0700 7fe1889631c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:54:15.564-0700 7fed3851f1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:54:15.564-0700 7fed3851f1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40353,v1:10.10.1.2:40354] --print /tmp/ceph_monmap.369165 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.369165 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.369165 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42353 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.EO8w6bJQQU 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new abc68b1a-1c10-40b8-9c11-845d786738d6 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDcn6JgBSkFARAAyt0tSgAjttVxwjjxWCkAeQ== --osd-uuid abc68b1a-1c10-40b8-9c11-845d786738d6 
2021-05-17T09:54:52.377-0700 7fc79c575f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T09:54:52.377-0700 7fc79c575f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T09:54:52.377-0700 7fc79c575f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T09:54:52.453-0700 7fc79c575f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 7f1ff0ea-3764-4450-b6ef-d4b6396f483a -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T09:54:52.749-0700 7f77b277bf00 -1 Falling back to public interface
2021-05-17T09:54:52.761-0700 7f77b277bf00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDcn6JglWO8LBAA0GtiBDtLbm/MBhuTf3x9qQ== --osd-uuid 7f1ff0ea-3764-4450-b6ef-d4b6396f483a 
2021-05-17T09:54:53.085-0700 7f88ba856f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T09:54:53.085-0700 7f88ba856f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T09:54:53.085-0700 7f88ba856f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T09:54:53.133-0700 7f88ba856f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 5732c87a-1d47-4ce3-9238-6e358e6d2bdb -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T09:54:53.409-0700 7fb75cfa1f00 -1 Falling back to public interface
2021-05-17T09:54:53.421-0700 7fb75cfa1f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDdn6Jg1jeAGBAAcs0Y3XR+CpkHS1GArmRCHw== --osd-uuid 5732c87a-1d47-4ce3-9238-6e358e6d2bdb 
2021-05-17T09:54:53.757-0700 7f3539af9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T09:54:53.757-0700 7f3539af9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T09:54:53.757-0700 7f3539af9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T09:54:53.829-0700 7f3539af9f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T09:54:54.237-0700 7f4aa386bf00 -1 Falling back to public interface
2021-05-17T09:54:54.253-0700 7f4aa386bf00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:54:56,182571471-04:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:54:56,186484752-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:54:56,227707950-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:54:56,230821449-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f6812c5c-551c-4317-9a78-acd8d3d769dd', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zk8VuJ:/tmp/ceph-asok.zk8VuJ', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f6812c5c-551c-4317-9a78-acd8d3d769dd --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zk8VuJ:/tmp/ceph-asok.zk8VuJ -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:54:59,628143156-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:54:59,632197442-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f6812c5c-551c-4317-9a78-acd8d3d769dd', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zk8VuJ:/tmp/ceph-asok.zk8VuJ', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f6812c5c-551c-4317-9a78-acd8d3d769dd --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zk8VuJ:/tmp/ceph-asok.zk8VuJ -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:55:02,995416228-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:55:02,998964824-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f6812c5c-551c-4317-9a78-acd8d3d769dd', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zk8VuJ:/tmp/ceph-asok.zk8VuJ', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f6812c5c-551c-4317-9a78-acd8d3d769dd --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zk8VuJ:/tmp/ceph-asok.zk8VuJ -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:55:06,331856901-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:55:06,335638134-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f6812c5c-551c-4317-9a78-acd8d3d769dd', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zk8VuJ:/tmp/ceph-asok.zk8VuJ', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f6812c5c-551c-4317-9a78-acd8d3d769dd --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zk8VuJ:/tmp/ceph-asok.zk8VuJ -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:55:13,017756555-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:55:13,021552797-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f6812c5c-551c-4317-9a78-acd8d3d769dd', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zk8VuJ:/tmp/ceph-asok.zk8VuJ', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f6812c5c-551c-4317-9a78-acd8d3d769dd --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zk8VuJ:/tmp/ceph-asok.zk8VuJ -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:55:17,406704106-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:55:17,410556622-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f6812c5c-551c-4317-9a78-acd8d3d769dd', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zk8VuJ:/tmp/ceph-asok.zk8VuJ', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f6812c5c-551c-4317-9a78-acd8d3d769dd --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zk8VuJ:/tmp/ceph-asok.zk8VuJ -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:55:21,711487675-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:55:21,715401437-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f6812c5c-551c-4317-9a78-acd8d3d769dd', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zk8VuJ:/tmp/ceph-asok.zk8VuJ', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f6812c5c-551c-4317-9a78-acd8d3d769dd --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zk8VuJ:/tmp/ceph-asok.zk8VuJ -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:55:25,924341705-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:55:25,927981302-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f6812c5c-551c-4317-9a78-acd8d3d769dd', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zk8VuJ:/tmp/ceph-asok.zk8VuJ', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f6812c5c-551c-4317-9a78-acd8d3d769dd --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zk8VuJ:/tmp/ceph-asok.zk8VuJ -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:55:30,339556712-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:55:30,343687131-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f6812c5c-551c-4317-9a78-acd8d3d769dd', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zk8VuJ:/tmp/ceph-asok.zk8VuJ', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f6812c5c-551c-4317-9a78-acd8d3d769dd --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zk8VuJ:/tmp/ceph-asok.zk8VuJ -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:55:33,879771341-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:55:33,883905587-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f6812c5c-551c-4317-9a78-acd8d3d769dd', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zk8VuJ:/tmp/ceph-asok.zk8VuJ', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f6812c5c-551c-4317-9a78-acd8d3d769dd --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zk8VuJ:/tmp/ceph-asok.zk8VuJ -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:55:38,300714269-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:55:38,304755871-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f6812c5c-551c-4317-9a78-acd8d3d769dd', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zk8VuJ:/tmp/ceph-asok.zk8VuJ', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f6812c5c-551c-4317-9a78-acd8d3d769dd --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zk8VuJ:/tmp/ceph-asok.zk8VuJ -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:55:41,713408883-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:55:41,717082514-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f6812c5c-551c-4317-9a78-acd8d3d769dd', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zk8VuJ:/tmp/ceph-asok.zk8VuJ', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f6812c5c-551c-4317-9a78-acd8d3d769dd --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zk8VuJ:/tmp/ceph-asok.zk8VuJ -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  156 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:55:44,820437562-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:56:08,053345364-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (9s)
      [===================.........] (remaining: 4s)
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:56:16,393090776-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:56:24,798075855-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:56:33,108477411-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:56:41,932388542-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:56:50,727721784-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:56:59,584803795-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:56:59,592717070-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:57:08,079214710-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:57:08,087768559-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:57:16,559255215-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:57:16,567804624-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:57:24,846158352-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:57:24,854419441-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:57:33,143050909-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:57:33,151776500-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:57:33,157860419-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:57:33,161413243-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:57:33,169697805-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=409224
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_64KB_write.dat.1 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:57:33,175848570-04:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_64KB_write.log.1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:57:33,195121644-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:57:33,198438445-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f6812c5c-551c-4317-9a78-acd8d3d769dd', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zk8VuJ:/tmp/ceph-asok.zk8VuJ', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f6812c5c-551c-4317-9a78-acd8d3d769dd --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zk8VuJ:/tmp/ceph-asok.zk8VuJ -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T16:57:35.664+0000 7f391750bd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:57:35.668+0000 7f391750bd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:57:35.668+0000 7f391750bd00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T16:57:35.675633+0000 Maintaining 256 concurrent writes of 65536 bytes to objects of size 65536 for up to 20 seconds or 0 objects
2021-05-17T16:57:35.675642+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-17T16:57:35.680663+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T16:57:35.680663+0000     0       0         0         0         0         0           -           0
2021-05-17T16:57:36.680776+0000     1     256     15585     15329   958.004   958.062   0.0171672   0.0164954
2021-05-17T16:57:37.680851+0000     2     256     31405     31149    973.34    988.75   0.0891107   0.0162067
2021-05-17T16:57:38.680935+0000     3     256     46377     46121   960.784    935.75  0.00874698   0.0165634
2021-05-17T16:57:39.681034+0000     4     256     61952     61696   963.923   973.438  0.00373911    0.016545
2021-05-17T16:57:40.681110+0000     5     255     77865     77610   970.048   994.625  0.00707205    0.016448
2021-05-17T16:57:41.681180+0000     6     256     93448     93192   970.675   973.875  0.00452812     0.01644
2021-05-17T16:57:42.681257+0000     7     255    109256    109001   973.148   988.062   0.0104684   0.0164034
2021-05-17T16:57:43.681343+0000     8     256    125038    124782   974.783   986.312  0.00159447   0.0163634
2021-05-17T16:57:44.681430+0000     9     255    140795    140540   975.895   984.875   0.0423341   0.0163416
2021-05-17T16:57:45.681507+0000    10     256    156041    155785   973.579   952.812  0.00806901   0.0164168
2021-05-17T16:57:46.681592+0000    11     255    170771    170516   968.764   920.688   0.0204599   0.0164972
2021-05-17T16:57:47.681672+0000    12     255    184091    183836   957.403     832.5   0.0141508    0.016688
2021-05-17T16:57:48.681753+0000    13     255    198985    198730   955.356   930.875   0.0127603   0.0167325
2021-05-17T16:57:49.681822+0000    14     256    214166    213910    954.88    948.75   0.0114133   0.0167402
2021-05-17T16:57:50.681900+0000    15     256    228056    227800   949.092   868.125  0.00847242   0.0168417
2021-05-17T16:57:51.681985+0000    16     256    243632    243376   950.612     973.5  0.00284387   0.0168141
2021-05-17T16:57:52.682070+0000    17     256    257555    257299   945.877   870.188  0.00655242   0.0168997
2021-05-17T16:57:53.682140+0000    18     256    271541    271285   941.887   874.125  0.00555025   0.0169679
2021-05-17T16:57:54.682220+0000    19     256    284136    283880   933.742   787.188  0.00889599   0.0171226
2021-05-17T16:57:55.682303+0000 min lat: 0.000845719 max lat: 0.181977 avg lat: 0.0171835
2021-05-17T16:57:55.682303+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T16:57:55.682303+0000    20     229    297776    297547   929.761   854.188   0.0119439   0.0171835
2021-05-17T16:57:56.682461+0000 Total time run:         20.0658
Total writes made:      297776
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     927.5
Stddev Bandwidth:       61.0618
Max bandwidth (MB/sec): 994.625
Min bandwidth (MB/sec): 787.188
Average IOPS:           14839
Stddev IOPS:            976.989
Max IOPS:               15914
Min IOPS:               12595
Average Latency(s):     0.0172081
Stddev Latency(s):      0.0160792
Max latency(s):         0.181977
Min latency(s):         0.000845719

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:57:57,316845190-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 409224


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:57:57,321138114-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:58:20,464570560-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 297.78k objects, 18 GiB
    usage:   36 GiB used, 264 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:58:20,472640930-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:58:28,887215839-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 297.78k objects, 18 GiB
    usage:   36 GiB used, 264 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:58:28,896117591-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:58:37,484581286-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 297.78k objects, 18 GiB
    usage:   36 GiB used, 264 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:58:37,492716479-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:58:45,871568158-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 297.78k objects, 18 GiB
    usage:   36 GiB used, 264 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:58:45,880585457-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:58:54,037794743-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 297.78k objects, 18 GiB
    usage:   36 GiB used, 264 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:58:54,046626153-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:58:54,052781035-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:58:54,056345160-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:58:54,064483768-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=410599
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_64KB_seq.dat.1 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:58:54,070587845-04:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_64KB_seq.log.1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:58:54,089947332-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:58:54,093274763-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f6812c5c-551c-4317-9a78-acd8d3d769dd', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zk8VuJ:/tmp/ceph-asok.zk8VuJ', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f6812c5c-551c-4317-9a78-acd8d3d769dd --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.zk8VuJ:/tmp/ceph-asok.zk8VuJ -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T16:58:56.572+0000 7f73d7cbfd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:58:56.576+0000 7f73d7cbfd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T16:58:56.576+0000 7f73d7cbfd00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T16:58:56.592654+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T16:58:56.592654+0000     0      31        31         0         0         0           -           0
2021-05-17T16:58:57.592743+0000     1     255     37144     36889   2304.84   2305.56  0.00360545  0.00688476
2021-05-17T16:58:58.592837+0000     2     256     65649     65393   2043.12    1781.5    0.011712  0.00780283
2021-05-17T16:58:59.592947+0000     3     255     98100     97845   2038.09   2028.25  0.00129864  0.00782726
2021-05-17T16:59:00.593016+0000     4     255    129924    129669   2025.78      1989  0.00982012  0.00787721
2021-05-17T16:59:01.593085+0000     5     256    164373    164117   2051.19      2153 0.000747164  0.00778047
2021-05-17T16:59:02.593164+0000     6     255    193658    193403   2014.37   1830.38   0.0107638  0.00792608
2021-05-17T16:59:03.593243+0000     7     255    226068    225813   2015.95   2025.62  0.00994076  0.00792151
2021-05-17T16:59:04.593324+0000     8     255    260186    259931   2030.48   2132.38  0.00295126   0.0078645
2021-05-17T16:59:05.593416+0000     9     255    292642    292387   2030.24    2028.5  0.00954081  0.00786624
2021-05-17T16:59:06.593785+0000 Total time run:       9.16581
Total reads made:     297776
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   2030.48
Average IOPS:         32487
Stddev IOPS:          2560.31
Max IOPS:             36889
Min IOPS:             28504
Average Latency(s):   0.00786479
Max latency(s):       0.138839
Min latency(s):       0.000271761

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:59:07,182592297-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 410599


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:59:07,186986211-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:59:30,249184263-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 297.78k objects, 18 GiB
    usage:   36 GiB used, 264 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:59:30,257434501-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:59:38,561893466-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 297.78k objects, 18 GiB
    usage:   36 GiB used, 264 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:59:38,569534139-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:59:46,945121678-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 297.78k objects, 18 GiB
    usage:   36 GiB used, 264 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:59:46,953666680-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:59:55,260201793-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 297.78k objects, 18 GiB
    usage:   36 GiB used, 264 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T12:59:55,268246796-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:00:03,705454697-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 297.78k objects, 18 GiB
    usage:   36 GiB used, 264 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:00:03,713521701-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:00:03,720255549-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T13:00:03,722929493-04:00][RUNNING][ROUND 2/3/40] object_size=64KB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:00:03,726368863-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:00:03,734602209-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40063\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.370345\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid c0db87e1-334d-4d2b-af7d-d3476a8c3004\nsetting min_mon_release = octopus\nepoch 0\nfsid c0db87e1-334d-4d2b-af7d-d3476a8c3004\nlast_changed 2021-05-17T10:00:30.943164-0700\ncreated 2021-05-17T10:00:30.943164-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40063/0,v1:10.10.1.2:40064/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.370345 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 e77d899e-1a63-4e51-a564-c95e049d425d\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 c60ed97a-7394-458f-b96d-f19c45b75d55\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 4fbfded4-aee5-4de2-ad45-1d656112bc76\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42063\n  w/ user/pass: admin / c697bcee-9c56-40ee-b805-05953ae06dc1\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 13:00:43 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40063
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.370345
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid c0db87e1-334d-4d2b-af7d-d3476a8c3004
setting min_mon_release = octopus
epoch 0
fsid c0db87e1-334d-4d2b-af7d-d3476a8c3004
last_changed 2021-05-17T10:00:30.943164-0700
created 2021-05-17T10:00:30.943164-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40063/0,v1:10.10.1.2:40064/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.370345 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 e77d899e-1a63-4e51-a564-c95e049d425d
0
start osd.0
add osd1 c60ed97a-7394-458f-b96d-f19c45b75d55
1
start osd.1
add osd2 4fbfded4-aee5-4de2-ad45-1d656112bc76
2
start osd.2


restful urls: https://10.10.1.2:42063
  w/ user/pass: admin / c697bcee-9c56-40ee-b805-05953ae06dc1


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T10:00:04.691-0700 7f97abc241c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:00:04.695-0700 7f97abc241c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:00:04.711-0700 7f457b6801c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:00:04.711-0700 7f457b6801c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40063,v1:10.10.1.2:40064] --print /tmp/ceph_monmap.370345 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.370345 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.370345 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42063 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
../src/vstart.sh: line 28: 370825 Segmentation fault      PATH=$CEPH_BIN:$PATH "$@"
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.s5hZqjkM4I 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new e77d899e-1a63-4e51-a564-c95e049d425d -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQA3oaJgq9WJLxAAOOIJG2+AjVJxsFqLCeJBsw== --osd-uuid e77d899e-1a63-4e51-a564-c95e049d425d 
2021-05-17T10:00:40.136-0700 7f58d66b4f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T10:00:40.136-0700 7f58d66b4f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T10:00:40.136-0700 7f58d66b4f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T10:00:40.184-0700 7f58d66b4f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new c60ed97a-7394-458f-b96d-f19c45b75d55 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T10:00:40.488-0700 7fa57be7af00 -1 Falling back to public interface
2021-05-17T10:00:40.500-0700 7fa57be7af00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQA4oaJgy7UOHRAAXF5zdAVgh4jbFRYis8590Q== --osd-uuid c60ed97a-7394-458f-b96d-f19c45b75d55 
2021-05-17T10:00:40.816-0700 7f224d8fef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T10:00:40.816-0700 7f224d8fef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T10:00:40.816-0700 7f224d8fef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T10:00:40.860-0700 7f224d8fef00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 4fbfded4-aee5-4de2-ad45-1d656112bc76 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T10:00:41.204-0700 7fcc94cd6f00 -1 Falling back to public interface
2021-05-17T10:00:41.220-0700 7fcc94cd6f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQA5oaJgputmDBAAmn8Vv30Su1Vi8qV93ymK+w== --osd-uuid 4fbfded4-aee5-4de2-ad45-1d656112bc76 
2021-05-17T10:00:41.560-0700 7f352e5d6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T10:00:41.560-0700 7f352e5d6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T10:00:41.560-0700 7f352e5d6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T10:00:41.656-0700 7f352e5d6f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T10:00:42.008-0700 7f0407718f00 -1 Falling back to public interface
2021-05-17T10:00:42.024-0700 7f0407718f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:00:43,985939822-04:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:00:43,990602070-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:00:44,031374008-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:00:44,034820082-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cedd92ca-a8d8-4152-abd2-94bb697bd989', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.OHSFkU:/tmp/ceph-asok.OHSFkU', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cedd92ca-a8d8-4152-abd2-94bb697bd989 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.OHSFkU:/tmp/ceph-asok.OHSFkU -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:00:47,274916617-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:00:47,278693111-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cedd92ca-a8d8-4152-abd2-94bb697bd989', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.OHSFkU:/tmp/ceph-asok.OHSFkU', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cedd92ca-a8d8-4152-abd2-94bb697bd989 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.OHSFkU:/tmp/ceph-asok.OHSFkU -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:00:50,589815145-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:00:50,593388037-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cedd92ca-a8d8-4152-abd2-94bb697bd989', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.OHSFkU:/tmp/ceph-asok.OHSFkU', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cedd92ca-a8d8-4152-abd2-94bb697bd989 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.OHSFkU:/tmp/ceph-asok.OHSFkU -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:00:53,846899201-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:00:53,850433700-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cedd92ca-a8d8-4152-abd2-94bb697bd989', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.OHSFkU:/tmp/ceph-asok.OHSFkU', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cedd92ca-a8d8-4152-abd2-94bb697bd989 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.OHSFkU:/tmp/ceph-asok.OHSFkU -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:01:00,498453942-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:01:00,502023838-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cedd92ca-a8d8-4152-abd2-94bb697bd989', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.OHSFkU:/tmp/ceph-asok.OHSFkU', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cedd92ca-a8d8-4152-abd2-94bb697bd989 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.OHSFkU:/tmp/ceph-asok.OHSFkU -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:01:03,973843179-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:01:03,977410951-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cedd92ca-a8d8-4152-abd2-94bb697bd989', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.OHSFkU:/tmp/ceph-asok.OHSFkU', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cedd92ca-a8d8-4152-abd2-94bb697bd989 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.OHSFkU:/tmp/ceph-asok.OHSFkU -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:01:07,431180447-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:01:07,434909171-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cedd92ca-a8d8-4152-abd2-94bb697bd989', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.OHSFkU:/tmp/ceph-asok.OHSFkU', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cedd92ca-a8d8-4152-abd2-94bb697bd989 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.OHSFkU:/tmp/ceph-asok.OHSFkU -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:01:11,903530779-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:01:11,907091828-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cedd92ca-a8d8-4152-abd2-94bb697bd989', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.OHSFkU:/tmp/ceph-asok.OHSFkU', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cedd92ca-a8d8-4152-abd2-94bb697bd989 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.OHSFkU:/tmp/ceph-asok.OHSFkU -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:01:15,334662012-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:01:15,338098948-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cedd92ca-a8d8-4152-abd2-94bb697bd989', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.OHSFkU:/tmp/ceph-asok.OHSFkU', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cedd92ca-a8d8-4152-abd2-94bb697bd989 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.OHSFkU:/tmp/ceph-asok.OHSFkU -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:01:18,767164220-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:01:18,770751890-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cedd92ca-a8d8-4152-abd2-94bb697bd989', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.OHSFkU:/tmp/ceph-asok.OHSFkU', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cedd92ca-a8d8-4152-abd2-94bb697bd989 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.OHSFkU:/tmp/ceph-asok.OHSFkU -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:01:23,106499883-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:01:23,110078235-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cedd92ca-a8d8-4152-abd2-94bb697bd989', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.OHSFkU:/tmp/ceph-asok.OHSFkU', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cedd92ca-a8d8-4152-abd2-94bb697bd989 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.OHSFkU:/tmp/ceph-asok.OHSFkU -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:01:26,279205935-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:01:26,282823751-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cedd92ca-a8d8-4152-abd2-94bb697bd989', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.OHSFkU:/tmp/ceph-asok.OHSFkU', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cedd92ca-a8d8-4152-abd2-94bb697bd989 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.OHSFkU:/tmp/ceph-asok.OHSFkU -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  155 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:01:29,451854280-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:01:52,740547253-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (4s)
      [=============...............] (remaining: 5s)
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:02:01,061133417-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:02:09,518389535-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:02:17,807222030-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:02:26,202550137-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:02:34,631648866-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:02:42,998299041-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:02:43,006371004-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:02:51,254836818-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:02:51,263447603-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:02:59,641719559-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:02:59,649793285-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:03:07,923299795-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:03:07,931267673-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:03:16,330306106-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:03:16,338264906-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:03:16,345050513-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:03:16,349013547-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:03:16,357032771-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=417199
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_64KB_write.dat.2 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:03:16,363426371-04:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_64KB_write.log.2
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:03:16,382728980-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:03:16,386183550-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cedd92ca-a8d8-4152-abd2-94bb697bd989', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.OHSFkU:/tmp/ceph-asok.OHSFkU', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cedd92ca-a8d8-4152-abd2-94bb697bd989 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.OHSFkU:/tmp/ceph-asok.OHSFkU -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T17:03:18.869+0000 7f98863d7d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T17:03:18.873+0000 7f98863d7d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T17:03:18.873+0000 7f98863d7d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T17:03:18.886018+0000 Maintaining 256 concurrent writes of 65536 bytes to objects of size 65536 for up to 20 seconds or 0 objects
2021-05-17T17:03:18.886027+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-17T17:03:18.891335+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T17:03:18.891335+0000     0       0         0         0         0         0           -           0
2021-05-17T17:03:19.891424+0000     1     256     11855     11599   724.914   724.938  0.00451845   0.0215463
2021-05-17T17:03:20.891494+0000     2     256     23434     23178   724.275   723.688   0.0360302     0.02199
2021-05-17T17:03:21.891609+0000     3     255     35294     35039   729.926   741.312   0.0324602   0.0218105
2021-05-17T17:03:22.891714+0000     4     256     46600     46344   724.066   706.562  0.00140433   0.0219329
2021-05-17T17:03:23.891785+0000     5     256     58002     57746   721.768   712.625  0.00175508   0.0220405
2021-05-17T17:03:24.891857+0000     6     256     69224     68968   718.361   701.375  0.00522806   0.0222039
2021-05-17T17:03:25.891935+0000     7     255     80342     80087   715.007   694.938  0.00907929   0.0223289
2021-05-17T17:03:26.892044+0000     8     255     91595     91340   713.535   703.312  0.00697141   0.0223747
2021-05-17T17:03:27.892115+0000     9     256    101433    101177   702.562   614.812  0.00104941   0.0226187
2021-05-17T17:03:28.892190+0000    10     256    111979    111723   698.213   659.125  0.00924273   0.0228399
2021-05-17T17:03:29.892279+0000    11     256    122793    122537   696.177   675.875  0.00105706   0.0229351
2021-05-17T17:03:30.892388+0000    12     255    133849    133594   695.744   691.062   0.0148378   0.0229588
2021-05-17T17:03:31.892458+0000    13     255    145102    144847   696.323   703.312   0.0524636   0.0229277
2021-05-17T17:03:32.892528+0000    14     255    155420    155165   692.645   644.875   0.0531987     0.02307
2021-05-17T17:03:33.892599+0000    15     256    164798    164542   685.536   586.062 0.000996922   0.0231915
2021-05-17T17:03:34.892699+0000    16     255    175605    175350   684.905     675.5  0.00685505   0.0233368
2021-05-17T17:03:35.892769+0000    17     256    186747    186491   685.573   696.312  0.00162599   0.0233013
2021-05-17T17:03:36.892846+0000    18     256    196400    196144       681   603.312  0.00134764   0.0234576
2021-05-17T17:03:37.892923+0000    19     256    207819    207563   682.718   713.688   0.0063925   0.0234154
2021-05-17T17:03:38.893024+0000 min lat: 0.000800724 max lat: 0.18252 avg lat: 0.0234476
2021-05-17T17:03:38.893024+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T17:03:38.893024+0000    20     256    218323    218067   681.404     656.5  0.00091539   0.0234476
2021-05-17T17:03:39.893413+0000 Total time run:         20.017
Total writes made:      218323
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     681.679
Stddev Bandwidth:       42.2515
Max bandwidth (MB/sec): 741.312
Min bandwidth (MB/sec): 586.062
Average IOPS:           10906
Stddev IOPS:            676.023
Max IOPS:               11861
Min IOPS:               9377
Average Latency(s):     0.0234604
Stddev Latency(s):      0.0220584
Max latency(s):         0.18252
Min latency(s):         0.000800724

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:03:40,418349911-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 417199


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:03:40,423007470-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:04:03,640585183-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 218.32k objects, 13 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:04:03,649658577-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:04:11,820160498-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 218.32k objects, 13 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:04:11,828372134-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:04:20,258874844-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 218.32k objects, 13 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:04:20,267093984-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:04:28,554692125-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 218.32k objects, 13 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:04:28,563123644-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:04:36,791928258-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 218.32k objects, 13 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:04:36,800289094-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:04:36,806780717-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:04:36,810722983-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:04:36,818922446-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=418582
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_64KB_seq.dat.2 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:04:36,825586433-04:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_64KB_seq.log.2
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:04:36,844890636-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:04:36,848379810-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cedd92ca-a8d8-4152-abd2-94bb697bd989', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.OHSFkU:/tmp/ceph-asok.OHSFkU', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cedd92ca-a8d8-4152-abd2-94bb697bd989 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.OHSFkU:/tmp/ceph-asok.OHSFkU -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T17:04:39.313+0000 7fa7de645d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T17:04:39.321+0000 7fa7de645d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T17:04:39.321+0000 7fa7de645d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T17:04:39.333638+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T17:04:39.333638+0000     0       0         0         0         0         0           -           0
2021-05-17T17:04:40.333733+0000     1     256     35754     35498    2218.3   2218.62  0.00869935  0.00715955
2021-05-17T17:04:41.333811+0000     2     255     71207     70952      2217   2215.88   0.0127724   0.0071898
2021-05-17T17:04:42.333890+0000     3     255    103206    102951    2144.6   1999.94  0.00466928  0.00743415
2021-05-17T17:04:43.333995+0000     4     255    136472    136217   2128.17   2079.12   0.0122116  0.00749907
2021-05-17T17:04:44.334100+0000     5     255    171508    171253   2140.44   2189.75   0.0121533  0.00745755
2021-05-17T17:04:45.334202+0000     6     255    206390    206135   2147.02   2180.12   0.0132991  0.00743527
2021-05-17T17:04:46.334399+0000 Total time run:       6.34806
Total reads made:     218323
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   2149.51
Average IOPS:         34392
Stddev IOPS:          1413.68
Max IOPS:             35498
Min IOPS:             31999
Average Latency(s):   0.0074258
Max latency(s):       0.0676581
Min latency(s):       0.000294784

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:04:46,888616911-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 418582


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:04:46,892862656-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:05:10,322848786-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 218.32k objects, 13 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:05:10,331157043-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:05:18,649699138-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 218.32k objects, 13 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:05:18,658230213-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:05:26,953886811-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 218.32k objects, 13 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:05:26,962418478-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:05:35,244776208-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 218.32k objects, 13 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:05:35,252334827-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:05:43,504208569-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 218.32k objects, 13 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:05:43,512934811-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:05:43,519061059-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T13:05:43,521653298-04:00][RUNNING][ROUND 3/3/40] object_size=64KB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:05:43,525012088-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:05:43,533579011-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40629\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.371459\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid e6f9f1a8-a4e1-4f9f-bca9-10047f9833db\nsetting min_mon_release = octopus\nepoch 0\nfsid e6f9f1a8-a4e1-4f9f-bca9-10047f9833db\nlast_changed 2021-05-17T10:06:14.468947-0700\ncreated 2021-05-17T10:06:14.468947-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40629/0,v1:10.10.1.2:40630/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.371459 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 ec396810-a745-4a64-a25c-a4c3b6deb727\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 eae2fc81-6365-4108-ac8a-e7b96e525bda\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 403e3b24-56d4-426f-854c-92a603b0b5f8\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\n'
10.10.1.2: b'restful urls: https://10.10.1.2:42629\n  w/ user/pass: admin / 6a850e1d-ca68-454b-aaa0-c7cf5ca98949\n'
10.10.1.2: b'\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 13:06:27 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40629
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.371459
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid e6f9f1a8-a4e1-4f9f-bca9-10047f9833db
setting min_mon_release = octopus
epoch 0
fsid e6f9f1a8-a4e1-4f9f-bca9-10047f9833db
last_changed 2021-05-17T10:06:14.468947-0700
created 2021-05-17T10:06:14.468947-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40629/0,v1:10.10.1.2:40630/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.371459 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 ec396810-a745-4a64-a25c-a4c3b6deb727
0
start osd.0
add osd1 eae2fc81-6365-4108-ac8a-e7b96e525bda
1
start osd.1
add osd2 403e3b24-56d4-426f-854c-92a603b0b5f8
2
start osd.2


restful urls: https://10.10.1.2:42629
  w/ user/pass: admin / 6a850e1d-ca68-454b-aaa0-c7cf5ca98949


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T10:05:44.458-0700 7ff2f10e11c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:05:44.458-0700 7ff2f10e11c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:05:44.474-0700 7f92e39171c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:05:44.474-0700 7f92e39171c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40629,v1:10.10.1.2:40630] --print /tmp/ceph_monmap.371459 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.371459 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.371459 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42629 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.ougFyUfOWN 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new ec396810-a745-4a64-a25c-a4c3b6deb727 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCPoqJgTz7eEhAA3E5sby/6RY8tOTFfuhmtHg== --osd-uuid ec396810-a745-4a64-a25c-a4c3b6deb727 
2021-05-17T10:06:23.667-0700 7faddede2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T10:06:23.667-0700 7faddede2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T10:06:23.667-0700 7faddede2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T10:06:23.747-0700 7faddede2f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new eae2fc81-6365-4108-ac8a-e7b96e525bda -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T10:06:24.043-0700 7fba7f009f00 -1 Falling back to public interface
2021-05-17T10:06:24.055-0700 7fba7f009f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCQoqJgbpdqAhAA6Xv6WvLaINidI51brF3NCg== --osd-uuid eae2fc81-6365-4108-ac8a-e7b96e525bda 
2021-05-17T10:06:24.371-0700 7f3ee1945f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T10:06:24.371-0700 7f3ee1945f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T10:06:24.371-0700 7f3ee1945f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T10:06:24.415-0700 7f3ee1945f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 403e3b24-56d4-426f-854c-92a603b0b5f8 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T10:06:24.711-0700 7fdd4d1cff00 -1 Falling back to public interface
2021-05-17T10:06:24.723-0700 7fdd4d1cff00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCQoqJgowNPKhAAcy2gWoSE72rfzB4APvlEXw== --osd-uuid 403e3b24-56d4-426f-854c-92a603b0b5f8 
2021-05-17T10:06:25.055-0700 7fdf105baf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T10:06:25.059-0700 7fdf105baf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T10:06:25.059-0700 7fdf105baf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T10:06:25.119-0700 7fdf105baf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T10:06:25.515-0700 7f82c8533f00 -1 Falling back to public interface
2021-05-17T10:06:25.527-0700 7f82c8533f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:06:27,490257194-04:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:06:27,494093992-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:06:27,534586236-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:06:27,538066874-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd22900c8-963c-4dd1-b753-f65b4e3c0f41', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aOTbLc:/tmp/ceph-asok.aOTbLc', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d22900c8-963c-4dd1-b753-f65b4e3c0f41 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aOTbLc:/tmp/ceph-asok.aOTbLc -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:06:30,843324110-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:06:30,846998733-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd22900c8-963c-4dd1-b753-f65b4e3c0f41', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aOTbLc:/tmp/ceph-asok.aOTbLc', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d22900c8-963c-4dd1-b753-f65b4e3c0f41 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aOTbLc:/tmp/ceph-asok.aOTbLc -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:06:34,324668203-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:06:34,328174479-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd22900c8-963c-4dd1-b753-f65b4e3c0f41', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aOTbLc:/tmp/ceph-asok.aOTbLc', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d22900c8-963c-4dd1-b753-f65b4e3c0f41 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aOTbLc:/tmp/ceph-asok.aOTbLc -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:06:37,538267833-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:06:37,541813033-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd22900c8-963c-4dd1-b753-f65b4e3c0f41', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aOTbLc:/tmp/ceph-asok.aOTbLc', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d22900c8-963c-4dd1-b753-f65b4e3c0f41 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aOTbLc:/tmp/ceph-asok.aOTbLc -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:06:44,302762478-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:06:44,306540405-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd22900c8-963c-4dd1-b753-f65b4e3c0f41', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aOTbLc:/tmp/ceph-asok.aOTbLc', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d22900c8-963c-4dd1-b753-f65b4e3c0f41 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aOTbLc:/tmp/ceph-asok.aOTbLc -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:06:48,564425209-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:06:48,567504623-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd22900c8-963c-4dd1-b753-f65b4e3c0f41', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aOTbLc:/tmp/ceph-asok.aOTbLc', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d22900c8-963c-4dd1-b753-f65b4e3c0f41 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aOTbLc:/tmp/ceph-asok.aOTbLc -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:06:52,457032356-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:06:52,460808670-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd22900c8-963c-4dd1-b753-f65b4e3c0f41', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aOTbLc:/tmp/ceph-asok.aOTbLc', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d22900c8-963c-4dd1-b753-f65b4e3c0f41 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aOTbLc:/tmp/ceph-asok.aOTbLc -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:06:56,227348352-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:06:56,230725155-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd22900c8-963c-4dd1-b753-f65b4e3c0f41', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aOTbLc:/tmp/ceph-asok.aOTbLc', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d22900c8-963c-4dd1-b753-f65b4e3c0f41 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aOTbLc:/tmp/ceph-asok.aOTbLc -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:07:00,583197863-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:07:00,586792145-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd22900c8-963c-4dd1-b753-f65b4e3c0f41', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aOTbLc:/tmp/ceph-asok.aOTbLc', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d22900c8-963c-4dd1-b753-f65b4e3c0f41 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aOTbLc:/tmp/ceph-asok.aOTbLc -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:07:04,219423705-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:07:04,222934270-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd22900c8-963c-4dd1-b753-f65b4e3c0f41', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aOTbLc:/tmp/ceph-asok.aOTbLc', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d22900c8-963c-4dd1-b753-f65b4e3c0f41 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aOTbLc:/tmp/ceph-asok.aOTbLc -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:07:08,760304527-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:07:08,763811495-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd22900c8-963c-4dd1-b753-f65b4e3c0f41', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aOTbLc:/tmp/ceph-asok.aOTbLc', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d22900c8-963c-4dd1-b753-f65b4e3c0f41 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aOTbLc:/tmp/ceph-asok.aOTbLc -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:07:11,981609478-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:07:11,985574025-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd22900c8-963c-4dd1-b753-f65b4e3c0f41', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aOTbLc:/tmp/ceph-asok.aOTbLc', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d22900c8-963c-4dd1-b753-f65b4e3c0f41 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aOTbLc:/tmp/ceph-asok.aOTbLc -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  154 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:07:15,286650496-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:07:38,472224345-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (9s)
      [=========================...] 
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:07:46,775113597-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:07:55,005580099-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:08:03,420442429-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:08:11,824258704-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:08:20,303135392-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:08:28,584817718-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   223 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:08:28,592849786-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:08:37,071083144-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:08:37,079532987-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:08:45,508160795-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:08:45,516390635-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:08:53,844320148-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:08:53,853723391-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:09:02,265772615-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:09:02,273578970-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:09:02,280461789-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:09:02,284384637-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:09:02,292598888-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=425152
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_64KB_write.dat.3 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:09:02,299113145-04:00] INFO: > Run rados bench[0m
# ./bench-rados:192 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_64KB_write.log.3
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:09:02,319549214-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:09:02,322804459-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd22900c8-963c-4dd1-b753-f65b4e3c0f41', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aOTbLc:/tmp/ceph-asok.aOTbLc', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d22900c8-963c-4dd1-b753-f65b4e3c0f41 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aOTbLc:/tmp/ceph-asok.aOTbLc -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T17:09:04.850+0000 7fc12b275d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T17:09:04.854+0000 7fc12b275d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T17:09:04.854+0000 7fc12b275d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T17:09:04.887225+0000 Maintaining 256 concurrent writes of 65536 bytes to objects of size 65536 for up to 20 seconds or 0 objects
2021-05-17T17:09:04.887233+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-17T17:09:04.892330+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T17:09:04.892330+0000     0       0         0         0         0         0           -           0
2021-05-17T17:09:05.892442+0000     1     255      8978      8723   545.165   545.188   0.0580572   0.0285352
2021-05-17T17:09:06.892550+0000     2     256     18134     17878   558.646   572.188   0.0108964   0.0284442
2021-05-17T17:09:07.892618+0000     3     255     27275     27020   562.876   571.375   0.0531499   0.0283172
2021-05-17T17:09:08.892731+0000     4     255     36457     36202   565.609   573.875   0.0360656   0.0282303
2021-05-17T17:09:09.892809+0000     5     256     45493     45237   565.416   564.688  0.00114497   0.0281354
2021-05-17T17:09:10.892920+0000     6     256     54889     54633   569.044    587.25  0.00204481   0.0280484
2021-05-17T17:09:11.892988+0000     7     255     64230     63975   571.157   583.875   0.0042962   0.0279566
2021-05-17T17:09:12.893108+0000     8     255     73268     73013   570.364   564.875  0.00135001   0.0279411
2021-05-17T17:09:13.893176+0000     9     256     82353     82097   570.069    567.75  0.00154383   0.0279988
2021-05-17T17:09:14.893264+0000    10     256     91624     91368   571.001   579.438  0.00836729   0.0279709
2021-05-17T17:09:15.893342+0000    11     256     99734     99478   565.168   506.875  0.00686189   0.0281752
2021-05-17T17:09:16.893459+0000    12     256    107879    107623   560.487   509.062   0.0110398   0.0284972
2021-05-17T17:09:17.893525+0000    13     255    116220    115965   557.476   521.375   0.0582072   0.0286251
2021-05-17T17:09:18.893624+0000    14     256    125015    124759   556.911   549.625 0.000946528    0.028663
2021-05-17T17:09:19.893682+0000    15     256    134206    133950   558.077   574.438   0.0335714   0.0286369
2021-05-17T17:09:20.893754+0000    16     256    142840    142584   556.922   539.625  0.00988736   0.0286913
2021-05-17T17:09:21.893828+0000    17     256    150844    150588   553.586    500.25  0.00108013   0.0288687
2021-05-17T17:09:22.893933+0000    18     256    159352    159096    552.37    531.75   0.0103571   0.0289414
2021-05-17T17:09:23.893989+0000    19     255    168654    168399   553.898   581.438   0.0380496   0.0288698
2021-05-17T17:09:24.894057+0000 min lat: 0.000805393 max lat: 0.147462 avg lat: 0.0289654
2021-05-17T17:09:24.894057+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T17:09:24.894057+0000    20     216    176895    176679   552.076     517.5    0.012596   0.0289654
2021-05-17T17:09:25.894204+0000 Total time run:         20.0172
Total writes made:      176895
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     552.323
Stddev Bandwidth:       28.5793
Max bandwidth (MB/sec): 587.25
Min bandwidth (MB/sec): 500.25
Average IOPS:           8837
Stddev IOPS:            457.268
Max IOPS:               9396
Min IOPS:               8004
Average Latency(s):     0.0289577
Stddev Latency(s):      0.0206698
Max latency(s):         0.147462
Min latency(s):         0.000805393

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:09:26,472531192-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 425152


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:09:26,477568445-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:09:50,060262683-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 176.90k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:09:50,068817653-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:09:58,637157595-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 176.90k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:09:58,645549710-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:10:07,010839687-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 176.90k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:10:07,019621083-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:10:15,266772992-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 176.90k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:10:15,275309558-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:10:23,599228007-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 176.90k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:10:23,608023359-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:10:23,614364581-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:10:23,617821154-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:10:23,625946758-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=426519
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_64KB_seq.dat.3 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:10:23,631864986-04:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_64KB_seq.log.3
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:10:23,650321045-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:10:23,653744897-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd22900c8-963c-4dd1-b753-f65b4e3c0f41', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aOTbLc:/tmp/ceph-asok.aOTbLc', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d22900c8-963c-4dd1-b753-f65b4e3c0f41 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aOTbLc:/tmp/ceph-asok.aOTbLc -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T17:10:26.030+0000 7fc63d276d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T17:10:26.038+0000 7fc63d276d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T17:10:26.038+0000 7fc63d276d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T17:10:26.049688+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T17:10:26.049688+0000     0       0         0         0         0         0           -           0
2021-05-17T17:10:27.049813+0000     1     255     38025     37770   2360.19   2360.62  0.00159902  0.00673347
2021-05-17T17:10:28.049909+0000     2     255     73468     73213   2287.58   2215.19   0.0132279  0.00696278
2021-05-17T17:10:29.050032+0000     3     255    107679    107424    2237.7   2138.19  0.00468731  0.00712837
2021-05-17T17:10:30.050154+0000     4     255    143189    142934   2233.05   2219.38   0.0020286  0.00714544
2021-05-17T17:10:31.050305+0000 Total time run:       4.98133
Total reads made:     176895
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   2219.48
Average IOPS:         35511
Stddev IOPS:          1483.2
Max IOPS:             37770
Min IOPS:             34211
Average Latency(s):   0.00718953
Max latency(s):       0.0353254
Min latency(s):       0.000274476

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:10:31,735499486-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 426519


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:10:31,739602674-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:10:55,079459107-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 176.90k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:10:55,087962441-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:11:03,516687617-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 176.90k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:11:03,525542811-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:11:11,775216013-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 176.90k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:11:11,783945361-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:11:20,148258887-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 176.90k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:11:20,156546526-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:11:28,653005031-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 176.90k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:11:28,660963731-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:11:28,667593636-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T13:11:28,669782357-04:00][RUNNING][ROUND 4/3/40] object_size=64KB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:11:28,673490293-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:11:28,681611758-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40676\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.372570\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid bf87ac90-9601-4c05-ab15-a6beb1bd7fd9\nsetting min_mon_release = octopus\nepoch 0\nfsid bf87ac90-9601-4c05-ab15-a6beb1bd7fd9\nlast_changed 2021-05-17T10:11:56.080735-0700\ncreated 2021-05-17T10:11:56.080735-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40676/0,v1:10.10.1.2:40677/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.372570 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 6d8e2bf8-a1a3-4ef4-b76a-151be4f71dff\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 655f2482-2e57-46cf-968c-583baa84a23d\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 e6ef20e4-8e4f-471a-ae6c-025c43d43c4d\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42676\n  w/ user/pass: admin / b5f12f14-b434-4ec6-b6fb-d3580c958fd1\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\nCEPH_DEV=1\n'
[1] 13:12:09 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40676
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.372570
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid bf87ac90-9601-4c05-ab15-a6beb1bd7fd9
setting min_mon_release = octopus
epoch 0
fsid bf87ac90-9601-4c05-ab15-a6beb1bd7fd9
last_changed 2021-05-17T10:11:56.080735-0700
created 2021-05-17T10:11:56.080735-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40676/0,v1:10.10.1.2:40677/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.372570 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 6d8e2bf8-a1a3-4ef4-b76a-151be4f71dff
0
start osd.0
add osd1 655f2482-2e57-46cf-968c-583baa84a23d
1
start osd.1
add osd2 e6ef20e4-8e4f-471a-ae6c-025c43d43c4d
2
start osd.2


restful urls: https://10.10.1.2:42676
  w/ user/pass: admin / b5f12f14-b434-4ec6-b6fb-d3580c958fd1


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T10:11:29.613-0700 7f710e60f1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:11:29.613-0700 7f710e60f1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:11:29.633-0700 7f038dc3d1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:11:29.633-0700 7f038dc3d1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40676,v1:10.10.1.2:40677] --print /tmp/ceph_monmap.372570 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.372570 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.372570 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42676 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.HnCbtrlPS3 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 6d8e2bf8-a1a3-4ef4-b76a-151be4f71dff -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDko6JgC43ANBAAkRVKowIGucQc+w9u1bOdYQ== --osd-uuid 6d8e2bf8-a1a3-4ef4-b76a-151be4f71dff 
2021-05-17T10:12:05.234-0700 7fe8cc251f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T10:12:05.234-0700 7fe8cc251f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T10:12:05.234-0700 7fe8cc251f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T10:12:05.314-0700 7fe8cc251f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 655f2482-2e57-46cf-968c-583baa84a23d -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T10:12:05.574-0700 7f0858b60f00 -1 Falling back to public interface
2021-05-17T10:12:05.586-0700 7f0858b60f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDlo6Jg5CRBIhAA97SXwSwio/1dRhupGwPJ2Q== --osd-uuid 655f2482-2e57-46cf-968c-583baa84a23d 
2021-05-17T10:12:05.922-0700 7f7ced2c9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T10:12:05.922-0700 7f7ced2c9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T10:12:05.922-0700 7f7ced2c9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T10:12:05.994-0700 7f7ced2c9f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new e6ef20e4-8e4f-471a-ae6c-025c43d43c4d -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T10:12:06.270-0700 7fa72958cf00 -1 Falling back to public interface
2021-05-17T10:12:06.282-0700 7fa72958cf00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDmo6JgBd8YEBAAmdvvWJTGTRmqLnsPXaD5FA== --osd-uuid e6ef20e4-8e4f-471a-ae6c-025c43d43c4d 
2021-05-17T10:12:06.622-0700 7f9bb443ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T10:12:06.622-0700 7f9bb443ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T10:12:06.622-0700 7f9bb443ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T10:12:06.678-0700 7f9bb443ff00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T10:12:07.074-0700 7fdff59b7f00 -1 Falling back to public interface
2021-05-17T10:12:07.086-0700 7fdff59b7f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:12:09,050039083-04:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:12:09,054025682-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:12:09,094668389-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:12:09,098186277-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '51ba911c-b546-4e96-b11a-198ca2ea858a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gRuv9k:/tmp/ceph-asok.gRuv9k', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 51ba911c-b546-4e96-b11a-198ca2ea858a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gRuv9k:/tmp/ceph-asok.gRuv9k -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:12:12,670573811-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:12:12,674241932-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '51ba911c-b546-4e96-b11a-198ca2ea858a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gRuv9k:/tmp/ceph-asok.gRuv9k', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 51ba911c-b546-4e96-b11a-198ca2ea858a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gRuv9k:/tmp/ceph-asok.gRuv9k -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:12:16,037564437-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:12:16,041322256-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '51ba911c-b546-4e96-b11a-198ca2ea858a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gRuv9k:/tmp/ceph-asok.gRuv9k', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 51ba911c-b546-4e96-b11a-198ca2ea858a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gRuv9k:/tmp/ceph-asok.gRuv9k -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:12:19,252950564-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:12:19,256864647-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '51ba911c-b546-4e96-b11a-198ca2ea858a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gRuv9k:/tmp/ceph-asok.gRuv9k', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 51ba911c-b546-4e96-b11a-198ca2ea858a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gRuv9k:/tmp/ceph-asok.gRuv9k -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:12:25,973306614-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:12:25,976767556-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '51ba911c-b546-4e96-b11a-198ca2ea858a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gRuv9k:/tmp/ceph-asok.gRuv9k', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 51ba911c-b546-4e96-b11a-198ca2ea858a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gRuv9k:/tmp/ceph-asok.gRuv9k -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:12:30,494779991-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:12:30,498321814-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '51ba911c-b546-4e96-b11a-198ca2ea858a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gRuv9k:/tmp/ceph-asok.gRuv9k', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 51ba911c-b546-4e96-b11a-198ca2ea858a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gRuv9k:/tmp/ceph-asok.gRuv9k -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:12:34,910411444-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:12:34,913972784-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '51ba911c-b546-4e96-b11a-198ca2ea858a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gRuv9k:/tmp/ceph-asok.gRuv9k', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 51ba911c-b546-4e96-b11a-198ca2ea858a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gRuv9k:/tmp/ceph-asok.gRuv9k -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:12:39,165843714-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:12:39,169385067-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '51ba911c-b546-4e96-b11a-198ca2ea858a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gRuv9k:/tmp/ceph-asok.gRuv9k', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 51ba911c-b546-4e96-b11a-198ca2ea858a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gRuv9k:/tmp/ceph-asok.gRuv9k -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:12:42,782899112-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:12:42,786740358-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '51ba911c-b546-4e96-b11a-198ca2ea858a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gRuv9k:/tmp/ceph-asok.gRuv9k', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 51ba911c-b546-4e96-b11a-198ca2ea858a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gRuv9k:/tmp/ceph-asok.gRuv9k -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:12:47,104606481-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:12:47,108684722-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '51ba911c-b546-4e96-b11a-198ca2ea858a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gRuv9k:/tmp/ceph-asok.gRuv9k', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 51ba911c-b546-4e96-b11a-198ca2ea858a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gRuv9k:/tmp/ceph-asok.gRuv9k -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:12:50,711549636-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:12:50,714977585-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '51ba911c-b546-4e96-b11a-198ca2ea858a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gRuv9k:/tmp/ceph-asok.gRuv9k', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 51ba911c-b546-4e96-b11a-198ca2ea858a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gRuv9k:/tmp/ceph-asok.gRuv9k -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:12:54,185848047-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:12:54,189488044-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '51ba911c-b546-4e96-b11a-198ca2ea858a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gRuv9k:/tmp/ceph-asok.gRuv9k', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 51ba911c-b546-4e96-b11a-198ca2ea858a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gRuv9k:/tmp/ceph-asok.gRuv9k -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  155 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:12:57,724994135-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:13:21,085470536-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (9s)
      [=========================...] 
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:13:29,618130741-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:13:37,890483160-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:13:46,329096535-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:13:54,800415721-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:14:03,199913715-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:14:11,763917607-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     0.521% pgs not active
             191 active+clean
             1   activating
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:14:11,772785516-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:14:20,303678904-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:14:20,312031294-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:14:28,765079263-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:14:28,773459215-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:14:37,288129686-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:14:37,296240472-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:14:45,802447949-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:14:45,810772998-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:14:45,817495426-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:14:45,821388810-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:14:45,829977554-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=433068
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_64KB_write.dat.4 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:14:45,836243164-04:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_64KB_write.log.4
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:14:45,854924277-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:14:45,858415615-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '51ba911c-b546-4e96-b11a-198ca2ea858a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gRuv9k:/tmp/ceph-asok.gRuv9k', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 51ba911c-b546-4e96-b11a-198ca2ea858a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gRuv9k:/tmp/ceph-asok.gRuv9k -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T17:14:48.439+0000 7f1125641d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T17:14:48.447+0000 7f1125641d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T17:14:48.447+0000 7f1125641d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T17:14:48.454572+0000 Maintaining 256 concurrent writes of 65536 bytes to objects of size 65536 for up to 20 seconds or 0 objects
2021-05-17T17:14:48.454582+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-17T17:14:48.459795+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T17:14:48.459795+0000     0       0         0         0         0         0           -           0
2021-05-17T17:14:49.459896+0000     1     256     13438     13182   823.845   823.875  0.00102335   0.0190889
2021-05-17T17:14:50.459976+0000     2     256     27194     26938   841.764    859.75   0.0322607   0.0188748
2021-05-17T17:14:51.460070+0000     3     256     40883     40627   846.337   855.562  0.00385051   0.0188052
2021-05-17T17:14:52.460153+0000     4     255     54747     54492   851.375   866.562   0.0139269   0.0187374
2021-05-17T17:14:53.460225+0000     5     256     68496     68240   852.938    859.25   0.0263542    0.018695
2021-05-17T17:14:54.460298+0000     6     256     81930     81674   850.709   839.625   0.0123633   0.0187566
2021-05-17T17:14:55.460370+0000     7     255     95411     95156   849.545   842.625    0.021901   0.0187936
2021-05-17T17:14:56.460476+0000     8     256    108839    108583   848.239   839.188  0.00566095   0.0188313
2021-05-17T17:14:57.460561+0000     9     256    119073    118817   825.054   639.625  0.00840486   0.0193642
2021-05-17T17:14:58.460671+0000    10     256    131407    131151   819.627   770.875   0.0381718   0.0194859
2021-05-17T17:14:59.460774+0000    11     256    144646    144390    820.33   827.438  0.00136373   0.0194729
2021-05-17T17:15:00.460858+0000    12     256    156977    156721   816.187   770.688   0.0020331   0.0195768
2021-05-17T17:15:01.460943+0000    13     256    167169    166913     802.4       637    0.156379   0.0198825
2021-05-17T17:15:02.461014+0000    14     256    178241    177985    794.51       692    0.023134    0.020113
2021-05-17T17:15:03.461097+0000    15     255    188560    188305   784.539       645   0.0027203   0.0203707
2021-05-17T17:15:04.461176+0000    16     255    201184    200929   784.814       789  0.00839457   0.0203679
2021-05-17T17:15:05.461254+0000    17     255    212988    212733   782.042    737.75   0.0329169   0.0204372
2021-05-17T17:15:06.461327+0000    18     256    224965    224709   780.176     748.5  0.00672869   0.0204902
2021-05-17T17:15:07.461411+0000    19     256    235578    235322   774.022   663.312   0.0169374   0.0206561
2021-05-17T17:15:08.461461+0000 min lat: 0.000781348 max lat: 0.175086 avg lat: 0.0206455
2021-05-17T17:15:08.461461+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T17:15:08.461461+0000    20     218    248019    247801   774.316   779.938   0.0100953   0.0206455
2021-05-17T17:15:09.461598+0000 Total time run:         20.0124
Total writes made:      248019
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     774.578
Stddev Bandwidth:       80.5031
Max bandwidth (MB/sec): 866.562
Min bandwidth (MB/sec): 637
Average IOPS:           12393
Stddev IOPS:            1288.05
Max IOPS:               13865
Min IOPS:               10192
Average Latency(s):     0.0206483
Stddev Latency(s):      0.0163199
Max latency(s):         0.175086
Min latency(s):         0.000781348

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:15:10,064752907-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 433068


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:15:10,069441785-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:15:33,387309117-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 248.02k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:15:33,395125971-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:15:41,834392423-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 248.02k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:15:41,842941402-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:15:50,354991963-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 248.02k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:15:50,363240148-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:15:58,803829833-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 248.02k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:15:58,812050335-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:16:07,144242949-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 248.02k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:16:07,152806245-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:16:07,159662755-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:16:07,163420274-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:16:07,172570112-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=434456
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_64KB_seq.dat.4 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:16:07,178527292-04:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_64KB_seq.log.4
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:16:07,197729564-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:16:07,200903847-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '51ba911c-b546-4e96-b11a-198ca2ea858a', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gRuv9k:/tmp/ceph-asok.gRuv9k', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 51ba911c-b546-4e96-b11a-198ca2ea858a --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gRuv9k:/tmp/ceph-asok.gRuv9k -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T17:16:09.571+0000 7faebd573d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T17:16:09.575+0000 7faebd573d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T17:16:09.579+0000 7faebd573d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T17:16:09.589701+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T17:16:09.589701+0000     0       0         0         0         0         0           -           0
2021-05-17T17:16:10.589772+0000     1     255     38624     38369   2397.74   2398.06   0.0154343  0.00662225
2021-05-17T17:16:11.589867+0000     2     255     76021     75766   2367.41   2337.31   0.0135808  0.00672838
2021-05-17T17:16:12.589947+0000     3     255    111174    110919   2310.57   2197.06   0.0123569  0.00690365
2021-05-17T17:16:13.590034+0000     4     255    148291    148036   2312.83   2319.81  0.00616299  0.00690117
2021-05-17T17:16:14.590115+0000     5     255    184120    183865   2298.09   2239.31   0.0123575  0.00694556
2021-05-17T17:16:15.590185+0000     6     255    220687    220432   2295.96   2285.44  0.00781681  0.00695319
2021-05-17T17:16:16.590286+0000 Total time run:       6.80881
Total reads made:     248019
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   2276.64
Average IOPS:         36426
Stddev IOPS:          1149.42
Max IOPS:             38369
Min IOPS:             35153
Average Latency(s):   0.00701385
Max latency(s):       0.0455515
Min latency(s):       0.000275668

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:16:17,228090503-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 434456


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:16:17,232509234-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:16:40,480376622-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 248.02k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:16:40,489021571-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:16:48,800617358-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 248.02k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:16:48,808621343-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:16:57,032081797-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 248.02k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:16:57,040822746-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:17:05,328075476-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 248.02k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:17:05,336662836-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:17:13,757092916-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 248.02k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:17:13,765522661-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:17:13,772177522-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T13:17:13,774739544-04:00][RUNNING][ROUND 5/3/40] object_size=64KB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:17:13,778089166-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:17:13,786566470-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40462\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.373690\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid de64e342-d14d-4614-a0c1-003425d7336d\nsetting min_mon_release = octopus\nepoch 0\nfsid de64e342-d14d-4614-a0c1-003425d7336d\nlast_changed 2021-05-17T10:17:44.722811-0700\ncreated 2021-05-17T10:17:44.722811-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40462/0,v1:10.10.1.2:40463/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.373690 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 7bc6f669-accf-4c9d-9bab-207a74c618e1\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 e092a722-2f68-4ea5-a56b-ddb7ce5b4a82\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 ca28455a-10bf-433e-8757-589ff20cbf35\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42462\n  w/ user/pass: admin / e40681c8-bc41-406d-90ef-c681a31bfb59\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\nCEPH_DEV=1\n'
[1] 13:17:57 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40462
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.373690
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid de64e342-d14d-4614-a0c1-003425d7336d
setting min_mon_release = octopus
epoch 0
fsid de64e342-d14d-4614-a0c1-003425d7336d
last_changed 2021-05-17T10:17:44.722811-0700
created 2021-05-17T10:17:44.722811-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40462/0,v1:10.10.1.2:40463/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.373690 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 7bc6f669-accf-4c9d-9bab-207a74c618e1
0
start osd.0
add osd1 e092a722-2f68-4ea5-a56b-ddb7ce5b4a82
1
start osd.1
add osd2 ca28455a-10bf-433e-8757-589ff20cbf35
2
start osd.2


restful urls: https://10.10.1.2:42462
  w/ user/pass: admin / e40681c8-bc41-406d-90ef-c681a31bfb59


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T10:17:14.712-0700 7f508add81c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:17:14.712-0700 7f508add81c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:17:14.728-0700 7fb38ee211c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:17:14.728-0700 7fb38ee211c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40462,v1:10.10.1.2:40463] --print /tmp/ceph_monmap.373690 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.373690 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.373690 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42462 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.fVaUA2Ip9N 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 7bc6f669-accf-4c9d-9bab-207a74c618e1 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBBpaJgkEcQIxAA5ufzpAOCTRVKgNrpsLBHBg== --osd-uuid 7bc6f669-accf-4c9d-9bab-207a74c618e1 
2021-05-17T10:17:53.929-0700 7f8a1be26f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T10:17:53.929-0700 7f8a1be26f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T10:17:53.929-0700 7f8a1be26f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T10:17:53.977-0700 7f8a1be26f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new e092a722-2f68-4ea5-a56b-ddb7ce5b4a82 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T10:17:54.273-0700 7f1b5d575f00 -1 Falling back to public interface
2021-05-17T10:17:54.285-0700 7f1b5d575f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBCpaJgd9gzEBAAKa6FqOMRQbnmV1YMVG3wKA== --osd-uuid e092a722-2f68-4ea5-a56b-ddb7ce5b4a82 
2021-05-17T10:17:54.609-0700 7fa6681ddf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T10:17:54.609-0700 7fa6681ddf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T10:17:54.609-0700 7fa6681ddf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T10:17:54.661-0700 7fa6681ddf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new ca28455a-10bf-433e-8757-589ff20cbf35 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T10:17:55.033-0700 7f8514082f00 -1 Falling back to public interface
2021-05-17T10:17:55.045-0700 7f8514082f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBDpaJgGpPyARAAvTROJmggiMFNMkzBuIx3bw== --osd-uuid ca28455a-10bf-433e-8757-589ff20cbf35 
2021-05-17T10:17:55.381-0700 7fee725adf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T10:17:55.381-0700 7fee725adf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T10:17:55.381-0700 7fee725adf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T10:17:55.481-0700 7fee725adf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T10:17:55.817-0700 7f87ab958f00 -1 Falling back to public interface
2021-05-17T10:17:55.833-0700 7f87ab958f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:17:57,810405050-04:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:17:57,814688225-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:17:57,855433423-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:17:57,859202483-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ef2dbdb5-ac12-4a7d-bc32-b1b262c82132', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.katoX6:/tmp/ceph-asok.katoX6', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ef2dbdb5-ac12-4a7d-bc32-b1b262c82132 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.katoX6:/tmp/ceph-asok.katoX6 -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:18:01,287418601-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:18:01,291476845-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ef2dbdb5-ac12-4a7d-bc32-b1b262c82132', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.katoX6:/tmp/ceph-asok.katoX6', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ef2dbdb5-ac12-4a7d-bc32-b1b262c82132 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.katoX6:/tmp/ceph-asok.katoX6 -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:18:04,769851414-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:18:04,773387676-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ef2dbdb5-ac12-4a7d-bc32-b1b262c82132', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.katoX6:/tmp/ceph-asok.katoX6', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ef2dbdb5-ac12-4a7d-bc32-b1b262c82132 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.katoX6:/tmp/ceph-asok.katoX6 -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:18:08,065803934-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:18:08,069612348-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ef2dbdb5-ac12-4a7d-bc32-b1b262c82132', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.katoX6:/tmp/ceph-asok.katoX6', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ef2dbdb5-ac12-4a7d-bc32-b1b262c82132 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.katoX6:/tmp/ceph-asok.katoX6 -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:18:15,291265878-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:18:15,295156216-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ef2dbdb5-ac12-4a7d-bc32-b1b262c82132', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.katoX6:/tmp/ceph-asok.katoX6', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ef2dbdb5-ac12-4a7d-bc32-b1b262c82132 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.katoX6:/tmp/ceph-asok.katoX6 -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:18:19,675189974-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:18:19,678993920-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ef2dbdb5-ac12-4a7d-bc32-b1b262c82132', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.katoX6:/tmp/ceph-asok.katoX6', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ef2dbdb5-ac12-4a7d-bc32-b1b262c82132 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.katoX6:/tmp/ceph-asok.katoX6 -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:18:23,031153576-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:18:23,035035387-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ef2dbdb5-ac12-4a7d-bc32-b1b262c82132', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.katoX6:/tmp/ceph-asok.katoX6', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ef2dbdb5-ac12-4a7d-bc32-b1b262c82132 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.katoX6:/tmp/ceph-asok.katoX6 -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:18:26,628786640-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:18:26,632681988-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ef2dbdb5-ac12-4a7d-bc32-b1b262c82132', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.katoX6:/tmp/ceph-asok.katoX6', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ef2dbdb5-ac12-4a7d-bc32-b1b262c82132 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.katoX6:/tmp/ceph-asok.katoX6 -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:18:31,107125433-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:18:31,110839259-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ef2dbdb5-ac12-4a7d-bc32-b1b262c82132', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.katoX6:/tmp/ceph-asok.katoX6', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ef2dbdb5-ac12-4a7d-bc32-b1b262c82132 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.katoX6:/tmp/ceph-asok.katoX6 -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:18:35,619820053-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:18:35,623496178-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ef2dbdb5-ac12-4a7d-bc32-b1b262c82132', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.katoX6:/tmp/ceph-asok.katoX6', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ef2dbdb5-ac12-4a7d-bc32-b1b262c82132 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.katoX6:/tmp/ceph-asok.katoX6 -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:18:39,859763726-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:18:39,863701383-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ef2dbdb5-ac12-4a7d-bc32-b1b262c82132', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.katoX6:/tmp/ceph-asok.katoX6', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ef2dbdb5-ac12-4a7d-bc32-b1b262c82132 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.katoX6:/tmp/ceph-asok.katoX6 -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:18:43,347728627-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:18:43,350919480-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ef2dbdb5-ac12-4a7d-bc32-b1b262c82132', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.katoX6:/tmp/ceph-asok.katoX6', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ef2dbdb5-ac12-4a7d-bc32-b1b262c82132 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.katoX6:/tmp/ceph-asok.katoX6 -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  154 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:18:46,769738948-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:19:10,106429400-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [===================.........] (remaining: 4s)
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:19:18,529521329-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:19:26,850613131-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:19:35,320022364-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:19:43,833578951-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:19:52,137808811-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:20:00,469529577-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:20:00,477901483-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:20:08,904422341-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:20:08,912488964-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:20:17,359211194-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:20:17,367512919-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:20:25,597446606-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:20:25,605670805-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:20:34,044998193-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:20:34,053996205-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:20:34,060485726-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:20:34,064433371-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:20:34,072627634-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=441082
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_64KB_write.dat.5 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:20:34,078473595-04:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_64KB_write.log.5
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:20:34,098060698-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:20:34,101269175-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ef2dbdb5-ac12-4a7d-bc32-b1b262c82132', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.katoX6:/tmp/ceph-asok.katoX6', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ef2dbdb5-ac12-4a7d-bc32-b1b262c82132 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.katoX6:/tmp/ceph-asok.katoX6 -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T17:20:36.604+0000 7f95ba9f7d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T17:20:36.608+0000 7f95ba9f7d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T17:20:36.608+0000 7f95ba9f7d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T17:20:36.620173+0000 Maintaining 256 concurrent writes of 65536 bytes to objects of size 65536 for up to 20 seconds or 0 objects
2021-05-17T17:20:36.620189+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-17T17:20:36.625480+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T17:20:36.625480+0000     0       0         0         0         0         0           -           0
2021-05-17T17:20:37.625550+0000     1     255     11638     11383   711.411   711.438   0.0285434   0.0221601
2021-05-17T17:20:38.625621+0000     2     256     23677     23421   731.866   752.375  0.00242007   0.0216744
2021-05-17T17:20:39.625703+0000     3     256     35882     35626   742.161   762.812  0.00127782   0.0212902
2021-05-17T17:20:40.625823+0000     4     256     47852     47596    743.63   748.125   0.0254016     0.02143
2021-05-17T17:20:41.625934+0000     5     256     60183     59927   749.024   770.688   0.0027397   0.0212764
2021-05-17T17:20:42.626019+0000     6     255     72220     71965   749.572   752.375  0.00352364   0.0212756
2021-05-17T17:20:43.626097+0000     7     256     84013     83757   747.768       737 0.000933754   0.0213256
2021-05-17T17:20:44.626202+0000     8     255     95803     95548   746.404   736.938   0.0170401   0.0214137
2021-05-17T17:20:45.626314+0000     9     256    107255    106999   742.982   715.688  0.00608694   0.0214972
2021-05-17T17:20:46.626400+0000    10     255    118953    118698   741.797   731.188  0.00447866   0.0215294
2021-05-17T17:20:47.626478+0000    11     256    129252    128996   732.867   643.625  0.00791964   0.0218129
2021-05-17T17:20:48.626584+0000    12     256    140473    140217   730.232   701.312   0.0052534    0.021883
2021-05-17T17:20:49.626692+0000    13     255    152642    152387   732.563   760.625  0.00654066   0.0218201
2021-05-17T17:20:50.626771+0000    14     256    162779    162523   725.484     633.5  0.00324917   0.0220183
2021-05-17T17:20:51.626870+0000    15     256    173902    173646    723.46   695.188   0.0291616   0.0220907
2021-05-17T17:20:52.626946+0000    16     256    183705    183449   716.533   612.688 0.000898188    0.022272
2021-05-17T17:20:53.627023+0000    17     256    193336    193080    709.79   601.938 0.000821353   0.0224872
2021-05-17T17:20:54.627124+0000    18     256    203061    202805   704.121   607.812  0.00087268   0.0226842
2021-05-17T17:20:55.627229+0000    19     256    213973    213717   702.953       682  0.00257917     0.02274
2021-05-17T17:20:56.627313+0000 min lat: 0.000796186 max lat: 0.199032 avg lat: 0.0227812
2021-05-17T17:20:56.627313+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T17:20:56.627313+0000    20     256    224868    224612   701.849   680.938  0.00668137   0.0227812
2021-05-17T17:20:57.627454+0000 Total time run:         20.0206
Total writes made:      224868
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     701.99
Stddev Bandwidth:       55.479
Max bandwidth (MB/sec): 770.688
Min bandwidth (MB/sec): 601.938
Average IOPS:           11231
Stddev IOPS:            887.664
Max IOPS:               12331
Min IOPS:               9631
Average Latency(s):     0.0227803
Stddev Latency(s):      0.0213751
Max latency(s):         0.199032
Min latency(s):         0.000796186

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:20:58,357082561-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 441082


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:20:58,361006222-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:21:21,551521780-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 224.87k objects, 14 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:21:21,560710631-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:21:29,954710694-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 224.87k objects, 14 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:21:29,962405268-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:21:38,754488848-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 224.87k objects, 14 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:21:38,762407303-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:21:47,100961282-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 224.87k objects, 14 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:21:47,108810436-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:21:55,538793185-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 224.87k objects, 14 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:21:55,546830612-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:21:55,553728941-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:21:55,557603028-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:21:55,566181533-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=442513
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_64KB_seq.dat.5 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:21:55,572634504-04:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_64KB_seq.log.5
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:21:55,591771001-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:21:55,595426528-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'ef2dbdb5-ac12-4a7d-bc32-b1b262c82132', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.katoX6:/tmp/ceph-asok.katoX6', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid ef2dbdb5-ac12-4a7d-bc32-b1b262c82132 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.katoX6:/tmp/ceph-asok.katoX6 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T17:21:58.157+0000 7f1afad5ed00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T17:21:58.161+0000 7f1afad5ed00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T17:21:58.165+0000 7f1afad5ed00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T17:21:58.177719+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T17:21:58.177719+0000     0       0         0         0         0         0           -           0
2021-05-17T17:21:59.177843+0000     1     255     35957     35702   2230.93   2231.38   0.0015979  0.00708345
2021-05-17T17:22:00.177940+0000     2     255     70462     70207   2193.64   2156.56 0.000977266  0.00725466
2021-05-17T17:22:01.178044+0000     3     255    106883    106628   2221.12   2276.31  0.00858326  0.00718476
2021-05-17T17:22:02.178119+0000     4     256    142669    142413   2224.94   2236.56  0.00221712  0.00717214
2021-05-17T17:22:03.178219+0000     5     255    178091    177836   2222.69   2213.94  0.00640383   0.0071813
2021-05-17T17:22:04.178294+0000     6     256    212969    212713   2215.52   2179.81  0.00300304  0.00720568
2021-05-17T17:22:05.178460+0000 Total time run:       6.3458
Total reads made:     224868
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   2214.73
Average IOPS:         35435
Stddev IOPS:          684.197
Max IOPS:             36421
Min IOPS:             34505
Average Latency(s):   0.00720912
Max latency(s):       0.0310713
Min latency(s):       0.00028736

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:22:05,817214363-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 442513


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:22:05,821674421-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:22:29,066576261-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 224.87k objects, 14 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:22:29,074653865-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:22:37,468322290-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 224.87k objects, 14 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:22:37,476877000-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:22:45,927297547-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 224.87k objects, 14 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:22:45,935799197-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:22:54,279920849-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 224.87k objects, 14 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:22:54,288322171-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:23:02,535388634-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 224.87k objects, 14 GiB
    usage:   27 GiB used, 273 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:23:02,544087885-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:23:02,550078298-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T13:23:02,554310588-04:00][RUNNING][ROUND 1/4/40] object_size=256KB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:23:02,558216625-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:23:02,567071138-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40687\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.377010\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid c4dc76e3-ed5b-4ff2-a655-7500c5132a1c\nsetting min_mon_release = octopus\nepoch 0\nfsid c4dc76e3-ed5b-4ff2-a655-7500c5132a1c\nlast_changed 2021-05-17T10:23:31.400285-0700\ncreated 2021-05-17T10:23:31.400285-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40687/0,v1:10.10.1.2:40688/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.377010 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 8fa17075-4a8b-4b21-9fb3-0fa9632114a4\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 6d1ad3a5-9853-42bf-a428-53869b6e05d5\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 644707a5-9fc4-42a4-a377-dbe7cdc9863d\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42687\n  w/ user/pass: admin / 4c080d55-8482-48b0-89d1-6185eff25210\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 13:23:44 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40687
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.377010
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid c4dc76e3-ed5b-4ff2-a655-7500c5132a1c
setting min_mon_release = octopus
epoch 0
fsid c4dc76e3-ed5b-4ff2-a655-7500c5132a1c
last_changed 2021-05-17T10:23:31.400285-0700
created 2021-05-17T10:23:31.400285-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40687/0,v1:10.10.1.2:40688/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.377010 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 8fa17075-4a8b-4b21-9fb3-0fa9632114a4
0
start osd.0
add osd1 6d1ad3a5-9853-42bf-a428-53869b6e05d5
1
start osd.1
add osd2 644707a5-9fc4-42a4-a377-dbe7cdc9863d
2
start osd.2


restful urls: https://10.10.1.2:42687
  w/ user/pass: admin / 4c080d55-8482-48b0-89d1-6185eff25210


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T10:23:03.471-0700 7fd9772df1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:23:03.471-0700 7fd9772df1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:23:03.487-0700 7f2cae50a1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:23:03.487-0700 7f2cae50a1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40687,v1:10.10.1.2:40688] --print /tmp/ceph_monmap.377010 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.377010 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.377010 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42687 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.IDw9i7rBT6 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 8fa17075-4a8b-4b21-9fb3-0fa9632114a4 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCcpqJg2Qj9DRAAf4QruTrlOvWSk88n5BTKzA== --osd-uuid 8fa17075-4a8b-4b21-9fb3-0fa9632114a4 
2021-05-17T10:23:40.556-0700 7fafc1e1bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T10:23:40.556-0700 7fafc1e1bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T10:23:40.556-0700 7fafc1e1bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T10:23:40.636-0700 7fafc1e1bf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 6d1ad3a5-9853-42bf-a428-53869b6e05d5 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T10:23:40.964-0700 7faddadb0f00 -1 Falling back to public interface
2021-05-17T10:23:40.976-0700 7faddadb0f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCcpqJg6G6dORAAaOcnOxitkTyS+BlOVKvPPw== --osd-uuid 6d1ad3a5-9853-42bf-a428-53869b6e05d5 
2021-05-17T10:23:41.296-0700 7fb65cc4bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T10:23:41.296-0700 7fb65cc4bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T10:23:41.296-0700 7fb65cc4bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T10:23:41.340-0700 7fb65cc4bf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 644707a5-9fc4-42a4-a377-dbe7cdc9863d -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T10:23:41.592-0700 7f28ea2faf00 -1 Falling back to public interface
2021-05-17T10:23:41.604-0700 7f28ea2faf00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCdpqJgqU9qIxAAiUk5zNfTuuvljFAyQpSK6w== --osd-uuid 644707a5-9fc4-42a4-a377-dbe7cdc9863d 
2021-05-17T10:23:41.924-0700 7f08e265af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T10:23:41.924-0700 7f08e265af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T10:23:41.924-0700 7f08e265af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T10:23:41.984-0700 7f08e265af00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T10:23:42.304-0700 7f9f3396ef00 -1 Falling back to public interface
2021-05-17T10:23:42.320-0700 7f9f3396ef00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:23:44,349954259-04:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:23:44,354388699-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:23:44,395436776-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:23:44,398643089-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f3b40365-f77a-4b26-bc7f-bccee69b03da', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6NpS0A:/tmp/ceph-asok.6NpS0A', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f3b40365-f77a-4b26-bc7f-bccee69b03da --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6NpS0A:/tmp/ceph-asok.6NpS0A -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:23:47,892323907-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:23:47,896304424-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f3b40365-f77a-4b26-bc7f-bccee69b03da', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6NpS0A:/tmp/ceph-asok.6NpS0A', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f3b40365-f77a-4b26-bc7f-bccee69b03da --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6NpS0A:/tmp/ceph-asok.6NpS0A -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:23:51,309659203-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:23:51,313410459-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f3b40365-f77a-4b26-bc7f-bccee69b03da', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6NpS0A:/tmp/ceph-asok.6NpS0A', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f3b40365-f77a-4b26-bc7f-bccee69b03da --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6NpS0A:/tmp/ceph-asok.6NpS0A -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:23:54,616135364-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:23:54,619894285-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f3b40365-f77a-4b26-bc7f-bccee69b03da', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6NpS0A:/tmp/ceph-asok.6NpS0A', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f3b40365-f77a-4b26-bc7f-bccee69b03da --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6NpS0A:/tmp/ceph-asok.6NpS0A -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:24:01,555493371-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:24:01,559222546-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f3b40365-f77a-4b26-bc7f-bccee69b03da', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6NpS0A:/tmp/ceph-asok.6NpS0A', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f3b40365-f77a-4b26-bc7f-bccee69b03da --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6NpS0A:/tmp/ceph-asok.6NpS0A -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:24:05,787035781-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:24:05,790722767-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f3b40365-f77a-4b26-bc7f-bccee69b03da', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6NpS0A:/tmp/ceph-asok.6NpS0A', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f3b40365-f77a-4b26-bc7f-bccee69b03da --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6NpS0A:/tmp/ceph-asok.6NpS0A -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:24:09,335003048-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:24:09,338883848-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f3b40365-f77a-4b26-bc7f-bccee69b03da', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6NpS0A:/tmp/ceph-asok.6NpS0A', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f3b40365-f77a-4b26-bc7f-bccee69b03da --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6NpS0A:/tmp/ceph-asok.6NpS0A -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:24:13,740141133-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:24:13,743799626-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f3b40365-f77a-4b26-bc7f-bccee69b03da', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6NpS0A:/tmp/ceph-asok.6NpS0A', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f3b40365-f77a-4b26-bc7f-bccee69b03da --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6NpS0A:/tmp/ceph-asok.6NpS0A -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:24:17,213337448-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:24:17,216407355-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f3b40365-f77a-4b26-bc7f-bccee69b03da', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6NpS0A:/tmp/ceph-asok.6NpS0A', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f3b40365-f77a-4b26-bc7f-bccee69b03da --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6NpS0A:/tmp/ceph-asok.6NpS0A -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:24:21,453656478-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:24:21,457425578-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f3b40365-f77a-4b26-bc7f-bccee69b03da', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6NpS0A:/tmp/ceph-asok.6NpS0A', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f3b40365-f77a-4b26-bc7f-bccee69b03da --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6NpS0A:/tmp/ceph-asok.6NpS0A -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:24:25,309932737-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:24:25,313969319-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f3b40365-f77a-4b26-bc7f-bccee69b03da', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6NpS0A:/tmp/ceph-asok.6NpS0A', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f3b40365-f77a-4b26-bc7f-bccee69b03da --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6NpS0A:/tmp/ceph-asok.6NpS0A -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:24:28,717287292-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:24:28,720701726-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f3b40365-f77a-4b26-bc7f-bccee69b03da', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6NpS0A:/tmp/ceph-asok.6NpS0A', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f3b40365-f77a-4b26-bc7f-bccee69b03da --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6NpS0A:/tmp/ceph-asok.6NpS0A -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  155 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:24:32,038482411-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:24:55,316215963-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=========================...] 
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:25:03,899672295-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:25:12,388155166-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:25:20,852135434-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:25:29,198429417-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:25:37,670428546-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:25:46,092708976-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:25:46,101183565-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:25:54,433425082-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:25:54,442240402-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:26:03,021733640-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:26:03,030545913-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:26:11,478008078-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:26:11,486627710-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:26:20,065635103-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:26:20,074225270-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:26:20,081030644-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:26:20,085034685-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:26:20,094012620-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=449121
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_256KB_write.dat.1 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:26:20,099978687-04:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_256KB_write.log.1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:26:20,119997071-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:26:20,123471057-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f3b40365-f77a-4b26-bc7f-bccee69b03da', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6NpS0A:/tmp/ceph-asok.6NpS0A', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f3b40365-f77a-4b26-bc7f-bccee69b03da --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6NpS0A:/tmp/ceph-asok.6NpS0A -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T17:26:22.693+0000 7fbbb8d99d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T17:26:22.697+0000 7fbbb8d99d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T17:26:22.697+0000 7fbbb8d99d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T17:26:22.709714+0000 Maintaining 256 concurrent writes of 262144 bytes to objects of size 262144 for up to 20 seconds or 0 objects
2021-05-17T17:26:22.709727+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-17T17:26:22.726072+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T17:26:22.726072+0000     0       0         0         0         0         0           -           0
2021-05-17T17:26:23.726158+0000     1     255      4074      3819    954.71    954.75   0.0456546   0.0639124
2021-05-17T17:26:24.726259+0000     2     255      8122      7867   983.305      1012   0.0389801   0.0634059
2021-05-17T17:26:25.726374+0000     3     255     12257     12002   1000.08   1033.75   0.0477086    0.062933
2021-05-17T17:26:26.726484+0000     4     255     16413     16158   1009.78      1039   0.0550171   0.0625577
2021-05-17T17:26:27.726593+0000     5     255     20570     20315   1015.65   1039.25   0.0990905    0.062401
2021-05-17T17:26:28.726697+0000     6     255     24687     24432    1017.9   1029.25    0.043289   0.0623404
2021-05-17T17:26:29.726803+0000     7     255     28797     28542   1019.26    1027.5   0.0927989   0.0623496
2021-05-17T17:26:30.726919+0000     8     255     32858     32603   1018.74   1015.25   0.0473412   0.0624319
2021-05-17T17:26:31.727041+0000     9     255     36945     36690   1019.06   1021.75   0.0966411   0.0624509
2021-05-17T17:26:32.727145+0000    10     255     40934     40679   1016.87    997.25   0.0896774   0.0626378
2021-05-17T17:26:33.727250+0000    11     255     44720     44465   1010.46     946.5   0.0461108   0.0630451
2021-05-17T17:26:34.727353+0000    12     255     48252     47997   999.834       883   0.0704205   0.0637293
2021-05-17T17:26:35.727462+0000    13     255     52241     51986   999.627    997.25    0.103203   0.0637681
2021-05-17T17:26:36.727581+0000    14     255     56124     55869   997.556    970.75   0.0547759    0.063897
2021-05-17T17:26:37.727696+0000    15     255     60265     60010   1000.06   1035.25   0.0515655   0.0637728
2021-05-17T17:26:38.727801+0000    16     255     64287     64032   1000.39    1005.5   0.0506577   0.0637696
2021-05-17T17:26:39.727915+0000    17     255     68106     67851   997.703    954.75    0.055545   0.0639527
2021-05-17T17:26:40.728018+0000    18     255     72212     71957   999.297    1026.5   0.0478271   0.0638581
2021-05-17T17:26:41.728125+0000    19     255     76069     75814   997.447    964.25   0.0548247   0.0639816
2021-05-17T17:26:42.728232+0000 min lat: 0.00304008 max lat: 0.198565 avg lat: 0.0638241
2021-05-17T17:26:42.728232+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T17:26:42.728232+0000    20     108     80206     80098   1001.12      1071  0.00304049   0.0638241
2021-05-17T17:26:43.728463+0000 Total time run:         20.0194
Total writes made:      80206
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     1001.6
Stddev Bandwidth:       43.7957
Max bandwidth (MB/sec): 1071
Min bandwidth (MB/sec): 883
Average IOPS:           4006
Stddev IOPS:            175.183
Max IOPS:               4284
Min IOPS:               3532
Average Latency(s):     0.0638017
Stddev Latency(s):      0.023351
Max latency(s):         0.198565
Min latency(s):         0.00304008

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:26:44,313564034-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 449121


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:26:44,318280394-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:27:07,759327029-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 80.21k objects, 20 GiB
    usage:   39 GiB used, 261 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:27:07,768353345-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:27:16,216398588-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 80.21k objects, 20 GiB
    usage:   39 GiB used, 261 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:27:16,225444871-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:27:24,576207003-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 80.21k objects, 20 GiB
    usage:   39 GiB used, 261 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:27:24,584923968-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:27:32,908868779-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 80.21k objects, 20 GiB
    usage:   39 GiB used, 261 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:27:32,917023938-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:27:41,189458622-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 80.21k objects, 20 GiB
    usage:   39 GiB used, 261 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:27:41,198327512-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:27:41,204929944-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:27:41,208596502-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:27:41,217623188-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=450486
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_256KB_seq.dat.1 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:27:41,224346868-04:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_256KB_seq.log.1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:27:41,243624551-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:27:41,247361571-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f3b40365-f77a-4b26-bc7f-bccee69b03da', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6NpS0A:/tmp/ceph-asok.6NpS0A', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f3b40365-f77a-4b26-bc7f-bccee69b03da --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.6NpS0A:/tmp/ceph-asok.6NpS0A -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T17:27:43.730+0000 7fc57da2cd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T17:27:43.734+0000 7fc57da2cd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T17:27:43.734+0000 7fc57da2cd00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T17:27:43.745861+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T17:27:43.745861+0000     0       0         0         0         0         0           -           0
2021-05-17T17:27:44.745987+0000     1     255     19897     19642   4909.58    4910.5  0.00930878    0.012885
2021-05-17T17:27:45.746113+0000     2     255     38825     38570    4820.5      4732  0.00604728   0.0131956
2021-05-17T17:27:46.746288+0000     3     256     57435     57179   4764.14   4652.25   0.0107001   0.0132497
2021-05-17T17:27:47.746470+0000     4     255     75880     75625   4725.77    4611.5   0.0196666   0.0134821
2021-05-17T17:27:48.746680+0000 Total time run:       4.2614
Total reads made:     80206
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   4705.38
Average IOPS:         18821
Stddev IOPS:          529.776
Max IOPS:             19642
Min IOPS:             18446
Average Latency(s):   0.0135315
Max latency(s):       0.0772103
Min latency(s):       0.000460174

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:27:49,395049375-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 450486


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:27:49,399511647-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:28:12,771791384-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 80.21k objects, 20 GiB
    usage:   39 GiB used, 261 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:28:12,780412669-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:28:21,136104669-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 80.21k objects, 20 GiB
    usage:   39 GiB used, 261 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:28:21,144851690-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:28:29,572753690-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 80.21k objects, 20 GiB
    usage:   39 GiB used, 261 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:28:29,581927383-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:28:37,920045979-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 80.21k objects, 20 GiB
    usage:   39 GiB used, 261 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:28:37,929618221-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:28:46,238515520-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 80.21k objects, 20 GiB
    usage:   39 GiB used, 261 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:28:46,246150502-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:28:46,252376818-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T13:28:46,254815229-04:00][RUNNING][ROUND 2/4/40] object_size=256KB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:28:46,258319382-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:28:46,266695516-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40717\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.378195\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 5421707a-9940-41af-b3e7-34695becd238\nsetting min_mon_release = octopus\nepoch 0\nfsid 5421707a-9940-41af-b3e7-34695becd238\nlast_changed 2021-05-17T10:29:14.625974-0700\ncreated 2021-05-17T10:29:14.625974-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40717/0,v1:10.10.1.2:40718/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.378195 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 208ee58a-b530-4c6a-bed3-d2ad998ad73e\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 40e98f1e-baf3-46ec-ab7d-a8e19d4c5c03\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 52798125-a955-4495-a7f8-b0dbb27b90b5\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42717\n  w/ user/pass: admin / c48ca1f8-91db-4e21-8aa2-00e851dd8aae\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\nCEPH_DEV=1\n'
[1] 13:29:27 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40717
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.378195
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 5421707a-9940-41af-b3e7-34695becd238
setting min_mon_release = octopus
epoch 0
fsid 5421707a-9940-41af-b3e7-34695becd238
last_changed 2021-05-17T10:29:14.625974-0700
created 2021-05-17T10:29:14.625974-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40717/0,v1:10.10.1.2:40718/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.378195 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 208ee58a-b530-4c6a-bed3-d2ad998ad73e
0
start osd.0
add osd1 40e98f1e-baf3-46ec-ab7d-a8e19d4c5c03
1
start osd.1
add osd2 52798125-a955-4495-a7f8-b0dbb27b90b5
2
start osd.2


restful urls: https://10.10.1.2:42717
  w/ user/pass: admin / c48ca1f8-91db-4e21-8aa2-00e851dd8aae


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T10:28:47.178-0700 7f45521d41c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:28:47.178-0700 7f45521d41c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:28:47.194-0700 7f3669db11c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:28:47.194-0700 7f3669db11c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40717,v1:10.10.1.2:40718] --print /tmp/ceph_monmap.378195 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.378195 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.378195 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42717 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.UE8xRPLgAc 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 208ee58a-b530-4c6a-bed3-d2ad998ad73e -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDzp6Jg1Bd5HBAA12KtDkzOD6UAHj3QbuxCmg== --osd-uuid 208ee58a-b530-4c6a-bed3-d2ad998ad73e 
2021-05-17T10:29:23.827-0700 7fe40b41ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T10:29:23.827-0700 7fe40b41ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T10:29:23.827-0700 7fe40b41ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T10:29:23.899-0700 7fe40b41ef00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 40e98f1e-baf3-46ec-ab7d-a8e19d4c5c03 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T10:29:24.187-0700 7fe00bd03f00 -1 Falling back to public interface
2021-05-17T10:29:24.199-0700 7fe00bd03f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQD0p6JgGQMDCxAAS0N09vCy1s7Ypgmj69wJug== --osd-uuid 40e98f1e-baf3-46ec-ab7d-a8e19d4c5c03 
2021-05-17T10:29:24.515-0700 7f2853b42f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T10:29:24.515-0700 7f2853b42f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T10:29:24.515-0700 7f2853b42f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T10:29:24.559-0700 7f2853b42f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 52798125-a955-4495-a7f8-b0dbb27b90b5 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T10:29:24.863-0700 7fb854fe3f00 -1 Falling back to public interface
2021-05-17T10:29:24.875-0700 7fb854fe3f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQD0p6Jg1RJaMxAArrPdSh2v3ESEQAv80H40Jg== --osd-uuid 52798125-a955-4495-a7f8-b0dbb27b90b5 
2021-05-17T10:29:25.235-0700 7f3a1119bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T10:29:25.235-0700 7f3a1119bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T10:29:25.235-0700 7f3a1119bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T10:29:25.287-0700 7f3a1119bf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T10:29:25.667-0700 7f893cccaf00 -1 Falling back to public interface
2021-05-17T10:29:25.683-0700 7f893cccaf00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:29:27,658968991-04:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:29:27,662957504-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:29:27,703697684-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:29:27,707132175-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8565450f-b3d2-4a52-ae93-c1ef26680c0d', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kPgEpV:/tmp/ceph-asok.kPgEpV', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8565450f-b3d2-4a52-ae93-c1ef26680c0d --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kPgEpV:/tmp/ceph-asok.kPgEpV -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:29:31,127641292-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:29:31,130940319-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8565450f-b3d2-4a52-ae93-c1ef26680c0d', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kPgEpV:/tmp/ceph-asok.kPgEpV', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8565450f-b3d2-4a52-ae93-c1ef26680c0d --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kPgEpV:/tmp/ceph-asok.kPgEpV -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:29:34,602315286-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:29:34,606007061-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8565450f-b3d2-4a52-ae93-c1ef26680c0d', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kPgEpV:/tmp/ceph-asok.kPgEpV', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8565450f-b3d2-4a52-ae93-c1ef26680c0d --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kPgEpV:/tmp/ceph-asok.kPgEpV -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:29:37,851705479-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:29:37,855082673-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8565450f-b3d2-4a52-ae93-c1ef26680c0d', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kPgEpV:/tmp/ceph-asok.kPgEpV', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8565450f-b3d2-4a52-ae93-c1ef26680c0d --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kPgEpV:/tmp/ceph-asok.kPgEpV -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:29:44,521442657-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:29:44,525505499-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8565450f-b3d2-4a52-ae93-c1ef26680c0d', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kPgEpV:/tmp/ceph-asok.kPgEpV', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8565450f-b3d2-4a52-ae93-c1ef26680c0d --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kPgEpV:/tmp/ceph-asok.kPgEpV -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:29:48,944041579-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:29:48,947706764-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8565450f-b3d2-4a52-ae93-c1ef26680c0d', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kPgEpV:/tmp/ceph-asok.kPgEpV', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8565450f-b3d2-4a52-ae93-c1ef26680c0d --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kPgEpV:/tmp/ceph-asok.kPgEpV -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:29:53,266382828-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:29:53,269795479-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8565450f-b3d2-4a52-ae93-c1ef26680c0d', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kPgEpV:/tmp/ceph-asok.kPgEpV', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8565450f-b3d2-4a52-ae93-c1ef26680c0d --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kPgEpV:/tmp/ceph-asok.kPgEpV -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:29:57,511320889-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:29:57,514832916-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8565450f-b3d2-4a52-ae93-c1ef26680c0d', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kPgEpV:/tmp/ceph-asok.kPgEpV', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8565450f-b3d2-4a52-ae93-c1ef26680c0d --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kPgEpV:/tmp/ceph-asok.kPgEpV -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:30:01,803481186-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:30:01,807223546-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8565450f-b3d2-4a52-ae93-c1ef26680c0d', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kPgEpV:/tmp/ceph-asok.kPgEpV', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8565450f-b3d2-4a52-ae93-c1ef26680c0d --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kPgEpV:/tmp/ceph-asok.kPgEpV -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:30:05,499006553-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:30:05,502712896-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8565450f-b3d2-4a52-ae93-c1ef26680c0d', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kPgEpV:/tmp/ceph-asok.kPgEpV', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8565450f-b3d2-4a52-ae93-c1ef26680c0d --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kPgEpV:/tmp/ceph-asok.kPgEpV -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:30:08,963568098-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:30:08,967331869-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8565450f-b3d2-4a52-ae93-c1ef26680c0d', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kPgEpV:/tmp/ceph-asok.kPgEpV', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8565450f-b3d2-4a52-ae93-c1ef26680c0d --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kPgEpV:/tmp/ceph-asok.kPgEpV -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:30:12,384000538-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:30:12,387530289-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8565450f-b3d2-4a52-ae93-c1ef26680c0d', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kPgEpV:/tmp/ceph-asok.kPgEpV', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8565450f-b3d2-4a52-ae93-c1ef26680c0d --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kPgEpV:/tmp/ceph-asok.kPgEpV -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  155 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:30:15,832768878-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:30:39,109601713-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:30:47,591005670-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:30:56,052684909-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:31:04,580784553-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:31:13,174413396-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:31:21,607750396-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:31:29,972185990-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:31:29,980680587-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:31:38,508771701-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:31:38,517155790-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:31:47,028784703-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:31:47,037530201-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:31:55,506616292-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:31:55,515708471-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:32:03,871803332-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:32:03,880996251-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:32:03,887711696-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:32:03,891661345-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:32:03,900626216-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=457132
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_256KB_write.dat.2 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:32:03,906256021-04:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_256KB_write.log.2
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:32:03,925755511-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:32:03,929040011-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8565450f-b3d2-4a52-ae93-c1ef26680c0d', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kPgEpV:/tmp/ceph-asok.kPgEpV', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8565450f-b3d2-4a52-ae93-c1ef26680c0d --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kPgEpV:/tmp/ceph-asok.kPgEpV -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T17:32:06.790+0000 7fea5d701d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T17:32:06.802+0000 7fea5d701d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T17:32:06.802+0000 7fea5d701d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T17:32:06.812209+0000 Maintaining 256 concurrent writes of 262144 bytes to objects of size 262144 for up to 20 seconds or 0 objects
2021-05-17T17:32:06.812221+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-17T17:32:06.828966+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T17:32:06.828966+0000     0       0         0         0         0         0           -           0
2021-05-17T17:32:07.829073+0000     1     255      3601      3346   836.463     836.5   0.0567155   0.0723817
2021-05-17T17:32:08.829177+0000     2     255      7645      7390   923.681      1011    0.048145   0.0674169
2021-05-17T17:32:09.829294+0000     3     255     11737     11482   956.749      1023    0.111635   0.0657892
2021-05-17T17:32:10.829406+0000     4     255     15815     15560   972.408    1019.5   0.0636206   0.0648731
2021-05-17T17:32:11.829512+0000     5     255     19910     19655   982.655   1023.75    0.115012   0.0644446
2021-05-17T17:32:12.829601+0000     6     255     23967     23712   987.906   1014.25   0.0552281   0.0641964
2021-05-17T17:32:13.829669+0000     7     255     27933     27678    988.41     991.5   0.0550213   0.0641717
2021-05-17T17:32:14.829747+0000     8     255     32074     31819   994.254   1035.25   0.0995923   0.0639571
2021-05-17T17:32:15.829825+0000     9     255     36084     35829   995.162    1002.5   0.0698248   0.0638671
2021-05-17T17:32:16.829891+0000    10     255     39972     39717   992.839       972    0.113515   0.0639897
2021-05-17T17:32:17.829958+0000    11     255     44003     43748   994.189   1007.75   0.0541219    0.064049
2021-05-17T17:32:18.830031+0000    12     255     48031     47776    995.25      1007    0.107782   0.0640187
2021-05-17T17:32:19.830108+0000    13     255     51603     51348    987.38       893    0.060911   0.0645364
2021-05-17T17:32:20.830175+0000    14     255     55784     55529   991.508   1045.25   0.0531309    0.064333
2021-05-17T17:32:21.830241+0000    15     255     60014     59759   995.903    1057.5    0.109007   0.0640598
2021-05-17T17:32:22.830313+0000    16     255     63912     63657   994.561     974.5   0.0506413   0.0641356
2021-05-17T17:32:23.830386+0000    17     255     67950     67695   995.435    1009.5   0.0517706   0.0641079
2021-05-17T17:32:24.830453+0000    18     255     71653     71398    991.56    925.75    0.108895   0.0643356
2021-05-17T17:32:25.830520+0000    19     255     75825     75570   994.264      1043   0.0503678   0.0641932
2021-05-17T17:32:26.830586+0000 min lat: 0.0233479 max lat: 0.192347 avg lat: 0.0642297
2021-05-17T17:32:26.830586+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T17:32:26.830586+0000    20     255     79784     79529   994.035    989.75   0.0487309   0.0642297
2021-05-17T17:32:27.830731+0000 Total time run:         20.0249
Total writes made:      79785
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     996.071
Stddev Bandwidth:       53.8326
Max bandwidth (MB/sec): 1057.5
Min bandwidth (MB/sec): 836.5
Average IOPS:           3984
Stddev IOPS:            215.33
Max IOPS:               4230
Min IOPS:               3346
Average Latency(s):     0.0641428
Stddev Latency(s):      0.0212244
Max latency(s):         0.192347
Min latency(s):         0.00437335

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:32:28,368137562-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 457132


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:32:28,372704932-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:32:51,897777132-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 79.79k objects, 19 GiB
    usage:   39 GiB used, 261 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:32:51,906266750-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:33:00,328298829-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 79.79k objects, 19 GiB
    usage:   39 GiB used, 261 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:33:00,337139145-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:33:08,880056181-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 79.79k objects, 19 GiB
    usage:   39 GiB used, 261 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:33:08,888501896-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:33:17,317426260-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 79.79k objects, 19 GiB
    usage:   39 GiB used, 261 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:33:17,326054468-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:33:25,900801935-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 79.79k objects, 19 GiB
    usage:   39 GiB used, 261 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:33:25,909619037-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:33:25,916888373-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:33:25,920491251-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:33:25,929157701-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=458552
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_256KB_seq.dat.2 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:33:25,936003431-04:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_256KB_seq.log.2
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:33:25,956041652-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:33:25,959521880-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8565450f-b3d2-4a52-ae93-c1ef26680c0d', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kPgEpV:/tmp/ceph-asok.kPgEpV', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8565450f-b3d2-4a52-ae93-c1ef26680c0d --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kPgEpV:/tmp/ceph-asok.kPgEpV -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T17:33:28.759+0000 7f8f9be33d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T17:33:28.763+0000 7f8f9be33d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T17:33:28.763+0000 7f8f9be33d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T17:33:28.774694+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T17:33:28.774694+0000     0       0         0         0         0         0           -           0
2021-05-17T17:33:29.774760+0000     1     256     19106     18850   4711.95    4712.5   0.0213661   0.0134088
2021-05-17T17:33:30.774870+0000     2     255     37416     37161    4644.6   4577.75  0.00673679   0.0136973
2021-05-17T17:33:31.774940+0000     3     255     56834     56579   4714.45    4854.5  0.00696811   0.0134894
2021-05-17T17:33:32.774999+0000     4     255     75944     75689   4730.14    4777.5  0.00438823    0.013472
2021-05-17T17:33:33.775268+0000 Total time run:       4.21524
Total reads made:     79785
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   4731.94
Average IOPS:         18927
Stddev IOPS:          468.994
Max IOPS:             19418
Min IOPS:             18311
Average Latency(s):   0.0134666
Max latency(s):       0.053999
Min latency(s):       0.000498537

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:33:34,360588922-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 458552


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:33:34,365277460-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:33:57,617840835-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 79.79k objects, 19 GiB
    usage:   39 GiB used, 261 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:33:57,626482498-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:34:05,967457821-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 79.79k objects, 19 GiB
    usage:   39 GiB used, 261 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:34:05,975944974-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:34:14,397188686-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 79.79k objects, 19 GiB
    usage:   39 GiB used, 261 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:34:14,406217877-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:34:22,812976262-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 79.79k objects, 19 GiB
    usage:   39 GiB used, 261 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:34:22,821191024-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:34:31,126848814-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 79.79k objects, 19 GiB
    usage:   39 GiB used, 261 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:34:31,135443089-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:34:31,142412912-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T13:34:31,145035829-04:00][RUNNING][ROUND 3/4/40] object_size=256KB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:34:31,148326240-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:34:31,157462352-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40971\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.379413\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 33ba86b9-ab87-403f-974d-bee97538d454\nsetting min_mon_release = octopus\nepoch 0\nfsid 33ba86b9-ab87-403f-974d-bee97538d454\nlast_changed 2021-05-17T10:34:59.647279-0700\ncreated 2021-05-17T10:34:59.647279-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40971/0,v1:10.10.1.2:40972/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.379413 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 117a7809-34fe-4f77-a89d-14499001ff92\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 5fd21929-bb0b-4d5c-adcf-2e2250be5b45\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 1398e452-41f0-4710-a861-3ea914f884eb\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42971\n  w/ user/pass: admin / 40da5c5e-a4d9-44a7-bd95-bfed3c59e1b3\n'
10.10.1.2: b'\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\nCEPH_DEV=1\n'
[1] 13:35:12 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40971
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.379413
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 33ba86b9-ab87-403f-974d-bee97538d454
setting min_mon_release = octopus
epoch 0
fsid 33ba86b9-ab87-403f-974d-bee97538d454
last_changed 2021-05-17T10:34:59.647279-0700
created 2021-05-17T10:34:59.647279-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40971/0,v1:10.10.1.2:40972/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.379413 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 117a7809-34fe-4f77-a89d-14499001ff92
0
start osd.0
add osd1 5fd21929-bb0b-4d5c-adcf-2e2250be5b45
1
start osd.1
add osd2 1398e452-41f0-4710-a861-3ea914f884eb
2
start osd.2


restful urls: https://10.10.1.2:42971
  w/ user/pass: admin / 40da5c5e-a4d9-44a7-bd95-bfed3c59e1b3


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T10:34:32.069-0700 7f901511e1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:34:32.069-0700 7f901511e1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:34:32.089-0700 7fa555f1c1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:34:32.089-0700 7fa555f1c1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40971,v1:10.10.1.2:40972] --print /tmp/ceph_monmap.379413 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.379413 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.379413 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42971 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.NAtIGnMmss 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 117a7809-34fe-4f77-a89d-14499001ff92 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBMqaJgYFtfIRAAl5+3mImpsmLjY6KTFentag== --osd-uuid 117a7809-34fe-4f77-a89d-14499001ff92 
2021-05-17T10:35:08.934-0700 7f53a0c88f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T10:35:08.934-0700 7f53a0c88f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T10:35:08.934-0700 7f53a0c88f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T10:35:08.978-0700 7f53a0c88f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 5fd21929-bb0b-4d5c-adcf-2e2250be5b45 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T10:35:09.258-0700 7f828d92df00 -1 Falling back to public interface
2021-05-17T10:35:09.270-0700 7f828d92df00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBNqaJgbuRlDxAAHVWIMCw2/zqmWkHuH1q9gw== --osd-uuid 5fd21929-bb0b-4d5c-adcf-2e2250be5b45 
2021-05-17T10:35:09.590-0700 7f750ccccf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T10:35:09.590-0700 7f750ccccf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T10:35:09.590-0700 7f750ccccf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T10:35:09.634-0700 7f750ccccf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 1398e452-41f0-4710-a861-3ea914f884eb -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T10:35:10.010-0700 7fd76fa36f00 -1 Falling back to public interface
2021-05-17T10:35:10.026-0700 7fd76fa36f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBOqaJggXOrABAAjjsPbLC/MINtdKueA5W0Ag== --osd-uuid 1398e452-41f0-4710-a861-3ea914f884eb 
2021-05-17T10:35:10.342-0700 7fb3ef66ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T10:35:10.342-0700 7fb3ef66ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T10:35:10.342-0700 7fb3ef66ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T10:35:10.410-0700 7fb3ef66ff00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T10:35:10.722-0700 7f29b481cf00 -1 Falling back to public interface
2021-05-17T10:35:10.738-0700 7f29b481cf00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:35:12,767154976-04:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:35:12,771147376-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:35:12,812147726-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:35:12,815372744-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7af33de4-3243-4696-809e-f51196ba9593', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vbWdCk:/tmp/ceph-asok.vbWdCk', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7af33de4-3243-4696-809e-f51196ba9593 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vbWdCk:/tmp/ceph-asok.vbWdCk -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:35:16,557541563-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:35:16,561546857-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7af33de4-3243-4696-809e-f51196ba9593', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vbWdCk:/tmp/ceph-asok.vbWdCk', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7af33de4-3243-4696-809e-f51196ba9593 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vbWdCk:/tmp/ceph-asok.vbWdCk -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:35:20,068071628-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:35:20,071845156-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7af33de4-3243-4696-809e-f51196ba9593', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vbWdCk:/tmp/ceph-asok.vbWdCk', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7af33de4-3243-4696-809e-f51196ba9593 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vbWdCk:/tmp/ceph-asok.vbWdCk -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:35:23,452611812-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:35:23,456543618-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7af33de4-3243-4696-809e-f51196ba9593', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vbWdCk:/tmp/ceph-asok.vbWdCk', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7af33de4-3243-4696-809e-f51196ba9593 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vbWdCk:/tmp/ceph-asok.vbWdCk -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:35:30,100315003-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:35:30,103691275-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7af33de4-3243-4696-809e-f51196ba9593', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vbWdCk:/tmp/ceph-asok.vbWdCk', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7af33de4-3243-4696-809e-f51196ba9593 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vbWdCk:/tmp/ceph-asok.vbWdCk -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:35:33,831130769-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:35:33,834861898-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7af33de4-3243-4696-809e-f51196ba9593', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vbWdCk:/tmp/ceph-asok.vbWdCk', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7af33de4-3243-4696-809e-f51196ba9593 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vbWdCk:/tmp/ceph-asok.vbWdCk -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:35:37,371067177-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:35:37,374990126-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7af33de4-3243-4696-809e-f51196ba9593', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vbWdCk:/tmp/ceph-asok.vbWdCk', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7af33de4-3243-4696-809e-f51196ba9593 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vbWdCk:/tmp/ceph-asok.vbWdCk -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:35:41,561065431-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:35:41,565040207-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7af33de4-3243-4696-809e-f51196ba9593', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vbWdCk:/tmp/ceph-asok.vbWdCk', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7af33de4-3243-4696-809e-f51196ba9593 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vbWdCk:/tmp/ceph-asok.vbWdCk -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:35:46,059533808-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:35:46,063477136-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7af33de4-3243-4696-809e-f51196ba9593', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vbWdCk:/tmp/ceph-asok.vbWdCk', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7af33de4-3243-4696-809e-f51196ba9593 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vbWdCk:/tmp/ceph-asok.vbWdCk -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:35:50,598661396-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:35:50,602546824-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7af33de4-3243-4696-809e-f51196ba9593', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vbWdCk:/tmp/ceph-asok.vbWdCk', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7af33de4-3243-4696-809e-f51196ba9593 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vbWdCk:/tmp/ceph-asok.vbWdCk -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:35:54,862565214-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:35:54,866031035-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7af33de4-3243-4696-809e-f51196ba9593', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vbWdCk:/tmp/ceph-asok.vbWdCk', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7af33de4-3243-4696-809e-f51196ba9593 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vbWdCk:/tmp/ceph-asok.vbWdCk -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:35:58,144460585-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:35:58,148171366-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7af33de4-3243-4696-809e-f51196ba9593', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vbWdCk:/tmp/ceph-asok.vbWdCk', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7af33de4-3243-4696-809e-f51196ba9593 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vbWdCk:/tmp/ceph-asok.vbWdCk -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  155 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:36:01,615033808-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:36:25,004103299-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [===================.........] (remaining: 4s)
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:36:33,404543382-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:36:41,772337082-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:36:50,139833556-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:36:58,609410293-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:37:07,025512200-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:37:15,464965928-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:37:15,473790235-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:37:23,804925048-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:37:23,813565148-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:37:32,426335291-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:37:32,435069689-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:37:40,947277196-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:37:40,956078209-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:37:49,295177444-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:37:49,303383599-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:37:49,310078195-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:37:49,314158280-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:37:49,322381657-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=465200
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_256KB_write.dat.3 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:37:49,328511713-04:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_256KB_write.log.3
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:37:49,346408602-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:37:49,349742194-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7af33de4-3243-4696-809e-f51196ba9593', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vbWdCk:/tmp/ceph-asok.vbWdCk', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7af33de4-3243-4696-809e-f51196ba9593 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vbWdCk:/tmp/ceph-asok.vbWdCk -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T17:37:51.928+0000 7f42448edd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T17:37:51.936+0000 7f42448edd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T17:37:51.936+0000 7f42448edd00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T17:37:51.942919+0000 Maintaining 256 concurrent writes of 262144 bytes to objects of size 262144 for up to 20 seconds or 0 objects
2021-05-17T17:37:51.942930+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-17T17:37:51.959292+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T17:37:51.959292+0000     0       0         0         0         0         0           -           0
2021-05-17T17:37:52.959412+0000     1     255      4129      3874   968.433     968.5   0.0492263   0.0626047
2021-05-17T17:37:53.959522+0000     2     255      8143      7888   985.912    1003.5   0.0629098   0.0631232
2021-05-17T17:37:54.959671+0000     3     255     12270     12015   1001.14   1031.75     0.05054   0.0628618
2021-05-17T17:37:55.959791+0000     4     255     16391     16136   1008.39   1030.25   0.0428504   0.0626486
2021-05-17T17:37:56.959930+0000     5     255     20458     20203   1010.03   1016.75    0.102064   0.0626822
2021-05-17T17:37:57.960041+0000     6     255     24524     24269   1011.09    1016.5   0.0561316   0.0627236
2021-05-17T17:37:58.960181+0000     7     255     28598     28343   1012.13    1018.5    0.106739   0.0627297
2021-05-17T17:37:59.960301+0000     8     255     32758     32503    1015.6      1040   0.0452491   0.0626146
2021-05-17T17:38:00.960439+0000     9     255     36538     36283   1007.74       945   0.0483734    0.063083
2021-05-17T17:38:01.960577+0000    10     255     40476     40221    1005.4     984.5   0.0581598   0.0632952
2021-05-17T17:38:02.960718+0000    11     255     44444     44189   1004.17       992   0.0508672   0.0634263
2021-05-17T17:38:03.960855+0000    12     255     48557     48302   1006.16   1028.25    0.052922   0.0632885
2021-05-17T17:38:04.960993+0000    13     255     52101     51846   996.912       886   0.0519306   0.0639232
2021-05-17T17:38:05.961104+0000    14     255     56054     55799   996.285    988.25    0.047134   0.0640063
2021-05-17T17:38:06.961249+0000    15     255     59936     59681   994.557     970.5    0.115686   0.0641012
2021-05-17T17:38:07.961348+0000    16     255     63765     63510   992.219    957.25   0.0411478   0.0642505
2021-05-17T17:38:08.961420+0000    17     255     67686     67431   991.511    980.25   0.0490145   0.0643638
2021-05-17T17:38:09.961490+0000    18     255     71644     71389   991.396     989.5   0.0448845   0.0643809
2021-05-17T17:38:10.961600+0000    19     255     75360     75105   988.106       929   0.0946902   0.0645831
2021-05-17T17:38:11.961671+0000 min lat: 0.0019729 max lat: 0.248985 avg lat: 0.0650223
2021-05-17T17:38:11.961671+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T17:38:11.961671+0000    20     131     78763     78632   982.786    881.75  0.00682829   0.0650223
2021-05-17T17:38:12.962055+0000 Total time run:         20.0256
Total writes made:      78763
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     983.279
Stddev Bandwidth:       45.3397
Max bandwidth (MB/sec): 1040
Min bandwidth (MB/sec): 881.75
Average IOPS:           3933
Stddev IOPS:            181.359
Max IOPS:               4160
Min IOPS:               3527
Average Latency(s):     0.0649798
Stddev Latency(s):      0.0268156
Max latency(s):         0.248985
Min latency(s):         0.0019729

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:38:13,743177927-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 465200


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:38:13,747754715-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:38:37,090260231-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 78.76k objects, 19 GiB
    usage:   38 GiB used, 262 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:38:37,099013544-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:38:45,612333248-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 78.76k objects, 19 GiB
    usage:   38 GiB used, 262 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:38:45,621135603-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:38:54,201963245-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 78.76k objects, 19 GiB
    usage:   38 GiB used, 262 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:38:54,210334300-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:39:02,553369550-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 78.76k objects, 19 GiB
    usage:   38 GiB used, 262 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:39:02,562176233-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:39:11,070613957-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 78.76k objects, 19 GiB
    usage:   38 GiB used, 262 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:39:11,078380576-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:39:11,084817789-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:39:11,088837530-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:39:11,097458315-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=466634
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_256KB_seq.dat.3 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:39:11,104167338-04:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_256KB_seq.log.3
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:39:11,123579594-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:39:11,126917514-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7af33de4-3243-4696-809e-f51196ba9593', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vbWdCk:/tmp/ceph-asok.vbWdCk', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7af33de4-3243-4696-809e-f51196ba9593 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.vbWdCk:/tmp/ceph-asok.vbWdCk -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T17:39:14.140+0000 7fb8f4aded00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T17:39:14.168+0000 7fb8f4aded00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T17:39:14.168+0000 7fb8f4aded00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T17:39:14.179253+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T17:39:14.179253+0000     0       0         0         0         0         0           -           0
2021-05-17T17:39:15.179367+0000     1     255     18841     18586   4645.68    4646.5     0.01389   0.0136474
2021-05-17T17:39:16.179478+0000     2     255     37478     37223   4652.21   4659.25  0.00583397   0.0136695
2021-05-17T17:39:17.179569+0000     3     255     55851     55596   4632.42   4593.25  0.00982156   0.0137284
2021-05-17T17:39:18.179674+0000     4     255     74075     73820   4613.19      4556  0.00566575   0.0137978
2021-05-17T17:39:19.179900+0000 Total time run:       4.2747
Total reads made:     78763
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   4606.35
Average IOPS:         18425
Stddev IOPS:          191.807
Max IOPS:             18637
Min IOPS:             18224
Average Latency(s):   0.0138114
Max latency(s):       0.0707573
Min latency(s):       0.00051677

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:39:19,824217903-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 466634


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:39:19,828673282-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:39:43,201584982-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 78.76k objects, 19 GiB
    usage:   38 GiB used, 262 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:39:43,209951849-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:39:51,826748394-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 78.76k objects, 19 GiB
    usage:   38 GiB used, 262 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:39:51,835165936-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:40:00,172234531-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 78.76k objects, 19 GiB
    usage:   38 GiB used, 262 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:40:00,180970341-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:40:08,585372145-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 78.76k objects, 19 GiB
    usage:   38 GiB used, 262 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:40:08,593880318-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:40:17,124731122-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 78.76k objects, 19 GiB
    usage:   38 GiB used, 262 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:40:17,133363378-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:40:17,139956814-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T13:40:17,142386568-04:00][RUNNING][ROUND 4/4/40] object_size=256KB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:40:17,146068124-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:40:17,154116303-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40521\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.380672\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid b9647f74-3dfe-4021-b4b8-bc99c361771a\nsetting min_mon_release = octopus\nepoch 0\nfsid b9647f74-3dfe-4021-b4b8-bc99c361771a\nlast_changed 2021-05-17T10:40:54.259461-0700\ncreated 2021-05-17T10:40:54.259461-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40521/0,v1:10.10.1.2:40522/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.380672 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 c792b472-5404-4476-a4c1-8f43f528067f\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 45ed8a00-5ad9-4148-9c59-55c44ad751e2\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 06ca8a65-93bd-4203-be6b-3f83aedfacaa\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42521\n  w/ user/pass: admin / 9f3f6223-9bde-4996-9dec-cc6f192c139f\n\n'
10.10.1.2: b'\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 13:41:07 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40521
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.380672
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid b9647f74-3dfe-4021-b4b8-bc99c361771a
setting min_mon_release = octopus
epoch 0
fsid b9647f74-3dfe-4021-b4b8-bc99c361771a
last_changed 2021-05-17T10:40:54.259461-0700
created 2021-05-17T10:40:54.259461-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40521/0,v1:10.10.1.2:40522/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.380672 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 c792b472-5404-4476-a4c1-8f43f528067f
0
start osd.0
add osd1 45ed8a00-5ad9-4148-9c59-55c44ad751e2
1
start osd.1
add osd2 06ca8a65-93bd-4203-be6b-3f83aedfacaa
2
start osd.2


restful urls: https://10.10.1.2:42521
  w/ user/pass: admin / 9f3f6223-9bde-4996-9dec-cc6f192c139f


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T10:40:18.076-0700 7f36ab6741c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:40:18.080-0700 7f36ab6741c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:40:18.096-0700 7f82e0e5e1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:40:18.096-0700 7f82e0e5e1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40521,v1:10.10.1.2:40522] --print /tmp/ceph_monmap.380672 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.380672 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.380672 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42521 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.mv4UNbESnJ 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new c792b472-5404-4476-a4c1-8f43f528067f -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCvqqJgZLcjCxAA6WPU3YTPNNgthdrAYLEdHg== --osd-uuid c792b472-5404-4476-a4c1-8f43f528067f 
2021-05-17T10:41:03.537-0700 7fa821564f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T10:41:03.537-0700 7fa821564f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T10:41:03.537-0700 7fa821564f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T10:41:03.581-0700 7fa821564f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 45ed8a00-5ad9-4148-9c59-55c44ad751e2 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T10:41:03.825-0700 7fce56f55f00 -1 Falling back to public interface
2021-05-17T10:41:03.837-0700 7fce56f55f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCvqqJg0rw6MRAAi7uyVJ7Cb74JcQzSdP7agQ== --osd-uuid 45ed8a00-5ad9-4148-9c59-55c44ad751e2 
2021-05-17T10:41:04.157-0700 7fe97ae6ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T10:41:04.157-0700 7fe97ae6ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T10:41:04.157-0700 7fe97ae6ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T10:41:04.225-0700 7fe97ae6ef00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 06ca8a65-93bd-4203-be6b-3f83aedfacaa -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T10:41:04.545-0700 7fe0e09f8f00 -1 Falling back to public interface
2021-05-17T10:41:04.561-0700 7fe0e09f8f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCwqqJgrvWkIBAAFn+RCUTSNTWcv4vEqlJAkQ== --osd-uuid 06ca8a65-93bd-4203-be6b-3f83aedfacaa 
2021-05-17T10:41:04.897-0700 7f26758a8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T10:41:04.901-0700 7f26758a8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T10:41:04.901-0700 7f26758a8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T10:41:04.957-0700 7f26758a8f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T10:41:05.333-0700 7fc158fa5f00 -1 Falling back to public interface
2021-05-17T10:41:05.345-0700 7fc158fa5f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:41:07,327019851-04:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:41:07,331315360-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:41:07,370868071-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:41:07,374259602-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f08468c9-6466-49bc-a807-fb1827b24b24', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gnUY1n:/tmp/ceph-asok.gnUY1n', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f08468c9-6466-49bc-a807-fb1827b24b24 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gnUY1n:/tmp/ceph-asok.gnUY1n -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:41:10,738941279-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:41:10,742634376-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f08468c9-6466-49bc-a807-fb1827b24b24', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gnUY1n:/tmp/ceph-asok.gnUY1n', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f08468c9-6466-49bc-a807-fb1827b24b24 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gnUY1n:/tmp/ceph-asok.gnUY1n -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:41:14,078678388-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:41:14,082428763-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f08468c9-6466-49bc-a807-fb1827b24b24', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gnUY1n:/tmp/ceph-asok.gnUY1n', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f08468c9-6466-49bc-a807-fb1827b24b24 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gnUY1n:/tmp/ceph-asok.gnUY1n -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:41:17,374430290-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:41:17,378118949-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f08468c9-6466-49bc-a807-fb1827b24b24', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gnUY1n:/tmp/ceph-asok.gnUY1n', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f08468c9-6466-49bc-a807-fb1827b24b24 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gnUY1n:/tmp/ceph-asok.gnUY1n -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:41:24,437105972-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:41:24,440632397-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f08468c9-6466-49bc-a807-fb1827b24b24', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gnUY1n:/tmp/ceph-asok.gnUY1n', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f08468c9-6466-49bc-a807-fb1827b24b24 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gnUY1n:/tmp/ceph-asok.gnUY1n -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:41:28,358292880-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:41:28,362052412-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f08468c9-6466-49bc-a807-fb1827b24b24', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gnUY1n:/tmp/ceph-asok.gnUY1n', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f08468c9-6466-49bc-a807-fb1827b24b24 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gnUY1n:/tmp/ceph-asok.gnUY1n -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:41:32,720408082-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:41:32,723963260-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f08468c9-6466-49bc-a807-fb1827b24b24', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gnUY1n:/tmp/ceph-asok.gnUY1n', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f08468c9-6466-49bc-a807-fb1827b24b24 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gnUY1n:/tmp/ceph-asok.gnUY1n -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:41:37,125953750-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:41:37,129608756-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f08468c9-6466-49bc-a807-fb1827b24b24', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gnUY1n:/tmp/ceph-asok.gnUY1n', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f08468c9-6466-49bc-a807-fb1827b24b24 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gnUY1n:/tmp/ceph-asok.gnUY1n -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:41:41,492477435-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:41:41,496321857-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f08468c9-6466-49bc-a807-fb1827b24b24', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gnUY1n:/tmp/ceph-asok.gnUY1n', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f08468c9-6466-49bc-a807-fb1827b24b24 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gnUY1n:/tmp/ceph-asok.gnUY1n -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:41:45,096373197-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:41:45,100164960-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f08468c9-6466-49bc-a807-fb1827b24b24', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gnUY1n:/tmp/ceph-asok.gnUY1n', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f08468c9-6466-49bc-a807-fb1827b24b24 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gnUY1n:/tmp/ceph-asok.gnUY1n -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:41:49,450169185-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:41:49,453864306-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f08468c9-6466-49bc-a807-fb1827b24b24', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gnUY1n:/tmp/ceph-asok.gnUY1n', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f08468c9-6466-49bc-a807-fb1827b24b24 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gnUY1n:/tmp/ceph-asok.gnUY1n -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:41:52,824145859-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:41:52,828158237-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f08468c9-6466-49bc-a807-fb1827b24b24', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gnUY1n:/tmp/ceph-asok.gnUY1n', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f08468c9-6466-49bc-a807-fb1827b24b24 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gnUY1n:/tmp/ceph-asok.gnUY1n -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  155 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:41:56,073007242-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:42:19,425182282-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (9s)
      [===================.........] (remaining: 4s)
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:42:27,711856908-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:42:36,181020598-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:42:44,622777106-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:42:52,884511000-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:43:01,347270610-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:43:09,791215299-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:43:09,800076955-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:43:18,047952736-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:43:18,057009690-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:43:26,377353938-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:43:26,385840561-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:43:34,815516094-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:43:34,824176302-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:43:43,127185222-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:43:43,135972979-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:43:43,141959946-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:43:43,145681698-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:43:43,154884305-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=473286
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_256KB_write.dat.4 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:43:43,161622022-04:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_256KB_write.log.4
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:43:43,181431955-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:43:43,185014205-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f08468c9-6466-49bc-a807-fb1827b24b24', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gnUY1n:/tmp/ceph-asok.gnUY1n', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f08468c9-6466-49bc-a807-fb1827b24b24 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gnUY1n:/tmp/ceph-asok.gnUY1n -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T17:43:45.825+0000 7fa1cf5fad00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T17:43:45.829+0000 7fa1cf5fad00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T17:43:45.829+0000 7fa1cf5fad00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T17:43:45.840532+0000 Maintaining 256 concurrent writes of 262144 bytes to objects of size 262144 for up to 20 seconds or 0 objects
2021-05-17T17:43:45.840540+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-17T17:43:45.856652+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T17:43:45.856652+0000     0       1         1         0         0         0           -           0
2021-05-17T17:43:46.856787+0000     1     255      3714      3459   864.554    864.75   0.0506159   0.0699531
2021-05-17T17:43:47.856895+0000     2     255      7350      7095   886.727       909    0.064957   0.0696084
2021-05-17T17:43:48.857049+0000     3     255     11060     10805    900.27     927.5    0.116852   0.0696232
2021-05-17T17:43:49.857197+0000     4     255     14841     14586    911.48    945.25   0.0526175   0.0691968
2021-05-17T17:43:50.857347+0000     5     255     18514     18259   912.807    918.25   0.0620013   0.0693111
2021-05-17T17:43:51.857487+0000     6     255     22163     21908   912.692    912.25   0.0512791   0.0693987
2021-05-17T17:43:52.857622+0000     7     255     25678     25423   907.827    878.75   0.0532763   0.0698853
2021-05-17T17:43:53.857763+0000     8     255     29375     29120   909.863    924.25   0.0632151   0.0697842
2021-05-17T17:43:54.857925+0000     9     255     33108     32853   912.445    933.25   0.0579065   0.0695805
2021-05-17T17:43:55.858077+0000    10     255     36558     36303   907.437     862.5   0.0624281   0.0701135
2021-05-17T17:43:56.858216+0000    11     255     40114     39859    905.75       889   0.0601604   0.0702869
2021-05-17T17:43:57.858359+0000    12     255     43643     43388   903.781    882.25   0.0487112   0.0705066
2021-05-17T17:43:58.858507+0000    13     255     47310     47055   904.768    916.75   0.0513574   0.0704116
2021-05-17T17:43:59.858653+0000    14     255     50930     50675   904.776       905   0.0370672   0.0704883
2021-05-17T17:44:00.858794+0000    15     255     54394     54139   902.182       866   0.0565519    0.070649
2021-05-17T17:44:01.858934+0000    16     255     57659     57404   896.804    816.25     0.05518   0.0711264
2021-05-17T17:44:02.859077+0000    17     255     61341     61086   898.191     920.5   0.0422695   0.0710176
2021-05-17T17:44:03.859198+0000    18     255     65016     64761   899.327    918.75   0.0604883   0.0709283
2021-05-17T17:44:04.859303+0000    19     255     68576     68321   898.831       890   0.0504974   0.0709597
2021-05-17T17:44:05.859405+0000 min lat: 0.0264257 max lat: 0.289396 avg lat: 0.0715236
2021-05-17T17:44:05.859405+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T17:44:05.859405+0000    20     255     71659     71404   892.423    770.75   0.0484783   0.0715236
2021-05-17T17:44:06.859615+0000 Total time run:         20.0281
Total writes made:      71660
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     894.494
Stddev Bandwidth:       42.0156
Max bandwidth (MB/sec): 945.25
Min bandwidth (MB/sec): 770.75
Average IOPS:           3577
Stddev IOPS:            168.063
Max IOPS:               3781
Min IOPS:               3083
Average Latency(s):     0.0714274
Stddev Latency(s):      0.0298677
Max latency(s):         0.289396
Min latency(s):         0.00800065

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:44:07,442533386-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 473286


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:44:07,446973327-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:44:30,937117567-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 71.66k objects, 17 GiB
    usage:   35 GiB used, 265 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:44:30,945816468-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:44:39,466647404-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 71.66k objects, 17 GiB
    usage:   35 GiB used, 265 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:44:39,476165074-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:44:48,111974374-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 71.66k objects, 17 GiB
    usage:   35 GiB used, 265 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:44:48,120698152-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:44:56,777580562-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 71.66k objects, 17 GiB
    usage:   35 GiB used, 265 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:44:56,785717888-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:45:05,346577660-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 71.66k objects, 17 GiB
    usage:   35 GiB used, 265 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:45:05,355207441-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:45:05,361795216-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:45:05,365635881-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:45:05,374499071-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=474684
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_256KB_seq.dat.4 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:45:05,381097617-04:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_256KB_seq.log.4
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:45:05,399923401-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:45:05,403431301-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'f08468c9-6466-49bc-a807-fb1827b24b24', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gnUY1n:/tmp/ceph-asok.gnUY1n', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid f08468c9-6466-49bc-a807-fb1827b24b24 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.gnUY1n:/tmp/ceph-asok.gnUY1n -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T17:45:07.949+0000 7f05fb0afd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T17:45:07.953+0000 7f05fb0afd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T17:45:07.953+0000 7f05fb0afd00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T17:45:07.968201+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T17:45:07.968201+0000     0       0         0         0         0         0           -           0
2021-05-17T17:45:08.968306+0000     1     255     18597     18342   4584.84    4585.5  0.00386988   0.0137407
2021-05-17T17:45:09.968381+0000     2     255     36187     35932   4491.01    4397.5  0.00580969   0.0141047
2021-05-17T17:45:10.968482+0000     3     256     53786     53530   4460.36    4399.5  0.00362654   0.0142373
2021-05-17T17:45:11.968680+0000     4     255     71118     70863   4428.36   4333.25   0.0411987   0.0143697
2021-05-17T17:45:12.968834+0000 Total time run:       4.06631
Total reads made:     71660
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   4405.72
Average IOPS:         17622
Stddev IOPS:          435.264
Max IOPS:             18342
Min IOPS:             17333
Average Latency(s):   0.01443
Max latency(s):       0.0479212
Min latency(s):       0.000418577

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:45:13,676337837-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 474684


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:45:13,681041542-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:45:36,966340504-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 71.66k objects, 17 GiB
    usage:   35 GiB used, 265 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:45:36,974683988-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:45:45,489473600-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 71.66k objects, 17 GiB
    usage:   35 GiB used, 265 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:45:45,498242082-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:45:53,861541599-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 71.66k objects, 17 GiB
    usage:   35 GiB used, 265 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:45:53,870280294-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:46:02,196921119-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 71.66k objects, 17 GiB
    usage:   35 GiB used, 265 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:46:02,205804016-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:46:10,680337966-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 71.66k objects, 17 GiB
    usage:   35 GiB used, 265 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:46:10,689032099-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:46:10,695929185-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T13:46:10,698416147-04:00][RUNNING][ROUND 5/4/40] object_size=256KB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:46:10,702326843-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:46:10,711184193-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40729\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.381906\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid fc116832-a305-40b7-8336-860640032478\nsetting min_mon_release = octopus\nepoch 0\nfsid fc116832-a305-40b7-8336-860640032478\nlast_changed 2021-05-17T10:46:39.261342-0700\ncreated 2021-05-17T10:46:39.261342-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40729/0,v1:10.10.1.2:40730/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.381906 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 005ecb9c-c747-485d-af30-4e2e0296b812\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 4dd65bb5-fcca-407e-897e-c4004120e969\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 6295a5e9-ae35-48aa-a60a-697b3856fe1d\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42729\n  w/ user/pass: admin / db4c9f6c-8255-4b61-ab3b-0eb77ecc1a03\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 13:46:52 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40729
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.381906
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid fc116832-a305-40b7-8336-860640032478
setting min_mon_release = octopus
epoch 0
fsid fc116832-a305-40b7-8336-860640032478
last_changed 2021-05-17T10:46:39.261342-0700
created 2021-05-17T10:46:39.261342-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40729/0,v1:10.10.1.2:40730/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.381906 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 005ecb9c-c747-485d-af30-4e2e0296b812
0
start osd.0
add osd1 4dd65bb5-fcca-407e-897e-c4004120e969
1
start osd.1
add osd2 6295a5e9-ae35-48aa-a60a-697b3856fe1d
2
start osd.2


restful urls: https://10.10.1.2:42729
  w/ user/pass: admin / db4c9f6c-8255-4b61-ab3b-0eb77ecc1a03


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T10:46:11.647-0700 7fdd2e6bb1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:46:11.651-0700 7fdd2e6bb1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:46:11.667-0700 7fba4d7c71c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:46:11.667-0700 7fba4d7c71c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40729,v1:10.10.1.2:40730] --print /tmp/ceph_monmap.381906 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.381906 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.381906 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42729 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.75KpxG7b0B 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 005ecb9c-c747-485d-af30-4e2e0296b812 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAIrKJgDuPJChAAwFApQAphDapNRNXixTWx8Q== --osd-uuid 005ecb9c-c747-485d-af30-4e2e0296b812 
2021-05-17T10:46:48.512-0700 7fee1389ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T10:46:48.512-0700 7fee1389ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T10:46:48.512-0700 7fee1389ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T10:46:48.560-0700 7fee1389ff00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 4dd65bb5-fcca-407e-897e-c4004120e969 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T10:46:48.844-0700 7f045c8d3f00 -1 Falling back to public interface
2021-05-17T10:46:48.856-0700 7f045c8d3f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAIrKJgd7lxMhAA7k2FZr5+uwvuTGAM3+vpOA== --osd-uuid 4dd65bb5-fcca-407e-897e-c4004120e969 
2021-05-17T10:46:49.172-0700 7f8fee532f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T10:46:49.176-0700 7f8fee532f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T10:46:49.176-0700 7f8fee532f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T10:46:49.220-0700 7f8fee532f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 6295a5e9-ae35-48aa-a60a-697b3856fe1d -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T10:46:49.564-0700 7f667ef19f00 -1 Falling back to public interface
2021-05-17T10:46:49.576-0700 7f667ef19f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAJrKJgeiLMIRAAC28MGeToZzJX+F5e95svRw== --osd-uuid 6295a5e9-ae35-48aa-a60a-697b3856fe1d 
2021-05-17T10:46:49.896-0700 7fd5c340af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T10:46:49.896-0700 7fd5c340af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T10:46:49.896-0700 7fd5c340af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T10:46:49.956-0700 7fd5c340af00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T10:46:50.300-0700 7f504bd38f00 -1 Falling back to public interface
2021-05-17T10:46:50.316-0700 7f504bd38f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:46:52,338443992-04:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:46:52,342415994-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:46:52,383417226-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:46:52,386733927-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7ec756bc-8104-464e-bd5b-e7f9524aba85', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8zhgsi:/tmp/ceph-asok.8zhgsi', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7ec756bc-8104-464e-bd5b-e7f9524aba85 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8zhgsi:/tmp/ceph-asok.8zhgsi -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:46:55,963672058-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:46:55,967224662-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7ec756bc-8104-464e-bd5b-e7f9524aba85', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8zhgsi:/tmp/ceph-asok.8zhgsi', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7ec756bc-8104-464e-bd5b-e7f9524aba85 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8zhgsi:/tmp/ceph-asok.8zhgsi -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:46:59,336645145-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:46:59,340372006-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7ec756bc-8104-464e-bd5b-e7f9524aba85', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8zhgsi:/tmp/ceph-asok.8zhgsi', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7ec756bc-8104-464e-bd5b-e7f9524aba85 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8zhgsi:/tmp/ceph-asok.8zhgsi -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:47:02,487328843-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:47:02,491041377-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7ec756bc-8104-464e-bd5b-e7f9524aba85', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8zhgsi:/tmp/ceph-asok.8zhgsi', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7ec756bc-8104-464e-bd5b-e7f9524aba85 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8zhgsi:/tmp/ceph-asok.8zhgsi -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:47:09,320288205-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:47:09,324166421-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7ec756bc-8104-464e-bd5b-e7f9524aba85', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8zhgsi:/tmp/ceph-asok.8zhgsi', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7ec756bc-8104-464e-bd5b-e7f9524aba85 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8zhgsi:/tmp/ceph-asok.8zhgsi -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:47:13,619151633-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:47:13,623094751-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7ec756bc-8104-464e-bd5b-e7f9524aba85', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8zhgsi:/tmp/ceph-asok.8zhgsi', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7ec756bc-8104-464e-bd5b-e7f9524aba85 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8zhgsi:/tmp/ceph-asok.8zhgsi -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:47:18,223914066-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:47:18,227774589-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7ec756bc-8104-464e-bd5b-e7f9524aba85', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8zhgsi:/tmp/ceph-asok.8zhgsi', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7ec756bc-8104-464e-bd5b-e7f9524aba85 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8zhgsi:/tmp/ceph-asok.8zhgsi -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:47:22,488875400-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:47:22,492484751-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7ec756bc-8104-464e-bd5b-e7f9524aba85', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8zhgsi:/tmp/ceph-asok.8zhgsi', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7ec756bc-8104-464e-bd5b-e7f9524aba85 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8zhgsi:/tmp/ceph-asok.8zhgsi -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:47:27,094866442-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:47:27,098216074-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7ec756bc-8104-464e-bd5b-e7f9524aba85', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8zhgsi:/tmp/ceph-asok.8zhgsi', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7ec756bc-8104-464e-bd5b-e7f9524aba85 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8zhgsi:/tmp/ceph-asok.8zhgsi -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:47:31,451328796-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:47:31,455129876-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7ec756bc-8104-464e-bd5b-e7f9524aba85', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8zhgsi:/tmp/ceph-asok.8zhgsi', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7ec756bc-8104-464e-bd5b-e7f9524aba85 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8zhgsi:/tmp/ceph-asok.8zhgsi -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:47:36,156589522-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:47:36,160334507-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7ec756bc-8104-464e-bd5b-e7f9524aba85', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8zhgsi:/tmp/ceph-asok.8zhgsi', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7ec756bc-8104-464e-bd5b-e7f9524aba85 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8zhgsi:/tmp/ceph-asok.8zhgsi -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:47:39,732164951-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:47:39,735848681-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7ec756bc-8104-464e-bd5b-e7f9524aba85', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8zhgsi:/tmp/ceph-asok.8zhgsi', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7ec756bc-8104-464e-bd5b-e7f9524aba85 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8zhgsi:/tmp/ceph-asok.8zhgsi -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  155 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:47:43,221727401-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:48:06,692653834-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:48:15,338415399-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:48:23,777392593-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:48:32,266713173-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:48:40,687089752-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:48:49,102073112-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:48:49,110156327-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:48:57,475043633-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:48:57,484004687-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:49:06,045817859-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:49:06,055003925-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:49:14,728835773-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:49:14,737411122-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:49:23,201975946-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:49:23,210434425-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:49:23,217465003-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:49:23,221500574-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:49:23,230375836-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=481192
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_256KB_write.dat.5 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:49:23,237618472-04:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_256KB_write.log.5
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:49:23,257917214-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:49:23,261767577-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7ec756bc-8104-464e-bd5b-e7f9524aba85', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8zhgsi:/tmp/ceph-asok.8zhgsi', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7ec756bc-8104-464e-bd5b-e7f9524aba85 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8zhgsi:/tmp/ceph-asok.8zhgsi -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T17:49:25.678+0000 7fcf045aad00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T17:49:25.686+0000 7fcf045aad00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T17:49:25.686+0000 7fcf045aad00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T17:49:25.694315+0000 Maintaining 256 concurrent writes of 262144 bytes to objects of size 262144 for up to 20 seconds or 0 objects
2021-05-17T17:49:25.694326+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-17T17:49:25.710645+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T17:49:25.710645+0000     0       0         0         0         0         0           -           0
2021-05-17T17:49:26.710741+0000     1     255      3638      3383   845.709    845.75   0.0638071   0.0706949
2021-05-17T17:49:27.710840+0000     2     255      7252      6997   874.561     903.5   0.0958766   0.0708507
2021-05-17T17:49:28.710914+0000     3     255     10743     10488   873.936    872.75   0.0550471   0.0710233
2021-05-17T17:49:29.711019+0000     4     255     14105     13850   865.554     840.5   0.0663651   0.0725725
2021-05-17T17:49:30.711088+0000     5     255     17657     17402   870.031       888   0.0512204   0.0727061
2021-05-17T17:49:31.711187+0000     6     255     21224     20969   873.636    891.75   0.0459807   0.0726442
2021-05-17T17:49:32.711260+0000     7     255     24806     24551    876.75     895.5   0.0446886   0.0723747
2021-05-17T17:49:33.711360+0000     8     255     28428     28173   880.333     905.5   0.0837948   0.0722293
2021-05-17T17:49:34.711429+0000     9     255     31982     31727   881.234     888.5   0.0481342   0.0721253
2021-05-17T17:49:35.711536+0000    10     255     35369     35114   877.776    846.75   0.0785895   0.0724201
2021-05-17T17:49:36.711617+0000    11     255     38932     38677   878.949    890.75   0.0461384   0.0724463
2021-05-17T17:49:37.711721+0000    12     255     42287     42032   875.592    838.75    0.075268   0.0727159
2021-05-17T17:49:38.711794+0000    13     255     45865     45610   877.041     894.5   0.0456035   0.0726469
2021-05-17T17:49:39.711903+0000    14     255     49297     49042   875.674       858   0.0695382   0.0727818
2021-05-17T17:49:40.711987+0000    15     255     52737     52482   874.625       860    0.112875    0.072882
2021-05-17T17:49:41.712055+0000    16     255     56029     55774   871.395       823   0.0470572   0.0731572
2021-05-17T17:49:42.712127+0000    17     255     59305     59050   868.309       819   0.0518119   0.0734336
2021-05-17T17:49:43.712208+0000    18     255     62411     62156   863.205     776.5    0.038385   0.0739066
2021-05-17T17:49:44.712284+0000    19     255     65910     65655   863.809    874.75   0.0631148   0.0738633
2021-05-17T17:49:45.712353+0000 min lat: 0.00587019 max lat: 0.275742 avg lat: 0.0740249
2021-05-17T17:49:45.712353+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T17:49:45.712353+0000    20     105     69157     69052   863.078    849.25  0.00587019   0.0740249
2021-05-17T17:49:46.712514+0000 Total time run:         20.0165
Total writes made:      69157
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     863.749
Stddev Bandwidth:       33.6364
Max bandwidth (MB/sec): 905.5
Min bandwidth (MB/sec): 776.5
Average IOPS:           3454
Stddev IOPS:            134.546
Max IOPS:               3622
Min IOPS:               3106
Average Latency(s):     0.0739866
Stddev Latency(s):      0.0318979
Max latency(s):         0.275742
Min latency(s):         0.00457818

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:49:47,300158938-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 481192


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:49:47,304863676-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:50:10,634981497-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 69.16k objects, 17 GiB
    usage:   34 GiB used, 266 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:50:10,643966325-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:50:19,144221234-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 69.16k objects, 17 GiB
    usage:   34 GiB used, 266 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:50:19,152936326-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:50:27,854881663-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 69.16k objects, 17 GiB
    usage:   34 GiB used, 266 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:50:27,863613436-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:50:36,263969676-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 69.16k objects, 17 GiB
    usage:   34 GiB used, 266 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:50:36,272572267-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:50:44,833662837-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 69.16k objects, 17 GiB
    usage:   34 GiB used, 266 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:50:44,842924426-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:50:44,849693191-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:50:44,853774207-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:50:44,862310714-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=482631
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_256KB_seq.dat.5 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:50:44,869671181-04:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_256KB_seq.log.5
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:50:44,890304982-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:50:44,894462983-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7ec756bc-8104-464e-bd5b-e7f9524aba85', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8zhgsi:/tmp/ceph-asok.8zhgsi', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7ec756bc-8104-464e-bd5b-e7f9524aba85 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8zhgsi:/tmp/ceph-asok.8zhgsi -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T17:50:47.502+0000 7f8bf9441d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T17:50:47.506+0000 7f8bf9441d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T17:50:47.506+0000 7f8bf9441d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T17:50:47.520046+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T17:50:47.520046+0000     0       0         0         0         0         0           -           0
2021-05-17T17:50:48.520155+0000     1     256     18535     18279   4568.95   4569.75  0.00763455   0.0138715
2021-05-17T17:50:49.520303+0000     2     255     36754     36499   4561.64      4555  0.00228393   0.0139115
2021-05-17T17:50:50.520439+0000     3     255     54539     54284   4522.97   4446.25  0.00426329   0.0140508
2021-05-17T17:50:51.520597+0000 Total time run:       3.84886
Total reads made:     69157
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   4492.04
Average IOPS:         17968
Stddev IOPS:          269.797
Max IOPS:             18279
Min IOPS:             17785
Average Latency(s):   0.0141654
Max latency(s):       0.0635534
Min latency(s):       0.000424998

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:50:52,263018463-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 482631


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:50:52,267744701-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:51:15,815688111-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 69.16k objects, 17 GiB
    usage:   34 GiB used, 266 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:51:15,824459098-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:51:24,397490133-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 69.16k objects, 17 GiB
    usage:   34 GiB used, 266 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:51:24,406270357-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:51:33,109991062-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 69.16k objects, 17 GiB
    usage:   34 GiB used, 266 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:51:33,118774632-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:51:41,592164323-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 69.16k objects, 17 GiB
    usage:   34 GiB used, 266 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:51:41,601069682-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:51:50,130496809-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 69.16k objects, 17 GiB
    usage:   34 GiB used, 266 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:51:50,138982217-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:51:50,145905271-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T13:51:50,150289826-04:00][RUNNING][ROUND 1/5/40] object_size=1MB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:51:50,154317492-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:51:50,162582025-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40770\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.383168\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid d833afaf-b0ab-41da-a7cf-6de99dfcde0b\nsetting min_mon_release = octopus\nepoch 0\nfsid d833afaf-b0ab-41da-a7cf-6de99dfcde0b\nlast_changed 2021-05-17T10:52:18.246954-0700\ncreated 2021-05-17T10:52:18.246954-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40770/0,v1:10.10.1.2:40771/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.383168 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 b9e61349-d665-4581-9746-bed8ba706c63\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 d1d2d38f-0267-4895-81b2-c24037a96ee2\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 5035b404-1f15-4850-80cb-fab53a77efd0\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42770\n  w/ user/pass: admin / 866d8448-4ed0-40e7-94fd-022367980957\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\nCEPH_DEV=1\n'
[1] 13:52:31 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40770
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.383168
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid d833afaf-b0ab-41da-a7cf-6de99dfcde0b
setting min_mon_release = octopus
epoch 0
fsid d833afaf-b0ab-41da-a7cf-6de99dfcde0b
last_changed 2021-05-17T10:52:18.246954-0700
created 2021-05-17T10:52:18.246954-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40770/0,v1:10.10.1.2:40771/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.383168 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 b9e61349-d665-4581-9746-bed8ba706c63
0
start osd.0
add osd1 d1d2d38f-0267-4895-81b2-c24037a96ee2
1
start osd.1
add osd2 5035b404-1f15-4850-80cb-fab53a77efd0
2
start osd.2


restful urls: https://10.10.1.2:42770
  w/ user/pass: admin / 866d8448-4ed0-40e7-94fd-022367980957


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T10:51:51.090-0700 7fef194ec1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:51:51.090-0700 7fef194ec1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:51:51.106-0700 7fa3f17b61c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:51:51.106-0700 7fa3f17b61c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40770,v1:10.10.1.2:40771] --print /tmp/ceph_monmap.383168 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.383168 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.383168 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42770 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.pOn5P8fn5J 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new b9e61349-d665-4581-9746-bed8ba706c63 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBaraJgMkZAOhAACDeD1KRJKu/yZk7Du88KLg== --osd-uuid b9e61349-d665-4581-9746-bed8ba706c63 
2021-05-17T10:52:27.302-0700 7f9b048fbf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T10:52:27.302-0700 7f9b048fbf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T10:52:27.302-0700 7f9b048fbf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T10:52:27.366-0700 7f9b048fbf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new d1d2d38f-0267-4895-81b2-c24037a96ee2 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T10:52:27.610-0700 7f744ed6df00 -1 Falling back to public interface
2021-05-17T10:52:27.622-0700 7f744ed6df00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBbraJgnI1HJBAACfJyS0sovUUX357JZdIX3w== --osd-uuid d1d2d38f-0267-4895-81b2-c24037a96ee2 
2021-05-17T10:52:27.962-0700 7f0cf1c97f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T10:52:27.962-0700 7f0cf1c97f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T10:52:27.962-0700 7f0cf1c97f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T10:52:28.010-0700 7f0cf1c97f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 5035b404-1f15-4850-80cb-fab53a77efd0 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T10:52:28.366-0700 7f18a9beaf00 -1 Falling back to public interface
2021-05-17T10:52:28.378-0700 7f18a9beaf00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBcraJgDwgaFRAAjCVVLM6OL42NRrT8ngsEqA== --osd-uuid 5035b404-1f15-4850-80cb-fab53a77efd0 
2021-05-17T10:52:28.682-0700 7f4781dbcf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T10:52:28.682-0700 7f4781dbcf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T10:52:28.682-0700 7f4781dbcf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T10:52:28.746-0700 7f4781dbcf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T10:52:29.142-0700 7f6b7b7c8f00 -1 Falling back to public interface
2021-05-17T10:52:29.154-0700 7f6b7b7c8f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:52:31,115152144-04:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:52:31,119440258-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:52:31,161345109-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:52:31,164849581-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c370a11d-d6b0-41bb-b04d-69285df241ff', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kkPWVN:/tmp/ceph-asok.kkPWVN', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c370a11d-d6b0-41bb-b04d-69285df241ff --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kkPWVN:/tmp/ceph-asok.kkPWVN -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:52:34,664322250-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:52:34,668463168-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c370a11d-d6b0-41bb-b04d-69285df241ff', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kkPWVN:/tmp/ceph-asok.kkPWVN', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c370a11d-d6b0-41bb-b04d-69285df241ff --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kkPWVN:/tmp/ceph-asok.kkPWVN -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:52:38,031048802-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:52:38,034647211-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c370a11d-d6b0-41bb-b04d-69285df241ff', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kkPWVN:/tmp/ceph-asok.kkPWVN', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c370a11d-d6b0-41bb-b04d-69285df241ff --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kkPWVN:/tmp/ceph-asok.kkPWVN -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:52:41,568280666-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:52:41,571891919-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c370a11d-d6b0-41bb-b04d-69285df241ff', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kkPWVN:/tmp/ceph-asok.kkPWVN', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c370a11d-d6b0-41bb-b04d-69285df241ff --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kkPWVN:/tmp/ceph-asok.kkPWVN -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:52:48,431453738-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:52:48,435146804-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c370a11d-d6b0-41bb-b04d-69285df241ff', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kkPWVN:/tmp/ceph-asok.kkPWVN', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c370a11d-d6b0-41bb-b04d-69285df241ff --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kkPWVN:/tmp/ceph-asok.kkPWVN -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:52:52,286793508-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:52:52,290651034-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c370a11d-d6b0-41bb-b04d-69285df241ff', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kkPWVN:/tmp/ceph-asok.kkPWVN', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c370a11d-d6b0-41bb-b04d-69285df241ff --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kkPWVN:/tmp/ceph-asok.kkPWVN -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:52:56,624740363-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:52:56,628631382-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c370a11d-d6b0-41bb-b04d-69285df241ff', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kkPWVN:/tmp/ceph-asok.kkPWVN', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c370a11d-d6b0-41bb-b04d-69285df241ff --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kkPWVN:/tmp/ceph-asok.kkPWVN -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:53:01,055599999-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:53:01,059132064-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c370a11d-d6b0-41bb-b04d-69285df241ff', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kkPWVN:/tmp/ceph-asok.kkPWVN', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c370a11d-d6b0-41bb-b04d-69285df241ff --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kkPWVN:/tmp/ceph-asok.kkPWVN -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:53:05,146339697-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:53:05,149955528-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c370a11d-d6b0-41bb-b04d-69285df241ff', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kkPWVN:/tmp/ceph-asok.kkPWVN', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c370a11d-d6b0-41bb-b04d-69285df241ff --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kkPWVN:/tmp/ceph-asok.kkPWVN -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:53:08,658623937-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:53:08,662137066-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c370a11d-d6b0-41bb-b04d-69285df241ff', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kkPWVN:/tmp/ceph-asok.kkPWVN', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c370a11d-d6b0-41bb-b04d-69285df241ff --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kkPWVN:/tmp/ceph-asok.kkPWVN -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:53:12,572553543-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:53:12,576747661-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c370a11d-d6b0-41bb-b04d-69285df241ff', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kkPWVN:/tmp/ceph-asok.kkPWVN', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c370a11d-d6b0-41bb-b04d-69285df241ff --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kkPWVN:/tmp/ceph-asok.kkPWVN -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:53:16,151074078-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:53:16,154445640-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c370a11d-d6b0-41bb-b04d-69285df241ff', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kkPWVN:/tmp/ceph-asok.kkPWVN', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c370a11d-d6b0-41bb-b04d-69285df241ff --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kkPWVN:/tmp/ceph-asok.kkPWVN -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  154 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:53:19,884214557-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:53:43,176348942-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:53:51,923328976-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:54:00,680046154-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:54:09,192883034-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:54:17,721050374-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:54:26,125468509-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:54:34,572367791-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:54:34,581428920-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:54:43,251062533-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:54:43,259389332-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:54:51,733080769-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:54:51,741720426-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:55:00,492245447-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:55:00,500528917-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:55:09,072573199-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:55:09,080791646-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:55:09,087458879-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:55:09,091528172-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:55:09,101656366-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=489354
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_1MB_write.dat.1 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:55:09,108232428-04:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_1MB_write.log.1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:55:09,128220873-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:55:09,131892960-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c370a11d-d6b0-41bb-b04d-69285df241ff', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kkPWVN:/tmp/ceph-asok.kkPWVN', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c370a11d-d6b0-41bb-b04d-69285df241ff --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kkPWVN:/tmp/ceph-asok.kkPWVN -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T17:55:11.843+0000 7f864d24ed00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T17:55:11.851+0000 7f864d24ed00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T17:55:11.851+0000 7f864d24ed00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T17:55:11.861010+0000 Maintaining 256 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 20 seconds or 0 objects
2021-05-17T17:55:11.861022+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-17T17:55:11.918502+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T17:55:11.918502+0000     0       0         0         0         0         0           -           0
2021-05-17T17:55:12.918583+0000     1     255      1206       951   950.973       951    0.221437    0.210946
2021-05-17T17:55:13.918659+0000     2     255      2331      2076   1037.95      1125    0.217877     0.22054
2021-05-17T17:55:14.918779+0000     3     255      3530      3275   1091.58      1199    0.207251    0.218384
2021-05-17T17:55:15.918868+0000     4     255      4748      4493   1123.16      1218    0.195211    0.216395
2021-05-17T17:55:16.918971+0000     5     255      5964      5709    1141.7      1216    0.214513    0.214954
2021-05-17T17:55:17.919044+0000     6     255      7035      6780   1129.91      1071    0.307688    0.216562
2021-05-17T17:55:18.919154+0000     7     255      8213      7958   1136.76      1178    0.214886    0.218456
2021-05-17T17:55:19.919224+0000     8     255      9344      9089   1136.03      1131    0.226337    0.219224
2021-05-17T17:55:20.919318+0000     9     255     10508     10253   1139.13      1164    0.207628    0.219675
2021-05-17T17:55:21.919436+0000    10     255     11695     11440    1143.9      1187    0.214123     0.21913
2021-05-17T17:55:22.919581+0000    11     255     12756     12501   1136.35      1061     0.21348    0.221106
2021-05-17T17:55:23.919687+0000    12     255     13929     13674   1139.39      1173    0.218321    0.220632
2021-05-17T17:55:24.919804+0000    13     255     14999     14744   1134.04      1070    0.198536    0.222205
2021-05-17T17:55:25.919916+0000    14     255     16171     15916   1136.75      1172    0.212247    0.221882
2021-05-17T17:55:26.920061+0000    15     255     17331     17076   1138.29      1160    0.201769     0.22185
2021-05-17T17:55:27.920171+0000    16     255     18504     18249   1140.45      1173    0.226839    0.221435
2021-05-17T17:55:28.920285+0000    17     255     19713     19458   1144.47      1209    0.222874    0.220914
2021-05-17T17:55:29.920393+0000    18     255     20892     20637   1146.38      1179    0.228501    0.220663
2021-05-17T17:55:30.920533+0000    19     255     22070     21815   1148.04      1178    0.214792    0.220547
2021-05-17T17:55:31.920637+0000 min lat: 0.00961865 max lat: 0.337015 avg lat: 0.219329
2021-05-17T17:55:31.920637+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T17:55:31.920637+0000    20      32     23258     23226   1161.18      1411  0.00961865    0.219329
2021-05-17T17:55:32.920828+0000 Total time run:         20.0224
Total writes made:      23258
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     1161.6
Stddev Bandwidth:       88.0174
Max bandwidth (MB/sec): 1411
Min bandwidth (MB/sec): 951
Average IOPS:           1161
Stddev IOPS:            88.0174
Max IOPS:               1411
Min IOPS:               951
Average Latency(s):     0.219073
Stddev Latency(s):      0.0231916
Max latency(s):         0.337015
Min latency(s):         0.00693264

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:55:33,513130600-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 489354


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:55:33,517831399-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:55:56,912103861-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 23.26k objects, 23 GiB
    usage:   45 GiB used, 255 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:55:56,920804744-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:56:05,456561658-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 23.26k objects, 23 GiB
    usage:   45 GiB used, 255 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:56:05,464651083-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:56:13,749705221-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 23.26k objects, 23 GiB
    usage:   45 GiB used, 255 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:56:13,758661213-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:56:22,247283823-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 23.26k objects, 23 GiB
    usage:   45 GiB used, 255 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:56:22,255972443-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:56:30,824824194-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 23.26k objects, 23 GiB
    usage:   45 GiB used, 255 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:56:30,833936028-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:56:30,840885511-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:56:30,844645644-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:56:30,852889679-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=490765
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_1MB_seq.dat.1 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:56:30,859069227-04:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_1MB_seq.log.1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:56:30,878249944-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:56:30,881578546-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c370a11d-d6b0-41bb-b04d-69285df241ff', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kkPWVN:/tmp/ceph-asok.kkPWVN', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c370a11d-d6b0-41bb-b04d-69285df241ff --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kkPWVN:/tmp/ceph-asok.kkPWVN -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T17:56:33.515+0000 7f853c644d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T17:56:33.519+0000 7f853c644d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T17:56:33.519+0000 7f853c644d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T17:56:33.533708+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T17:56:33.533708+0000     0       0         0         0         0         0           -           0
2021-05-17T17:56:34.533805+0000     1     255      4031      3776   3775.37      3776   0.0643684   0.0646286
2021-05-17T17:56:35.533927+0000     2     255      7790      7535   3766.96      3759   0.0575595   0.0660912
2021-05-17T17:56:36.534038+0000     3     255     11304     11049   3682.51      3514   0.0692482   0.0682744
2021-05-17T17:56:37.534178+0000     4     255     14939     14684    3670.5      3635   0.0699099    0.068791
2021-05-17T17:56:38.534320+0000     5     255     18559     18304    3660.3      3620   0.0714674   0.0691678
2021-05-17T17:56:39.534426+0000     6     255     22156     21901   3649.69      3597   0.0706841   0.0694921
2021-05-17T17:56:40.534603+0000 Total time run:       6.35313
Total reads made:     23258
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   3660.87
Average IOPS:         3660
Stddev IOPS:          100.187
Max IOPS:             3776
Min IOPS:             3514
Average Latency(s):   0.0693749
Max latency(s):       0.148277
Min latency(s):       0.0168947

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:56:41,162925006-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 490765


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:56:41,167444155-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:57:04,430008325-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 23.26k objects, 23 GiB
    usage:   45 GiB used, 255 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:57:04,438454339-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:57:12,816660948-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 23.26k objects, 23 GiB
    usage:   45 GiB used, 255 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:57:12,826190929-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:57:21,171936881-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 23.26k objects, 23 GiB
    usage:   45 GiB used, 255 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:57:21,180396020-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:57:29,565663450-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 23.26k objects, 23 GiB
    usage:   45 GiB used, 255 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:57:29,574956566-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:57:37,822397849-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 23.26k objects, 23 GiB
    usage:   45 GiB used, 255 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:57:37,830454562-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:57:37,837449892-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T13:57:37,840081344-04:00][RUNNING][ROUND 2/5/40] object_size=1MB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:57:37,844029891-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:57:37,853210475-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40643\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.384380\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 149b6d49-ac81-41a7-8533-2148f0bb733f\nsetting min_mon_release = octopus\nepoch 0\nfsid 149b6d49-ac81-41a7-8533-2148f0bb733f\nlast_changed 2021-05-17T10:58:05.331963-0700\ncreated 2021-05-17T10:58:05.331963-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40643/0,v1:10.10.1.2:40644/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.384380 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 ed8aeae9-0639-49a3-8c7c-58027b10f782\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 d79d672b-2725-473c-885c-38507668e518\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 e7fa3734-81b8-4e7a-ac51-977084d4eebc\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42643\n  w/ user/pass: admin / 9c9877d9-291a-4762-937f-224dc29bd7f2\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 13:58:19 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40643
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.384380
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 149b6d49-ac81-41a7-8533-2148f0bb733f
setting min_mon_release = octopus
epoch 0
fsid 149b6d49-ac81-41a7-8533-2148f0bb733f
last_changed 2021-05-17T10:58:05.331963-0700
created 2021-05-17T10:58:05.331963-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40643/0,v1:10.10.1.2:40644/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.384380 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 ed8aeae9-0639-49a3-8c7c-58027b10f782
0
start osd.0
add osd1 d79d672b-2725-473c-885c-38507668e518
1
start osd.1
add osd2 e7fa3734-81b8-4e7a-ac51-977084d4eebc
2
start osd.2


restful urls: https://10.10.1.2:42643
  w/ user/pass: admin / 9c9877d9-291a-4762-937f-224dc29bd7f2


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T10:57:38.765-0700 7f13db6fb1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:57:38.765-0700 7f13db6fb1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:57:38.781-0700 7f670233b1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:57:38.781-0700 7f670233b1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40643,v1:10.10.1.2:40644] --print /tmp/ceph_monmap.384380 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.384380 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.384380 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42643 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.f3BXp7bwrI 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new ed8aeae9-0639-49a3-8c7c-58027b10f782 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQC2rqJgu2/zNRAA+Sx1z578/DhUFvh3gHGwjA== --osd-uuid ed8aeae9-0639-49a3-8c7c-58027b10f782 
2021-05-17T10:58:15.237-0700 7f7b472c2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T10:58:15.237-0700 7f7b472c2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T10:58:15.237-0700 7f7b472c2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T10:58:15.357-0700 7f7b472c2f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new d79d672b-2725-473c-885c-38507668e518 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T10:58:15.637-0700 7f426958af00 -1 Falling back to public interface
2021-05-17T10:58:15.649-0700 7f426958af00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQC3rqJgzGb9JRAAE2FqUtl6xHiTd0FLmFaBTQ== --osd-uuid d79d672b-2725-473c-885c-38507668e518 
2021-05-17T10:58:15.965-0700 7f61db100f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T10:58:15.965-0700 7f61db100f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T10:58:15.965-0700 7f61db100f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T10:58:16.061-0700 7f61db100f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new e7fa3734-81b8-4e7a-ac51-977084d4eebc -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T10:58:16.337-0700 7f3c24f2df00 -1 Falling back to public interface
2021-05-17T10:58:16.353-0700 7f3c24f2df00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQC4rqJgSpE+FBAAtzsElwcgKH8pfZOP6Wc/3w== --osd-uuid e7fa3734-81b8-4e7a-ac51-977084d4eebc 
2021-05-17T10:58:16.685-0700 7f524db2ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T10:58:16.685-0700 7f524db2ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T10:58:16.685-0700 7f524db2ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T10:58:16.733-0700 7f524db2ff00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T10:58:17.033-0700 7f6346b02f00 -1 Falling back to public interface
2021-05-17T10:58:17.045-0700 7f6346b02f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:58:19,110943045-04:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:58:19,115294639-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:58:19,156593261-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:58:19,159522904-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5d242596-b096-4066-a794-b9f7a84a61fe', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9DD1K6:/tmp/ceph-asok.9DD1K6', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5d242596-b096-4066-a794-b9f7a84a61fe --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9DD1K6:/tmp/ceph-asok.9DD1K6 -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:58:22,673962783-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:58:22,677454762-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5d242596-b096-4066-a794-b9f7a84a61fe', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9DD1K6:/tmp/ceph-asok.9DD1K6', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5d242596-b096-4066-a794-b9f7a84a61fe --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9DD1K6:/tmp/ceph-asok.9DD1K6 -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:58:26,204162396-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:58:26,207638084-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5d242596-b096-4066-a794-b9f7a84a61fe', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9DD1K6:/tmp/ceph-asok.9DD1K6', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5d242596-b096-4066-a794-b9f7a84a61fe --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9DD1K6:/tmp/ceph-asok.9DD1K6 -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:58:29,552473362-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:58:29,556212675-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5d242596-b096-4066-a794-b9f7a84a61fe', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9DD1K6:/tmp/ceph-asok.9DD1K6', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5d242596-b096-4066-a794-b9f7a84a61fe --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9DD1K6:/tmp/ceph-asok.9DD1K6 -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:58:36,449732950-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:58:36,453395940-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5d242596-b096-4066-a794-b9f7a84a61fe', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9DD1K6:/tmp/ceph-asok.9DD1K6', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5d242596-b096-4066-a794-b9f7a84a61fe --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9DD1K6:/tmp/ceph-asok.9DD1K6 -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:58:40,031822240-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:58:40,035498576-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5d242596-b096-4066-a794-b9f7a84a61fe', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9DD1K6:/tmp/ceph-asok.9DD1K6', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5d242596-b096-4066-a794-b9f7a84a61fe --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9DD1K6:/tmp/ceph-asok.9DD1K6 -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:58:44,564353164-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:58:44,568519089-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5d242596-b096-4066-a794-b9f7a84a61fe', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9DD1K6:/tmp/ceph-asok.9DD1K6', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5d242596-b096-4066-a794-b9f7a84a61fe --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9DD1K6:/tmp/ceph-asok.9DD1K6 -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:58:49,063247729-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:58:49,066556443-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5d242596-b096-4066-a794-b9f7a84a61fe', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9DD1K6:/tmp/ceph-asok.9DD1K6', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5d242596-b096-4066-a794-b9f7a84a61fe --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9DD1K6:/tmp/ceph-asok.9DD1K6 -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:58:52,946131614-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:58:52,949664580-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5d242596-b096-4066-a794-b9f7a84a61fe', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9DD1K6:/tmp/ceph-asok.9DD1K6', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5d242596-b096-4066-a794-b9f7a84a61fe --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9DD1K6:/tmp/ceph-asok.9DD1K6 -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:58:57,336109659-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:58:57,339653305-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5d242596-b096-4066-a794-b9f7a84a61fe', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9DD1K6:/tmp/ceph-asok.9DD1K6', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5d242596-b096-4066-a794-b9f7a84a61fe --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9DD1K6:/tmp/ceph-asok.9DD1K6 -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:59:01,933313004-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:59:01,937002314-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5d242596-b096-4066-a794-b9f7a84a61fe', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9DD1K6:/tmp/ceph-asok.9DD1K6', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5d242596-b096-4066-a794-b9f7a84a61fe --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9DD1K6:/tmp/ceph-asok.9DD1K6 -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:59:05,331403489-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:59:05,335238132-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5d242596-b096-4066-a794-b9f7a84a61fe', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9DD1K6:/tmp/ceph-asok.9DD1K6', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5d242596-b096-4066-a794-b9f7a84a61fe --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9DD1K6:/tmp/ceph-asok.9DD1K6 -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  155 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:59:08,594674230-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:59:31,823242623-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:59:40,122463917-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:59:48,727005898-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T13:59:57,139088902-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:00:05,556070352-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:00:14,019083027-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:00:22,424874255-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:00:22,433942608-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:00:30,814236578-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:00:30,822524816-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:00:39,353136337-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:00:39,361909606-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:00:47,775657533-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:00:47,784297682-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:00:56,282021768-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:00:56,290988961-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:00:56,298171713-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:00:56,302047493-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:00:56,311020527-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=497376
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_1MB_write.dat.2 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:00:56,318062404-04:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_1MB_write.log.2
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:00:56,338602506-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:00:56,341971935-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5d242596-b096-4066-a794-b9f7a84a61fe', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9DD1K6:/tmp/ceph-asok.9DD1K6', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5d242596-b096-4066-a794-b9f7a84a61fe --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9DD1K6:/tmp/ceph-asok.9DD1K6 -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T18:00:59.028+0000 7fbbd8635d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T18:00:59.032+0000 7fbbd8635d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T18:00:59.032+0000 7fbbd8635d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T18:00:59.044328+0000 Maintaining 256 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 20 seconds or 0 objects
2021-05-17T18:00:59.044343+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-17T18:00:59.101921+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T18:00:59.101921+0000     0       0         0         0         0         0           -           0
2021-05-17T18:01:00.102009+0000     1     255       757       502   501.978       502    0.343535    0.346015
2021-05-17T18:01:01.102088+0000     2     255      1453      1198   598.963       696    0.368344    0.354277
2021-05-17T18:01:02.102159+0000     3     255      2179      1924   641.292       726    0.372874    0.355785
2021-05-17T18:01:03.102271+0000     4     255      2894      2639     659.7       715     0.34104    0.356644
2021-05-17T18:01:04.102348+0000     5     255      3621      3366   673.149       727    0.352928    0.355475
2021-05-17T18:01:05.102419+0000     6     255      4353      4098   682.948       732    0.334197    0.354521
2021-05-17T18:01:06.102500+0000     7     255      5092      4837   690.947       739    0.320778    0.353556
2021-05-17T18:01:07.102592+0000     8     255      5786      5531   691.321       694    0.361103    0.354568
2021-05-17T18:01:08.102661+0000     9     255      6514      6259   695.391       728    0.335818    0.354633
2021-05-17T18:01:09.102730+0000    10     255      7236      6981   698.047       722    0.359294    0.354462
2021-05-17T18:01:10.102841+0000    11     255      7933      7678   697.944       697    0.384621    0.355041
2021-05-17T18:01:11.102932+0000    12     255      8647      8392   699.277       714    0.352641    0.356215
2021-05-17T18:01:12.103001+0000    13     255      9380      9125   701.867       733     0.36408    0.355155
2021-05-17T18:01:13.103071+0000    14     255     10137      9882   705.801       757    0.343996    0.354589
2021-05-17T18:01:14.103139+0000    15     255     10866     10611   707.345       729    0.372224    0.353797
2021-05-17T18:01:15.103232+0000    16     255     11561     11306   706.569       695    0.356126    0.355039
2021-05-17T18:01:16.103311+0000    17     255     12344     12089   711.061       783    0.321271    0.353472
2021-05-17T18:01:17.103382+0000    18     255     13088     12833   712.888       744      0.3429    0.352957
2021-05-17T18:01:18.103453+0000    19     255     13821     13566   713.944       733    0.302933    0.352719
2021-05-17T18:01:19.103560+0000 min lat: 0.00748608 max lat: 0.423812 avg lat: 0.35035
2021-05-17T18:01:19.103560+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T18:01:19.103560+0000    20      33     14534     14501   724.992       935  0.00748608     0.35035
2021-05-17T18:01:20.103726+0000 Total time run:         20.0281
Total writes made:      14534
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     725.681
Stddev Bandwidth:       73.5194
Max bandwidth (MB/sec): 935
Min bandwidth (MB/sec): 502
Average IOPS:           725
Stddev IOPS:            73.5194
Max IOPS:               935
Min IOPS:               502
Average Latency(s):     0.349644
Stddev Latency(s):      0.0330157
Max latency(s):         0.423812
Min latency(s):         0.00748608

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:01:20,712915552-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 497376


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:01:20,718008869-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:01:44,098926818-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.54k objects, 14 GiB
    usage:   28 GiB used, 272 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:01:44,107993979-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:01:52,621855775-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.54k objects, 14 GiB
    usage:   28 GiB used, 272 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:01:52,630778705-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:02:01,253178957-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.54k objects, 14 GiB
    usage:   28 GiB used, 272 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:02:01,261872797-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:02:09,808876092-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.54k objects, 14 GiB
    usage:   28 GiB used, 272 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:02:09,818092354-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:02:18,238959967-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.54k objects, 14 GiB
    usage:   28 GiB used, 272 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:02:18,247711466-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:02:18,254626004-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:02:18,258127631-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:02:18,266586650-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=498761
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_1MB_seq.dat.2 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:02:18,274027186-04:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_1MB_seq.log.2
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:02:18,294006303-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:02:18,297081640-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5d242596-b096-4066-a794-b9f7a84a61fe', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9DD1K6:/tmp/ceph-asok.9DD1K6', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5d242596-b096-4066-a794-b9f7a84a61fe --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.9DD1K6:/tmp/ceph-asok.9DD1K6 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T18:02:20.748+0000 7fbaa727fd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T18:02:20.752+0000 7fbaa727fd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T18:02:20.752+0000 7fbaa727fd00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T18:02:20.768959+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T18:02:20.768959+0000     0       0         0         0         0         0           -           0
2021-05-17T18:02:21.769045+0000     1     255      3791      3536   3535.46      3536   0.0590772   0.0684076
2021-05-17T18:02:22.769155+0000     2     256      7367      7111   3555.03      3575   0.0684399   0.0699892
2021-05-17T18:02:23.769232+0000     3     255     10860     10605    3534.6      3494   0.0739951   0.0711143
2021-05-17T18:02:24.769320+0000     4     255     14391     14136   3533.62      3531   0.0709741   0.0714849
2021-05-17T18:02:25.769519+0000 Total time run:       4.09278
Total reads made:     14534
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   3551.13
Average IOPS:         3551
Stddev IOPS:          33.1361
Max IOPS:             3575
Min IOPS:             3494
Average Latency(s):   0.0710835
Max latency(s):       0.145906
Min latency(s):       0.0147415

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:02:26,398369637-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 498761


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:02:26,402994875-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:02:49,746524583-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.54k objects, 14 GiB
    usage:   28 GiB used, 272 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:02:49,755664591-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:02:58,148535687-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.54k objects, 14 GiB
    usage:   28 GiB used, 272 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:02:58,157579544-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:03:06,598672832-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.54k objects, 14 GiB
    usage:   28 GiB used, 272 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:03:06,607400896-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:03:14,819192079-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.54k objects, 14 GiB
    usage:   28 GiB used, 272 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:03:14,828089472-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:03:23,152505717-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.54k objects, 14 GiB
    usage:   28 GiB used, 272 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:03:23,161577798-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:03:23,168885204-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T14:03:23,171052154-04:00][RUNNING][ROUND 3/5/40] object_size=1MB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:03:23,174994970-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:03:23,183349382-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40388\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.385488\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid c44d6079-792f-4060-98a7-6cddafeafc71\nsetting min_mon_release = octopus\nepoch 0\nfsid c44d6079-792f-4060-98a7-6cddafeafc71\nlast_changed 2021-05-17T11:03:51.852788-0700\ncreated 2021-05-17T11:03:51.852788-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40388/0,v1:10.10.1.2:40389/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.385488 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 5a404e19-f883-4618-a9a3-377113de07a8\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 f0a8b88c-0a97-451e-855e-bedf2b412b02\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 646eebb3-ef24-41eb-baed-3aa582e448f3\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42388\n  w/ user/pass: admin / f7caea02-3b8a-4300-982f-45537ea7fb72\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 14:04:04 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40388
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.385488
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid c44d6079-792f-4060-98a7-6cddafeafc71
setting min_mon_release = octopus
epoch 0
fsid c44d6079-792f-4060-98a7-6cddafeafc71
last_changed 2021-05-17T11:03:51.852788-0700
created 2021-05-17T11:03:51.852788-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40388/0,v1:10.10.1.2:40389/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.385488 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 5a404e19-f883-4618-a9a3-377113de07a8
0
start osd.0
add osd1 f0a8b88c-0a97-451e-855e-bedf2b412b02
1
start osd.1
add osd2 646eebb3-ef24-41eb-baed-3aa582e448f3
2
start osd.2


restful urls: https://10.10.1.2:42388
  w/ user/pass: admin / f7caea02-3b8a-4300-982f-45537ea7fb72


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T11:03:24.176-0700 7f6f94e511c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T11:03:24.176-0700 7f6f94e511c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T11:03:24.192-0700 7f3dd63af1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T11:03:24.192-0700 7f3dd63af1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40388,v1:10.10.1.2:40389] --print /tmp/ceph_monmap.385488 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.385488 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.385488 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42388 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.OSJUmELvTD 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 5a404e19-f883-4618-a9a3-377113de07a8 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAQsKJgrgK6JxAA0SFBEA4cENdEELPncK5gTw== --osd-uuid 5a404e19-f883-4618-a9a3-377113de07a8 
2021-05-17T11:04:01.040-0700 7fc36c29df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T11:04:01.040-0700 7fc36c29df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T11:04:01.040-0700 7fc36c29df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T11:04:01.092-0700 7fc36c29df00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new f0a8b88c-0a97-451e-855e-bedf2b412b02 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T11:04:01.344-0700 7fb45261bf00 -1 Falling back to public interface
2021-05-17T11:04:01.360-0700 7fb45261bf00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQARsKJgJJrCFBAAeiZmIR7pb25CHfFb0fnj8w== --osd-uuid f0a8b88c-0a97-451e-855e-bedf2b412b02 
2021-05-17T11:04:01.676-0700 7fbb83187f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T11:04:01.676-0700 7fbb83187f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T11:04:01.676-0700 7fbb83187f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T11:04:01.720-0700 7fbb83187f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 646eebb3-ef24-41eb-baed-3aa582e448f3 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T11:04:02.052-0700 7f24b7d4af00 -1 Falling back to public interface
2021-05-17T11:04:02.068-0700 7f24b7d4af00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQASsKJgModRAxAA4iQDEmmBjmOiBOcaYQXZUw== --osd-uuid 646eebb3-ef24-41eb-baed-3aa582e448f3 
2021-05-17T11:04:02.388-0700 7f2f33a61f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T11:04:02.388-0700 7f2f33a61f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T11:04:02.388-0700 7f2f33a61f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T11:04:02.440-0700 7f2f33a61f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T11:04:02.776-0700 7faebebcbf00 -1 Falling back to public interface
2021-05-17T11:04:02.792-0700 7faebebcbf00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:04:04,815474194-04:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:04:04,820045881-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:04:04,860701026-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:04:04,863622694-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1f8ecfba-770d-4953-8fd3-00f4b2215aa3', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fg5cp2:/tmp/ceph-asok.Fg5cp2', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1f8ecfba-770d-4953-8fd3-00f4b2215aa3 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fg5cp2:/tmp/ceph-asok.Fg5cp2 -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:04:08,607719850-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:04:08,611343927-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1f8ecfba-770d-4953-8fd3-00f4b2215aa3', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fg5cp2:/tmp/ceph-asok.Fg5cp2', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1f8ecfba-770d-4953-8fd3-00f4b2215aa3 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fg5cp2:/tmp/ceph-asok.Fg5cp2 -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:04:12,146728710-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:04:12,150134918-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1f8ecfba-770d-4953-8fd3-00f4b2215aa3', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fg5cp2:/tmp/ceph-asok.Fg5cp2', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1f8ecfba-770d-4953-8fd3-00f4b2215aa3 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fg5cp2:/tmp/ceph-asok.Fg5cp2 -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:04:15,587533780-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:04:15,591044284-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1f8ecfba-770d-4953-8fd3-00f4b2215aa3', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fg5cp2:/tmp/ceph-asok.Fg5cp2', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1f8ecfba-770d-4953-8fd3-00f4b2215aa3 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fg5cp2:/tmp/ceph-asok.Fg5cp2 -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:04:22,459868949-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:04:22,463806675-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1f8ecfba-770d-4953-8fd3-00f4b2215aa3', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fg5cp2:/tmp/ceph-asok.Fg5cp2', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1f8ecfba-770d-4953-8fd3-00f4b2215aa3 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fg5cp2:/tmp/ceph-asok.Fg5cp2 -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:04:26,926501352-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:04:26,930136510-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1f8ecfba-770d-4953-8fd3-00f4b2215aa3', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fg5cp2:/tmp/ceph-asok.Fg5cp2', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1f8ecfba-770d-4953-8fd3-00f4b2215aa3 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fg5cp2:/tmp/ceph-asok.Fg5cp2 -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:04:31,338209279-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:04:31,341966777-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1f8ecfba-770d-4953-8fd3-00f4b2215aa3', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fg5cp2:/tmp/ceph-asok.Fg5cp2', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1f8ecfba-770d-4953-8fd3-00f4b2215aa3 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fg5cp2:/tmp/ceph-asok.Fg5cp2 -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:04:35,602334355-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:04:35,605718451-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1f8ecfba-770d-4953-8fd3-00f4b2215aa3', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fg5cp2:/tmp/ceph-asok.Fg5cp2', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1f8ecfba-770d-4953-8fd3-00f4b2215aa3 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fg5cp2:/tmp/ceph-asok.Fg5cp2 -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:04:39,947314906-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:04:39,951177111-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1f8ecfba-770d-4953-8fd3-00f4b2215aa3', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fg5cp2:/tmp/ceph-asok.Fg5cp2', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1f8ecfba-770d-4953-8fd3-00f4b2215aa3 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fg5cp2:/tmp/ceph-asok.Fg5cp2 -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:04:43,586948471-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:04:43,590380597-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1f8ecfba-770d-4953-8fd3-00f4b2215aa3', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fg5cp2:/tmp/ceph-asok.Fg5cp2', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1f8ecfba-770d-4953-8fd3-00f4b2215aa3 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fg5cp2:/tmp/ceph-asok.Fg5cp2 -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:04:47,983695760-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:04:47,986770235-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1f8ecfba-770d-4953-8fd3-00f4b2215aa3', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fg5cp2:/tmp/ceph-asok.Fg5cp2', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1f8ecfba-770d-4953-8fd3-00f4b2215aa3 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fg5cp2:/tmp/ceph-asok.Fg5cp2 -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:04:51,514924684-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:04:51,518310985-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1f8ecfba-770d-4953-8fd3-00f4b2215aa3', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fg5cp2:/tmp/ceph-asok.Fg5cp2', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1f8ecfba-770d-4953-8fd3-00f4b2215aa3 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fg5cp2:/tmp/ceph-asok.Fg5cp2 -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  156 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:04:54,782675194-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:05:18,132381778-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:05:26,569131813-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:05:35,082905470-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:05:43,436408615-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:05:51,931991381-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:06:00,285995335-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:06:08,706613117-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:06:08,715051748-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:06:17,170485240-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:06:17,179190061-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:06:25,564202452-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:06:25,572920046-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:06:34,028244562-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:06:34,036075511-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:06:42,527449855-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:06:42,536241689-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:06:42,543011425-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:06:42,547061111-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:06:42,555330644-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=505360
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_1MB_write.dat.3 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:06:42,561699137-04:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_1MB_write.log.3
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:06:42,581586623-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:06:42,585269300-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1f8ecfba-770d-4953-8fd3-00f4b2215aa3', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fg5cp2:/tmp/ceph-asok.Fg5cp2', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1f8ecfba-770d-4953-8fd3-00f4b2215aa3 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fg5cp2:/tmp/ceph-asok.Fg5cp2 -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T18:06:45.181+0000 7f971b554d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T18:06:45.189+0000 7f971b554d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T18:06:45.189+0000 7f971b554d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T18:06:45.201051+0000 Maintaining 256 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 20 seconds or 0 objects
2021-05-17T18:06:45.201062+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-17T18:06:45.258500+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T18:06:45.258500+0000     0       0         0         0         0         0           -           0
2021-05-17T18:06:46.258585+0000     1     255       951       696   695.975       696    0.298414    0.267796
2021-05-17T18:06:47.258686+0000     2     255      1948      1693   846.442       997     0.26667    0.262144
2021-05-17T18:06:48.258758+0000     3     255      2909      2654   884.605       961    0.269971    0.265913
2021-05-17T18:06:49.258833+0000     4     255      3861      3606   901.436       952    0.275826    0.265537
2021-05-17T18:06:50.258908+0000     5     255      4825      4570   913.934       964     0.28544    0.265426
2021-05-17T18:06:51.259008+0000     6     255      5794      5539   923.096       969    0.253381    0.265707
2021-05-17T18:06:52.259081+0000     7     255      6800      6545   934.929      1006    0.235893     0.26465
2021-05-17T18:06:53.259168+0000     8     255      7780      7525   940.552       980    0.243943    0.264119
2021-05-17T18:06:54.259285+0000     9     255      8771      8516   946.145       991    0.251281    0.263293
2021-05-17T18:06:55.259422+0000    10     255      9749      9494   949.317       978    0.281144    0.262632
2021-05-17T18:06:56.259558+0000    11     255     10715     10460   950.822       966    0.252381    0.263344
2021-05-17T18:06:57.259696+0000    12     255     11636     11381   948.326       921    0.264086    0.264314
2021-05-17T18:06:58.259784+0000    13     255     12649     12394   953.294      1013    0.253804    0.263516
2021-05-17T18:06:59.259918+0000    14     255     13623     13368   954.764       974    0.247952    0.263534
2021-05-17T18:07:00.260026+0000    15     255     14628     14373   958.106      1005    0.241321    0.262895
2021-05-17T18:07:01.260152+0000    16     255     15636     15381   961.216      1008     0.23827    0.262454
2021-05-17T18:07:02.260266+0000    17     255     16607     16352   961.785       971    0.269204    0.262258
2021-05-17T18:07:03.260402+0000    18     255     17630     17375   965.178      1023    0.240432    0.261743
2021-05-17T18:07:04.260543+0000    19     255     18556     18301   963.109       926     0.25052    0.261549
2021-05-17T18:07:05.260645+0000 min lat: 0.143062 max lat: 0.360514 avg lat: 0.263356
2021-05-17T18:07:05.260645+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T18:07:05.260645+0000    20     141     19432     19291   964.449       990    0.143062    0.263356
2021-05-17T18:07:06.260824+0000 Total time run:         20.032
Total writes made:      19432
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     970.047
Stddev Bandwidth:       68.6911
Max bandwidth (MB/sec): 1023
Min bandwidth (MB/sec): 696
Average IOPS:           970
Stddev IOPS:            68.6911
Max IOPS:               1023
Min IOPS:               696
Average Latency(s):     0.262009
Stddev Latency(s):      0.024321
Max latency(s):         0.360514
Min latency(s):         0.0072517

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:07:06,839589729-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 505360


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:07:06,844765972-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:07:30,050550886-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 19.43k objects, 19 GiB
    usage:   38 GiB used, 262 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:07:30,059544409-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:07:38,309754826-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 19.43k objects, 19 GiB
    usage:   38 GiB used, 262 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:07:38,318752116-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:07:46,686334182-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 19.43k objects, 19 GiB
    usage:   38 GiB used, 262 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:07:46,694925960-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:07:55,170622278-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 19.43k objects, 19 GiB
    usage:   38 GiB used, 262 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:07:55,179246628-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:08:03,440286417-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 19.43k objects, 19 GiB
    usage:   38 GiB used, 262 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:08:03,448823923-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:08:03,456106593-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:08:03,459866906-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:08:03,468647278-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=506752
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_1MB_seq.dat.3 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:08:03,475500111-04:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_1MB_seq.log.3
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:08:03,495112470-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:08:03,499093307-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '1f8ecfba-770d-4953-8fd3-00f4b2215aa3', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fg5cp2:/tmp/ceph-asok.Fg5cp2', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 1f8ecfba-770d-4953-8fd3-00f4b2215aa3 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Fg5cp2:/tmp/ceph-asok.Fg5cp2 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T18:08:05.989+0000 7f1354aa7d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T18:08:05.993+0000 7f1354aa7d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T18:08:05.997+0000 7f1354aa7d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T18:08:06.008285+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T18:08:06.008285+0000     0       0         0         0         0         0           -           0
2021-05-17T18:08:07.008375+0000     1     255      3697      3442   3441.51      3442   0.0706482   0.0706153
2021-05-17T18:08:08.008519+0000     2     255      7200      6945      3472      3503   0.0732506   0.0717839
2021-05-17T18:08:09.008707+0000     3     255     10616     10361   3453.12      3416   0.0702941   0.0727656
2021-05-17T18:08:10.008814+0000     4     255     13994     13739   3434.25      3378   0.0776912    0.073487
2021-05-17T18:08:11.008922+0000     5     255     17369     17114   3422.33      3375   0.0775568   0.0739433
2021-05-17T18:08:12.009069+0000 Total time run:       5.65591
Total reads made:     19432
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   3435.7
Average IOPS:         3435
Stddev IOPS:          52.7703
Max IOPS:             3503
Min IOPS:             3375
Average Latency(s):   0.0738942
Max latency(s):       0.155902
Min latency(s):       0.0173073

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:08:12,661860359-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 506752


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:08:12,666430173-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:08:35,923795254-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 19.43k objects, 19 GiB
    usage:   38 GiB used, 262 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:08:35,933009632-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:08:44,221918086-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 19.43k objects, 19 GiB
    usage:   38 GiB used, 262 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:08:44,230640961-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:08:52,796294556-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 19.43k objects, 19 GiB
    usage:   38 GiB used, 262 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:08:52,804964301-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:09:01,086826176-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 19.43k objects, 19 GiB
    usage:   38 GiB used, 262 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:09:01,096155299-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:09:09,616442746-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 19.43k objects, 19 GiB
    usage:   38 GiB used, 262 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:09:09,625821129-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:09:09,632924169-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T14:09:09,635273140-04:00][RUNNING][ROUND 4/5/40] object_size=1MB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:09:09,638856840-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:09:09,647547241-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40886\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.386660\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 02e58793-7c24-4fc7-ab56-bd8f70001476\nsetting min_mon_release = octopus\nepoch 0\nfsid 02e58793-7c24-4fc7-ab56-bd8f70001476\nlast_changed 2021-05-17T11:09:38.119426-0700\ncreated 2021-05-17T11:09:38.119426-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40886/0,v1:10.10.1.2:40887/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.386660 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 925b943b-2df7-4cd3-a306-0dfe4e7abcb7\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 810465d8-ea97-40dd-9f29-50ff8d70d84c\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 003581bd-4031-4bf4-ab6d-dcd0e093f989\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42886\n  w/ user/pass: admin / 441c50be-09cb-43db-83ac-95171bfea620\n'
10.10.1.2: b'\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 14:09:51 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40886
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.386660
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 02e58793-7c24-4fc7-ab56-bd8f70001476
setting min_mon_release = octopus
epoch 0
fsid 02e58793-7c24-4fc7-ab56-bd8f70001476
last_changed 2021-05-17T11:09:38.119426-0700
created 2021-05-17T11:09:38.119426-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40886/0,v1:10.10.1.2:40887/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.386660 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 925b943b-2df7-4cd3-a306-0dfe4e7abcb7
0
start osd.0
add osd1 810465d8-ea97-40dd-9f29-50ff8d70d84c
1
start osd.1
add osd2 003581bd-4031-4bf4-ab6d-dcd0e093f989
2
start osd.2


restful urls: https://10.10.1.2:42886
  w/ user/pass: admin / 441c50be-09cb-43db-83ac-95171bfea620


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T11:09:10.575-0700 7f1744dcd1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T11:09:10.575-0700 7f1744dcd1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T11:09:10.591-0700 7f00281071c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T11:09:10.591-0700 7f00281071c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40886,v1:10.10.1.2:40887] --print /tmp/ceph_monmap.386660 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.386660 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.386660 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42886 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.8VWQOi8coH 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 925b943b-2df7-4cd3-a306-0dfe4e7abcb7 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBqsaJgJ3q9OBAAN+vFccGWM4rq/QucGxbV4Q== --osd-uuid 925b943b-2df7-4cd3-a306-0dfe4e7abcb7 
2021-05-17T11:09:47.323-0700 7f2e6a8e2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T11:09:47.327-0700 7f2e6a8e2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T11:09:47.327-0700 7f2e6a8e2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T11:09:47.371-0700 7f2e6a8e2f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 810465d8-ea97-40dd-9f29-50ff8d70d84c -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T11:09:47.647-0700 7fcb811f0f00 -1 Falling back to public interface
2021-05-17T11:09:47.659-0700 7fcb811f0f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBrsaJgdGDbJhAA0mWM27FxCLvaerBrNFi/Tw== --osd-uuid 810465d8-ea97-40dd-9f29-50ff8d70d84c 
2021-05-17T11:09:47.983-0700 7f8292715f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T11:09:47.987-0700 7f8292715f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T11:09:47.987-0700 7f8292715f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T11:09:48.031-0700 7f8292715f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 003581bd-4031-4bf4-ab6d-dcd0e093f989 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T11:09:48.419-0700 7f6e520c2f00 -1 Falling back to public interface
2021-05-17T11:09:48.431-0700 7f6e520c2f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBssaJglzYZGRAAcz3SyoPqmLBTBjF9lKCb5w== --osd-uuid 003581bd-4031-4bf4-ab6d-dcd0e093f989 
2021-05-17T11:09:48.779-0700 7f21a5adcf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T11:09:48.779-0700 7f21a5adcf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T11:09:48.779-0700 7f21a5adcf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T11:09:48.859-0700 7f21a5adcf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T11:09:49.179-0700 7f5fe8d02f00 -1 Falling back to public interface
2021-05-17T11:09:49.195-0700 7f5fe8d02f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:09:51,207344411-04:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:09:51,211671527-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:09:51,251887154-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:09:51,255297218-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'af27c88f-d26a-4892-893c-5a8592548a52', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QjGzvm:/tmp/ceph-asok.QjGzvm', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid af27c88f-d26a-4892-893c-5a8592548a52 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QjGzvm:/tmp/ceph-asok.QjGzvm -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:09:54,598845498-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:09:54,603079911-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'af27c88f-d26a-4892-893c-5a8592548a52', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QjGzvm:/tmp/ceph-asok.QjGzvm', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid af27c88f-d26a-4892-893c-5a8592548a52 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QjGzvm:/tmp/ceph-asok.QjGzvm -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:09:58,221538117-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:09:58,225162904-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'af27c88f-d26a-4892-893c-5a8592548a52', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QjGzvm:/tmp/ceph-asok.QjGzvm', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid af27c88f-d26a-4892-893c-5a8592548a52 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QjGzvm:/tmp/ceph-asok.QjGzvm -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:10:01,419312192-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:10:01,423184033-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'af27c88f-d26a-4892-893c-5a8592548a52', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QjGzvm:/tmp/ceph-asok.QjGzvm', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid af27c88f-d26a-4892-893c-5a8592548a52 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QjGzvm:/tmp/ceph-asok.QjGzvm -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:10:08,197796752-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:10:08,201661641-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'af27c88f-d26a-4892-893c-5a8592548a52', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QjGzvm:/tmp/ceph-asok.QjGzvm', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid af27c88f-d26a-4892-893c-5a8592548a52 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QjGzvm:/tmp/ceph-asok.QjGzvm -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:10:12,495641852-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:10:12,498867741-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'af27c88f-d26a-4892-893c-5a8592548a52', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QjGzvm:/tmp/ceph-asok.QjGzvm', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid af27c88f-d26a-4892-893c-5a8592548a52 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QjGzvm:/tmp/ceph-asok.QjGzvm -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:10:16,959923042-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:10:16,963486304-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'af27c88f-d26a-4892-893c-5a8592548a52', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QjGzvm:/tmp/ceph-asok.QjGzvm', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid af27c88f-d26a-4892-893c-5a8592548a52 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QjGzvm:/tmp/ceph-asok.QjGzvm -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:10:21,268123038-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:10:21,271370637-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'af27c88f-d26a-4892-893c-5a8592548a52', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QjGzvm:/tmp/ceph-asok.QjGzvm', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid af27c88f-d26a-4892-893c-5a8592548a52 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QjGzvm:/tmp/ceph-asok.QjGzvm -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:10:25,845926518-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:10:25,850192460-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'af27c88f-d26a-4892-893c-5a8592548a52', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QjGzvm:/tmp/ceph-asok.QjGzvm', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid af27c88f-d26a-4892-893c-5a8592548a52 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QjGzvm:/tmp/ceph-asok.QjGzvm -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:10:30,131412015-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:10:30,135138774-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'af27c88f-d26a-4892-893c-5a8592548a52', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QjGzvm:/tmp/ceph-asok.QjGzvm', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid af27c88f-d26a-4892-893c-5a8592548a52 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QjGzvm:/tmp/ceph-asok.QjGzvm -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:10:33,871633250-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:10:33,874953426-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'af27c88f-d26a-4892-893c-5a8592548a52', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QjGzvm:/tmp/ceph-asok.QjGzvm', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid af27c88f-d26a-4892-893c-5a8592548a52 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QjGzvm:/tmp/ceph-asok.QjGzvm -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:10:37,422016083-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:10:37,426024361-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'af27c88f-d26a-4892-893c-5a8592548a52', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QjGzvm:/tmp/ceph-asok.QjGzvm', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid af27c88f-d26a-4892-893c-5a8592548a52 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QjGzvm:/tmp/ceph-asok.QjGzvm -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  155 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:10:40,760864825-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:11:04,048771406-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:11:12,413171841-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:11:20,860097551-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:11:29,088690054-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:11:37,353780690-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:11:45,537698704-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:11:53,889770569-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:11:53,898808322-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:12:02,383486067-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:12:02,392312885-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:12:10,917477274-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:12:10,926142759-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:12:19,177751897-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:12:19,186800952-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:12:27,545630675-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:12:27,554689418-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:12:27,561598814-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:12:27,565551728-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:12:27,574269561-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=513383
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_1MB_write.dat.4 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:12:27,581574971-04:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_1MB_write.log.4
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:12:27,600465128-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:12:27,603896042-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'af27c88f-d26a-4892-893c-5a8592548a52', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QjGzvm:/tmp/ceph-asok.QjGzvm', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid af27c88f-d26a-4892-893c-5a8592548a52 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QjGzvm:/tmp/ceph-asok.QjGzvm -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T18:12:30.070+0000 7f4891549d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T18:12:30.074+0000 7f4891549d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T18:12:30.074+0000 7f4891549d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T18:12:30.084923+0000 Maintaining 256 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 20 seconds or 0 objects
2021-05-17T18:12:30.084934+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-17T18:12:30.143010+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T18:12:30.143010+0000     0       0         0         0         0         0           -           0
2021-05-17T18:12:31.143100+0000     1     255      1072       817   816.971       817    0.240236    0.241277
2021-05-17T18:12:32.143172+0000     2     255      2176      1921   960.449      1104    0.229171    0.237187
2021-05-17T18:12:33.143245+0000     3     255      3258      3003   1000.94      1082    0.238138    0.236926
2021-05-17T18:12:34.143323+0000     4     255      4360      4105   1026.18      1102    0.222735    0.235632
2021-05-17T18:12:35.143400+0000     5     255      5484      5229   1045.73      1124    0.224855    0.234261
2021-05-17T18:12:36.143486+0000     6     255      6548      6293   1048.76      1064    0.233579    0.235048
2021-05-17T18:12:37.143558+0000     7     255      7600      7345   1049.21      1052    0.227612    0.236122
2021-05-17T18:12:38.143636+0000     8     255      8681      8426   1053.17      1081    0.219131    0.236562
2021-05-17T18:12:39.143747+0000     9     255      9687      9432   1047.92      1006    0.308184     0.23771
2021-05-17T18:12:40.143813+0000    10     255     10723     10468   1046.72      1036    0.202448    0.239563
2021-05-17T18:12:41.143891+0000    11     255     11763     11508    1046.1      1040    0.218928    0.239897
2021-05-17T18:12:42.143959+0000    12     255     12884     12629   1052.34      1121    0.215928    0.238974
2021-05-17T18:12:43.144054+0000    13     255     13921     13666   1051.15      1037     0.26745    0.239281
2021-05-17T18:12:44.144126+0000    14     255     14879     14624   1044.49       958    0.231415    0.241228
2021-05-17T18:12:45.144199+0000    15     255     15946     15691   1045.99      1067     0.24056    0.241086
2021-05-17T18:12:46.144270+0000    16     255     17082     16827   1051.61      1136     0.22186    0.240194
2021-05-17T18:12:47.144366+0000    17     255     18200     17945   1055.51      1118    0.225426    0.239433
2021-05-17T18:12:48.144434+0000    18     255     19298     19043   1057.86      1098    0.234556    0.238976
2021-05-17T18:12:49.144512+0000    19     255     20400     20145   1060.18      1102    0.248763    0.238507
2021-05-17T18:12:50.144586+0000 min lat: 0.0165685 max lat: 0.425684 avg lat: 0.236816
2021-05-17T18:12:50.144586+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T18:12:50.144586+0000    20      33     21540     21507   1075.27      1362   0.0165685    0.236816
2021-05-17T18:12:51.144755+0000 Total time run:         20.0138
Total writes made:      21540
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     1076.26
Stddev Bandwidth:       98.7513
Max bandwidth (MB/sec): 1362
Min bandwidth (MB/sec): 817
Average IOPS:           1076
Stddev IOPS:            98.7513
Max IOPS:               1362
Min IOPS:               817
Average Latency(s):     0.236498
Stddev Latency(s):      0.0278246
Max latency(s):         0.425684
Min latency(s):         0.00957426

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:12:51,740699487-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 513383


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:12:51,745509530-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:13:14,871212701-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 21.54k objects, 21 GiB
    usage:   42 GiB used, 258 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:13:14,880003621-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:13:23,276592547-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 21.54k objects, 21 GiB
    usage:   42 GiB used, 258 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:13:23,285879879-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:13:31,782890022-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 21.54k objects, 21 GiB
    usage:   42 GiB used, 258 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:13:31,791875698-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:13:40,212764423-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 21.54k objects, 21 GiB
    usage:   42 GiB used, 258 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:13:40,221749839-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:13:48,592712103-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 21.54k objects, 21 GiB
    usage:   42 GiB used, 258 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:13:48,601609363-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:13:48,608845333-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:13:48,612583063-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:13:48,621607782-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=514781
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_1MB_seq.dat.4 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:13:48,628762329-04:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_1MB_seq.log.4
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:13:48,647592314-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:13:48,651105963-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'af27c88f-d26a-4892-893c-5a8592548a52', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QjGzvm:/tmp/ceph-asok.QjGzvm', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid af27c88f-d26a-4892-893c-5a8592548a52 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.QjGzvm:/tmp/ceph-asok.QjGzvm -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T18:13:51.134+0000 7f5e981e7d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T18:13:51.138+0000 7f5e981e7d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T18:13:51.138+0000 7f5e981e7d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T18:13:51.150555+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T18:13:51.150555+0000     0       0         0         0         0         0           -           0
2021-05-17T18:13:52.150636+0000     1     255      3966      3711   3710.42      3711   0.0714944   0.0652701
2021-05-17T18:13:53.150713+0000     2     255      7461      7206   3602.58      3495   0.0713345   0.0692096
2021-05-17T18:13:54.150860+0000     3     255     11019     10764   3587.54      3558    0.125992   0.0699924
2021-05-17T18:13:55.150980+0000     4     255     14346     14091   3522.31      3327   0.0702072   0.0716995
2021-05-17T18:13:56.151050+0000     5     255     17821     17566    3512.8      3475   0.0736925    0.072066
2021-05-17T18:13:57.151135+0000     6     255     21420     21165   3527.11      3599   0.0682331   0.0718907
2021-05-17T18:13:58.151262+0000 Total time run:       6.07823
Total reads made:     21540
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   3543.8
Average IOPS:         3543
Stddev IOPS:          129.404
Max IOPS:             3711
Min IOPS:             3327
Average Latency(s):   0.0716157
Max latency(s):       0.14443
Min latency(s):       0.016237

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:13:58,744702586-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 514781


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:13:58,749146091-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:14:22,067087035-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 21.54k objects, 21 GiB
    usage:   42 GiB used, 258 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:14:22,076996706-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:14:30,504884190-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 21.54k objects, 21 GiB
    usage:   42 GiB used, 258 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:14:30,513749951-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:14:38,785477766-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 21.54k objects, 21 GiB
    usage:   42 GiB used, 258 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:14:38,794181843-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:14:47,082582767-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 21.54k objects, 21 GiB
    usage:   42 GiB used, 258 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:14:47,091196364-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:14:55,420650245-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 21.54k objects, 21 GiB
    usage:   42 GiB used, 258 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:14:55,430010094-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:14:55,436663921-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T14:14:55,439112148-04:00][RUNNING][ROUND 5/5/40] object_size=1MB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:14:55,443246463-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:14:55,452791429-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40392\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.387775\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 8baba8d6-3e9e-41a1-a9b6-079970641bcb\nsetting min_mon_release = octopus\nepoch 0\nfsid 8baba8d6-3e9e-41a1-a9b6-079970641bcb\nlast_changed 2021-05-17T11:15:23.958809-0700\ncreated 2021-05-17T11:15:23.958809-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40392/0,v1:10.10.1.2:40393/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.387775 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 ebb883b2-60d3-4a66-b39e-da1e60966386\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 9d084860-90e2-4665-9d6b-803eff14cac2\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 0a0b434d-99fe-4ac5-826a-a265720a6ddf\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42392\n  w/ user/pass: admin / f6f0b576-2884-4b71-8260-22a1ac6cf924\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\nCEPH_DEV=1\n'
[1] 14:15:37 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40392
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.387775
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 8baba8d6-3e9e-41a1-a9b6-079970641bcb
setting min_mon_release = octopus
epoch 0
fsid 8baba8d6-3e9e-41a1-a9b6-079970641bcb
last_changed 2021-05-17T11:15:23.958809-0700
created 2021-05-17T11:15:23.958809-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40392/0,v1:10.10.1.2:40393/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.387775 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 ebb883b2-60d3-4a66-b39e-da1e60966386
0
start osd.0
add osd1 9d084860-90e2-4665-9d6b-803eff14cac2
1
start osd.1
add osd2 0a0b434d-99fe-4ac5-826a-a265720a6ddf
2
start osd.2


restful urls: https://10.10.1.2:42392
  w/ user/pass: admin / f6f0b576-2884-4b71-8260-22a1ac6cf924


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T11:14:56.370-0700 7f9e322131c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T11:14:56.370-0700 7f9e322131c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T11:14:56.386-0700 7f68534151c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T11:14:56.386-0700 7f68534151c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40392,v1:10.10.1.2:40393] --print /tmp/ceph_monmap.387775 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.387775 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.387775 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42392 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.wRvWEc3voq 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new ebb883b2-60d3-4a66-b39e-da1e60966386 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDEsqJggXR5LxAAYK30kGgwfZzvvwsmEKertg== --osd-uuid ebb883b2-60d3-4a66-b39e-da1e60966386 
2021-05-17T11:15:33.122-0700 7fbb2b071f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T11:15:33.122-0700 7fbb2b071f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T11:15:33.122-0700 7fbb2b071f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T11:15:33.190-0700 7fbb2b071f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 9d084860-90e2-4665-9d6b-803eff14cac2 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T11:15:33.470-0700 7fbd28b9ff00 -1 Falling back to public interface
2021-05-17T11:15:33.482-0700 7fbd28b9ff00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDFsqJgGEf7GxAA4C/bjmpiuoVQtrIKAa/HEQ== --osd-uuid 9d084860-90e2-4665-9d6b-803eff14cac2 
2021-05-17T11:15:33.822-0700 7f82107d2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T11:15:33.826-0700 7f82107d2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T11:15:33.826-0700 7f82107d2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T11:15:33.870-0700 7f82107d2f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 0a0b434d-99fe-4ac5-826a-a265720a6ddf -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T11:15:34.222-0700 7f00d970df00 -1 Falling back to public interface
2021-05-17T11:15:34.234-0700 7f00d970df00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDGsqJghb5PDRAAY1eUqjc2nN+7Jsq05epa8Q== --osd-uuid 0a0b434d-99fe-4ac5-826a-a265720a6ddf 
2021-05-17T11:15:34.610-0700 7fc7d4017f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T11:15:34.610-0700 7fc7d4017f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T11:15:34.610-0700 7fc7d4017f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T11:15:34.706-0700 7fc7d4017f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T11:15:35.062-0700 7f0bf924df00 -1 Falling back to public interface
2021-05-17T11:15:35.074-0700 7f0bf924df00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:15:37,034331631-04:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:15:37,038601371-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:15:37,079066449-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:15:37,082625102-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e4baabff-3077-4c10-9c6f-cad32c070147', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RmZbrk:/tmp/ceph-asok.RmZbrk', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e4baabff-3077-4c10-9c6f-cad32c070147 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RmZbrk:/tmp/ceph-asok.RmZbrk -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:15:40,438034707-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:15:40,442161057-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e4baabff-3077-4c10-9c6f-cad32c070147', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RmZbrk:/tmp/ceph-asok.RmZbrk', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e4baabff-3077-4c10-9c6f-cad32c070147 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RmZbrk:/tmp/ceph-asok.RmZbrk -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:15:43,694230639-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:15:43,698018834-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e4baabff-3077-4c10-9c6f-cad32c070147', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RmZbrk:/tmp/ceph-asok.RmZbrk', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e4baabff-3077-4c10-9c6f-cad32c070147 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RmZbrk:/tmp/ceph-asok.RmZbrk -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:15:46,926565849-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:15:46,929934356-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e4baabff-3077-4c10-9c6f-cad32c070147', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RmZbrk:/tmp/ceph-asok.RmZbrk', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e4baabff-3077-4c10-9c6f-cad32c070147 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RmZbrk:/tmp/ceph-asok.RmZbrk -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:15:53,465081718-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:15:53,468213039-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e4baabff-3077-4c10-9c6f-cad32c070147', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RmZbrk:/tmp/ceph-asok.RmZbrk', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e4baabff-3077-4c10-9c6f-cad32c070147 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RmZbrk:/tmp/ceph-asok.RmZbrk -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:15:56,887574856-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:15:56,891158877-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e4baabff-3077-4c10-9c6f-cad32c070147', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RmZbrk:/tmp/ceph-asok.RmZbrk', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e4baabff-3077-4c10-9c6f-cad32c070147 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RmZbrk:/tmp/ceph-asok.RmZbrk -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:16:00,301828110-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:16:00,305242022-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e4baabff-3077-4c10-9c6f-cad32c070147', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RmZbrk:/tmp/ceph-asok.RmZbrk', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e4baabff-3077-4c10-9c6f-cad32c070147 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RmZbrk:/tmp/ceph-asok.RmZbrk -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:16:03,754086276-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:16:03,758393846-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e4baabff-3077-4c10-9c6f-cad32c070147', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RmZbrk:/tmp/ceph-asok.RmZbrk', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e4baabff-3077-4c10-9c6f-cad32c070147 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RmZbrk:/tmp/ceph-asok.RmZbrk -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:16:08,029803547-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:16:08,033860346-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e4baabff-3077-4c10-9c6f-cad32c070147', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RmZbrk:/tmp/ceph-asok.RmZbrk', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e4baabff-3077-4c10-9c6f-cad32c070147 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RmZbrk:/tmp/ceph-asok.RmZbrk -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:16:11,852382679-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:16:11,856069463-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e4baabff-3077-4c10-9c6f-cad32c070147', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RmZbrk:/tmp/ceph-asok.RmZbrk', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e4baabff-3077-4c10-9c6f-cad32c070147 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RmZbrk:/tmp/ceph-asok.RmZbrk -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:16:16,266056848-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:16:16,270140368-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e4baabff-3077-4c10-9c6f-cad32c070147', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RmZbrk:/tmp/ceph-asok.RmZbrk', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e4baabff-3077-4c10-9c6f-cad32c070147 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RmZbrk:/tmp/ceph-asok.RmZbrk -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:16:19,415366067-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:16:19,418715007-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e4baabff-3077-4c10-9c6f-cad32c070147', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RmZbrk:/tmp/ceph-asok.RmZbrk', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e4baabff-3077-4c10-9c6f-cad32c070147 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RmZbrk:/tmp/ceph-asok.RmZbrk -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  156 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:16:22,760218274-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:16:46,014649136-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (5s)
      [=============...............] (remaining: 5s)
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:16:54,293449542-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:17:02,625129317-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:17:10,868869283-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:17:19,123493404-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:17:27,370911916-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:17:35,816546351-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:17:35,825617798-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:17:44,212795076-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:17:44,221737030-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:17:52,567658988-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:17:52,577097144-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:18:00,888769289-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:18:00,897891072-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:18:09,259303571-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:18:09,268694549-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:18:09,276087093-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:18:09,280610018-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:18:09,289546552-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=521385
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_1MB_write.dat.5 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:18:09,295721660-04:00] INFO: > Run rados bench[0m
# ./bench-rados:192 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_1MB_write.log.5
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:18:09,315726302-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:18:09,318937834-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e4baabff-3077-4c10-9c6f-cad32c070147', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RmZbrk:/tmp/ceph-asok.RmZbrk', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e4baabff-3077-4c10-9c6f-cad32c070147 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RmZbrk:/tmp/ceph-asok.RmZbrk -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T18:18:11.883+0000 7fd4ea3aed00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T18:18:11.887+0000 7fd4ea3aed00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T18:18:11.887+0000 7fd4ea3aed00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T18:18:11.897348+0000 Maintaining 256 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 20 seconds or 0 objects
2021-05-17T18:18:11.897359+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-17T18:18:11.954635+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T18:18:11.954635+0000     0       0         0         0         0         0           -           0
2021-05-17T18:18:12.954738+0000     1     255      1280      1025   1024.97      1025     0.20547    0.201344
2021-05-17T18:18:13.954840+0000     2     255      2539      2284   1141.92      1259    0.196827    0.202756
2021-05-17T18:18:14.954965+0000     3     255      3833      3578   1192.56      1294    0.176837    0.201945
2021-05-17T18:18:15.955075+0000     4     255      5141      4886   1221.39      1308    0.192145    0.199753
2021-05-17T18:18:16.955182+0000     5     255      6422      6167   1233.28      1281    0.200356    0.199867
2021-05-17T18:18:17.955300+0000     6     255      7698      7443   1240.38      1276     0.18902    0.199983
2021-05-17T18:18:18.955444+0000     7     255      8953      8698   1242.44      1255    0.212966     0.20031
2021-05-17T18:18:19.955556+0000     8     255     10245      9990   1248.62      1292     0.21535    0.199857
2021-05-17T18:18:20.955661+0000     9     255     11518     11263   1251.31      1273    0.206206    0.200099
2021-05-17T18:18:21.955773+0000    10     255     12830     12575   1257.37      1312    0.196453    0.199737
2021-05-17T18:18:22.955915+0000    11     255     14100     13845    1258.5      1270     0.19766    0.199881
2021-05-17T18:18:23.956027+0000    12     255     15366     15111   1259.11      1266    0.192746    0.200062
2021-05-17T18:18:24.956135+0000    13     255     16627     16372   1259.25      1261    0.190462    0.200485
2021-05-17T18:18:25.956242+0000    14     255     17905     17650   1260.58      1278    0.198062    0.200327
2021-05-17T18:18:26.956389+0000    15     255     19167     18912   1260.66      1262    0.201445     0.20048
2021-05-17T18:18:27.956496+0000    16     255     20494     20239    1264.8      1327    0.187872    0.200058
2021-05-17T18:18:28.956605+0000    17     255     21797     21542   1267.03      1303    0.190497     0.19979
2021-05-17T18:18:29.956737+0000    18     255     23102     22847   1269.13      1305    0.180171    0.199734
2021-05-17T18:18:30.956850+0000    19     255     24324     24069   1266.65      1222    0.191091    0.200142
2021-05-17T18:18:31.956934+0000 min lat: 0.170006 max lat: 0.312794 avg lat: 0.201536
2021-05-17T18:18:31.956934+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T18:18:31.956934+0000    20     255     25414     25159   1257.81      1090    0.200028    0.201536
2021-05-17T18:18:32.957325+0000 Total time run:         20.0146
Total writes made:      25415
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     1269.82
Stddev Bandwidth:       73.4105
Max bandwidth (MB/sec): 1327
Min bandwidth (MB/sec): 1025
Average IOPS:           1269
Stddev IOPS:            73.4105
Max IOPS:               1327
Min IOPS:               1025
Average Latency(s):     0.200582
Stddev Latency(s):      0.0184582
Max latency(s):         0.312794
Min latency(s):         0.00664442

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:18:33,641808436-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 521385


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:18:33,646828524-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:18:56,766087001-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 25.42k objects, 25 GiB
    usage:   50 GiB used, 250 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:18:56,774992006-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:19:05,130668583-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 25.42k objects, 25 GiB
    usage:   50 GiB used, 250 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:19:05,139884763-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:19:13,343853242-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 25.42k objects, 25 GiB
    usage:   50 GiB used, 250 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:19:13,352531621-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:19:21,639867470-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 25.42k objects, 25 GiB
    usage:   50 GiB used, 250 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:19:21,648715538-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:19:29,935936994-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 25.42k objects, 25 GiB
    usage:   50 GiB used, 250 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:19:29,944969709-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:19:29,952038315-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:19:29,955663194-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:19:29,964563730-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=522753
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_1MB_seq.dat.5 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:19:29,970849696-04:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_1MB_seq.log.5
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:19:29,990866140-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:19:29,994251559-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e4baabff-3077-4c10-9c6f-cad32c070147', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RmZbrk:/tmp/ceph-asok.RmZbrk', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e4baabff-3077-4c10-9c6f-cad32c070147 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RmZbrk:/tmp/ceph-asok.RmZbrk -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T18:19:32.383+0000 7f5faa50cd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T18:19:32.391+0000 7f5faa50cd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T18:19:32.391+0000 7f5faa50cd00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T18:19:32.401478+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T18:19:32.401478+0000     0       0         0         0         0         0           -           0
2021-05-17T18:19:33.401633+0000     1     255      3697      3442   3441.31      3442   0.0676135   0.0707387
2021-05-17T18:19:34.401755+0000     2     255      7369      7114   3556.42      3672   0.0691933   0.0701314
2021-05-17T18:19:35.401942+0000     3     255     11036     10781   3593.06      3667   0.0673707   0.0699644
2021-05-17T18:19:36.402034+0000     4     255     14570     14315   3578.21      3534   0.0702933   0.0705795
2021-05-17T18:19:37.402158+0000     5     255     18118     17863   3572.08      3548    0.132261   0.0708571
2021-05-17T18:19:38.402248+0000     6     255     21704     21449   3574.35      3586   0.0696081   0.0709621
2021-05-17T18:19:39.402367+0000     7     255     25264     25009   3572.24      3560   0.0685166   0.0710351
2021-05-17T18:19:40.402578+0000 Total time run:       7.09305
Total reads made:     25415
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   3583.08
Average IOPS:         3583
Stddev IOPS:          79.8764
Max IOPS:             3672
Min IOPS:             3442
Average Latency(s):   0.0708467
Max latency(s):       0.154397
Min latency(s):       0.0148221

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:19:40,995403648-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 522753


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:19:41,000331013-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:20:04,281693406-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 25.42k objects, 25 GiB
    usage:   50 GiB used, 250 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:20:04,290966684-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:20:12,658835166-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 25.42k objects, 25 GiB
    usage:   50 GiB used, 250 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:20:12,668007694-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:20:21,014940359-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 25.42k objects, 25 GiB
    usage:   50 GiB used, 250 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:20:21,024403462-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:20:29,302282494-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 25.42k objects, 25 GiB
    usage:   50 GiB used, 250 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:20:29,310974530-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:20:37,440302378-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 25.42k objects, 25 GiB
    usage:   50 GiB used, 250 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:20:37,449603578-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:20:37,456408349-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T14:20:37,460654023-04:00][RUNNING][ROUND 1/6/40] object_size=4MB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:20:37,464499205-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:20:37,473982005-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40421\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.388887\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 6dd66efe-6c79-4924-a393-3df0eb8429d3\nsetting min_mon_release = octopus\nepoch 0\nfsid 6dd66efe-6c79-4924-a393-3df0eb8429d3\nlast_changed 2021-05-17T11:21:04.538666-0700\ncreated 2021-05-17T11:21:04.538666-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40421/0,v1:10.10.1.2:40422/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.388887 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 cad3d44c-9214-4668-866b-63ed6730f252\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 21388eae-af83-453c-9872-5df20def5027\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 3ae9d185-fb11-4865-bd3a-420f0e97e8d9\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42421\n  w/ user/pass: admin / 9ffff74d-198c-4d1b-b10d-5ac5136f212d\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 14:21:17 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40421
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.388887
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 6dd66efe-6c79-4924-a393-3df0eb8429d3
setting min_mon_release = octopus
epoch 0
fsid 6dd66efe-6c79-4924-a393-3df0eb8429d3
last_changed 2021-05-17T11:21:04.538666-0700
created 2021-05-17T11:21:04.538666-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40421/0,v1:10.10.1.2:40422/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.388887 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 cad3d44c-9214-4668-866b-63ed6730f252
0
start osd.0
add osd1 21388eae-af83-453c-9872-5df20def5027
1
start osd.1
add osd2 3ae9d185-fb11-4865-bd3a-420f0e97e8d9
2
start osd.2


restful urls: https://10.10.1.2:42421
  w/ user/pass: admin / 9ffff74d-198c-4d1b-b10d-5ac5136f212d


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T11:20:38.385-0700 7fa83fe2a1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T11:20:38.385-0700 7fa83fe2a1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T11:20:38.401-0700 7f68a7a7d1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T11:20:38.401-0700 7f68a7a7d1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40421,v1:10.10.1.2:40422] --print /tmp/ceph_monmap.388887 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.388887 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.388887 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42421 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.zgCQGqUnlL 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new cad3d44c-9214-4668-866b-63ed6730f252 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAZtKJg067sFBAAWU3klFwZ3BLkeD4xCUKI6A== --osd-uuid cad3d44c-9214-4668-866b-63ed6730f252 
2021-05-17T11:21:13.693-0700 7f9220991f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T11:21:13.693-0700 7f9220991f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T11:21:13.693-0700 7f9220991f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T11:21:13.761-0700 7f9220991f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 21388eae-af83-453c-9872-5df20def5027 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T11:21:14.065-0700 7f3aabfd1f00 -1 Falling back to public interface
2021-05-17T11:21:14.077-0700 7f3aabfd1f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAatKJg0cLzAxAAV17z/y6fudwew31GGE3h5g== --osd-uuid 21388eae-af83-453c-9872-5df20def5027 
2021-05-17T11:21:14.393-0700 7f1433d53f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T11:21:14.397-0700 7f1433d53f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T11:21:14.397-0700 7f1433d53f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T11:21:14.449-0700 7f1433d53f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 3ae9d185-fb11-4865-bd3a-420f0e97e8d9 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T11:21:14.765-0700 7fb4bc6fcf00 -1 Falling back to public interface
2021-05-17T11:21:14.777-0700 7fb4bc6fcf00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAatKJgWEjuKxAAenwIvVuzHEPWTMf5FL8SFQ== --osd-uuid 3ae9d185-fb11-4865-bd3a-420f0e97e8d9 
2021-05-17T11:21:15.065-0700 7f3296ff6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T11:21:15.065-0700 7f3296ff6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T11:21:15.065-0700 7f3296ff6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T11:21:15.257-0700 7f3296ff6f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T11:21:15.837-0700 7f717bd5ef00 -1 Falling back to public interface
2021-05-17T11:21:15.849-0700 7f717bd5ef00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:21:17,503492856-04:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:21:17,507404443-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:21:17,547814210-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:21:17,551374477-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8aa5f50d-ef8b-42c1-a333-018aa62fda5e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CRUmnT:/tmp/ceph-asok.CRUmnT', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8aa5f50d-ef8b-42c1-a333-018aa62fda5e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CRUmnT:/tmp/ceph-asok.CRUmnT -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:21:20,840694857-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:21:20,844249253-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8aa5f50d-ef8b-42c1-a333-018aa62fda5e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CRUmnT:/tmp/ceph-asok.CRUmnT', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8aa5f50d-ef8b-42c1-a333-018aa62fda5e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CRUmnT:/tmp/ceph-asok.CRUmnT -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:21:24,155916818-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:21:24,160089304-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8aa5f50d-ef8b-42c1-a333-018aa62fda5e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CRUmnT:/tmp/ceph-asok.CRUmnT', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8aa5f50d-ef8b-42c1-a333-018aa62fda5e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CRUmnT:/tmp/ceph-asok.CRUmnT -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:21:27,279334896-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:21:27,283426410-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8aa5f50d-ef8b-42c1-a333-018aa62fda5e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CRUmnT:/tmp/ceph-asok.CRUmnT', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8aa5f50d-ef8b-42c1-a333-018aa62fda5e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CRUmnT:/tmp/ceph-asok.CRUmnT -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:21:33,780218928-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:21:33,783696159-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8aa5f50d-ef8b-42c1-a333-018aa62fda5e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CRUmnT:/tmp/ceph-asok.CRUmnT', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8aa5f50d-ef8b-42c1-a333-018aa62fda5e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CRUmnT:/tmp/ceph-asok.CRUmnT -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:21:37,584445351-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:21:37,588395260-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8aa5f50d-ef8b-42c1-a333-018aa62fda5e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CRUmnT:/tmp/ceph-asok.CRUmnT', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8aa5f50d-ef8b-42c1-a333-018aa62fda5e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CRUmnT:/tmp/ceph-asok.CRUmnT -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:21:40,893289082-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:21:40,897097746-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8aa5f50d-ef8b-42c1-a333-018aa62fda5e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CRUmnT:/tmp/ceph-asok.CRUmnT', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8aa5f50d-ef8b-42c1-a333-018aa62fda5e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CRUmnT:/tmp/ceph-asok.CRUmnT -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:21:44,282131097-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:21:44,286099470-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8aa5f50d-ef8b-42c1-a333-018aa62fda5e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CRUmnT:/tmp/ceph-asok.CRUmnT', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8aa5f50d-ef8b-42c1-a333-018aa62fda5e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CRUmnT:/tmp/ceph-asok.CRUmnT -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:21:47,656239382-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:21:47,659855373-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8aa5f50d-ef8b-42c1-a333-018aa62fda5e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CRUmnT:/tmp/ceph-asok.CRUmnT', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8aa5f50d-ef8b-42c1-a333-018aa62fda5e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CRUmnT:/tmp/ceph-asok.CRUmnT -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:21:51,111227096-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:21:51,115310495-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8aa5f50d-ef8b-42c1-a333-018aa62fda5e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CRUmnT:/tmp/ceph-asok.CRUmnT', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8aa5f50d-ef8b-42c1-a333-018aa62fda5e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CRUmnT:/tmp/ceph-asok.CRUmnT -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:21:55,568044715-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:21:55,571792134-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8aa5f50d-ef8b-42c1-a333-018aa62fda5e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CRUmnT:/tmp/ceph-asok.CRUmnT', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8aa5f50d-ef8b-42c1-a333-018aa62fda5e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CRUmnT:/tmp/ceph-asok.CRUmnT -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:21:58,892657233-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:21:58,896434498-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8aa5f50d-ef8b-42c1-a333-018aa62fda5e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CRUmnT:/tmp/ceph-asok.CRUmnT', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8aa5f50d-ef8b-42c1-a333-018aa62fda5e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CRUmnT:/tmp/ceph-asok.CRUmnT -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   47 KiB   0 B   0 B   0 B  100 GiB     0  0.95   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.95/1.03  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:22:02,138370566-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:22:25,370732613-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   206 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
    Global Recovery Event (4s)
      [=========...................] (remaining: 9s)
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:22:33,768289239-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:22:42,133688182-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:22:50,288345567-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:22:58,635029429-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:23:06,881611221-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:23:15,130036357-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:23:15,138288987-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:23:23,494374774-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:23:23,503506465-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:23:31,814932303-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:23:31,824020282-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:23:40,095435272-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:23:40,104669676-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:23:48,360561021-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:23:48,369876317-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:23:48,376896102-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:23:48,380688525-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:23:48,389788717-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=529403
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_4MB_write.dat.1 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:23:48,396434750-04:00] INFO: > Run rados bench[0m
# ./bench-rados:192 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_4MB_write.log.1
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:23:48,415328135-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:23:48,418711229-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8aa5f50d-ef8b-42c1-a333-018aa62fda5e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CRUmnT:/tmp/ceph-asok.CRUmnT', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8aa5f50d-ef8b-42c1-a333-018aa62fda5e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CRUmnT:/tmp/ceph-asok.CRUmnT -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T18:23:50.796+0000 7f6b4982dd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T18:23:50.800+0000 7f6b4982dd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T18:23:50.800+0000 7f6b4982dd00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T18:23:50.815344+0000 Maintaining 256 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 20 seconds or 0 objects
2021-05-17T18:23:50.815356+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-17T18:23:51.040448+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T18:23:51.040448+0000     0       0         0         0         0         0           -           0
2021-05-17T18:23:52.040553+0000     1     243       243         0         0         0           -           0
2021-05-17T18:23:53.040621+0000     2     255       470       215   429.978       430     1.11755     1.10106
2021-05-17T18:23:54.040733+0000     3     255       703       448   597.291       932     1.08798     1.10005
2021-05-17T18:23:55.040831+0000     4     255       935       680   679.947       928     1.11066     1.09632
2021-05-17T18:23:56.040938+0000     5     255      1142       887   709.541       828     1.20887     1.11611
2021-05-17T18:23:57.041018+0000     6     255      1381      1126   750.604       956     1.07672      1.1239
2021-05-17T18:23:58.041117+0000     7     255      1605      1350   771.363       896     1.11436     1.12105
2021-05-17T18:23:59.041209+0000     8     255      1823      1568   783.932       872     1.16811     1.12488
2021-05-17T18:24:00.041280+0000     9     255      2045      1790   795.488       888     1.14304     1.13102
2021-05-17T18:24:01.041381+0000    10     255      2256      2001   800.331       844     1.17915      1.1363
2021-05-17T18:24:02.041474+0000    11     255      2477      2222    807.93       884     1.15352     1.14138
2021-05-17T18:24:03.041575+0000    12     255      2709      2454   817.928       928     1.10119     1.13989
2021-05-17T18:24:04.041671+0000    13     255      2938      2683   825.465       916     1.10746     1.13743
2021-05-17T18:24:05.041745+0000    14     255      3171      2916    833.07       932     1.08853     1.13477
2021-05-17T18:24:06.041852+0000    15     255      3408      3153   840.725       948     1.07577     1.13182
2021-05-17T18:24:07.041944+0000    16     255      3641      3386   846.425       932     1.09545     1.12826
2021-05-17T18:24:08.042013+0000    17     255      3871      3616   850.749       920     1.11817     1.12656
2021-05-17T18:24:09.042092+0000    18     255      4109      3854    856.37       952     1.07953     1.12466
2021-05-17T18:24:10.042165+0000    19     255      4341      4086   860.136       928     1.11043     1.12282
2021-05-17T18:24:11.042266+0000 min lat: 1.05369 max lat: 1.24128 avg lat: 1.12152
2021-05-17T18:24:11.042266+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T18:24:11.042266+0000    20     255      4572      4317   863.325       924     1.11298     1.12152
2021-05-17T18:24:12.042647+0000 Total time run:         20.0245
Total writes made:      4573
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     913.481
Stddev Bandwidth:       228.001
Max bandwidth (MB/sec): 956
Min bandwidth (MB/sec): 0
Average IOPS:           228
Stddev IOPS:            57.048
Max IOPS:               239
Min IOPS:               0
Average Latency(s):     1.09031
Stddev Latency(s):      0.154144
Max latency(s):         1.24128
Min latency(s):         0.0196812

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:24:12,641188655-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 529403


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:24:12,645861161-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:24:35,795835180-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.57k objects, 18 GiB
    usage:   36 GiB used, 264 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:24:35,805629015-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:24:44,051579125-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.57k objects, 18 GiB
    usage:   36 GiB used, 264 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:24:44,061467798-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:24:52,274142509-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.57k objects, 18 GiB
    usage:   36 GiB used, 264 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:24:52,282726501-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:25:00,576349864-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.57k objects, 18 GiB
    usage:   36 GiB used, 264 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:25:00,585389512-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:25:09,083997635-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.57k objects, 18 GiB
    usage:   36 GiB used, 264 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:25:09,093476137-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:25:09,100847332-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:25:09,104841805-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:25:09,113512280-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=530775
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_4MB_seq.dat.1 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:25:09,120092078-04:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_4MB_seq.log.1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:25:09,139387379-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:25:09,143132233-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '8aa5f50d-ef8b-42c1-a333-018aa62fda5e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CRUmnT:/tmp/ceph-asok.CRUmnT', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 8aa5f50d-ef8b-42c1-a333-018aa62fda5e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.CRUmnT:/tmp/ceph-asok.CRUmnT -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T18:25:11.532+0000 7f3478f79d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T18:25:11.536+0000 7f3478f79d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T18:25:11.536+0000 7f3478f79d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T18:25:11.550433+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T18:25:11.550433+0000     0       0         0         0         0         0           -           0
2021-05-17T18:25:12.550550+0000     1     255       889       634   2535.51      2536    0.345695     0.29179
2021-05-17T18:25:13.550629+0000     2     255      1464      1209   2417.67      2300    0.318884    0.367567
2021-05-17T18:25:14.550815+0000     3     255      1992      1737   2315.65      2112    0.298802     0.40408
2021-05-17T18:25:15.575144+0000     4     255      2563      2308   2293.79      2284    0.621263    0.403266
2021-05-17T18:25:16.575270+0000     5     255      3150      2895   2304.52      2348    0.352519     0.41831
2021-05-17T18:25:17.575377+0000     6     255      3849      3594   2386.05      2796    0.344483    0.408916
2021-05-17T18:25:18.575483+0000     7     255      4549      4294   2444.94      2800    0.363851    0.400944
2021-05-17T18:25:19.575692+0000 Total time run:       7.15672
Total reads made:     4573
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   2555.92
Average IOPS:         638
Stddev IOPS:          66.4752
Max IOPS:             700
Min IOPS:             528
Average Latency(s):   0.39129
Max latency(s):       0.829311
Min latency(s):       0.0876372

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:25:20,161376516-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 530775


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:25:20,166103565-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:25:43,284864973-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.57k objects, 18 GiB
    usage:   36 GiB used, 264 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:25:43,294547288-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:25:51,878591448-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.57k objects, 18 GiB
    usage:   36 GiB used, 264 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:25:51,887246854-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:26:00,023225312-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.57k objects, 18 GiB
    usage:   36 GiB used, 264 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:26:00,032982197-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:26:08,232863219-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.57k objects, 18 GiB
    usage:   36 GiB used, 264 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:26:08,241643991-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:26:16,620308281-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.57k objects, 18 GiB
    usage:   36 GiB used, 264 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:26:16,629535693-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:26:16,636739544-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T14:26:16,639017923-04:00][RUNNING][ROUND 2/6/40] object_size=4MB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:26:16,643131348-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:26:16,652175656-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40641\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.390023\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 994b427b-44d6-4866-9c08-ba188a45158f\nsetting min_mon_release = octopus\nepoch 0\nfsid 994b427b-44d6-4866-9c08-ba188a45158f\nlast_changed 2021-05-17T11:26:43.995453-0700\ncreated 2021-05-17T11:26:43.995453-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40641/0,v1:10.10.1.2:40642/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.390023 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 e54883e3-b36a-4bd1-b73d-6b9e7321dc3c\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 0ff09a63-8dc8-41e7-b3a4-0903452715bd\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 2d5dfdae-8dc3-40d2-84dc-ce4488b95491\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42641\n  w/ user/pass: admin / 38058ab8-d1f4-4fa7-a2c2-45b70247e8f1\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 14:26:56 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40641
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.390023
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 994b427b-44d6-4866-9c08-ba188a45158f
setting min_mon_release = octopus
epoch 0
fsid 994b427b-44d6-4866-9c08-ba188a45158f
last_changed 2021-05-17T11:26:43.995453-0700
created 2021-05-17T11:26:43.995453-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40641/0,v1:10.10.1.2:40642/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.390023 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 e54883e3-b36a-4bd1-b73d-6b9e7321dc3c
0
start osd.0
add osd1 0ff09a63-8dc8-41e7-b3a4-0903452715bd
1
start osd.1
add osd2 2d5dfdae-8dc3-40d2-84dc-ce4488b95491
2
start osd.2


restful urls: https://10.10.1.2:42641
  w/ user/pass: admin / 38058ab8-d1f4-4fa7-a2c2-45b70247e8f1


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T11:26:17.596-0700 7fc07b1d61c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T11:26:17.596-0700 7fc07b1d61c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T11:26:17.616-0700 7f3cee5f51c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T11:26:17.616-0700 7f3cee5f51c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40641,v1:10.10.1.2:40642] --print /tmp/ceph_monmap.390023 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.390023 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.390023 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42641 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.rnXO2bkcUJ 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new e54883e3-b36a-4bd1-b73d-6b9e7321dc3c -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBstaJgkWjZLxAA/umEfIHvN4yGBNGjZUrlfg== --osd-uuid e54883e3-b36a-4bd1-b73d-6b9e7321dc3c 
2021-05-17T11:26:53.124-0700 7f32aeb0ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T11:26:53.124-0700 7f32aeb0ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T11:26:53.124-0700 7f32aeb0ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T11:26:53.188-0700 7f32aeb0ef00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 0ff09a63-8dc8-41e7-b3a4-0903452715bd -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T11:26:53.464-0700 7efc70128f00 -1 Falling back to public interface
2021-05-17T11:26:53.476-0700 7efc70128f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBttaJgPSfIGxAAJkTV9xwaTl+oALGwCKtK8w== --osd-uuid 0ff09a63-8dc8-41e7-b3a4-0903452715bd 
2021-05-17T11:26:53.796-0700 7f133ba00f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T11:26:53.796-0700 7f133ba00f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T11:26:53.796-0700 7f133ba00f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T11:26:53.840-0700 7f133ba00f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 2d5dfdae-8dc3-40d2-84dc-ce4488b95491 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T11:26:54.128-0700 7fe7d0e39f00 -1 Falling back to public interface
2021-05-17T11:26:54.140-0700 7fe7d0e39f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQButaJgD5O2BxAAxYBSfe5jYGI6GJF7zPYr0g== --osd-uuid 2d5dfdae-8dc3-40d2-84dc-ce4488b95491 
2021-05-17T11:26:54.472-0700 7fab6b2f9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T11:26:54.476-0700 7fab6b2f9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T11:26:54.476-0700 7fab6b2f9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T11:26:54.528-0700 7fab6b2f9f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T11:26:54.924-0700 7febda6c1f00 -1 Falling back to public interface
2021-05-17T11:26:54.940-0700 7febda6c1f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:26:56,902338693-04:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:26:56,906867149-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:26:56,945935619-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:26:56,949078682-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '10a17d87-b324-4704-a516-b47604077e04', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Pd2TXk:/tmp/ceph-asok.Pd2TXk', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 10a17d87-b324-4704-a516-b47604077e04 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Pd2TXk:/tmp/ceph-asok.Pd2TXk -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:27:00,345950376-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:27:00,349974384-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '10a17d87-b324-4704-a516-b47604077e04', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Pd2TXk:/tmp/ceph-asok.Pd2TXk', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 10a17d87-b324-4704-a516-b47604077e04 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Pd2TXk:/tmp/ceph-asok.Pd2TXk -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:27:03,679136885-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:27:03,683005932-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '10a17d87-b324-4704-a516-b47604077e04', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Pd2TXk:/tmp/ceph-asok.Pd2TXk', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 10a17d87-b324-4704-a516-b47604077e04 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Pd2TXk:/tmp/ceph-asok.Pd2TXk -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:27:07,066356002-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:27:07,069825799-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '10a17d87-b324-4704-a516-b47604077e04', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Pd2TXk:/tmp/ceph-asok.Pd2TXk', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 10a17d87-b324-4704-a516-b47604077e04 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Pd2TXk:/tmp/ceph-asok.Pd2TXk -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:27:13,755008304-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:27:13,758650294-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '10a17d87-b324-4704-a516-b47604077e04', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Pd2TXk:/tmp/ceph-asok.Pd2TXk', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 10a17d87-b324-4704-a516-b47604077e04 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Pd2TXk:/tmp/ceph-asok.Pd2TXk -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:27:18,059866702-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:27:18,063368870-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '10a17d87-b324-4704-a516-b47604077e04', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Pd2TXk:/tmp/ceph-asok.Pd2TXk', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 10a17d87-b324-4704-a516-b47604077e04 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Pd2TXk:/tmp/ceph-asok.Pd2TXk -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:27:21,618217034-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:27:21,622308088-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '10a17d87-b324-4704-a516-b47604077e04', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Pd2TXk:/tmp/ceph-asok.Pd2TXk', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 10a17d87-b324-4704-a516-b47604077e04 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Pd2TXk:/tmp/ceph-asok.Pd2TXk -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:27:25,710308848-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:27:25,714146506-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '10a17d87-b324-4704-a516-b47604077e04', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Pd2TXk:/tmp/ceph-asok.Pd2TXk', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 10a17d87-b324-4704-a516-b47604077e04 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Pd2TXk:/tmp/ceph-asok.Pd2TXk -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:27:30,108474588-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:27:30,112327775-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '10a17d87-b324-4704-a516-b47604077e04', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Pd2TXk:/tmp/ceph-asok.Pd2TXk', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 10a17d87-b324-4704-a516-b47604077e04 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Pd2TXk:/tmp/ceph-asok.Pd2TXk -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:27:33,864114616-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:27:33,868129728-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '10a17d87-b324-4704-a516-b47604077e04', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Pd2TXk:/tmp/ceph-asok.Pd2TXk', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 10a17d87-b324-4704-a516-b47604077e04 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Pd2TXk:/tmp/ceph-asok.Pd2TXk -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:27:38,276387709-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:27:38,280003170-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '10a17d87-b324-4704-a516-b47604077e04', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Pd2TXk:/tmp/ceph-asok.Pd2TXk', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 10a17d87-b324-4704-a516-b47604077e04 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Pd2TXk:/tmp/ceph-asok.Pd2TXk -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:27:41,667556663-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:27:41,671164700-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '10a17d87-b324-4704-a516-b47604077e04', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Pd2TXk:/tmp/ceph-asok.Pd2TXk', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 10a17d87-b324-4704-a516-b47604077e04 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Pd2TXk:/tmp/ceph-asok.Pd2TXk -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  155 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:27:45,061490411-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:28:08,251233135-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=========================...] 
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:28:16,680181825-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:28:25,150955312-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:28:33,504339102-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:28:41,909721890-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:28:50,290659865-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:28:58,427299612-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:28:58,436962511-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:29:06,832802886-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:29:06,842274426-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:29:15,119462240-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:29:15,128837580-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:29:23,580753881-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:29:23,589559109-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:29:31,842365398-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:29:31,851383606-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:29:31,858538064-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:29:31,862414475-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:29:31,871684807-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=537403
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_4MB_write.dat.2 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:29:31,878964961-04:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_4MB_write.log.2
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:29:31,899232329-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:29:31,902820047-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '10a17d87-b324-4704-a516-b47604077e04', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Pd2TXk:/tmp/ceph-asok.Pd2TXk', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 10a17d87-b324-4704-a516-b47604077e04 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Pd2TXk:/tmp/ceph-asok.Pd2TXk -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T18:29:34.461+0000 7f3829d10d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T18:29:34.465+0000 7f3829d10d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T18:29:34.465+0000 7f3829d10d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T18:29:34.477681+0000 Maintaining 256 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 20 seconds or 0 objects
2021-05-17T18:29:34.477692+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-17T18:29:34.698288+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T18:29:34.698288+0000     0       0         0         0         0         0           -           0
2021-05-17T18:29:35.698380+0000     1     219       219         0         0         0           -           0
2021-05-17T18:29:36.698478+0000     2     255       449       194   387.972       388     1.11139     1.15249
2021-05-17T18:29:37.698571+0000     3     255       668       413   550.623       876     1.17489     1.15136
2021-05-17T18:29:38.698654+0000     4     255       887       632   631.949       876      1.1638     1.15793
2021-05-17T18:29:39.698738+0000     5     255      1092       837   669.546       820     1.24878     1.16823
2021-05-17T18:29:40.698807+0000     6     255      1290      1035   689.946       792     1.25353      1.1897
2021-05-17T18:29:41.698876+0000     7     255      1486      1231   703.374       784     1.26463     1.19721
2021-05-17T18:29:42.698947+0000     8     255      1695      1440   719.945       836     1.31324     1.20766
2021-05-17T18:29:43.699025+0000     9     255      1906      1651   733.721       844      1.2533     1.21291
2021-05-17T18:29:44.699094+0000    10     255      2105      1850   739.944       796     1.27676     1.21619
2021-05-17T18:29:45.699178+0000    11     255      2321      2066   751.215       864     1.23206     1.22117
2021-05-17T18:29:46.699247+0000    12     255      2545      2290   763.275       896     1.14472     1.21473
2021-05-17T18:29:47.699362+0000    13     255      2754      2499   768.862       836     1.22646     1.21277
2021-05-17T18:29:48.699438+0000    14     255      2980      2725    778.51       904     1.13824     1.20949
2021-05-17T18:29:49.699519+0000    15     255      3216      2961   789.538       944     1.09266     1.20105
2021-05-17T18:29:50.699588+0000    16     255      3440      3185   796.188       896     1.14241     1.19347
2021-05-17T18:29:51.699666+0000    17     255      3664      3409   802.055       896     1.17402     1.19226
2021-05-17T18:29:52.699738+0000    18     255      3880      3625   805.493       864     1.16345     1.19116
2021-05-17T18:29:53.699809+0000    19     255      4096      3841   808.569       864     1.17323     1.18929
2021-05-17T18:29:54.699883+0000 min lat: 1.05825 max lat: 1.33219 avg lat: 1.18744
2021-05-17T18:29:54.699883+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T18:29:54.699883+0000    20     255      4321      4066   813.137       900     1.14266     1.18744
2021-05-17T18:29:55.700124+0000 Total time run:         20.0481
Total writes made:      4322
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     862.326
Stddev Bandwidth:       218.521
Max bandwidth (MB/sec): 944
Min bandwidth (MB/sec): 0
Average IOPS:           215
Stddev IOPS:            54.6303
Max IOPS:               236
Min IOPS:               0
Average Latency(s):     1.15217
Stddev Latency(s):      0.171269
Max latency(s):         1.33219
Min latency(s):         0.0242826

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:29:56,284817088-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 537403


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:29:56,289831386-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:30:19,588330121-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.32k objects, 17 GiB
    usage:   34 GiB used, 266 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:30:19,598365801-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:30:27,866099963-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.32k objects, 17 GiB
    usage:   34 GiB used, 266 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:30:27,875633760-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:30:36,163393785-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.32k objects, 17 GiB
    usage:   34 GiB used, 266 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:30:36,172314982-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:30:44,550010441-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.32k objects, 17 GiB
    usage:   34 GiB used, 266 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:30:44,559007149-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:30:52,872225992-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.32k objects, 17 GiB
    usage:   34 GiB used, 266 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:30:52,881492557-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:30:52,888541988-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:30:52,892564554-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:30:52,902527597-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=538781
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_4MB_seq.dat.2 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:30:52,909762416-04:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_4MB_seq.log.2
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:30:52,928337776-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:30:52,931915346-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '10a17d87-b324-4704-a516-b47604077e04', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Pd2TXk:/tmp/ceph-asok.Pd2TXk', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 10a17d87-b324-4704-a516-b47604077e04 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Pd2TXk:/tmp/ceph-asok.Pd2TXk -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T18:30:55.333+0000 7ff3ed5b1d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T18:30:55.337+0000 7ff3ed5b1d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T18:30:55.337+0000 7ff3ed5b1d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T18:30:55.353763+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T18:30:55.353763+0000     0       0         0         0         0         0           -           0
2021-05-17T18:30:56.354008+0000     1     255       895       640   2559.21      2560    0.319293      0.2938
2021-05-17T18:30:57.354116+0000     2     255      1700      1445    2889.4      3220    0.287931    0.310213
2021-05-17T18:30:58.354563+0000     3     255      2501      2246    2993.8      3204    0.298282    0.313375
2021-05-17T18:30:59.354691+0000     4     255      3338      3083   3082.24      3348     0.32471    0.309617
2021-05-17T18:31:00.354838+0000     5     255      4159      3904   3122.49      3284    0.316843    0.310189
2021-05-17T18:31:01.355259+0000 Total time run:       5.28546
Total reads made:     4322
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   3270.86
Average IOPS:         817
Stddev IOPS:          79.9887
Max IOPS:             837
Min IOPS:             640
Average Latency(s):   0.304568
Max latency(s):       0.350411
Min latency(s):       0.068982

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:31:01,999661303-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 538781


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:31:02,004391327-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:31:25,275125782-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.32k objects, 17 GiB
    usage:   34 GiB used, 266 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:31:25,284631356-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:31:33,628920604-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.32k objects, 17 GiB
    usage:   34 GiB used, 266 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:31:33,638329497-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:31:41,875825115-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.32k objects, 17 GiB
    usage:   34 GiB used, 266 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:31:41,885015567-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:31:50,030014920-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.32k objects, 17 GiB
    usage:   34 GiB used, 266 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:31:50,039949570-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:31:58,399609275-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.32k objects, 17 GiB
    usage:   34 GiB used, 266 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:31:58,408758981-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:31:58,416180421-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T14:31:58,418979218-04:00][RUNNING][ROUND 3/6/40] object_size=4MB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:31:58,422942642-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:31:58,432482070-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40330\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.391125\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 5c46af3e-98a1-487c-b28a-ad637d448f3a\nsetting min_mon_release = octopus\nepoch 0\nfsid 5c46af3e-98a1-487c-b28a-ad637d448f3a\nlast_changed 2021-05-17T11:32:26.287934-0700\ncreated 2021-05-17T11:32:26.287934-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40330/0,v1:10.10.1.2:40331/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.391125 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 d4850e89-935a-4b87-9286-f94ec4ef742f\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 150307ee-22f6-4cb5-aad3-190dec4e41f8\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 7b304ac9-dfce-4474-aaee-8d14c2cbac7b\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42330\n  w/ user/pass: admin / 9000a6b7-ef0e-477c-b31e-8d688ff7509b\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\nCEPH_DEV=1\n'
[1] 14:32:39 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40330
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.391125
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 5c46af3e-98a1-487c-b28a-ad637d448f3a
setting min_mon_release = octopus
epoch 0
fsid 5c46af3e-98a1-487c-b28a-ad637d448f3a
last_changed 2021-05-17T11:32:26.287934-0700
created 2021-05-17T11:32:26.287934-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40330/0,v1:10.10.1.2:40331/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.391125 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 d4850e89-935a-4b87-9286-f94ec4ef742f
0
start osd.0
add osd1 150307ee-22f6-4cb5-aad3-190dec4e41f8
1
start osd.1
add osd2 7b304ac9-dfce-4474-aaee-8d14c2cbac7b
2
start osd.2


restful urls: https://10.10.1.2:42330
  w/ user/pass: admin / 9000a6b7-ef0e-477c-b31e-8d688ff7509b


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T11:31:59.359-0700 7fdd8a0b21c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T11:31:59.359-0700 7fdd8a0b21c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T11:31:59.375-0700 7f252b8071c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T11:31:59.375-0700 7f252b8071c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40330,v1:10.10.1.2:40331] --print /tmp/ceph_monmap.391125 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.391125 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.391125 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42330 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.JtnlslCihk 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new d4850e89-935a-4b87-9286-f94ec4ef742f -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDDtqJgSXKZChAA0ozX2Rb3ilG7u1DlICq32A== --osd-uuid d4850e89-935a-4b87-9286-f94ec4ef742f 
2021-05-17T11:32:35.591-0700 7fa560d79f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T11:32:35.595-0700 7fa560d79f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T11:32:35.595-0700 7fa560d79f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T11:32:35.639-0700 7fa560d79f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 150307ee-22f6-4cb5-aad3-190dec4e41f8 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T11:32:35.931-0700 7f9e0eb58f00 -1 Falling back to public interface
2021-05-17T11:32:35.943-0700 7f9e0eb58f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDDtqJgWJqsNxAAlPxDKvMOiNg2TT3xR7t+/Q== --osd-uuid 150307ee-22f6-4cb5-aad3-190dec4e41f8 
2021-05-17T11:32:36.255-0700 7f67f042cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T11:32:36.255-0700 7f67f042cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T11:32:36.255-0700 7f67f042cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T11:32:36.291-0700 7f67f042cf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 7b304ac9-dfce-4474-aaee-8d14c2cbac7b -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T11:32:36.611-0700 7fad0367ff00 -1 Falling back to public interface
2021-05-17T11:32:36.623-0700 7fad0367ff00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDEtqJgQDmYJBAA0FjPnyWOc9HrFZHoUjo74w== --osd-uuid 7b304ac9-dfce-4474-aaee-8d14c2cbac7b 
2021-05-17T11:32:36.939-0700 7f13ddae9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T11:32:36.939-0700 7f13ddae9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T11:32:36.939-0700 7f13ddae9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T11:32:37.003-0700 7f13ddae9f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T11:32:37.423-0700 7fa12cfcef00 -1 Falling back to public interface
2021-05-17T11:32:37.439-0700 7fa12cfcef00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:32:39,367338006-04:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:32:39,372223061-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:32:39,417869408-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:32:39,421827403-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9ed17252-76b4-4011-b912-a76c5814f2eb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fZY5SC:/tmp/ceph-asok.fZY5SC', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9ed17252-76b4-4011-b912-a76c5814f2eb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fZY5SC:/tmp/ceph-asok.fZY5SC -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:32:42,774586604-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:32:42,778320767-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9ed17252-76b4-4011-b912-a76c5814f2eb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fZY5SC:/tmp/ceph-asok.fZY5SC', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9ed17252-76b4-4011-b912-a76c5814f2eb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fZY5SC:/tmp/ceph-asok.fZY5SC -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:32:46,023551830-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:32:46,027296584-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9ed17252-76b4-4011-b912-a76c5814f2eb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fZY5SC:/tmp/ceph-asok.fZY5SC', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9ed17252-76b4-4011-b912-a76c5814f2eb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fZY5SC:/tmp/ceph-asok.fZY5SC -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:32:49,379037916-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:32:49,382228458-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9ed17252-76b4-4011-b912-a76c5814f2eb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fZY5SC:/tmp/ceph-asok.fZY5SC', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9ed17252-76b4-4011-b912-a76c5814f2eb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fZY5SC:/tmp/ceph-asok.fZY5SC -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:32:56,049605719-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:32:56,053300870-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9ed17252-76b4-4011-b912-a76c5814f2eb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fZY5SC:/tmp/ceph-asok.fZY5SC', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9ed17252-76b4-4011-b912-a76c5814f2eb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fZY5SC:/tmp/ceph-asok.fZY5SC -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:32:59,469525815-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:32:59,472906976-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9ed17252-76b4-4011-b912-a76c5814f2eb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fZY5SC:/tmp/ceph-asok.fZY5SC', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9ed17252-76b4-4011-b912-a76c5814f2eb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fZY5SC:/tmp/ceph-asok.fZY5SC -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:33:02,823671218-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:33:02,826810425-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9ed17252-76b4-4011-b912-a76c5814f2eb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fZY5SC:/tmp/ceph-asok.fZY5SC', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9ed17252-76b4-4011-b912-a76c5814f2eb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fZY5SC:/tmp/ceph-asok.fZY5SC -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:33:07,190086207-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:33:07,193549052-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9ed17252-76b4-4011-b912-a76c5814f2eb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fZY5SC:/tmp/ceph-asok.fZY5SC', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9ed17252-76b4-4011-b912-a76c5814f2eb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fZY5SC:/tmp/ceph-asok.fZY5SC -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:33:11,402467307-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:33:11,406081705-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9ed17252-76b4-4011-b912-a76c5814f2eb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fZY5SC:/tmp/ceph-asok.fZY5SC', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9ed17252-76b4-4011-b912-a76c5814f2eb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fZY5SC:/tmp/ceph-asok.fZY5SC -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:33:15,123687130-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:33:15,127177867-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9ed17252-76b4-4011-b912-a76c5814f2eb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fZY5SC:/tmp/ceph-asok.fZY5SC', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9ed17252-76b4-4011-b912-a76c5814f2eb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fZY5SC:/tmp/ceph-asok.fZY5SC -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:33:19,314788695-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:33:19,318668191-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9ed17252-76b4-4011-b912-a76c5814f2eb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fZY5SC:/tmp/ceph-asok.fZY5SC', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9ed17252-76b4-4011-b912-a76c5814f2eb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fZY5SC:/tmp/ceph-asok.fZY5SC -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:33:22,597547771-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:33:22,601388145-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9ed17252-76b4-4011-b912-a76c5814f2eb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fZY5SC:/tmp/ceph-asok.fZY5SC', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9ed17252-76b4-4011-b912-a76c5814f2eb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fZY5SC:/tmp/ceph-asok.fZY5SC -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  155 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:33:26,013212586-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:33:49,383501250-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (5s)
      [============................] (remaining: 6s)
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:33:57,706145357-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:34:06,050679509-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:34:14,404253859-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:34:22,809019191-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:34:31,165201367-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:34:39,528401194-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:34:39,537608828-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:34:47,858184181-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:34:47,867212178-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:34:56,273427418-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:34:56,282068158-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:35:04,926770970-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:35:04,935855864-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:35:13,442078075-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:35:13,451813171-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:35:13,458503778-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:35:13,462446815-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:35:13,471998175-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=545424
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_4MB_write.dat.3 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:35:13,478234639-04:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_4MB_write.log.3
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:35:13,498397771-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:35:13,502128870-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9ed17252-76b4-4011-b912-a76c5814f2eb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fZY5SC:/tmp/ceph-asok.fZY5SC', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9ed17252-76b4-4011-b912-a76c5814f2eb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fZY5SC:/tmp/ceph-asok.fZY5SC -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T18:35:15.886+0000 7f46e8d47d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T18:35:15.890+0000 7f46e8d47d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T18:35:15.890+0000 7f46e8d47d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T18:35:15.903463+0000 Maintaining 256 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 20 seconds or 0 objects
2021-05-17T18:35:15.903475+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-17T18:35:16.124648+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T18:35:16.124648+0000     0       0         0         0         0         0           -           0
2021-05-17T18:35:17.124739+0000     1     211       211         0         0         0           -           0
2021-05-17T18:35:18.124804+0000     2     255       426       171   341.982       342     1.22002     1.22744
2021-05-17T18:35:19.124906+0000     3     255       639       384   511.965       852     1.20735     1.21594
2021-05-17T18:35:20.124982+0000     4     255       842       587   586.958       812     1.23152      1.2141
2021-05-17T18:35:21.125055+0000     5     255      1030       775   619.956       752     1.35276      1.2339
2021-05-17T18:35:22.125124+0000     6     255      1212       957   637.955       728     1.39179     1.25523
2021-05-17T18:35:23.125224+0000     7     255      1403      1148   655.951       764     1.40503     1.27917
2021-05-17T18:35:24.125314+0000     8     255      1616      1361   680.448       852     1.25693     1.28225
2021-05-17T18:35:25.125398+0000     9     255      1819      1564   695.057       812     1.26116     1.27278
2021-05-17T18:35:26.125498+0000    10     255      1998      1743   697.144       716     1.40912     1.27887
2021-05-17T18:35:27.125616+0000    11     255      2215      1960   712.668       868      1.2327      1.2835
2021-05-17T18:35:28.125693+0000    12     255      2423      2168   722.607       832     1.22217     1.27628
2021-05-17T18:35:29.125765+0000    13     255      2631      2376   731.017       832     1.20299     1.27203
2021-05-17T18:35:30.125837+0000    14     255      2835      2580   737.083       816     1.23218     1.27001
2021-05-17T18:35:31.125939+0000    15     255      3044      2789   743.672       836     1.22588     1.26905
2021-05-17T18:35:32.126009+0000    16     255      3257      3002   750.439       852     1.18031     1.26341
2021-05-17T18:35:33.126078+0000    17     255      3466      3211   755.468       836     1.23442     1.25928
2021-05-17T18:35:34.126148+0000    18     255      3674      3419   759.717       832     1.25286     1.25761
2021-05-17T18:35:35.126261+0000    19     255      3892      3637   765.621       872     1.20291     1.25584
2021-05-17T18:35:36.126334+0000 min lat: 1.14688 max lat: 1.43684 avg lat: 1.2533
2021-05-17T18:35:36.126334+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T18:35:36.126334+0000    20     255      4096      3841   768.137       816     1.21334      1.2533
2021-05-17T18:35:37.126484+0000 Total time run:         20.0227
Total writes made:      4097
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     818.473
Stddev Bandwidth:       210.534
Max bandwidth (MB/sec): 872
Min bandwidth (MB/sec): 0
Average IOPS:           204
Stddev IOPS:            52.6846
Max IOPS:               218
Min IOPS:               0
Average Latency(s):     1.2142
Stddev Latency(s):      0.18655
Max latency(s):         1.43684
Min latency(s):         0.0264207

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:35:37,816455273-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 545424


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:35:37,821722947-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:36:01,091985795-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.10k objects, 16 GiB
    usage:   32 GiB used, 268 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:36:01,100828996-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:36:09,222955037-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.10k objects, 16 GiB
    usage:   32 GiB used, 268 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:36:09,231938792-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:36:17,498099314-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.10k objects, 16 GiB
    usage:   32 GiB used, 268 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:36:17,507303012-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:36:25,879988922-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.10k objects, 16 GiB
    usage:   32 GiB used, 268 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:36:25,888190778-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:36:34,333828332-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.10k objects, 16 GiB
    usage:   32 GiB used, 268 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:36:34,343058630-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:36:34,350342041-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:36:34,354299284-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:36:34,363262549-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=546840
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_4MB_seq.dat.3 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:36:34,369733984-04:00] INFO: > Run rados bench[0m
# ./bench-rados:200 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_4MB_seq.log.3
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:36:34,389326225-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:36:34,392567834-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9ed17252-76b4-4011-b912-a76c5814f2eb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fZY5SC:/tmp/ceph-asok.fZY5SC', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9ed17252-76b4-4011-b912-a76c5814f2eb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.fZY5SC:/tmp/ceph-asok.fZY5SC -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T18:36:36.822+0000 7f18e531cd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T18:36:36.826+0000 7f18e531cd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T18:36:36.826+0000 7f18e531cd00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T18:36:36.838878+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T18:36:36.838878+0000     0       0         0         0         0         0           -           0
2021-05-17T18:36:37.838979+0000     1     255       873       618   2471.57      2472    0.350907    0.299353
2021-05-17T18:36:38.839080+0000     2     255      1621      1366   2731.63      2992    0.320231    0.326109
2021-05-17T18:36:39.839359+0000     3     255      2391      2136   2847.47      3080     0.29209    0.329811
2021-05-17T18:36:40.839437+0000     4     255      3213      2958   2957.53      3288    0.304124    0.324252
2021-05-17T18:36:41.839506+0000     5     255      4058      3803   3041.97      3380    0.305227    0.319474
2021-05-17T18:36:42.839902+0000 Total time run:       5.11304
Total reads made:     4097
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   3205.14
Average IOPS:         801
Stddev IOPS:          88.7063
Max IOPS:             845
Min IOPS:             618
Average Latency(s):   0.310842
Max latency(s):       0.375855
Min latency(s):       0.0659455

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:36:43,487182206-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 546840


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:36:43,492271705-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:37:06,783064903-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.10k objects, 16 GiB
    usage:   32 GiB used, 268 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:37:06,792409766-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:37:15,177797852-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.10k objects, 16 GiB
    usage:   32 GiB used, 268 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:37:15,186577143-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:37:23,506617782-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.10k objects, 16 GiB
    usage:   32 GiB used, 268 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:37:23,515925305-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:37:31,780548360-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.10k objects, 16 GiB
    usage:   32 GiB used, 268 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:37:31,790032223-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:37:40,080196672-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.10k objects, 16 GiB
    usage:   32 GiB used, 268 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:37:40,089522869-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:37:40,096414123-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T14:37:40,099127049-04:00][RUNNING][ROUND 4/6/40] object_size=4MB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:37:40,103576397-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:37:40,112868991-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40740\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.392253\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 8c7e8601-95f8-4ef1-aa24-484032078d56\nsetting min_mon_release = octopus\nepoch 0\nfsid 8c7e8601-95f8-4ef1-aa24-484032078d56\nlast_changed 2021-05-17T11:38:08.638686-0700\ncreated 2021-05-17T11:38:08.638686-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40740/0,v1:10.10.1.2:40741/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.392253 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 ccfef894-6b38-4897-a284-521189b678f5\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 8d029319-7720-4c05-9cba-88a5de202281\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 307bcc10-2153-49a7-ad84-82bcab3cab7e\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42740\n  w/ user/pass: admin / 02415977-5791-48f4-b981-3a85c353116d\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 14:38:21 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40740
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.392253
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 8c7e8601-95f8-4ef1-aa24-484032078d56
setting min_mon_release = octopus
epoch 0
fsid 8c7e8601-95f8-4ef1-aa24-484032078d56
last_changed 2021-05-17T11:38:08.638686-0700
created 2021-05-17T11:38:08.638686-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40740/0,v1:10.10.1.2:40741/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.392253 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 ccfef894-6b38-4897-a284-521189b678f5
0
start osd.0
add osd1 8d029319-7720-4c05-9cba-88a5de202281
1
start osd.1
add osd2 307bcc10-2153-49a7-ad84-82bcab3cab7e
2
start osd.2


restful urls: https://10.10.1.2:42740
  w/ user/pass: admin / 02415977-5791-48f4-b981-3a85c353116d


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T11:37:41.042-0700 7fdfff2c51c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T11:37:41.042-0700 7fdfff2c51c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T11:37:41.062-0700 7f96758371c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T11:37:41.062-0700 7f96758371c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40740,v1:10.10.1.2:40741] --print /tmp/ceph_monmap.392253 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.392253 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.392253 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42740 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.WFNAVKxHZW 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new ccfef894-6b38-4897-a284-521189b678f5 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAZuKJgqRDrKhAAbylIHuL0hFW3OSkUlPrrNg== --osd-uuid ccfef894-6b38-4897-a284-521189b678f5 
2021-05-17T11:38:18.070-0700 7fe7267a1f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T11:38:18.070-0700 7fe7267a1f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T11:38:18.070-0700 7fe7267a1f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T11:38:18.150-0700 7fe7267a1f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 8d029319-7720-4c05-9cba-88a5de202281 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T11:38:18.410-0700 7f4e0b2dbf00 -1 Falling back to public interface
2021-05-17T11:38:18.422-0700 7f4e0b2dbf00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAauKJgXkBvGBAAocNl8sC01H25pOdO+6fMRQ== --osd-uuid 8d029319-7720-4c05-9cba-88a5de202281 
2021-05-17T11:38:18.750-0700 7f6f3ebfdf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T11:38:18.750-0700 7f6f3ebfdf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T11:38:18.750-0700 7f6f3ebfdf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T11:38:18.794-0700 7f6f3ebfdf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 307bcc10-2153-49a7-ad84-82bcab3cab7e -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T11:38:19.162-0700 7f3c76454f00 -1 Falling back to public interface
2021-05-17T11:38:19.178-0700 7f3c76454f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAbuKJgvmzMCRAAleVVDCHFeSXkVwLbfT5fXw== --osd-uuid 307bcc10-2153-49a7-ad84-82bcab3cab7e 
2021-05-17T11:38:19.494-0700 7fedf2824f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T11:38:19.494-0700 7fedf2824f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T11:38:19.494-0700 7fedf2824f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T11:38:19.566-0700 7fedf2824f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T11:38:19.942-0700 7fb42d19bf00 -1 Falling back to public interface
2021-05-17T11:38:19.958-0700 7fb42d19bf00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:38:21,927237367-04:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:38:21,931743801-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:38:21,974357604-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:38:21,977883577-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '80ce042c-c3e6-4fdd-85ea-61c3df78b4eb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rt8zXj:/tmp/ceph-asok.rt8zXj', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 80ce042c-c3e6-4fdd-85ea-61c3df78b4eb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rt8zXj:/tmp/ceph-asok.rt8zXj -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:38:25,278694198-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:38:25,282141723-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '80ce042c-c3e6-4fdd-85ea-61c3df78b4eb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rt8zXj:/tmp/ceph-asok.rt8zXj', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 80ce042c-c3e6-4fdd-85ea-61c3df78b4eb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rt8zXj:/tmp/ceph-asok.rt8zXj -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:38:28,610074660-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:38:28,613801931-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '80ce042c-c3e6-4fdd-85ea-61c3df78b4eb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rt8zXj:/tmp/ceph-asok.rt8zXj', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 80ce042c-c3e6-4fdd-85ea-61c3df78b4eb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rt8zXj:/tmp/ceph-asok.rt8zXj -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:38:31,991827429-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:38:31,995531958-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '80ce042c-c3e6-4fdd-85ea-61c3df78b4eb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rt8zXj:/tmp/ceph-asok.rt8zXj', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 80ce042c-c3e6-4fdd-85ea-61c3df78b4eb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rt8zXj:/tmp/ceph-asok.rt8zXj -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:38:38,865733310-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:38:38,869433771-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '80ce042c-c3e6-4fdd-85ea-61c3df78b4eb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rt8zXj:/tmp/ceph-asok.rt8zXj', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 80ce042c-c3e6-4fdd-85ea-61c3df78b4eb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rt8zXj:/tmp/ceph-asok.rt8zXj -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:38:42,912488954-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:38:42,915682763-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '80ce042c-c3e6-4fdd-85ea-61c3df78b4eb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rt8zXj:/tmp/ceph-asok.rt8zXj', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 80ce042c-c3e6-4fdd-85ea-61c3df78b4eb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rt8zXj:/tmp/ceph-asok.rt8zXj -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:38:46,821856277-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:38:46,825456940-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '80ce042c-c3e6-4fdd-85ea-61c3df78b4eb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rt8zXj:/tmp/ceph-asok.rt8zXj', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 80ce042c-c3e6-4fdd-85ea-61c3df78b4eb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rt8zXj:/tmp/ceph-asok.rt8zXj -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:38:51,333761863-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:38:51,337439380-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '80ce042c-c3e6-4fdd-85ea-61c3df78b4eb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rt8zXj:/tmp/ceph-asok.rt8zXj', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 80ce042c-c3e6-4fdd-85ea-61c3df78b4eb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rt8zXj:/tmp/ceph-asok.rt8zXj -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:38:55,689832800-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:38:55,693800052-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '80ce042c-c3e6-4fdd-85ea-61c3df78b4eb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rt8zXj:/tmp/ceph-asok.rt8zXj', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 80ce042c-c3e6-4fdd-85ea-61c3df78b4eb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rt8zXj:/tmp/ceph-asok.rt8zXj -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:38:59,955190330-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:38:59,958769893-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '80ce042c-c3e6-4fdd-85ea-61c3df78b4eb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rt8zXj:/tmp/ceph-asok.rt8zXj', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 80ce042c-c3e6-4fdd-85ea-61c3df78b4eb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rt8zXj:/tmp/ceph-asok.rt8zXj -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:39:03,378206778-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:39:03,382006305-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '80ce042c-c3e6-4fdd-85ea-61c3df78b4eb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rt8zXj:/tmp/ceph-asok.rt8zXj', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 80ce042c-c3e6-4fdd-85ea-61c3df78b4eb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rt8zXj:/tmp/ceph-asok.rt8zXj -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:39:06,700494945-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:39:06,703998736-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '80ce042c-c3e6-4fdd-85ea-61c3df78b4eb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rt8zXj:/tmp/ceph-asok.rt8zXj', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 80ce042c-c3e6-4fdd-85ea-61c3df78b4eb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rt8zXj:/tmp/ceph-asok.rt8zXj -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  156 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:39:10,099831988-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:39:33,380354108-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (9s)
      [=========================...] 
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:39:41,509742493-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:39:49,763579822-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:39:58,119456484-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:40:06,407662194-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:40:14,693800758-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:40:23,040368225-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:40:23,049595147-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:40:31,352780775-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:40:31,361749020-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:40:39,699264029-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:40:39,708811161-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:40:48,079377799-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:40:48,088882533-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:40:56,346999708-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:40:56,356023748-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:40:56,362771843-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:40:56,366695333-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:40:56,375742517-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=553428
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_4MB_write.dat.4 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:40:56,382547259-04:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_4MB_write.log.4
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:40:56,402744176-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:40:56,406019177-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '80ce042c-c3e6-4fdd-85ea-61c3df78b4eb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rt8zXj:/tmp/ceph-asok.rt8zXj', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 80ce042c-c3e6-4fdd-85ea-61c3df78b4eb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rt8zXj:/tmp/ceph-asok.rt8zXj -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T18:40:58.819+0000 7f222ac38d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T18:40:58.827+0000 7f222ac38d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T18:40:58.827+0000 7f222ac38d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T18:40:58.836095+0000 Maintaining 256 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 20 seconds or 0 objects
2021-05-17T18:40:58.836108+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-17T18:40:59.057976+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T18:40:59.057976+0000     0       0         0         0         0         0           -           0
2021-05-17T18:41:00.058081+0000     1     255       257         2   7.99973         8    0.999111    0.994419
2021-05-17T18:41:01.058156+0000     2     255       508       253   505.973      1004     1.02312     1.01326
2021-05-17T18:41:02.058232+0000     3     255       758       503   670.626      1000      1.0108     1.01876
2021-05-17T18:41:03.058346+0000     4     255      1000       745   744.945       968     1.05555     1.01875
2021-05-17T18:41:04.058450+0000     5     255      1218       963   770.338       872     1.20033     1.03916
2021-05-17T18:41:05.058588+0000     6     255      1420      1165   776.597       808     1.30392     1.06539
2021-05-17T18:41:06.058709+0000     7     255      1654      1399   799.353       936     1.14287     1.09255
2021-05-17T18:41:07.058815+0000     8     255      1909      1654   826.921      1020     1.00576     1.08699
2021-05-17T18:41:08.058898+0000     9     255      2157      1902   845.254       992     1.03325     1.07932
2021-05-17T18:41:09.058972+0000    10     255      2408      2153    861.12      1004     1.01591     1.07197
2021-05-17T18:41:10.059041+0000    11     255      2659      2404   874.103      1004     1.01377     1.06709
2021-05-17T18:41:11.059151+0000    12     255      2911      2656   885.252      1008     1.00991     1.06216
2021-05-17T18:41:12.059233+0000    13     255      3158      2903   893.149       988     1.01631     1.05765
2021-05-17T18:41:13.059303+0000    14     255      3416      3161   903.062      1032    0.992353     1.05498
2021-05-17T18:41:14.059374+0000    15     255      3674      3419   911.653      1032     0.99416     1.05026
2021-05-17T18:41:15.059476+0000    16     255      3916      3661   915.168       968     1.04789     1.04861
2021-05-17T18:41:16.059545+0000    17     255      4165      3910   919.919       996     1.02832     1.04765
2021-05-17T18:41:17.059622+0000    18     255      4423      4168   926.141      1032    0.992319     1.04559
2021-05-17T18:41:18.059698+0000    19     255      4678      4423   931.077      1020    0.999043     1.04268
2021-05-17T18:41:19.059806+0000 min lat: 0.941686 max lat: 1.31499 avg lat: 1.03894
2021-05-17T18:41:19.059806+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T18:41:19.059806+0000    20     255      4947      4692   938.318      1076    0.949031     1.03894
2021-05-17T18:41:20.059964+0000 Total time run:         20.0263
Total writes made:      4948
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     988.302
Stddev Bandwidth:       226.826
Max bandwidth (MB/sec): 1076
Min bandwidth (MB/sec): 8
Average IOPS:           247
Stddev IOPS:            56.7064
Max IOPS:               269
Min IOPS:               2
Average Latency(s):     1.0101
Stddev Latency(s):      0.152031
Max latency(s):         1.31499
Min latency(s):         0.0232796

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:41:20,687777630-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 553428


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:41:20,692555565-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:41:43,848198881-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.95k objects, 19 GiB
    usage:   39 GiB used, 261 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:41:43,856968093-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:41:52,261727791-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.95k objects, 19 GiB
    usage:   39 GiB used, 261 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:41:52,270733508-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:42:00,598939638-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.95k objects, 19 GiB
    usage:   39 GiB used, 261 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:42:00,608521476-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:42:08,903817947-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.95k objects, 19 GiB
    usage:   39 GiB used, 261 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:42:08,913345083-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:42:17,249803467-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.95k objects, 19 GiB
    usage:   39 GiB used, 261 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:42:17,258904042-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:42:17,265826925-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:42:17,270339191-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:42:17,279868199-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=554813
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_4MB_seq.dat.4 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:42:17,286749475-04:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_4MB_seq.log.4
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:42:17,307198165-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:42:17,310629200-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '80ce042c-c3e6-4fdd-85ea-61c3df78b4eb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rt8zXj:/tmp/ceph-asok.rt8zXj', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 80ce042c-c3e6-4fdd-85ea-61c3df78b4eb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rt8zXj:/tmp/ceph-asok.rt8zXj -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T18:42:19.787+0000 7f30b45abd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T18:42:19.791+0000 7f30b45abd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T18:42:19.791+0000 7f30b45abd00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T18:42:19.802393+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T18:42:19.802393+0000     0       0         0         0         0         0           -           0
2021-05-17T18:42:20.802481+0000     1     255       864       609   2435.62      2436    0.327236    0.303229
2021-05-17T18:42:21.802596+0000     2     255      1654      1399   2797.62      3160    0.312332    0.316084
2021-05-17T18:42:22.802701+0000     3     255      2422      2167   2888.97      3072    0.337935    0.321039
2021-05-17T18:42:23.802885+0000     4     255      3201      2946   2945.59      3116    0.315368    0.323494
2021-05-17T18:42:24.803667+0000     5     255      4056      3801   3039.98      3420    0.284002    0.319541
2021-05-17T18:42:25.803782+0000     6     255      4929      4674   3115.24      3492    0.308354    0.313857
2021-05-17T18:42:26.803955+0000 Total time run:       6.09133
Total reads made:     4948
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   3249.21
Average IOPS:         812
Stddev IOPS:          93.5457
Max IOPS:             873
Min IOPS:             609
Average Latency(s):   0.307288
Max latency(s):       0.352478
Min latency(s):       0.0661221

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:42:27,435874989-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 554813


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:42:27,440567995-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:42:50,779317582-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.95k objects, 19 GiB
    usage:   39 GiB used, 261 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:42:50,788046649-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:42:59,125566220-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.95k objects, 19 GiB
    usage:   39 GiB used, 261 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:42:59,134718622-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:43:07,520264912-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.95k objects, 19 GiB
    usage:   39 GiB used, 261 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:43:07,529551646-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:43:15,791672314-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.95k objects, 19 GiB
    usage:   39 GiB used, 261 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:43:15,800513001-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:43:24,154143114-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.95k objects, 19 GiB
    usage:   39 GiB used, 261 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:43:24,162532221-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:43:24,169361449-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T14:43:24,171968156-04:00][RUNNING][ROUND 5/6/40] object_size=4MB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:43:24,175769907-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:43:24,185576417-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40192\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.393376\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 54a1ca04-1552-44fd-bbe0-a55247927102\nsetting min_mon_release = octopus\nepoch 0\nfsid 54a1ca04-1552-44fd-bbe0-a55247927102\nlast_changed 2021-05-17T11:43:59.584220-0700\ncreated 2021-05-17T11:43:59.584220-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40192/0,v1:10.10.1.2:40193/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.393376 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 44b8350f-14bc-4226-bda0-f5b727c80f96\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 6e379b4f-f248-4ac6-bfeb-a8422066208e\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 9b2c037d-69e1-4bfc-b42e-5fe9beeffb8c\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42192\n  w/ user/pass: admin / 528fec17-59c9-4af4-bc4b-dda31a1f687e\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\nCEPH_DEV=1\n'
[1] 14:44:12 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40192
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.393376
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 54a1ca04-1552-44fd-bbe0-a55247927102
setting min_mon_release = octopus
epoch 0
fsid 54a1ca04-1552-44fd-bbe0-a55247927102
last_changed 2021-05-17T11:43:59.584220-0700
created 2021-05-17T11:43:59.584220-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40192/0,v1:10.10.1.2:40193/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.393376 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 44b8350f-14bc-4226-bda0-f5b727c80f96
0
start osd.0
add osd1 6e379b4f-f248-4ac6-bfeb-a8422066208e
1
start osd.1
add osd2 9b2c037d-69e1-4bfc-b42e-5fe9beeffb8c
2
start osd.2


restful urls: https://10.10.1.2:42192
  w/ user/pass: admin / 528fec17-59c9-4af4-bc4b-dda31a1f687e


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T11:43:25.097-0700 7f658d24c1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T11:43:25.097-0700 7f658d24c1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T11:43:25.113-0700 7f7a472fb1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T11:43:25.113-0700 7f7a472fb1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40192,v1:10.10.1.2:40193] --print /tmp/ceph_monmap.393376 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.393376 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.393376 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42192 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.SEBF9a94NG 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 44b8350f-14bc-4226-bda0-f5b727c80f96 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQB4uaJg4GGtIBAAUtUkocTSD91XmIDWGJVspA== --osd-uuid 44b8350f-14bc-4226-bda0-f5b727c80f96 
2021-05-17T11:44:08.881-0700 7f2962802f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T11:44:08.881-0700 7f2962802f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T11:44:08.881-0700 7f2962802f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T11:44:08.925-0700 7f2962802f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 6e379b4f-f248-4ac6-bfeb-a8422066208e -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T11:44:09.213-0700 7f14c0f74f00 -1 Falling back to public interface
2021-05-17T11:44:09.225-0700 7f14c0f74f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQB5uaJg1xq9DBAA+AWUV70ENcAaw75l0IMgBQ== --osd-uuid 6e379b4f-f248-4ac6-bfeb-a8422066208e 
2021-05-17T11:44:09.549-0700 7f25713edf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T11:44:09.553-0700 7f25713edf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T11:44:09.553-0700 7f25713edf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T11:44:09.597-0700 7f25713edf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 9b2c037d-69e1-4bfc-b42e-5fe9beeffb8c -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T11:44:09.997-0700 7f33a0d3bf00 -1 Falling back to public interface
2021-05-17T11:44:10.013-0700 7f33a0d3bf00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQB5uaJgmQKOOxAA8KIclVbIPBW4BibtcOUWRw== --osd-uuid 9b2c037d-69e1-4bfc-b42e-5fe9beeffb8c 
2021-05-17T11:44:10.353-0700 7f52e9999f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T11:44:10.353-0700 7f52e9999f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T11:44:10.353-0700 7f52e9999f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T11:44:10.417-0700 7f52e9999f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T11:44:10.701-0700 7f6edbb20f00 -1 Falling back to public interface
2021-05-17T11:44:10.717-0700 7f6edbb20f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:44:12,763383485-04:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:44:12,767890649-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:44:12,809974839-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:44:12,813048912-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '49728a97-4394-4b9b-83b1-a10e7ef3383f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rHtT50:/tmp/ceph-asok.rHtT50', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 49728a97-4394-4b9b-83b1-a10e7ef3383f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rHtT50:/tmp/ceph-asok.rHtT50 -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:44:16,153369241-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:44:16,157329670-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '49728a97-4394-4b9b-83b1-a10e7ef3383f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rHtT50:/tmp/ceph-asok.rHtT50', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 49728a97-4394-4b9b-83b1-a10e7ef3383f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rHtT50:/tmp/ceph-asok.rHtT50 -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:44:19,624375056-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:44:19,627975318-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '49728a97-4394-4b9b-83b1-a10e7ef3383f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rHtT50:/tmp/ceph-asok.rHtT50', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 49728a97-4394-4b9b-83b1-a10e7ef3383f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rHtT50:/tmp/ceph-asok.rHtT50 -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:44:22,838388508-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:44:22,842253266-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '49728a97-4394-4b9b-83b1-a10e7ef3383f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rHtT50:/tmp/ceph-asok.rHtT50', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 49728a97-4394-4b9b-83b1-a10e7ef3383f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rHtT50:/tmp/ceph-asok.rHtT50 -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:44:29,441425948-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:44:29,445060064-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '49728a97-4394-4b9b-83b1-a10e7ef3383f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rHtT50:/tmp/ceph-asok.rHtT50', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 49728a97-4394-4b9b-83b1-a10e7ef3383f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rHtT50:/tmp/ceph-asok.rHtT50 -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:44:33,876617803-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:44:33,880156969-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '49728a97-4394-4b9b-83b1-a10e7ef3383f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rHtT50:/tmp/ceph-asok.rHtT50', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 49728a97-4394-4b9b-83b1-a10e7ef3383f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rHtT50:/tmp/ceph-asok.rHtT50 -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:44:37,317471288-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:44:37,321313764-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '49728a97-4394-4b9b-83b1-a10e7ef3383f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rHtT50:/tmp/ceph-asok.rHtT50', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 49728a97-4394-4b9b-83b1-a10e7ef3383f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rHtT50:/tmp/ceph-asok.rHtT50 -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:44:41,656383700-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:44:41,659727771-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '49728a97-4394-4b9b-83b1-a10e7ef3383f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rHtT50:/tmp/ceph-asok.rHtT50', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 49728a97-4394-4b9b-83b1-a10e7ef3383f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rHtT50:/tmp/ceph-asok.rHtT50 -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:44:45,328026132-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:44:45,331478256-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '49728a97-4394-4b9b-83b1-a10e7ef3383f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rHtT50:/tmp/ceph-asok.rHtT50', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 49728a97-4394-4b9b-83b1-a10e7ef3383f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rHtT50:/tmp/ceph-asok.rHtT50 -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:44:49,749880917-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:44:49,753792333-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '49728a97-4394-4b9b-83b1-a10e7ef3383f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rHtT50:/tmp/ceph-asok.rHtT50', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 49728a97-4394-4b9b-83b1-a10e7ef3383f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rHtT50:/tmp/ceph-asok.rHtT50 -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:44:53,363894118-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:44:53,367629413-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '49728a97-4394-4b9b-83b1-a10e7ef3383f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rHtT50:/tmp/ceph-asok.rHtT50', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 49728a97-4394-4b9b-83b1-a10e7ef3383f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rHtT50:/tmp/ceph-asok.rHtT50 -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:44:56,872726236-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:44:56,875909455-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '49728a97-4394-4b9b-83b1-a10e7ef3383f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rHtT50:/tmp/ceph-asok.rHtT50', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 49728a97-4394-4b9b-83b1-a10e7ef3383f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rHtT50:/tmp/ceph-asok.rHtT50 -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   47 KiB   0 B   0 B   0 B  100 GiB     0  0.95   70      up          osd.2  
                       TOTAL  300 GiB  151 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.95/1.03  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:45:00,116319742-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:45:23,394370338-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (4s)
      [===================.........] (remaining: 2s)
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:45:31,694359939-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:45:40,101631112-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:45:48,495328341-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:45:56,686804200-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:46:05,139694775-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:46:13,524752290-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   233 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:46:13,533979981-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:46:21,979891510-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:46:21,989546874-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:46:30,327068811-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:46:30,336386291-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:46:38,739616930-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:46:38,749655844-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:46:47,280832175-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:46:47,289894756-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:46:47,297277812-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:46:47,301618465-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:46:47,310288128-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=561425
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_4MB_write.dat.5 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:46:47,317541401-04:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_4MB_write.log.5
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:46:47,337806391-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:46:47,341363131-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '49728a97-4394-4b9b-83b1-a10e7ef3383f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rHtT50:/tmp/ceph-asok.rHtT50', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 49728a97-4394-4b9b-83b1-a10e7ef3383f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rHtT50:/tmp/ceph-asok.rHtT50 -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T18:46:49.988+0000 7f9502635d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T18:46:49.992+0000 7f9502635d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T18:46:49.992+0000 7f9502635d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T18:46:50.007551+0000 Maintaining 256 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 20 seconds or 0 objects
2021-05-17T18:46:50.007567+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-17T18:46:50.229585+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T18:46:50.229585+0000     0       0         0         0         0         0           -           0
2021-05-17T18:46:51.229715+0000     1     223       223         0         0         0           -           0
2021-05-17T18:46:52.229821+0000     2     255       451       196   391.966       392     1.14285     1.15657
2021-05-17T18:46:53.229896+0000     3     255       686       431   574.619       940     1.08378     1.13677
2021-05-17T18:46:54.229976+0000     4     255       901       646   645.947       860     1.17478     1.12915
2021-05-17T18:46:55.230079+0000     5     255      1113       858   686.341       848     1.22477     1.15477
2021-05-17T18:46:56.230147+0000     6     255      1312      1057   704.608       796     1.22761     1.17214
2021-05-17T18:46:57.230215+0000     7     255      1534      1279   730.798       888     1.16023     1.17808
2021-05-17T18:46:58.230288+0000     8     255      1739      1484   741.941       820     1.20625     1.17554
2021-05-17T18:46:59.230409+0000     9     255      1949      1694   752.825       840      1.2513     1.18598
2021-05-17T18:47:00.230481+0000    10     255      2168      1913   765.136       876     1.20857      1.1884
2021-05-17T18:47:01.230554+0000    11     255      2375      2120   770.846       828     1.20327     1.18819
2021-05-17T18:47:02.230624+0000    12     255      2577      2322   773.937       808      1.2819     1.19599
2021-05-17T18:47:03.230723+0000    13     255      2796      2541   781.782       876     1.18217     1.19476
2021-05-17T18:47:04.230796+0000    14     255      3012      2757    787.65       864      1.2056     1.19427
2021-05-17T18:47:05.230871+0000    15     255      3222      2967   791.136       840     1.20752     1.19394
2021-05-17T18:47:06.230940+0000    16     255      3456      3201   800.185       936     1.10623      1.1912
2021-05-17T18:47:07.231038+0000    17     255      3670      3415   803.464       856     1.18301     1.18919
2021-05-17T18:47:08.231109+0000    18     255      3882      3627   805.935       848     1.21255     1.19069
2021-05-17T18:47:09.231194+0000    19     255      4113      3858   812.145       924     1.10615     1.18906
2021-05-17T18:47:10.231271+0000 min lat: 1.05216 max lat: 1.34962 avg lat: 1.18261
2021-05-17T18:47:10.231271+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T18:47:10.231271+0000    20     255      4348      4093   818.534       940     1.08411     1.18261
2021-05-17T18:47:11.231690+0000 Total time run:         20.0425
Total writes made:      4349
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     867.956
Stddev Bandwidth:       219.894
Max bandwidth (MB/sec): 940
Min bandwidth (MB/sec): 0
Average IOPS:           216
Stddev IOPS:            54.9736
Max IOPS:               235
Min IOPS:               0
Average Latency(s):     1.14558
Stddev Latency(s):      0.175147
Max latency(s):         1.34962
Min latency(s):         0.0206607

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:47:11,870142691-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 561425


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:47:11,874874728-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:47:35,124528612-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.35k objects, 17 GiB
    usage:   34 GiB used, 266 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:47:35,133356212-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:47:43,404609084-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.35k objects, 17 GiB
    usage:   34 GiB used, 266 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:47:43,414342155-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:47:51,717102784-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.35k objects, 17 GiB
    usage:   34 GiB used, 266 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:47:51,726519751-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:48:00,089807199-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.35k objects, 17 GiB
    usage:   34 GiB used, 266 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:48:00,099462173-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:48:08,397017316-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.35k objects, 17 GiB
    usage:   34 GiB used, 266 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:48:08,405957347-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:48:08,413227030-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:48:08,417663052-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:48:08,427769944-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=562813
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_4MB_seq.dat.5 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:48:08,434749623-04:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_4MB_seq.log.5
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:48:08,453863452-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:48:08,457062601-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '49728a97-4394-4b9b-83b1-a10e7ef3383f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rHtT50:/tmp/ceph-asok.rHtT50', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 49728a97-4394-4b9b-83b1-a10e7ef3383f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.rHtT50:/tmp/ceph-asok.rHtT50 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T18:48:10.928+0000 7fde7a8a2d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T18:48:10.936+0000 7fde7a8a2d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T18:48:10.936+0000 7fde7a8a2d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T18:48:10.950791+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T18:48:10.950791+0000     0       0         0         0         0         0           -           0
2021-05-17T18:48:11.950899+0000     1     255      1044       789   3155.48      3156    0.268707    0.253545
2021-05-17T18:48:12.951168+0000     2     255      1881      1626   3251.29      3348    0.300952    0.278051
2021-05-17T18:48:13.951238+0000     3     255      2644      2389    3184.8      3052    0.307167    0.295635
2021-05-17T18:48:14.951622+0000     4     255      3406      3151    3150.3      3048    0.295092    0.305674
2021-05-17T18:48:15.951694+0000     5     255      4166      3911    3128.2      3040     0.31731    0.310959
2021-05-17T18:48:16.951827+0000 Total time run:       5.33422
Total reads made:     4349
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   3261.21
Average IOPS:         815
Stddev IOPS:          32.8588
Max IOPS:             837
Min IOPS:             760
Average Latency(s):   0.306605
Max latency(s):       0.383348
Min latency(s):       0.0726252

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:48:17,673751671-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 562813


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:48:17,678826913-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:48:40,996004538-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.35k objects, 17 GiB
    usage:   34 GiB used, 266 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:48:41,005414170-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:48:49,339760962-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.35k objects, 17 GiB
    usage:   34 GiB used, 266 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:48:49,349350703-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:48:57,622743953-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.35k objects, 17 GiB
    usage:   34 GiB used, 266 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:48:57,632210533-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:49:06,120807232-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.35k objects, 17 GiB
    usage:   34 GiB used, 266 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:49:06,130384960-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:49:14,500846792-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 4.35k objects, 17 GiB
    usage:   34 GiB used, 266 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:49:14,510055708-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:49:14,517560954-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T14:49:14,522037211-04:00][RUNNING][ROUND 1/7/40] object_size=16MB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:49:14,526371211-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:49:14,535301333-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40910\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.394484\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 6c6f7277-eb0e-4a1c-9209-fbc778524eac\nsetting min_mon_release = octopus\nepoch 0\nfsid 6c6f7277-eb0e-4a1c-9209-fbc778524eac\nlast_changed 2021-05-17T11:49:42.788384-0700\ncreated 2021-05-17T11:49:42.788384-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40910/0,v1:10.10.1.2:40911/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.394484 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 76e723a4-ab06-4bdd-ac9d-4d9119fc4f08\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 5d6cd697-e852-4e99-8c68-ecac850ddc21\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 dda9d292-bd38-458c-89d5-3f2dd1ad3c12\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42910\n  w/ user/pass: admin / 132a4faf-6cb9-4144-9333-efe3783a4016\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 14:49:56 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40910
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.394484
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 6c6f7277-eb0e-4a1c-9209-fbc778524eac
setting min_mon_release = octopus
epoch 0
fsid 6c6f7277-eb0e-4a1c-9209-fbc778524eac
last_changed 2021-05-17T11:49:42.788384-0700
created 2021-05-17T11:49:42.788384-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40910/0,v1:10.10.1.2:40911/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.394484 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 76e723a4-ab06-4bdd-ac9d-4d9119fc4f08
0
start osd.0
add osd1 5d6cd697-e852-4e99-8c68-ecac850ddc21
1
start osd.1
add osd2 dda9d292-bd38-458c-89d5-3f2dd1ad3c12
2
start osd.2


restful urls: https://10.10.1.2:42910
  w/ user/pass: admin / 132a4faf-6cb9-4144-9333-efe3783a4016


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T11:49:15.444-0700 7f7293e0e1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T11:49:15.444-0700 7f7293e0e1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T11:49:15.464-0700 7fb23f0a31c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T11:49:15.464-0700 7fb23f0a31c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40910,v1:10.10.1.2:40911] --print /tmp/ceph_monmap.394484 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.394484 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.394484 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42910 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.scWLYf03Xe 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 76e723a4-ab06-4bdd-ac9d-4d9119fc4f08 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDPuqJgdlPJLxAAblUouqaLkdIrwvzySTV+wg== --osd-uuid 76e723a4-ab06-4bdd-ac9d-4d9119fc4f08 
2021-05-17T11:49:52.140-0700 7fcdaa505f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T11:49:52.140-0700 7fcdaa505f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T11:49:52.140-0700 7fcdaa505f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T11:49:52.212-0700 7fcdaa505f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 5d6cd697-e852-4e99-8c68-ecac850ddc21 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T11:49:52.500-0700 7fd87137df00 -1 Falling back to public interface
2021-05-17T11:49:52.512-0700 7fd87137df00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDQuqJgDxPZHRAAIILnkPa7wdB+P1PNB56NiA== --osd-uuid 5d6cd697-e852-4e99-8c68-ecac850ddc21 
2021-05-17T11:49:52.828-0700 7f046f193f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T11:49:52.828-0700 7f046f193f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T11:49:52.828-0700 7f046f193f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T11:49:52.880-0700 7f046f193f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new dda9d292-bd38-458c-89d5-3f2dd1ad3c12 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T11:49:53.252-0700 7f54acd96f00 -1 Falling back to public interface
2021-05-17T11:49:53.264-0700 7f54acd96f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDRuqJgfdsVDxAAvCJGMt/9oElbonHDUNsWHg== --osd-uuid dda9d292-bd38-458c-89d5-3f2dd1ad3c12 
2021-05-17T11:49:53.588-0700 7f6f7c611f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T11:49:53.588-0700 7f6f7c611f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T11:49:53.588-0700 7f6f7c611f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T11:49:53.644-0700 7f6f7c611f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T11:49:53.964-0700 7fb0dd4b7f00 -1 Falling back to public interface
2021-05-17T11:49:53.976-0700 7fb0dd4b7f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:49:56,048002788-04:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:49:56,052747540-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:49:56,094171171-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:49:56,097803163-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '617ecab4-fbed-4f13-b050-f90f1b04d51e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.TkPsxe:/tmp/ceph-asok.TkPsxe', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 617ecab4-fbed-4f13-b050-f90f1b04d51e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.TkPsxe:/tmp/ceph-asok.TkPsxe -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:49:59,533382444-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:49:59,537049140-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '617ecab4-fbed-4f13-b050-f90f1b04d51e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.TkPsxe:/tmp/ceph-asok.TkPsxe', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 617ecab4-fbed-4f13-b050-f90f1b04d51e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.TkPsxe:/tmp/ceph-asok.TkPsxe -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:50:02,895928366-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:50:02,898917400-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '617ecab4-fbed-4f13-b050-f90f1b04d51e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.TkPsxe:/tmp/ceph-asok.TkPsxe', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 617ecab4-fbed-4f13-b050-f90f1b04d51e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.TkPsxe:/tmp/ceph-asok.TkPsxe -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:50:06,221858760-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:50:06,225662494-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '617ecab4-fbed-4f13-b050-f90f1b04d51e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.TkPsxe:/tmp/ceph-asok.TkPsxe', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 617ecab4-fbed-4f13-b050-f90f1b04d51e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.TkPsxe:/tmp/ceph-asok.TkPsxe -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:50:12,969056672-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:50:12,972349396-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '617ecab4-fbed-4f13-b050-f90f1b04d51e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.TkPsxe:/tmp/ceph-asok.TkPsxe', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 617ecab4-fbed-4f13-b050-f90f1b04d51e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.TkPsxe:/tmp/ceph-asok.TkPsxe -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:50:17,307611585-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:50:17,311357601-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '617ecab4-fbed-4f13-b050-f90f1b04d51e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.TkPsxe:/tmp/ceph-asok.TkPsxe', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 617ecab4-fbed-4f13-b050-f90f1b04d51e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.TkPsxe:/tmp/ceph-asok.TkPsxe -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:50:21,779948378-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:50:21,783914527-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '617ecab4-fbed-4f13-b050-f90f1b04d51e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.TkPsxe:/tmp/ceph-asok.TkPsxe', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 617ecab4-fbed-4f13-b050-f90f1b04d51e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.TkPsxe:/tmp/ceph-asok.TkPsxe -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:50:26,087126014-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:50:26,090733179-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '617ecab4-fbed-4f13-b050-f90f1b04d51e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.TkPsxe:/tmp/ceph-asok.TkPsxe', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 617ecab4-fbed-4f13-b050-f90f1b04d51e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.TkPsxe:/tmp/ceph-asok.TkPsxe -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:50:29,551121529-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:50:29,554572300-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '617ecab4-fbed-4f13-b050-f90f1b04d51e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.TkPsxe:/tmp/ceph-asok.TkPsxe', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 617ecab4-fbed-4f13-b050-f90f1b04d51e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.TkPsxe:/tmp/ceph-asok.TkPsxe -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:50:33,855782814-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:50:33,859628487-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '617ecab4-fbed-4f13-b050-f90f1b04d51e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.TkPsxe:/tmp/ceph-asok.TkPsxe', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 617ecab4-fbed-4f13-b050-f90f1b04d51e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.TkPsxe:/tmp/ceph-asok.TkPsxe -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:50:37,381556083-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:50:37,385322638-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '617ecab4-fbed-4f13-b050-f90f1b04d51e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.TkPsxe:/tmp/ceph-asok.TkPsxe', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 617ecab4-fbed-4f13-b050-f90f1b04d51e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.TkPsxe:/tmp/ceph-asok.TkPsxe -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:50:40,879659465-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:50:40,882941610-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '617ecab4-fbed-4f13-b050-f90f1b04d51e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.TkPsxe:/tmp/ceph-asok.TkPsxe', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 617ecab4-fbed-4f13-b050-f90f1b04d51e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.TkPsxe:/tmp/ceph-asok.TkPsxe -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  155 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:50:44,276726253-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:51:07,714892161-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (9s)
      [=========================...] 
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:51:16,069437555-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:51:24,370986049-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:51:32,730997748-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:51:41,153098671-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:51:49,438372314-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:51:57,717907860-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:51:57,727133127-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:52:06,147156785-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:52:06,156666607-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:52:14,554047847-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:52:14,564075331-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:52:22,991434264-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:52:23,000965356-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:52:31,422002946-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:52:31,431579422-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:52:31,438462069-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:52:31,442333781-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:52:31,451586810-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=569460
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_16MB_write.dat.1 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:52:31,458985116-04:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_16MB_write.log.1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:52:31,478989558-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:52:31,482563020-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '617ecab4-fbed-4f13-b050-f90f1b04d51e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.TkPsxe:/tmp/ceph-asok.TkPsxe', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 617ecab4-fbed-4f13-b050-f90f1b04d51e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.TkPsxe:/tmp/ceph-asok.TkPsxe -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T18:52:34.041+0000 7f83cf583d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T18:52:34.045+0000 7f83cf583d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T18:52:34.045+0000 7f83cf583d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T18:52:34.061714+0000 Maintaining 256 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 20 seconds or 0 objects
2021-05-17T18:52:34.061746+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-17T18:52:35.075826+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T18:52:35.075826+0000     0       0         0         0         0         0           -           0
2021-05-17T18:52:36.075925+0000     1      30        30         0         0         0           -           0
2021-05-17T18:52:37.076000+0000     2      58        58         0         0         0           -           0
2021-05-17T18:52:38.076074+0000     3      88        88         0         0         0           -           0
2021-05-17T18:52:39.076161+0000     4     118       118         0         0         0           -           0
2021-05-17T18:52:40.076228+0000     5     145       145         0         0         0           -           0
2021-05-17T18:52:41.076295+0000     6     175       175         0         0         0           -           0
2021-05-17T18:52:42.076375+0000     7     202       202         0         0         0           -           0
2021-05-17T18:52:43.076466+0000     8     230       230         0         0         0           -           0
2021-05-17T18:52:44.076539+0000     9     255       256         1   1.77765   1.77778     8.98681     8.98681
2021-05-17T18:52:45.076618+0000    10     255       284        29   46.3966       448     9.06025     9.07203
2021-05-17T18:52:46.076686+0000    11     255       312        57    82.903       448     9.06284     9.07067
2021-05-17T18:52:47.076776+0000    12     255       343        88   117.325       496     9.01282     9.06446
2021-05-17T18:52:48.076842+0000    13     255       372       117   143.989       464     9.06006     9.06267
2021-05-17T18:52:49.076909+0000    14     255       399       144   164.559       432     9.07164     9.07071
2021-05-17T18:52:50.076979+0000    15     255       427       172   183.453       448     9.09763     9.07193
2021-05-17T18:52:51.077068+0000    16     255       458       203   202.985       496     8.98426     9.06834
2021-05-17T18:52:52.077135+0000    17     255       486       231   217.396       448     8.98545     9.05653
2021-05-17T18:52:53.077203+0000    18     255       512       257   228.428       416     8.98881     9.05327
2021-05-17T18:52:54.077271+0000    19     255       542       287   241.666       480     8.92572     9.04064
2021-05-17T18:52:55.077539+0000 min lat: 8.88875 max lat: 9.14527 avg lat: 9.03068
2021-05-17T18:52:55.077539+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T18:52:55.077539+0000    20     255       570       315   251.979       448     8.90325     9.03068
2021-05-17T18:52:56.077908+0000 Total time run:         20.1028
Total writes made:      571
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     454.465
Stddev Bandwidth:       233.735
Max bandwidth (MB/sec): 496
Min bandwidth (MB/sec): 0
Average IOPS:           28
Stddev IOPS:            14.6147
Max IOPS:               31
Min IOPS:               0
Average Latency(s):     7.02229
Stddev Latency(s):      2.82694
Max latency(s):         9.14527
Min latency(s):         0.1148

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:52:56,691339618-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 569460


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:52:56,696264788-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:53:20,071396159-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 572 objects, 8.9 GiB
    usage:   18 GiB used, 282 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:53:20,080922141-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:53:28,433474511-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 572 objects, 8.9 GiB
    usage:   18 GiB used, 282 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:53:28,442202844-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:53:36,838833381-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 572 objects, 8.9 GiB
    usage:   18 GiB used, 282 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:53:36,848145821-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:53:44,992969681-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 572 objects, 8.9 GiB
    usage:   18 GiB used, 282 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:53:45,002448323-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:53:53,305371925-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 572 objects, 8.9 GiB
    usage:   18 GiB used, 282 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:53:53,314297249-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:53:53,320788200-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:53:53,325173457-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:53:53,334017768-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=570866
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_16MB_seq.dat.1 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:53:53,341172296-04:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_16MB_seq.log.1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:53:53,361720680-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:53:53,365202550-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '617ecab4-fbed-4f13-b050-f90f1b04d51e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.TkPsxe:/tmp/ceph-asok.TkPsxe', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 617ecab4-fbed-4f13-b050-f90f1b04d51e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.TkPsxe:/tmp/ceph-asok.TkPsxe -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T18:53:55.854+0000 7f71b1531d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T18:53:55.858+0000 7f71b1531d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T18:53:55.862+0000 7f71b1531d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T18:53:55.877239+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T18:53:55.877239+0000     0       0         0         0         0         0           -           0
2021-05-17T18:53:56.877327+0000     1     220       220         0         0         0           -           0
2021-05-17T18:53:57.877437+0000     2     255       401       146   1167.81      1168      1.3402     1.24806
2021-05-17T18:53:58.878069+0000     3     255       541       286   1524.84      2240     1.68185     1.38391
2021-05-17T18:53:59.878294+0000 Total time run:       3.63155
Total reads made:     571
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   2515.73
Average IOPS:         157
Stddev IOPS:          70.0214
Max IOPS:             140
Min IOPS:             0
Average Latency(s):   1.26606
Max latency(s):       1.70521
Min latency(s):       0.461649

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:54:00,733612244-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 570866


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:54:00,738568764-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:54:24,029407146-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 572 objects, 8.9 GiB
    usage:   18 GiB used, 282 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:54:24,038119279-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:54:32,415339815-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 572 objects, 8.9 GiB
    usage:   18 GiB used, 282 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:54:32,424936630-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:54:41,011684683-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 572 objects, 8.9 GiB
    usage:   18 GiB used, 282 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:54:41,021420138-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:54:49,357211859-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 572 objects, 8.9 GiB
    usage:   18 GiB used, 282 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:54:49,366505013-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:54:57,678985128-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 572 objects, 8.9 GiB
    usage:   18 GiB used, 282 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:54:57,688117631-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:54:57,695654817-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T14:54:57,698568800-04:00][RUNNING][ROUND 2/7/40] object_size=16MB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:54:57,702713795-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:54:57,711337683-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40381\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.395604\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid eea89c44-b276-41cb-90b5-50337d014615\nsetting min_mon_release = octopus\nepoch 0\nfsid eea89c44-b276-41cb-90b5-50337d014615\nlast_changed 2021-05-17T11:55:27.570776-0700\ncreated 2021-05-17T11:55:27.570776-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40381/0,v1:10.10.1.2:40382/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.395604 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 37a57208-0e7c-4bb4-a59d-acc3dbf9fdd0\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 fad0a06b-af1d-4d27-a3f3-064b578ce82b\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 1fa221a5-2b13-4695-8dea-e5c03214b8a9\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42381\n  w/ user/pass: admin / 93477098-4c53-4da9-9877-920affb80adc\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 14:55:40 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40381
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.395604
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid eea89c44-b276-41cb-90b5-50337d014615
setting min_mon_release = octopus
epoch 0
fsid eea89c44-b276-41cb-90b5-50337d014615
last_changed 2021-05-17T11:55:27.570776-0700
created 2021-05-17T11:55:27.570776-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40381/0,v1:10.10.1.2:40382/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.395604 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 37a57208-0e7c-4bb4-a59d-acc3dbf9fdd0
0
start osd.0
add osd1 fad0a06b-af1d-4d27-a3f3-064b578ce82b
1
start osd.1
add osd2 1fa221a5-2b13-4695-8dea-e5c03214b8a9
2
start osd.2


restful urls: https://10.10.1.2:42381
  w/ user/pass: admin / 93477098-4c53-4da9-9877-920affb80adc


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T11:54:58.643-0700 7fe5093301c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T11:54:58.643-0700 7fe5093301c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T11:54:58.659-0700 7f71e35fc1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T11:54:58.659-0700 7f71e35fc1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40381,v1:10.10.1.2:40382] --print /tmp/ceph_monmap.395604 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.395604 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.395604 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42381 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.dO9buUjCP8 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 37a57208-0e7c-4bb4-a59d-acc3dbf9fdd0 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAovKJg95HxGhAAfwKIHhOVDZM/4EdzbPUahA== --osd-uuid 37a57208-0e7c-4bb4-a59d-acc3dbf9fdd0 
2021-05-17T11:55:36.779-0700 7fa5c4439f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T11:55:36.779-0700 7fa5c4439f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T11:55:36.779-0700 7fa5c4439f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T11:55:36.851-0700 7fa5c4439f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new fad0a06b-af1d-4d27-a3f3-064b578ce82b -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T11:55:37.163-0700 7f56c1366f00 -1 Falling back to public interface
2021-05-17T11:55:37.175-0700 7f56c1366f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQApvKJgMO2ZCRAAZKYL+0++cABhgFaeafOFDQ== --osd-uuid fad0a06b-af1d-4d27-a3f3-064b578ce82b 
2021-05-17T11:55:37.515-0700 7f548e687f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T11:55:37.519-0700 7f548e687f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T11:55:37.519-0700 7f548e687f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T11:55:37.563-0700 7f548e687f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 1fa221a5-2b13-4695-8dea-e5c03214b8a9 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T11:55:37.851-0700 7f2cda468f00 -1 Falling back to public interface
2021-05-17T11:55:37.867-0700 7f2cda468f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQApvKJgiXEBMRAAErCGqOnhkbNje6F326ByFw== --osd-uuid 1fa221a5-2b13-4695-8dea-e5c03214b8a9 
2021-05-17T11:55:38.195-0700 7f11764eef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T11:55:38.195-0700 7f11764eef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T11:55:38.195-0700 7f11764eef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T11:55:38.247-0700 7f11764eef00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T11:55:38.567-0700 7f0b69b46f00 -1 Falling back to public interface
2021-05-17T11:55:38.583-0700 7f0b69b46f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:55:40,659916679-04:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:55:40,664484959-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:55:40,707757235-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:55:40,711135339-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '466576a4-95dc-44ca-bc43-03bcd5178bdb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pf9ftk:/tmp/ceph-asok.pf9ftk', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 466576a4-95dc-44ca-bc43-03bcd5178bdb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pf9ftk:/tmp/ceph-asok.pf9ftk -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:55:44,060727715-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:55:44,064060815-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '466576a4-95dc-44ca-bc43-03bcd5178bdb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pf9ftk:/tmp/ceph-asok.pf9ftk', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 466576a4-95dc-44ca-bc43-03bcd5178bdb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pf9ftk:/tmp/ceph-asok.pf9ftk -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:55:47,407604802-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:55:47,411377438-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '466576a4-95dc-44ca-bc43-03bcd5178bdb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pf9ftk:/tmp/ceph-asok.pf9ftk', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 466576a4-95dc-44ca-bc43-03bcd5178bdb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pf9ftk:/tmp/ceph-asok.pf9ftk -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:55:50,705547289-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:55:50,709391069-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '466576a4-95dc-44ca-bc43-03bcd5178bdb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pf9ftk:/tmp/ceph-asok.pf9ftk', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 466576a4-95dc-44ca-bc43-03bcd5178bdb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pf9ftk:/tmp/ceph-asok.pf9ftk -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:55:57,366446811-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:55:57,370050169-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '466576a4-95dc-44ca-bc43-03bcd5178bdb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pf9ftk:/tmp/ceph-asok.pf9ftk', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 466576a4-95dc-44ca-bc43-03bcd5178bdb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pf9ftk:/tmp/ceph-asok.pf9ftk -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:56:01,771038410-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:56:01,774521112-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '466576a4-95dc-44ca-bc43-03bcd5178bdb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pf9ftk:/tmp/ceph-asok.pf9ftk', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 466576a4-95dc-44ca-bc43-03bcd5178bdb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pf9ftk:/tmp/ceph-asok.pf9ftk -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:56:05,435899490-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:56:05,440170161-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '466576a4-95dc-44ca-bc43-03bcd5178bdb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pf9ftk:/tmp/ceph-asok.pf9ftk', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 466576a4-95dc-44ca-bc43-03bcd5178bdb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pf9ftk:/tmp/ceph-asok.pf9ftk -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:56:09,607785690-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:56:09,611824445-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '466576a4-95dc-44ca-bc43-03bcd5178bdb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pf9ftk:/tmp/ceph-asok.pf9ftk', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 466576a4-95dc-44ca-bc43-03bcd5178bdb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pf9ftk:/tmp/ceph-asok.pf9ftk -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:56:13,379492633-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:56:13,383601541-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '466576a4-95dc-44ca-bc43-03bcd5178bdb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pf9ftk:/tmp/ceph-asok.pf9ftk', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 466576a4-95dc-44ca-bc43-03bcd5178bdb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pf9ftk:/tmp/ceph-asok.pf9ftk -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:56:17,710398853-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:56:17,713958458-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '466576a4-95dc-44ca-bc43-03bcd5178bdb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pf9ftk:/tmp/ceph-asok.pf9ftk', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 466576a4-95dc-44ca-bc43-03bcd5178bdb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pf9ftk:/tmp/ceph-asok.pf9ftk -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:56:21,422138938-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:56:21,425970775-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '466576a4-95dc-44ca-bc43-03bcd5178bdb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pf9ftk:/tmp/ceph-asok.pf9ftk', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 466576a4-95dc-44ca-bc43-03bcd5178bdb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pf9ftk:/tmp/ceph-asok.pf9ftk -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:56:24,819817115-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:56:24,823144625-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '466576a4-95dc-44ca-bc43-03bcd5178bdb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pf9ftk:/tmp/ceph-asok.pf9ftk', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 466576a4-95dc-44ca-bc43-03bcd5178bdb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pf9ftk:/tmp/ceph-asok.pf9ftk -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  154 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:56:28,051440134-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:56:51,404761198-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=========================...] 
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:56:59,716522918-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:57:07,939059380-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:57:16,320745109-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:57:24,756822367-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:57:33,024185476-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:57:41,545093450-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:57:41,554385663-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:57:50,065815714-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:57:50,075082879-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:57:58,455747098-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:57:58,464886955-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:58:06,989180261-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:58:06,998048888-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:58:15,595055225-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:58:15,604304958-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:58:15,611544065-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:58:15,615657812-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:58:15,625410189-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=577474
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_16MB_write.dat.2 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:58:15,632302034-04:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_16MB_write.log.2
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:58:15,651771291-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:58:15,654931226-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '466576a4-95dc-44ca-bc43-03bcd5178bdb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pf9ftk:/tmp/ceph-asok.pf9ftk', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 466576a4-95dc-44ca-bc43-03bcd5178bdb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pf9ftk:/tmp/ceph-asok.pf9ftk -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T18:58:18.358+0000 7efd742f2d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T18:58:18.362+0000 7efd742f2d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T18:58:18.362+0000 7efd742f2d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T18:58:18.378765+0000 Maintaining 256 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 20 seconds or 0 objects
2021-05-17T18:58:18.378804+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-17T18:58:19.390120+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T18:58:19.390120+0000     0       0         0         0         0         0           -           0
2021-05-17T18:58:20.390213+0000     1      29        29         0         0         0           -           0
2021-05-17T18:58:21.390286+0000     2      59        59         0         0         0           -           0
2021-05-17T18:58:22.390359+0000     3      89        89         0         0         0           -           0
2021-05-17T18:58:23.390436+0000     4     119       119         0         0         0           -           0
2021-05-17T18:58:24.390504+0000     5     148       148         0         0         0           -           0
2021-05-17T18:58:25.390575+0000     6     175       175         0         0         0           -           0
2021-05-17T18:58:26.390643+0000     7     206       206         0         0         0           -           0
2021-05-17T18:58:27.390728+0000     8     237       237         0         0         0           -           0
2021-05-17T18:58:28.390800+0000     9     255       263         8   14.2212   14.2222     8.73598     8.76614
2021-05-17T18:58:29.390871+0000    10     255       292        37   59.1959       464     8.76181     8.75725
2021-05-17T18:58:30.390956+0000    11     255       319        64   93.0843       432     8.87694     8.78029
2021-05-17T18:58:31.391046+0000    12     255       350        95   126.657       496     8.85891     8.80617
2021-05-17T18:58:32.391119+0000    13     255       377       122   150.143       432     8.95576      8.8248
2021-05-17T18:58:33.391190+0000    14     255       406       151   172.559       464     8.95659     8.85049
2021-05-17T18:58:34.391261+0000    15     255       437       182   194.119       496     8.84072      8.8558
2021-05-17T18:58:35.391340+0000    16     255       466       211   210.985       464     8.88556     8.85592
2021-05-17T18:58:36.391449+0000    17     255       492       237   223.042       416     8.98685      8.8647
2021-05-17T18:58:37.391559+0000    18     255       521       266   236.426       464     8.95793     8.87877
2021-05-17T18:58:38.391632+0000    19     255       549       294    247.56       448     8.97778     8.88916
2021-05-17T18:58:39.391711+0000 min lat: 8.70028 max lat: 9.06177 avg lat: 8.89064
2021-05-17T18:58:39.391711+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T18:58:39.391711+0000    20     255       578       323    258.38       464     8.90814     8.89064
2021-05-17T18:58:40.392101+0000 Total time run:         20.0784
Total writes made:      579
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     461.392
Stddev Bandwidth:       233.786
Max bandwidth (MB/sec): 496
Min bandwidth (MB/sec): 0
Average IOPS:           28
Stddev IOPS:            14.6606
Max IOPS:               31
Min IOPS:               0
Average Latency(s):     6.95186
Stddev Latency(s):      2.77458
Max latency(s):         9.06177
Min latency(s):         0.0867678

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:58:40,974992099-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 577474


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:58:40,979898404-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:59:04,162304902-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 580 objects, 9.0 GiB
    usage:   18 GiB used, 282 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:59:04,170495085-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:59:12,566800597-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 580 objects, 9.0 GiB
    usage:   18 GiB used, 282 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:59:12,575906961-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:59:21,049614851-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 580 objects, 9.0 GiB
    usage:   18 GiB used, 282 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:59:21,058351861-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:59:29,389697738-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 580 objects, 9.0 GiB
    usage:   18 GiB used, 282 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:59:29,398707330-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:59:37,576817629-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 580 objects, 9.0 GiB
    usage:   18 GiB used, 282 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:59:37,586152973-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:59:37,593486037-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:59:37,597332571-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:59:37,606091252-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=578870
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_16MB_seq.dat.2 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:59:37,613340037-04:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_16MB_seq.log.2
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:59:37,633394444-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:59:37,637261147-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '466576a4-95dc-44ca-bc43-03bcd5178bdb', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pf9ftk:/tmp/ceph-asok.pf9ftk', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 466576a4-95dc-44ca-bc43-03bcd5178bdb --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.pf9ftk:/tmp/ceph-asok.pf9ftk -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T18:59:40.219+0000 7f8f17879d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T18:59:40.223+0000 7f8f17879d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T18:59:40.223+0000 7f8f17879d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T18:59:40.242221+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T18:59:40.242221+0000     0       0         0         0         0         0           -           0
2021-05-17T18:59:41.242358+0000     1     206       206         0         0         0           -           0
2021-05-17T18:59:42.242450+0000     2     255       369       114   911.832       912     1.47226     1.33703
2021-05-17T18:59:43.242611+0000     3     255       525       270   1439.75      2496     1.70026     1.48425
2021-05-17T18:59:44.242792+0000 Total time run:       3.71662
Total reads made:     579
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   2492.58
Average IOPS:         155
Stddev IOPS:          78.9367
Max IOPS:             156
Min IOPS:             0
Average Latency(s):   1.29431
Max latency(s):       1.70851
Min latency(s):       0.378577

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:59:45,037642172-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 578870


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T14:59:45,042378528-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:00:08,240366014-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 580 objects, 9.0 GiB
    usage:   18 GiB used, 282 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:00:08,249498747-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:00:16,663428066-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 580 objects, 9.0 GiB
    usage:   18 GiB used, 282 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:00:16,672922228-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:00:25,037238358-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 580 objects, 9.0 GiB
    usage:   18 GiB used, 282 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:00:25,046956631-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:00:33,425192723-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 580 objects, 9.0 GiB
    usage:   18 GiB used, 282 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:00:33,434605022-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:00:41,765394003-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 580 objects, 9.0 GiB
    usage:   18 GiB used, 282 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:00:41,774243024-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:00:41,781622865-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T15:00:41,784093816-04:00][RUNNING][ROUND 3/7/40] object_size=16MB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:00:41,788504471-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:00:41,797453459-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40613\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.396710\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 3c9ccf34-2577-4c7f-b19d-ca8e4f10f4c1\nsetting min_mon_release = octopus\nepoch 0\nfsid 3c9ccf34-2577-4c7f-b19d-ca8e4f10f4c1\nlast_changed 2021-05-17T12:01:09.820908-0700\ncreated 2021-05-17T12:01:09.820908-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40613/0,v1:10.10.1.2:40614/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.396710 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 2adfa278-e301-4ae4-bc23-564aaebb6b05\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 dad746c2-48f8-4a1b-9b87-e9bc7d1c43a0\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 b43f337c-4c58-4101-b33e-03f1a577bb1d\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42613\n  w/ user/pass: admin / 62bbd860-814a-4eea-960e-de2d70e4beb0\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\nCEPH_DEV=1\n'
[1] 15:01:22 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40613
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.396710
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 3c9ccf34-2577-4c7f-b19d-ca8e4f10f4c1
setting min_mon_release = octopus
epoch 0
fsid 3c9ccf34-2577-4c7f-b19d-ca8e4f10f4c1
last_changed 2021-05-17T12:01:09.820908-0700
created 2021-05-17T12:01:09.820908-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40613/0,v1:10.10.1.2:40614/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.396710 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 2adfa278-e301-4ae4-bc23-564aaebb6b05
0
start osd.0
add osd1 dad746c2-48f8-4a1b-9b87-e9bc7d1c43a0
1
start osd.1
add osd2 b43f337c-4c58-4101-b33e-03f1a577bb1d
2
start osd.2


restful urls: https://10.10.1.2:42613
  w/ user/pass: admin / 62bbd860-814a-4eea-960e-de2d70e4beb0


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T12:00:42.722-0700 7f10484e01c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T12:00:42.722-0700 7f10484e01c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T12:00:42.742-0700 7fe6fab5d1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T12:00:42.742-0700 7fe6fab5d1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40613,v1:10.10.1.2:40614] --print /tmp/ceph_monmap.396710 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.396710 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.396710 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42613 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.h0LNAykD1Y 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 2adfa278-e301-4ae4-bc23-564aaebb6b05 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQB+vaJgXZViKRAAlQed1BeppsyTWkdt5+mIFQ== --osd-uuid 2adfa278-e301-4ae4-bc23-564aaebb6b05 
2021-05-17T12:01:19.026-0700 7f9b864cbf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T12:01:19.030-0700 7f9b864cbf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T12:01:19.030-0700 7f9b864cbf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T12:01:19.074-0700 7f9b864cbf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new dad746c2-48f8-4a1b-9b87-e9bc7d1c43a0 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T12:01:19.366-0700 7f6f79f1df00 -1 Falling back to public interface
2021-05-17T12:01:19.378-0700 7f6f79f1df00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQB/vaJgxL/4FRAAXr0rB7UAaUxrnsZ9Lo4xRg== --osd-uuid dad746c2-48f8-4a1b-9b87-e9bc7d1c43a0 
2021-05-17T12:01:19.714-0700 7f2f8eb67f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T12:01:19.714-0700 7f2f8eb67f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T12:01:19.714-0700 7f2f8eb67f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T12:01:19.762-0700 7f2f8eb67f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new b43f337c-4c58-4101-b33e-03f1a577bb1d -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T12:01:20.106-0700 7f744c14ef00 -1 Falling back to public interface
2021-05-17T12:01:20.118-0700 7f744c14ef00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCAvaJgrnpHBhAAM3e2nt+rCniNWViYAdybtg== --osd-uuid b43f337c-4c58-4101-b33e-03f1a577bb1d 
2021-05-17T12:01:20.434-0700 7f08a5954f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T12:01:20.434-0700 7f08a5954f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T12:01:20.434-0700 7f08a5954f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T12:01:20.498-0700 7f08a5954f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T12:01:20.890-0700 7fc63eecff00 -1 Falling back to public interface
2021-05-17T12:01:20.906-0700 7fc63eecff00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:01:22,882651232-04:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:01:22,887855256-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:01:22,928620934-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:01:22,931550718-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '881f5e49-179e-4e59-beca-c5c344a8b81e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZglYWO:/tmp/ceph-asok.ZglYWO', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 881f5e49-179e-4e59-beca-c5c344a8b81e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZglYWO:/tmp/ceph-asok.ZglYWO -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:01:26,323297410-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:01:26,326839774-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '881f5e49-179e-4e59-beca-c5c344a8b81e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZglYWO:/tmp/ceph-asok.ZglYWO', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 881f5e49-179e-4e59-beca-c5c344a8b81e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZglYWO:/tmp/ceph-asok.ZglYWO -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:01:29,638237461-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:01:29,641970763-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '881f5e49-179e-4e59-beca-c5c344a8b81e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZglYWO:/tmp/ceph-asok.ZglYWO', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 881f5e49-179e-4e59-beca-c5c344a8b81e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZglYWO:/tmp/ceph-asok.ZglYWO -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:01:32,976367636-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:01:32,979775487-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '881f5e49-179e-4e59-beca-c5c344a8b81e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZglYWO:/tmp/ceph-asok.ZglYWO', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 881f5e49-179e-4e59-beca-c5c344a8b81e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZglYWO:/tmp/ceph-asok.ZglYWO -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:01:40,007178838-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:01:40,011235448-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '881f5e49-179e-4e59-beca-c5c344a8b81e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZglYWO:/tmp/ceph-asok.ZglYWO', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 881f5e49-179e-4e59-beca-c5c344a8b81e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZglYWO:/tmp/ceph-asok.ZglYWO -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:01:44,072060116-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:01:44,075465682-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '881f5e49-179e-4e59-beca-c5c344a8b81e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZglYWO:/tmp/ceph-asok.ZglYWO', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 881f5e49-179e-4e59-beca-c5c344a8b81e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZglYWO:/tmp/ceph-asok.ZglYWO -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:01:48,563369120-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:01:48,567057749-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '881f5e49-179e-4e59-beca-c5c344a8b81e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZglYWO:/tmp/ceph-asok.ZglYWO', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 881f5e49-179e-4e59-beca-c5c344a8b81e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZglYWO:/tmp/ceph-asok.ZglYWO -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:01:52,985563890-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:01:52,989365752-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '881f5e49-179e-4e59-beca-c5c344a8b81e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZglYWO:/tmp/ceph-asok.ZglYWO', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 881f5e49-179e-4e59-beca-c5c344a8b81e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZglYWO:/tmp/ceph-asok.ZglYWO -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:01:56,551891190-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:01:56,556227344-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '881f5e49-179e-4e59-beca-c5c344a8b81e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZglYWO:/tmp/ceph-asok.ZglYWO', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 881f5e49-179e-4e59-beca-c5c344a8b81e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZglYWO:/tmp/ceph-asok.ZglYWO -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:02:00,830428673-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:02:00,834084700-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '881f5e49-179e-4e59-beca-c5c344a8b81e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZglYWO:/tmp/ceph-asok.ZglYWO', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 881f5e49-179e-4e59-beca-c5c344a8b81e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZglYWO:/tmp/ceph-asok.ZglYWO -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:02:04,338414083-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:02:04,342080840-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '881f5e49-179e-4e59-beca-c5c344a8b81e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZglYWO:/tmp/ceph-asok.ZglYWO', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 881f5e49-179e-4e59-beca-c5c344a8b81e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZglYWO:/tmp/ceph-asok.ZglYWO -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:02:07,654214641-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:02:07,658036359-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '881f5e49-179e-4e59-beca-c5c344a8b81e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZglYWO:/tmp/ceph-asok.ZglYWO', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 881f5e49-179e-4e59-beca-c5c344a8b81e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZglYWO:/tmp/ceph-asok.ZglYWO -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  155 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:02:11,058987646-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:02:34,402217105-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:02:42,951326457-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (9s)
      [=======.....................] (remaining: 26s)
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:02:51,157223285-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [========....................] (remaining: 49s)
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:02:59,565127558-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [========....................] (remaining: 60s)
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:03:07,937537680-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [=========...................] (remaining: 73s)
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:03:16,508571230-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (40s)
      [=========...................] (remaining: 83s)
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:03:24,865478735-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [=========...................] (remaining: 104s)
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:03:33,312949145-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:03:33,322143536-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:03:41,774722334-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:03:41,784126159-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:03:50,170366036-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:03:50,179139827-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:03:58,567594957-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:03:58,577561388-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:04:06,970715206-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:04:06,980015936-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:04:06,986878798-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:04:06,990976725-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:04:07,000804526-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=585727
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_16MB_write.dat.3 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:04:07,007633974-04:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_16MB_write.log.3
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:04:07,026995633-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:04:07,030783428-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '881f5e49-179e-4e59-beca-c5c344a8b81e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZglYWO:/tmp/ceph-asok.ZglYWO', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 881f5e49-179e-4e59-beca-c5c344a8b81e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZglYWO:/tmp/ceph-asok.ZglYWO -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T19:04:09.407+0000 7f59678e2d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T19:04:09.411+0000 7f59678e2d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T19:04:09.411+0000 7f59678e2d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T19:04:09.425373+0000 Maintaining 256 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 20 seconds or 0 objects
2021-05-17T19:04:09.425405+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-17T19:04:10.440169+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T19:04:10.440169+0000     0       0         0         0         0         0           -           0
2021-05-17T19:04:11.440280+0000     1      29        29         0         0         0           -           0
2021-05-17T19:04:12.440390+0000     2      58        58         0         0         0           -           0
2021-05-17T19:04:13.440509+0000     3      88        88         0         0         0           -           0
2021-05-17T19:04:14.440578+0000     4     114       114         0         0         0           -           0
2021-05-17T19:04:15.440665+0000     5     143       143         0         0         0           -           0
2021-05-17T19:04:16.440733+0000     6     172       172         0         0         0           -           0
2021-05-17T19:04:17.440803+0000     7     201       201         0         0         0           -           0
2021-05-17T19:04:18.440880+0000     8     231       231         0         0         0           -           0
2021-05-17T19:04:19.440970+0000     9     255       257         2   3.55527   3.55556      8.9862     8.94753
2021-05-17T19:04:20.441039+0000    10     255       284        29   46.3963       432     9.03143     8.99983
2021-05-17T19:04:21.441110+0000    11     255       312        57   82.9025       448     9.05268     9.02105
2021-05-17T19:04:22.441180+0000    12     255       338        83   110.658       416     9.22179     9.05925
2021-05-17T19:04:23.441267+0000    13     255       367       112   137.835       464     9.11301     9.08787
2021-05-17T19:04:24.441339+0000    14     255       397       142   162.273       480     9.06066     9.08855
2021-05-17T19:04:25.441411+0000    15     255       422       167   178.119       400     9.16458     9.09313
2021-05-17T19:04:26.441527+0000    16     255       449       194   193.984       432      9.2672     9.11119
2021-05-17T19:04:27.441623+0000    17     255       477       222   208.924       448     9.32712      9.1363
2021-05-17T19:04:28.441717+0000    18     255       503       248   220.426       416     9.39471      9.1653
2021-05-17T19:04:29.441787+0000    19     255       532       277   233.244       464     9.32171     9.18335
2021-05-17T19:04:30.441858+0000 min lat: 8.90885 max lat: 9.47297 avg lat: 9.19914
2021-05-17T19:04:30.441858+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T19:04:30.441858+0000    20     255       556       301   240.781       384     9.43436     9.19914
2021-05-17T19:04:31.442054+0000 Total time run:         20.0755
Total writes made:      557
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     443.924
Stddev Bandwidth:       222.804
Max bandwidth (MB/sec): 480
Min bandwidth (MB/sec): 0
Average IOPS:           27
Stddev IOPS:            13.9377
Max IOPS:               30
Min IOPS:               0
Average Latency(s):     7.16641
Stddev Latency(s):      2.87093
Max latency(s):         9.47297
Min latency(s):         0.116247

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:04:32,018727341-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 585727


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:04:32,024108369-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:04:55,354723001-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 558 objects, 8.7 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:04:55,364317885-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:05:03,756521848-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 558 objects, 8.7 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:05:03,765955979-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:05:12,120420283-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 558 objects, 8.7 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:05:12,130197018-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:05:20,582379208-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 558 objects, 8.7 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:05:20,591806165-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:05:28,778865095-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 558 objects, 8.7 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:05:28,788311830-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:05:28,795964054-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:05:28,800123787-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:05:28,809430139-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=587119
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_16MB_seq.dat.3 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:05:28,816522752-04:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_16MB_seq.log.3
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:05:28,835500159-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:05:28,839157910-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '881f5e49-179e-4e59-beca-c5c344a8b81e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZglYWO:/tmp/ceph-asok.ZglYWO', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 881f5e49-179e-4e59-beca-c5c344a8b81e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ZglYWO:/tmp/ceph-asok.ZglYWO -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T19:05:31.256+0000 7f339d45ed00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T19:05:31.260+0000 7f339d45ed00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T19:05:31.260+0000 7f339d45ed00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T19:05:31.278551+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T19:05:31.278551+0000     0       0         0         0         0         0           -           0
2021-05-17T19:05:32.278686+0000     1     199       199         0         0         0           -           0
2021-05-17T19:05:33.278760+0000     2     255       367       112   895.841       896     1.50225     1.38034
2021-05-17T19:05:34.279386+0000     3     255       521       266    1418.2      2464     1.66017     1.49814
2021-05-17T19:05:35.279570+0000 Total time run:       3.59347
Total reads made:     557
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   2480.06
Average IOPS:         155
Stddev IOPS:          77.9487
Max IOPS:             154
Min IOPS:             0
Average Latency(s):   1.27918
Max latency(s):       1.66901
Min latency(s):       0.405961

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:05:35,966902909-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 587119


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:05:35,971730247-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:05:59,175701891-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 558 objects, 8.7 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:05:59,184916991-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:06:07,622796062-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 558 objects, 8.7 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:06:07,632090590-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:06:16,008050493-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 558 objects, 8.7 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:06:16,016871893-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:06:24,305905162-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 558 objects, 8.7 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:06:24,315248963-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:06:32,874702386-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 558 objects, 8.7 GiB
    usage:   17 GiB used, 283 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:06:32,883897518-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:06:32,891933493-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T15:06:32,894424662-04:00][RUNNING][ROUND 4/7/40] object_size=16MB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:06:32,898282248-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:06:32,907561178-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40314\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.397821\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 88fb16af-1dac-4205-b8e0-2421683bdd4a\nsetting min_mon_release = octopus\nepoch 0\nfsid 88fb16af-1dac-4205-b8e0-2421683bdd4a\nlast_changed 2021-05-17T12:07:02.125143-0700\ncreated 2021-05-17T12:07:02.125143-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40314/0,v1:10.10.1.2:40315/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.397821 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 e586427e-4303-4f79-807a-dbade5aab164\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 e9965544-9e87-40b7-a963-0f875f293ac2\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 06b61eac-8f4d-48af-8df4-cdcaba413b81\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42314\n  w/ user/pass: admin / 47c2749b-0ced-4a75-af14-a59057163299\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\nCEPH_DEV=1\n'
[1] 15:07:15 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40314
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.397821
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 88fb16af-1dac-4205-b8e0-2421683bdd4a
setting min_mon_release = octopus
epoch 0
fsid 88fb16af-1dac-4205-b8e0-2421683bdd4a
last_changed 2021-05-17T12:07:02.125143-0700
created 2021-05-17T12:07:02.125143-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40314/0,v1:10.10.1.2:40315/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.397821 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 e586427e-4303-4f79-807a-dbade5aab164
0
start osd.0
add osd1 e9965544-9e87-40b7-a963-0f875f293ac2
1
start osd.1
add osd2 06b61eac-8f4d-48af-8df4-cdcaba413b81
2
start osd.2


restful urls: https://10.10.1.2:42314
  w/ user/pass: admin / 47c2749b-0ced-4a75-af14-a59057163299


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T12:06:33.825-0700 7ffb106cb1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T12:06:33.825-0700 7ffb106cb1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T12:06:33.841-0700 7f05e24731c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T12:06:33.841-0700 7f05e24731c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40314,v1:10.10.1.2:40315] --print /tmp/ceph_monmap.397821 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.397821 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.397821 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42314 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.PV2CZgWdoA 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new e586427e-4303-4f79-807a-dbade5aab164 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDfvqJgkdUHKxAA0jOjnv85iUyCgS46M/uh3g== --osd-uuid e586427e-4303-4f79-807a-dbade5aab164 
2021-05-17T12:07:12.045-0700 7f933cbebf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T12:07:12.045-0700 7f933cbebf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T12:07:12.045-0700 7f933cbebf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T12:07:12.109-0700 7f933cbebf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new e9965544-9e87-40b7-a963-0f875f293ac2 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T12:07:12.393-0700 7fc1d1331f00 -1 Falling back to public interface
2021-05-17T12:07:12.405-0700 7fc1d1331f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDgvqJg35eRFxAA9bcIT8JtG8BBzM2LSeAJbg== --osd-uuid e9965544-9e87-40b7-a963-0f875f293ac2 
2021-05-17T12:07:12.725-0700 7f814f89df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T12:07:12.725-0700 7f814f89df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T12:07:12.725-0700 7f814f89df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T12:07:12.777-0700 7f814f89df00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 06b61eac-8f4d-48af-8df4-cdcaba413b81 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T12:07:13.049-0700 7fb7f0f83f00 -1 Falling back to public interface
2021-05-17T12:07:13.061-0700 7fb7f0f83f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDhvqJgjo/nAhAAwk4YMzxa6Yy6HPMoDvg+lg== --osd-uuid 06b61eac-8f4d-48af-8df4-cdcaba413b81 
2021-05-17T12:07:13.429-0700 7f62e2844f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T12:07:13.429-0700 7f62e2844f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T12:07:13.429-0700 7f62e2844f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T12:07:13.489-0700 7f62e2844f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T12:07:13.817-0700 7f6b4ce06f00 -1 Falling back to public interface
2021-05-17T12:07:13.833-0700 7f6b4ce06f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:07:15,854775223-04:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:07:15,859652084-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:07:15,902969492-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:07:15,906665554-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '514fbe7b-9831-404e-9285-741dcf1bc0b0', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.07cnqO:/tmp/ceph-asok.07cnqO', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 514fbe7b-9831-404e-9285-741dcf1bc0b0 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.07cnqO:/tmp/ceph-asok.07cnqO -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:07:19,317863379-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:07:19,321494230-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '514fbe7b-9831-404e-9285-741dcf1bc0b0', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.07cnqO:/tmp/ceph-asok.07cnqO', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 514fbe7b-9831-404e-9285-741dcf1bc0b0 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.07cnqO:/tmp/ceph-asok.07cnqO -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:07:22,831020056-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:07:22,834278968-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '514fbe7b-9831-404e-9285-741dcf1bc0b0', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.07cnqO:/tmp/ceph-asok.07cnqO', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 514fbe7b-9831-404e-9285-741dcf1bc0b0 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.07cnqO:/tmp/ceph-asok.07cnqO -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:07:26,194883247-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:07:26,198248789-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '514fbe7b-9831-404e-9285-741dcf1bc0b0', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.07cnqO:/tmp/ceph-asok.07cnqO', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 514fbe7b-9831-404e-9285-741dcf1bc0b0 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.07cnqO:/tmp/ceph-asok.07cnqO -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:07:32,724247312-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:07:32,728079321-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '514fbe7b-9831-404e-9285-741dcf1bc0b0', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.07cnqO:/tmp/ceph-asok.07cnqO', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 514fbe7b-9831-404e-9285-741dcf1bc0b0 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.07cnqO:/tmp/ceph-asok.07cnqO -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:07:37,002347003-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:07:37,005985167-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '514fbe7b-9831-404e-9285-741dcf1bc0b0', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.07cnqO:/tmp/ceph-asok.07cnqO', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 514fbe7b-9831-404e-9285-741dcf1bc0b0 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.07cnqO:/tmp/ceph-asok.07cnqO -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:07:41,516481245-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:07:41,519666398-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '514fbe7b-9831-404e-9285-741dcf1bc0b0', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.07cnqO:/tmp/ceph-asok.07cnqO', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 514fbe7b-9831-404e-9285-741dcf1bc0b0 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.07cnqO:/tmp/ceph-asok.07cnqO -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:07:44,897853687-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:07:44,901748083-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '514fbe7b-9831-404e-9285-741dcf1bc0b0', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.07cnqO:/tmp/ceph-asok.07cnqO', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 514fbe7b-9831-404e-9285-741dcf1bc0b0 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.07cnqO:/tmp/ceph-asok.07cnqO -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:07:48,677950170-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:07:48,682044221-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '514fbe7b-9831-404e-9285-741dcf1bc0b0', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.07cnqO:/tmp/ceph-asok.07cnqO', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 514fbe7b-9831-404e-9285-741dcf1bc0b0 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.07cnqO:/tmp/ceph-asok.07cnqO -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:07:53,166525744-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:07:53,169829970-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '514fbe7b-9831-404e-9285-741dcf1bc0b0', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.07cnqO:/tmp/ceph-asok.07cnqO', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 514fbe7b-9831-404e-9285-741dcf1bc0b0 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.07cnqO:/tmp/ceph-asok.07cnqO -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:07:56,630327713-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:07:56,634019598-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '514fbe7b-9831-404e-9285-741dcf1bc0b0', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.07cnqO:/tmp/ceph-asok.07cnqO', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 514fbe7b-9831-404e-9285-741dcf1bc0b0 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.07cnqO:/tmp/ceph-asok.07cnqO -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:08:00,099162577-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:08:00,102660718-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '514fbe7b-9831-404e-9285-741dcf1bc0b0', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.07cnqO:/tmp/ceph-asok.07cnqO', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 514fbe7b-9831-404e-9285-741dcf1bc0b0 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.07cnqO:/tmp/ceph-asok.07cnqO -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  155 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:08:03,515622449-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:08:26,776143162-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (5s)
      [===================.........] (remaining: 2s)
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:08:35,033268505-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:08:43,335105596-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:08:51,858395691-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:09:00,154092681-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:09:08,569510168-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:09:17,030427665-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:09:17,039716383-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:09:25,598590434-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:09:25,608271569-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:09:33,937381508-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:09:33,946704581-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:09:42,265033730-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:09:42,273967511-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:09:50,489686297-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:09:50,498882601-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:09:50,506558690-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:09:50,510393593-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:09:50,520728487-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=593703
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_16MB_write.dat.4 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:09:50,527934102-04:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_16MB_write.log.4
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:09:50,547502971-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:09:50,550784785-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '514fbe7b-9831-404e-9285-741dcf1bc0b0', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.07cnqO:/tmp/ceph-asok.07cnqO', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 514fbe7b-9831-404e-9285-741dcf1bc0b0 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.07cnqO:/tmp/ceph-asok.07cnqO -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T19:09:53.000+0000 7f96c809dd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T19:09:53.004+0000 7f96c809dd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T19:09:53.004+0000 7f96c809dd00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T19:09:53.023202+0000 Maintaining 256 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 20 seconds or 0 objects
2021-05-17T19:09:53.023234+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-17T19:09:54.033140+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T19:09:54.033140+0000     0       0         0         0         0         0           -           0
2021-05-17T19:09:55.033273+0000     1      32        32         0         0         0           -           0
2021-05-17T19:09:56.033339+0000     2      61        61         0         0         0           -           0
2021-05-17T19:09:57.033404+0000     3      91        91         0         0         0           -           0
2021-05-17T19:09:58.033469+0000     4     122       122         0         0         0           -           0
2021-05-17T19:09:59.033560+0000     5     150       150         0         0         0           -           0
2021-05-17T19:10:00.033626+0000     6     179       179         0         0         0           -           0
2021-05-17T19:10:01.033692+0000     7     206       206         0         0         0           -           0
2021-05-17T19:10:02.033770+0000     8     237       237         0         0         0           -           0
2021-05-17T19:10:03.033867+0000     9     255       262         7   12.4435   12.4444      8.7633     8.78645
2021-05-17T19:10:04.033934+0000    10     255       293        38   60.7956       496     8.81319     8.82427
2021-05-17T19:10:05.034003+0000    11     255       321        66   95.9931       448     8.87199     8.83001
2021-05-17T19:10:06.034073+0000    12     255       353        98   130.657       512     8.84245     8.84584
2021-05-17T19:10:07.034170+0000    13     255       381       126   155.066       448     8.85647     8.85331
2021-05-17T19:10:08.034239+0000    14     255       411       156   178.273       480     8.81745     8.85199
2021-05-17T19:10:09.034307+0000    15     255       441       186   198.386       480     8.79636     8.84596
2021-05-17T19:10:10.034378+0000    16     255       469       214   213.984       448     8.75877     8.83538
2021-05-17T19:10:11.034472+0000    17     255       494       239   224.925       400     8.93864       8.841
2021-05-17T19:10:12.034543+0000    18     255       523       268   238.205       464     8.82536     8.84618
2021-05-17T19:10:13.034612+0000    19     255       553       298   250.929       480     8.78845     8.84287
2021-05-17T19:10:14.034681+0000 min lat: 3.97351 max lat: 8.97832 avg lat: 8.10407
2021-05-17T19:10:14.034681+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T19:10:14.034681+0000    20     113       584       471   376.772      2768     3.97351     8.10407
2021-05-17T19:10:15.034856+0000 Total time run:         20.0587
Total writes made:      584
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     465.832
Stddev Bandwidth:       610.164
Max bandwidth (MB/sec): 2768
Min bandwidth (MB/sec): 0
Average IOPS:           29
Stddev IOPS:            38.1598
Max IOPS:               173
Min IOPS:               0
Average Latency(s):     6.90675
Stddev Latency(s):      2.77617
Max latency(s):         8.97832
Min latency(s):         0.0919012

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:10:15,656132284-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 593703


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:10:15,661281116-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:10:39,011111589-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 585 objects, 9.1 GiB
    usage:   18 GiB used, 282 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:10:39,020214088-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:10:47,391374938-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 585 objects, 9.1 GiB
    usage:   18 GiB used, 282 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:10:47,401066243-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:10:55,827454166-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 585 objects, 9.1 GiB
    usage:   18 GiB used, 282 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:10:55,836735861-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:11:04,340732308-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 585 objects, 9.1 GiB
    usage:   18 GiB used, 282 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:11:04,350194422-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:11:12,858301841-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 585 objects, 9.1 GiB
    usage:   18 GiB used, 282 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:11:12,867749227-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:11:12,874681018-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:11:12,878762706-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:11:12,889092920-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=595099
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_16MB_seq.dat.4 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:11:12,895208609-04:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_16MB_seq.log.4
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:11:12,914752010-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:11:12,918244210-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '514fbe7b-9831-404e-9285-741dcf1bc0b0', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.07cnqO:/tmp/ceph-asok.07cnqO', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 514fbe7b-9831-404e-9285-741dcf1bc0b0 --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.07cnqO:/tmp/ceph-asok.07cnqO -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T19:11:15.625+0000 7f2078daad00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T19:11:15.629+0000 7f2078daad00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T19:11:15.629+0000 7f2078daad00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T19:11:15.646347+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T19:11:15.646347+0000     0       0         0         0         0         0           -           0
2021-05-17T19:11:16.646477+0000     1     244       244         0         0         0           -           0
2021-05-17T19:11:17.646616+0000     2     255       428       173   1383.72      1384     1.31556     1.18017
2021-05-17T19:11:18.840191+0000     3       4       584       580   2905.47      6512    0.394502     1.09967
2021-05-17T19:11:19.840418+0000 Total time run:       3.19878
Total reads made:     584
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   2921.11
Average IOPS:         182
Stddev IOPS:          214.51
Max IOPS:             407
Min IOPS:             0
Average Latency(s):   1.09482
Max latency(s):       1.42659
Min latency(s):       0.386258

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:11:20,651655809-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 595099


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:11:20,656685326-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:11:44,082107577-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 585 objects, 9.1 GiB
    usage:   18 GiB used, 282 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:11:44,091515659-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:11:52,488206198-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 585 objects, 9.1 GiB
    usage:   18 GiB used, 282 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:11:52,497184874-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:12:00,950532890-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 585 objects, 9.1 GiB
    usage:   18 GiB used, 282 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:12:00,960424932-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:12:09,095868610-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 585 objects, 9.1 GiB
    usage:   18 GiB used, 282 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:12:09,105527213-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:12:17,501363707-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 585 objects, 9.1 GiB
    usage:   18 GiB used, 282 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:12:17,510167665-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:12:17,517428043-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T15:12:17,519970639-04:00][RUNNING][ROUND 5/7/40] object_size=16MB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:12:17,523834447-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:12:17,533595723-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40227\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.398935\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 8440fcab-6398-40eb-b9c1-6fcffc18f0de\nsetting min_mon_release = octopus\nepoch 0\nfsid 8440fcab-6398-40eb-b9c1-6fcffc18f0de\nlast_changed 2021-05-17T12:12:46.647798-0700\ncreated 2021-05-17T12:12:46.647798-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40227/0,v1:10.10.1.2:40228/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.398935 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 c975c3cd-4fa3-46b5-a62a-d6b81852acd2\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 f859cb4e-9cde-4c83-9ea9-6cfb58fabd18\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 4ef9d219-fd1b-4a68-93cd-157cdaa620c7\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42227\n  w/ user/pass: admin / dd57d64c-991a-40e0-a022-c96563cdf5e1\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\nCEPH_DEV=1\n'
[1] 15:12:59 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40227
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.398935
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 8440fcab-6398-40eb-b9c1-6fcffc18f0de
setting min_mon_release = octopus
epoch 0
fsid 8440fcab-6398-40eb-b9c1-6fcffc18f0de
last_changed 2021-05-17T12:12:46.647798-0700
created 2021-05-17T12:12:46.647798-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40227/0,v1:10.10.1.2:40228/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.398935 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 c975c3cd-4fa3-46b5-a62a-d6b81852acd2
0
start osd.0
add osd1 f859cb4e-9cde-4c83-9ea9-6cfb58fabd18
1
start osd.1
add osd2 4ef9d219-fd1b-4a68-93cd-157cdaa620c7
2
start osd.2


restful urls: https://10.10.1.2:42227
  w/ user/pass: admin / dd57d64c-991a-40e0-a022-c96563cdf5e1


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T12:12:18.476-0700 7f7c5f7451c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T12:12:18.476-0700 7f7c5f7451c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T12:12:18.492-0700 7f7e7f6b01c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T12:12:18.492-0700 7f7e7f6b01c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40227,v1:10.10.1.2:40228] --print /tmp/ceph_monmap.398935 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.398935 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.398935 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42227 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.f1zr9GBDlS 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new c975c3cd-4fa3-46b5-a62a-d6b81852acd2 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQA3wKJgF8s3HxAAi2In4sdQ/0oYSC39XGIIUQ== --osd-uuid c975c3cd-4fa3-46b5-a62a-d6b81852acd2 
2021-05-17T12:12:55.880-0700 7f9a8ae62f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T12:12:55.880-0700 7f9a8ae62f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T12:12:55.880-0700 7f9a8ae62f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T12:12:55.924-0700 7f9a8ae62f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new f859cb4e-9cde-4c83-9ea9-6cfb58fabd18 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T12:12:56.236-0700 7fedd0879f00 -1 Falling back to public interface
2021-05-17T12:12:56.248-0700 7fedd0879f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQA4wKJg3khEDhAAgw5MACH1bL/T+RggaAbKqg== --osd-uuid f859cb4e-9cde-4c83-9ea9-6cfb58fabd18 
2021-05-17T12:12:56.560-0700 7f2963f1af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T12:12:56.560-0700 7f2963f1af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T12:12:56.560-0700 7f2963f1af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T12:12:56.612-0700 7f2963f1af00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 4ef9d219-fd1b-4a68-93cd-157cdaa620c7 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T12:12:57.008-0700 7fa51f624f00 -1 Falling back to public interface
2021-05-17T12:12:57.020-0700 7fa51f624f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQA5wKJgJ3KJABAA+mixkduc7oD4GFSt4+RG7Q== --osd-uuid 4ef9d219-fd1b-4a68-93cd-157cdaa620c7 
2021-05-17T12:12:57.336-0700 7f15c8a1ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T12:12:57.336-0700 7f15c8a1ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T12:12:57.336-0700 7f15c8a1ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T12:12:57.448-0700 7f15c8a1ef00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T12:12:57.808-0700 7f28d51b6f00 -1 Falling back to public interface
2021-05-17T12:12:57.824-0700 7f28d51b6f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:12:59,766694334-04:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:12:59,771384654-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:12:59,812126015-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:12:59,814939320-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'db3e6310-75df-47e6-84fa-341dfbf0b34f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.YjDJ4x:/tmp/ceph-asok.YjDJ4x', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid db3e6310-75df-47e6-84fa-341dfbf0b34f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.YjDJ4x:/tmp/ceph-asok.YjDJ4x -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:13:03,350851850-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:13:03,354670863-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'db3e6310-75df-47e6-84fa-341dfbf0b34f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.YjDJ4x:/tmp/ceph-asok.YjDJ4x', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid db3e6310-75df-47e6-84fa-341dfbf0b34f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.YjDJ4x:/tmp/ceph-asok.YjDJ4x -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:13:06,842493298-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:13:06,846216272-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'db3e6310-75df-47e6-84fa-341dfbf0b34f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.YjDJ4x:/tmp/ceph-asok.YjDJ4x', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid db3e6310-75df-47e6-84fa-341dfbf0b34f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.YjDJ4x:/tmp/ceph-asok.YjDJ4x -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:13:10,249157441-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:13:10,253014787-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'db3e6310-75df-47e6-84fa-341dfbf0b34f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.YjDJ4x:/tmp/ceph-asok.YjDJ4x', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid db3e6310-75df-47e6-84fa-341dfbf0b34f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.YjDJ4x:/tmp/ceph-asok.YjDJ4x -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:13:17,137984477-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:13:17,141823639-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'db3e6310-75df-47e6-84fa-341dfbf0b34f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.YjDJ4x:/tmp/ceph-asok.YjDJ4x', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid db3e6310-75df-47e6-84fa-341dfbf0b34f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.YjDJ4x:/tmp/ceph-asok.YjDJ4x -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:13:21,651024947-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:13:21,654748171-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'db3e6310-75df-47e6-84fa-341dfbf0b34f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.YjDJ4x:/tmp/ceph-asok.YjDJ4x', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid db3e6310-75df-47e6-84fa-341dfbf0b34f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.YjDJ4x:/tmp/ceph-asok.YjDJ4x -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:13:26,118782655-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:13:26,122356469-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'db3e6310-75df-47e6-84fa-341dfbf0b34f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.YjDJ4x:/tmp/ceph-asok.YjDJ4x', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid db3e6310-75df-47e6-84fa-341dfbf0b34f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.YjDJ4x:/tmp/ceph-asok.YjDJ4x -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:13:30,480547601-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:13:30,484235128-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'db3e6310-75df-47e6-84fa-341dfbf0b34f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.YjDJ4x:/tmp/ceph-asok.YjDJ4x', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid db3e6310-75df-47e6-84fa-341dfbf0b34f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.YjDJ4x:/tmp/ceph-asok.YjDJ4x -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:13:34,820024400-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:13:34,823379552-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'db3e6310-75df-47e6-84fa-341dfbf0b34f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.YjDJ4x:/tmp/ceph-asok.YjDJ4x', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid db3e6310-75df-47e6-84fa-341dfbf0b34f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.YjDJ4x:/tmp/ceph-asok.YjDJ4x -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:13:39,494379512-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:13:39,497877322-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'db3e6310-75df-47e6-84fa-341dfbf0b34f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.YjDJ4x:/tmp/ceph-asok.YjDJ4x', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid db3e6310-75df-47e6-84fa-341dfbf0b34f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.YjDJ4x:/tmp/ceph-asok.YjDJ4x -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:13:43,832692685-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:13:43,836173002-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'db3e6310-75df-47e6-84fa-341dfbf0b34f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.YjDJ4x:/tmp/ceph-asok.YjDJ4x', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid db3e6310-75df-47e6-84fa-341dfbf0b34f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.YjDJ4x:/tmp/ceph-asok.YjDJ4x -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:13:47,266567171-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:13:47,269949745-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'db3e6310-75df-47e6-84fa-341dfbf0b34f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.YjDJ4x:/tmp/ceph-asok.YjDJ4x', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid db3e6310-75df-47e6-84fa-341dfbf0b34f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.YjDJ4x:/tmp/ceph-asok.YjDJ4x -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  156 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:13:50,873637608-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:14:14,237173291-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:14:22,573020108-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:14:30,970731195-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:14:39,345685584-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:14:47,692265062-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:14:56,014481997-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:14:56,024446084-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:15:04,652718129-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:15:04,662382102-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:15:13,086533606-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:15:13,096274323-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:15:21,364479199-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:15:21,374085023-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:15:29,810765602-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:15:29,820921901-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:15:29,828267168-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:15:29,832279576-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:15:29,841752410-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=601527
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_16MB_write.dat.5 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:15:29,849020152-04:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_16MB_write.log.5
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:15:29,867270394-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:15:29,870773214-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'db3e6310-75df-47e6-84fa-341dfbf0b34f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.YjDJ4x:/tmp/ceph-asok.YjDJ4x', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid db3e6310-75df-47e6-84fa-341dfbf0b34f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.YjDJ4x:/tmp/ceph-asok.YjDJ4x -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T19:15:32.442+0000 7fd618ba8d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T19:15:32.446+0000 7fd618ba8d00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T19:15:32.446+0000 7fd618ba8d00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T19:15:32.461261+0000 Maintaining 256 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 20 seconds or 0 objects
2021-05-17T19:15:32.461290+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-17T19:15:33.464029+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T19:15:33.464029+0000     0       0         0         0         0         0           -           0
2021-05-17T19:15:34.464135+0000     1      31        31         0         0         0           -           0
2021-05-17T19:15:35.464268+0000     2      60        60         0         0         0           -           0
2021-05-17T19:15:36.464398+0000     3      89        89         0         0         0           -           0
2021-05-17T19:15:37.464465+0000     4     117       117         0         0         0           -           0
2021-05-17T19:15:38.464531+0000     5     144       144         0         0         0           -           0
2021-05-17T19:15:39.464594+0000     6     172       172         0         0         0           -           0
2021-05-17T19:15:40.464662+0000     7     199       199         0         0         0           -           0
2021-05-17T19:15:41.464738+0000     8     226       226         0         0         0           -           0
2021-05-17T19:15:42.464807+0000     9     255       255         0         0         0           -           0
2021-05-17T19:15:43.464902+0000    10     255       283        28   44.7963      44.8     9.11157     9.09463
2021-05-17T19:15:44.464969+0000    11     255       310        55   79.9935       432      9.1555     9.10876
2021-05-17T19:15:45.465046+0000    12     255       338        83   110.658       448     9.26125     9.15361
2021-05-17T19:15:46.465118+0000    13     255       366       111   136.604       448     9.18209     9.17866
2021-05-17T19:15:47.465203+0000    14     255       396       141    161.13       480      9.1322     9.18172
2021-05-17T19:15:48.465307+0000    15     255       424       169   180.252       448     9.13812     9.17428
2021-05-17T19:15:49.465411+0000    16     255       452       197   196.984       448     9.00594     9.15942
2021-05-17T19:15:50.465492+0000    17     255       478       223   209.865       416     9.16391     9.15522
2021-05-17T19:15:51.465575+0000    18     255       503       248   220.426       400     9.29794     9.16603
2021-05-17T19:15:52.465680+0000    19     255       534       279   234.928       496     9.16658      9.1727
2021-05-17T19:15:53.465787+0000 min lat: 9.00594 max lat: 9.33204 avg lat: 9.17226
2021-05-17T19:15:53.465787+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T19:15:53.465787+0000    20     255       563       308   246.379       464     9.16449     9.17226
2021-05-17T19:15:54.466088+0000 Total time run:         20.0935
Total writes made:      564
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     449.101
Stddev Bandwidth:       228.557
Max bandwidth (MB/sec): 496
Min bandwidth (MB/sec): 0
Average IOPS:           28
Stddev IOPS:            14.3193
Max IOPS:               31
Min IOPS:               0
Average Latency(s):     7.09476
Stddev Latency(s):      2.90005
Max latency(s):         9.33204
Min latency(s):         0.127461

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:15:55,038847293-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 601527


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:15:55,043883223-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:16:18,370529825-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 565 objects, 8.8 GiB
    usage:   18 GiB used, 282 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:16:18,379838251-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:16:26,570732442-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 565 objects, 8.8 GiB
    usage:   18 GiB used, 282 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:16:26,580408137-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:16:35,091208308-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 565 objects, 8.8 GiB
    usage:   18 GiB used, 282 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:16:35,101081133-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:16:43,574699579-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 565 objects, 8.8 GiB
    usage:   18 GiB used, 282 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:16:43,584158457-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:16:51,822155489-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 565 objects, 8.8 GiB
    usage:   18 GiB used, 282 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:16:51,831553884-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:16:51,839235443-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:16:51,843223004-04:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:16:51,853178114-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=602922
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_16MB_seq.dat.5 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:16:51,860186318-04:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_16MB_seq.log.5
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:16:51,878989199-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:16:51,882725678-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'db3e6310-75df-47e6-84fa-341dfbf0b34f', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.YjDJ4x:/tmp/ceph-asok.YjDJ4x', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid db3e6310-75df-47e6-84fa-341dfbf0b34f --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.YjDJ4x:/tmp/ceph-asok.YjDJ4x -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T19:16:54.454+0000 7fddfbc3fd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T19:16:54.458+0000 7fddfbc3fd00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T19:16:54.458+0000 7fddfbc3fd00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T19:16:54.478226+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T19:16:54.478226+0000     0       0         0         0         0         0           -           0
2021-05-17T19:16:55.478375+0000     1     215       215         0         0         0           -           0
2021-05-17T19:16:56.478479+0000     2     255       387       132    1055.8      1056      1.4215     1.27693
2021-05-17T19:16:57.478600+0000     3     255       555       300   1599.73      2688      1.5828     1.40729
2021-05-17T19:16:58.478792+0000 Total time run:       3.4346
Total reads made:     564
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   2627.38
Average IOPS:         164
Stddev IOPS:          84.6404
Max IOPS:             168
Min IOPS:             0
Average Latency(s):   1.21175
Max latency(s):       1.59269
Min latency(s):       0.377519

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:16:59,313887106-04:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 602922


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:16:59,318795747-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:17:22,592948052-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 565 objects, 8.8 GiB
    usage:   18 GiB used, 282 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:17:22,602321850-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:17:31,161995337-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 565 objects, 8.8 GiB
    usage:   18 GiB used, 282 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:17:31,172041388-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:17:39,557921833-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 565 objects, 8.8 GiB
    usage:   18 GiB used, 282 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:17:39,567726701-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:17:48,036803246-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 565 objects, 8.8 GiB
    usage:   18 GiB used, 282 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:17:48,046404802-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:17:56,384896329-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 565 objects, 8.8 GiB
    usage:   18 GiB used, 282 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:17:56,394537189-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:17:56,402223207-04:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T15:17:56,406085041-04:00][RUNNING][ROUND 1/8/40] object_size=64MB[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:17:56,410099642-04:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /mnt/sda8/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:17:56,419663097-04:00] DEBUG: command from stdin[0m
# /mnt/sda8/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40507\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.400062\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 1a98cbac-b162-494b-89d4-c824a1b15617\nsetting min_mon_release = octopus\nepoch 0\nfsid 1a98cbac-b162-494b-89d4-c824a1b15617\nlast_changed 2021-05-17T12:18:25.512626-0700\ncreated 2021-05-17T12:18:25.512626-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40507/0,v1:10.10.1.2:40508/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.400062 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 3bd8da4c-8a0e-4145-9dce-c79c8ee4e831\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 75495fd1-adc1-4902-a087-6142f16ec43e\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 4e89777e-b359-4e66-b283-fec000939e74\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42507\n  w/ user/pass: admin / 8d9dd382-292c-4e56-a122-07efa9149c22\n\n'
10.10.1.2: b'\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 15:18:38 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40507
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.400062
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 1a98cbac-b162-494b-89d4-c824a1b15617
setting min_mon_release = octopus
epoch 0
fsid 1a98cbac-b162-494b-89d4-c824a1b15617
last_changed 2021-05-17T12:18:25.512626-0700
created 2021-05-17T12:18:25.512626-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40507/0,v1:10.10.1.2:40508/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.400062 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 3bd8da4c-8a0e-4145-9dce-c79c8ee4e831
0
start osd.0
add osd1 75495fd1-adc1-4902-a087-6142f16ec43e
1
start osd.1
add osd2 4e89777e-b359-4e66-b283-fec000939e74
2
start osd.2


restful urls: https://10.10.1.2:42507
  w/ user/pass: admin / 8d9dd382-292c-4e56-a122-07efa9149c22


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T12:17:57.339-0700 7f9d3d1791c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T12:17:57.339-0700 7f9d3d1791c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T12:17:57.355-0700 7fd23838b1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T12:17:57.355-0700 7fd23838b1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40507,v1:10.10.1.2:40508] --print /tmp/ceph_monmap.400062 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.400062 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.400062 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42507 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.WOoTfGD2zR 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 3bd8da4c-8a0e-4145-9dce-c79c8ee4e831 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCKwaJgj3ZEGhAA5Sn3INds0SQ1+B8wE4MtyA== --osd-uuid 3bd8da4c-8a0e-4145-9dce-c79c8ee4e831 
2021-05-17T12:18:34.823-0700 7fa97c9f4f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T12:18:34.827-0700 7fa97c9f4f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T12:18:34.827-0700 7fa97c9f4f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T12:18:34.871-0700 7fa97c9f4f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 75495fd1-adc1-4902-a087-6142f16ec43e -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T12:18:35.183-0700 7f5450616f00 -1 Falling back to public interface
2021-05-17T12:18:35.195-0700 7f5450616f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCLwaJgHv7LChAAYOOs1rwVBB1ru9fD7UIGgg== --osd-uuid 75495fd1-adc1-4902-a087-6142f16ec43e 
2021-05-17T12:18:35.519-0700 7ff8e6ec1f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T12:18:35.519-0700 7ff8e6ec1f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T12:18:35.519-0700 7ff8e6ec1f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T12:18:35.575-0700 7ff8e6ec1f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 4e89777e-b359-4e66-b283-fec000939e74 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T12:18:35.935-0700 7f7b167a5f00 -1 Falling back to public interface
2021-05-17T12:18:35.947-0700 7f7b167a5f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCLwaJgH/ehNxAA7GF4NlieDXSafYQuinQpjw== --osd-uuid 4e89777e-b359-4e66-b283-fec000939e74 
2021-05-17T12:18:36.275-0700 7f8c55c15f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T12:18:36.275-0700 7f8c55c15f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T12:18:36.275-0700 7f8c55c15f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T12:18:36.367-0700 7f8c55c15f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T12:18:36.679-0700 7fc004b28f00 -1 Falling back to public interface
2021-05-17T12:18:36.695-0700 7fc004b28f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:18:38,670900355-04:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:18:38,675449280-04:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:18:38,714804186-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:18:38,718135294-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2d5a62f5-e707-4acf-b349-79a283e9297e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ukzobm:/tmp/ceph-asok.Ukzobm', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2d5a62f5-e707-4acf-b349-79a283e9297e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ukzobm:/tmp/ceph-asok.Ukzobm -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:18:42,090138478-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:18:42,093809234-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2d5a62f5-e707-4acf-b349-79a283e9297e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ukzobm:/tmp/ceph-asok.Ukzobm', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2d5a62f5-e707-4acf-b349-79a283e9297e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ukzobm:/tmp/ceph-asok.Ukzobm -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:18:45,612439050-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:18:45,615748186-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2d5a62f5-e707-4acf-b349-79a283e9297e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ukzobm:/tmp/ceph-asok.Ukzobm', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2d5a62f5-e707-4acf-b349-79a283e9297e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ukzobm:/tmp/ceph-asok.Ukzobm -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:18:49,019019396-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:18:49,022324104-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2d5a62f5-e707-4acf-b349-79a283e9297e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ukzobm:/tmp/ceph-asok.Ukzobm', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2d5a62f5-e707-4acf-b349-79a283e9297e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ukzobm:/tmp/ceph-asok.Ukzobm -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:18:55,906878026-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:18:55,910518003-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2d5a62f5-e707-4acf-b349-79a283e9297e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ukzobm:/tmp/ceph-asok.Ukzobm', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2d5a62f5-e707-4acf-b349-79a283e9297e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ukzobm:/tmp/ceph-asok.Ukzobm -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:18:59,478168272-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:18:59,481849888-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2d5a62f5-e707-4acf-b349-79a283e9297e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ukzobm:/tmp/ceph-asok.Ukzobm', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2d5a62f5-e707-4acf-b349-79a283e9297e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ukzobm:/tmp/ceph-asok.Ukzobm -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:19:03,924158823-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:19:03,928543370-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2d5a62f5-e707-4acf-b349-79a283e9297e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ukzobm:/tmp/ceph-asok.Ukzobm', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2d5a62f5-e707-4acf-b349-79a283e9297e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ukzobm:/tmp/ceph-asok.Ukzobm -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:19:08,379691479-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:19:08,382545611-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2d5a62f5-e707-4acf-b349-79a283e9297e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ukzobm:/tmp/ceph-asok.Ukzobm', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2d5a62f5-e707-4acf-b349-79a283e9297e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ukzobm:/tmp/ceph-asok.Ukzobm -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:19:12,634140118-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:19:12,637793110-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2d5a62f5-e707-4acf-b349-79a283e9297e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ukzobm:/tmp/ceph-asok.Ukzobm', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2d5a62f5-e707-4acf-b349-79a283e9297e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ukzobm:/tmp/ceph-asok.Ukzobm -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:19:16,123925711-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:19:16,127729877-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2d5a62f5-e707-4acf-b349-79a283e9297e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ukzobm:/tmp/ceph-asok.Ukzobm', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2d5a62f5-e707-4acf-b349-79a283e9297e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ukzobm:/tmp/ceph-asok.Ukzobm -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:19:20,703182547-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:19:20,706390103-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2d5a62f5-e707-4acf-b349-79a283e9297e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ukzobm:/tmp/ceph-asok.Ukzobm', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2d5a62f5-e707-4acf-b349-79a283e9297e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ukzobm:/tmp/ceph-asok.Ukzobm -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:19:24,267605744-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:19:24,271044644-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2d5a62f5-e707-4acf-b349-79a283e9297e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ukzobm:/tmp/ceph-asok.Ukzobm', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2d5a62f5-e707-4acf-b349-79a283e9297e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ukzobm:/tmp/ceph-asok.Ukzobm -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  156 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:19:27,722914645-04:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:19:51,069780423-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (9s)
      [===================.........] (remaining: 4s)
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:19:59,373770732-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:20:07,711704813-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:20:15,982812433-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:20:24,504636063-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:20:33,007750017-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:20:41,309893547-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:20:41,318821227-04:00] INFO: >> Counting down idle state [1/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:20:49,768179834-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:20:49,777940068-04:00] INFO: >> Counting down idle state [2/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:20:58,499801590-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:20:58,509386905-04:00] INFO: >> Counting down idle state [3/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:21:07,154297738-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:21:07,164289568-04:00] INFO: >> Counting down idle state [4/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:21:15,611269647-04:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:21:15,620810419-04:00] INFO: >> Counting down idle state [5/5][0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:21:15,628540229-04:00] INFO: > The cluster is idle now.[0m


[1;33mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:21:15,632525796-04:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:21:15,642288174-04:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=609535
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /mnt/sda8/ceph-research/scripts/benchmarks/output/sys_activity_64MB_write.dat.1 2

[1;32mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:21:15,650127790-04:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 96-127 /mnt/sda8/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 67108864 -O 67108864 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /mnt/sda8/ceph-research/scripts/benchmarks/output/rados_bench_64MB_write.log.1
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:21:15,670157064-04:00] DEBUG: HOST_PARAMS: [][0m
[1;30mnode-1.bluefield2.ucsc-cmps107-pg0.clemson.cloudlab.us	[2021-05-17T15:21:15,673859048-04:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2d5a62f5-e707-4acf-b349-79a283e9297e', '--mount', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ukzobm:/tmp/ceph-asok.Ukzobm', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '67108864', '-O', '67108864', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2d5a62f5-e707-4acf-b349-79a283e9297e --mount /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /mnt/sda8/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Ukzobm:/tmp/ceph-asok.Ukzobm -- rados bench 20 write --pool bench_rados -b 67108864 -O 67108864 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:7bda5ef5bf4c06e8b720afefe24b22dd0fe2fdf7f3c34da265dc9238578563ff
2021-05-17T19:21:18.175+0000 7fd65765ed00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T19:21:18.183+0000 7fd65765ed00 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T19:21:18.183+0000 7fd65765ed00 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T19:21:18.205288+0000 Maintaining 256 concurrent writes of 67108864 bytes to objects of size 67108864 for up to 20 seconds or 0 objects
2021-05-17T19:21:18.205352+0000 Object prefix: benchmark_data_node-1.bluefield2.ucsc-cmps10_7
2021-05-17T19:21:22.340966+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T19:21:22.340966+0000     0       0         0         0         0         0           -           0
2021-05-17T19:21:23.341171+0000     1      10        10         0         0         0           -           0
2021-05-17T19:21:24.341240+0000     2      21        21         0         0         0           -           0
2021-05-17T19:21:25.341306+0000     3      31        31         0         0         0           -           0
2021-05-17T19:21:26.341397+0000     4      40        40         0         0         0           -           0
2021-05-17T19:21:27.341485+0000     5      50        50         0         0         0           -           0
2021-05-17T19:21:28.341565+0000     6      61        61         0         0         0           -           0
2021-05-17T19:21:29.341631+0000     7      72        72         0         0         0           -           0
2021-05-17T19:21:30.341699+0000     8      85        85         0         0         0           -           0
2021-05-17T19:21:31.341776+0000     9      97        97         0         0         0           -           0
2021-05-17T19:21:32.341854+0000    10     110       110         0         0         0           -           0
2021-05-17T19:21:33.341926+0000    11     122       122         0         0         0           -           0
2021-05-17T19:21:34.341991+0000    12     135       135         0         0         0           -           0
2021-05-17T19:21:35.342069+0000    13     148       148         0         0         0           -           0
2021-05-17T19:21:36.342135+0000    14     159       159         0         0         0           -           0
2021-05-17T19:21:37.342201+0000    15     169       169         0         0         0           -           0
2021-05-17T19:21:38.342271+0000    16     178       178         0         0         0           -           0
2021-05-17T19:21:39.342366+0000    17     187       187         0         0         0           -           0
2021-05-17T19:21:40.342445+0000    18     197       197         0         0         0           -           0
2021-05-17T19:21:41.342513+0000    19     208       208         0         0         0           -           0
2021-05-17T19:21:42.342590+0000 min lat: 9999 max lat: 0 avg lat: 0
2021-05-17T19:21:42.342590+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T19:21:42.342590+0000    20     220       220         0         0         0           -           0
2021-05-17T19:21:43.342677+0000    21     231       231         0         0         0           -           0
2021-05-17T19:21:44.342745+0000    22     242       242         0         0         0           -           0
2021-05-17T19:21:45.342818+0000    23     253       253         0         0         0           -           0
error during benchmark: (90) Message too long
error 90: (90) Message too long
