

[1;7;39;49m[2021-05-17T00:01:55,937757703-07:00][RUNNING][ROUND 1/1/40] object_size=4KB[0m


[1;33mlocalhost.localdomain	[2021-05-17T00:01:55,943676455-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T00:01:55,959990094-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40548\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.295639\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 3572f93d-48c4-4bc7-b574-364c3b230d81\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 3572f93d-48c4-4bc7-b574-364c3b230d81\nlast_changed 2021-05-17T00:02:05.815471-0700\ncreated 2021-05-17T00:02:05.815471-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40548/0,v1:10.10.1.2:40549/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.295639 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 95d3d9f4-2705-4f8c-b70c-026318e8a0c0\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 d4c165b1-cd3d-4c8c-8931-5a0e7180f651\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 b2536274-0590-4d8d-96bd-98f1a78cfda3\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\n'
10.10.1.2: b'restful urls: https://10.10.1.2:42548\n  w/ user/pass: admin / 6c23d724-4abd-4b3d-b3cd-44f22920a912\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 00:02:21 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40548
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.295639
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 3572f93d-48c4-4bc7-b574-364c3b230d81
setting min_mon_release = octopus
epoch 0
fsid 3572f93d-48c4-4bc7-b574-364c3b230d81
last_changed 2021-05-17T00:02:05.815471-0700
created 2021-05-17T00:02:05.815471-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40548/0,v1:10.10.1.2:40549/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.295639 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 95d3d9f4-2705-4f8c-b70c-026318e8a0c0
0
start osd.0
add osd1 d4c165b1-cd3d-4c8c-8931-5a0e7180f651
1
start osd.1
add osd2 b2536274-0590-4d8d-96bd-98f1a78cfda3
2
start osd.2


restful urls: https://10.10.1.2:42548
  w/ user/pass: admin / 6c23d724-4abd-4b3d-b3cd-44f22920a912


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T00:01:58.027-0700 7f2c52b3f1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T00:01:58.027-0700 7f2c52b3f1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T00:01:58.047-0700 7f5c1f9061c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T00:01:58.047-0700 7f5c1f9061c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40548,v1:10.10.1.2:40549] --print /tmp/ceph_monmap.295639 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.295639 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.295639 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42548 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.OGRa089dSd 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 95d3d9f4-2705-4f8c-b70c-026318e8a0c0 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQD2FKJgsGdzJhAAq3FnQpCHTHpc2dbZomwXsg== --osd-uuid 95d3d9f4-2705-4f8c-b70c-026318e8a0c0 
2021-05-17T00:02:14.971-0700 7fcb7242ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T00:02:14.971-0700 7fcb7242ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T00:02:14.971-0700 7fcb7242ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T00:02:15.023-0700 7fcb7242ff00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new d4c165b1-cd3d-4c8c-8931-5a0e7180f651 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T00:02:15.303-0700 7f03f08b8f00 -1 Falling back to public interface
2021-05-17T00:02:15.315-0700 7f03f08b8f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQD3FKJgVn4GEhAA2mSCXtE+IjE8zAtOE0yU6w== --osd-uuid d4c165b1-cd3d-4c8c-8931-5a0e7180f651 
2021-05-17T00:02:15.635-0700 7f4fd60bdf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T00:02:15.635-0700 7f4fd60bdf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T00:02:15.635-0700 7f4fd60bdf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T00:02:15.711-0700 7f4fd60bdf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new b2536274-0590-4d8d-96bd-98f1a78cfda3 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T00:02:15.987-0700 7f5f7c551f00 -1 Falling back to public interface
2021-05-17T00:02:16.003-0700 7f5f7c551f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQD3FKJgkE3nOhAAtHjRDOt2LDEmkCJdm6A9Ug== --osd-uuid b2536274-0590-4d8d-96bd-98f1a78cfda3 
2021-05-17T00:02:16.327-0700 7fed03c8df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T00:02:16.327-0700 7fed03c8df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T00:02:16.327-0700 7fed03c8df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T00:02:16.375-0700 7fed03c8df00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T00:02:16.735-0700 7fcddd693f00 -1 Falling back to public interface
2021-05-17T00:02:16.747-0700 7fcddd693f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T00:02:21,382230050-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T00:02:21,387351085-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T00:02:21,479009899-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:02:21,485745931-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cc82bf40-3d25-4bc1-9a73-d4a9827727a7', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wYC3Fl:/tmp/ceph-asok.wYC3Fl', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cc82bf40-3d25-4bc1-9a73-d4a9827727a7 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wYC3Fl:/tmp/ceph-asok.wYC3Fl -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T00:02:26,915083984-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:02:26,921839954-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cc82bf40-3d25-4bc1-9a73-d4a9827727a7', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wYC3Fl:/tmp/ceph-asok.wYC3Fl', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cc82bf40-3d25-4bc1-9a73-d4a9827727a7 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wYC3Fl:/tmp/ceph-asok.wYC3Fl -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T00:02:29,810795715-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:02:29,817242902-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cc82bf40-3d25-4bc1-9a73-d4a9827727a7', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wYC3Fl:/tmp/ceph-asok.wYC3Fl', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cc82bf40-3d25-4bc1-9a73-d4a9827727a7 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wYC3Fl:/tmp/ceph-asok.wYC3Fl -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T00:02:32,942460477-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:02:32,948877194-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cc82bf40-3d25-4bc1-9a73-d4a9827727a7', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wYC3Fl:/tmp/ceph-asok.wYC3Fl', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cc82bf40-3d25-4bc1-9a73-d4a9827727a7 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wYC3Fl:/tmp/ceph-asok.wYC3Fl -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T00:02:38,620921252-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:02:38,627357817-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cc82bf40-3d25-4bc1-9a73-d4a9827727a7', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wYC3Fl:/tmp/ceph-asok.wYC3Fl', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cc82bf40-3d25-4bc1-9a73-d4a9827727a7 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wYC3Fl:/tmp/ceph-asok.wYC3Fl -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T00:02:41,480725540-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:02:41,487869431-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cc82bf40-3d25-4bc1-9a73-d4a9827727a7', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wYC3Fl:/tmp/ceph-asok.wYC3Fl', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cc82bf40-3d25-4bc1-9a73-d4a9827727a7 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wYC3Fl:/tmp/ceph-asok.wYC3Fl -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T00:02:44,693825672-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:02:44,700179348-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cc82bf40-3d25-4bc1-9a73-d4a9827727a7', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wYC3Fl:/tmp/ceph-asok.wYC3Fl', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cc82bf40-3d25-4bc1-9a73-d4a9827727a7 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wYC3Fl:/tmp/ceph-asok.wYC3Fl -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T00:02:48,180013526-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:02:48,186454922-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cc82bf40-3d25-4bc1-9a73-d4a9827727a7', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wYC3Fl:/tmp/ceph-asok.wYC3Fl', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cc82bf40-3d25-4bc1-9a73-d4a9827727a7 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wYC3Fl:/tmp/ceph-asok.wYC3Fl -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T00:02:51,399818121-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:02:51,406406122-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cc82bf40-3d25-4bc1-9a73-d4a9827727a7', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wYC3Fl:/tmp/ceph-asok.wYC3Fl', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cc82bf40-3d25-4bc1-9a73-d4a9827727a7 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wYC3Fl:/tmp/ceph-asok.wYC3Fl -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T00:02:54,875827946-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:02:54,882535293-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cc82bf40-3d25-4bc1-9a73-d4a9827727a7', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wYC3Fl:/tmp/ceph-asok.wYC3Fl', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cc82bf40-3d25-4bc1-9a73-d4a9827727a7 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wYC3Fl:/tmp/ceph-asok.wYC3Fl -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T00:02:58,276089536-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:02:58,282338030-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cc82bf40-3d25-4bc1-9a73-d4a9827727a7', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wYC3Fl:/tmp/ceph-asok.wYC3Fl', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cc82bf40-3d25-4bc1-9a73-d4a9827727a7 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wYC3Fl:/tmp/ceph-asok.wYC3Fl -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T00:03:00,973959163-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:03:00,980441021-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cc82bf40-3d25-4bc1-9a73-d4a9827727a7', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wYC3Fl:/tmp/ceph-asok.wYC3Fl', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cc82bf40-3d25-4bc1-9a73-d4a9827727a7 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wYC3Fl:/tmp/ceph-asok.wYC3Fl -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T00:03:03,811424787-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:03:26,864219446-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (4s)
      [=============...............] (remaining: 5s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:03:34,839466756-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:03:42,781799195-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:03:50,751543261-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:03:58,804676298-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:04:06,792888908-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:04:14,667322886-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:04:14,678854427-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:04:22,593172700-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:04:22,605674697-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:04:30,657246877-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:04:30,668581465-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:04:38,629529267-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:04:38,640924239-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:04:46,514123279-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:04:46,525588254-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:04:46,534025096-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T00:04:46,539019765-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:04:46,550479218-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=3888246
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4KB_write.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-17T00:04:46,559646445-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4KB_write.log.1
[1;30mlocalhost.localdomain	[2021-05-17T00:04:46,599755651-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:04:46,606295659-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cc82bf40-3d25-4bc1-9a73-d4a9827727a7', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wYC3Fl:/tmp/ceph-asok.wYC3Fl', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cc82bf40-3d25-4bc1-9a73-d4a9827727a7 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wYC3Fl:/tmp/ceph-asok.wYC3Fl -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T07:04:48.169+0000 ffff900a3010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T07:04:48.177+0000 ffff900a3010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T07:04:48.177+0000 ffff900a3010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T07:04:48.203022+0000 Maintaining 256 concurrent writes of 4096 bytes to objects of size 4096 for up to 20 seconds or 0 objects
2021-05-17T07:04:48.203075+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T07:04:48.205334+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T07:04:48.205334+0000     0       0         0         0         0         0           -           0
2021-05-17T07:04:49.205478+0000     1     255     19259     19004   74.2382   74.2344  0.00276128   0.0130122
2021-05-17T07:04:50.205641+0000     2     255     36964     36709   71.6933   69.1602  0.00782856   0.0137862
2021-05-17T07:04:51.205850+0000     3     255     55706     55451   72.1941   73.2109  0.00635893   0.0137267
2021-05-17T07:04:52.206066+0000     4     255     73988     73733   71.9952   71.4141  0.00703727    0.013717
2021-05-17T07:04:53.206223+0000     5     255     91083     90828   70.9495   66.7773    0.019788   0.0140154
2021-05-17T07:04:54.206471+0000     6     255    109677    109422   71.2271   72.6328  0.00608282   0.0139341
2021-05-17T07:04:55.206752+0000     7     255    128282    128027   71.4312   72.6758   0.0063194   0.0139424
2021-05-17T07:04:56.206956+0000     8     255    146673    146418   71.4804   71.8398  0.00126803   0.0139309
2021-05-17T07:04:57.207108+0000     9     255    165035    164780   71.5066   71.7266   0.0637654   0.0139334
2021-05-17T07:04:58.207263+0000    10     256    181104    180848   70.6315   62.7656   0.0927533   0.0140733
2021-05-17T07:04:59.207422+0000    11     255    197841    197586   70.1534   65.3828   0.0662722   0.0141841
2021-05-17T07:05:00.207579+0000    12     255    216724    216469   70.4531   73.7617  0.00412923   0.0141481
2021-05-17T07:05:01.207748+0000    13     255    234262    234007   70.3026   68.5078  0.00594916   0.0141894
2021-05-17T07:05:02.208009+0000    14     255    252559    252304   70.3849   71.4727  0.00407123   0.0141583
2021-05-17T07:05:03.208200+0000    15     256    269175    268919   70.0185   64.9023  0.00429309   0.0142464
2021-05-17T07:05:04.208376+0000    16     256    285059    284803   69.5196   62.0469   0.0621953   0.0143573
2021-05-17T07:05:05.208553+0000    17     255    301663    301408    69.245   64.8633    0.010213   0.0144112
2021-05-17T07:05:06.208863+0000    18     255    317650    317395   68.8664   62.4492   0.0117012   0.0145057
2021-05-17T07:05:07.209078+0000    19     255    334506    334251   68.7065   65.8438   0.0127466   0.0145422
2021-05-17T07:05:08.209310+0000 min lat: 0.000663324 max lat: 0.20542 avg lat: 0.0145426
2021-05-17T07:05:08.209310+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T07:05:08.209310+0000    20     203    351614    351411    68.622   67.0312   0.0803023   0.0145426
2021-05-17T07:05:09.209545+0000 Total time run:         20.0562
Total writes made:      351614
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     68.4823
Stddev Bandwidth:       4.07462
Max bandwidth (MB/sec): 74.2344
Min bandwidth (MB/sec): 62.0469
Average IOPS:           17531
Stddev IOPS:            1043.1
Max IOPS:               19004
Min IOPS:               15884
Average Latency(s):     0.0145691
Stddev Latency(s):      0.0227263
Max latency(s):         0.20542
Min latency(s):         0.000663324

[1;32mlocalhost.localdomain	[2021-05-17T00:05:09,492981381-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 3888246


[1;33mlocalhost.localdomain	[2021-05-17T00:05:09,498548953-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:05:32,884016737-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 351.62k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:05:32,896029140-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:05:40,890722840-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 351.62k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:05:40,902419889-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:05:49,070013620-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 351.62k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:05:49,081834921-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:05:57,429300434-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 351.62k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:05:57,442911900-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:06:05,576139992-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 351.62k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:06:05,588003870-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:06:05,596484250-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T00:06:05,601680808-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:06:05,613565329-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=3891576
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4KB_seq.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-17T00:06:05,625124790-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4KB_seq.log.1
[1;30mlocalhost.localdomain	[2021-05-17T00:06:05,664215397-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:06:05,670621596-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'cc82bf40-3d25-4bc1-9a73-d4a9827727a7', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wYC3Fl:/tmp/ceph-asok.wYC3Fl', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid cc82bf40-3d25-4bc1-9a73-d4a9827727a7 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wYC3Fl:/tmp/ceph-asok.wYC3Fl -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T07:06:07.159+0000 ffff99d32010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T07:06:07.167+0000 ffff99d32010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T07:06:07.167+0000 ffff99d32010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T07:06:07.187124+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T07:06:07.187124+0000     0       0         0         0         0         0           -           0
2021-05-17T07:06:08.187285+0000     1     255     30124     29869   116.642   116.676   0.0050025  0.00851285
2021-05-17T07:06:09.187454+0000     2     255     60725     60470   118.078   119.535  0.00416276  0.00842688
2021-05-17T07:06:10.187614+0000     3     256     90841     90585   117.925   117.637    0.020792  0.00841709
2021-05-17T07:06:11.187787+0000     4     255    124963    124708   121.761   133.293  0.00222982  0.00818896
2021-05-17T07:06:12.187954+0000     5     255    153846    153591    119.97   112.824   0.0134964  0.00831716
2021-05-17T07:06:13.188132+0000     6     255    176680    176425   114.838   89.1953  0.00283299  0.00868617
2021-05-17T07:06:14.188425+0000     7     255    204722    204467   114.077   109.539  0.00709264   0.0087507
2021-05-17T07:06:15.188609+0000     8     255    233336    233081   113.786   111.773  0.00680521  0.00877499
2021-05-17T07:06:16.189183+0000     9     255    263474    263219   114.217   117.727  0.00541042    0.008742
2021-05-17T07:06:17.189374+0000    10     256    292694    292438   114.206   114.137   0.0115335  0.00869123
2021-05-17T07:06:18.189551+0000    11     255    322310    322055    114.34   115.691   0.0189511  0.00873035
2021-05-17T07:06:19.189739+0000    12     256    350478    350222   113.979   110.027  0.00844377  0.00876188
2021-05-17T07:06:20.190029+0000 Total time run:       12.0454
Total reads made:     351614
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   114.027
Average IOPS:         29190
Stddev IOPS:          2556.86
Max IOPS:             34123
Min IOPS:             22834
Average Latency(s):   0.00875878
Max latency(s):       0.109317
Min latency(s):       0.000291319

[1;32mlocalhost.localdomain	[2021-05-17T00:06:20,493672111-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 3891576


[1;33mlocalhost.localdomain	[2021-05-17T00:06:20,500453030-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:06:43,576628880-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 351.62k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:06:43,590333888-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:06:51,779940918-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 351.62k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:06:51,793717818-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:06:59,726515973-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 351.62k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:06:59,740492092-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:07:07,696846067-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 351.62k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:07:07,710808863-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:07:15,794620252-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 351.62k objects, 1.3 GiB
    usage:   2.7 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:07:15,808402013-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:07:15,819030354-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T00:07:15,823046286-07:00][RUNNING][ROUND 2/1/40] object_size=4KB[0m


[1;33mlocalhost.localdomain	[2021-05-17T00:07:15,829509360-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T00:07:15,845642008-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40136\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.301963\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 6b3b1105-5a4e-4426-bcc4-c3be7bd9b0b3\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 6b3b1105-5a4e-4426-bcc4-c3be7bd9b0b3\nlast_changed 2021-05-17T00:07:41.976958-0700\ncreated 2021-05-17T00:07:41.976958-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40136/0,v1:10.10.1.2:40137/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.301963 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 3e76ea3f-156d-46d0-a49d-dc28fa1ae78b\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 b089ca4f-05fe-443e-bd55-1b687d4ce520\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 1bc19a32-7c51-4d81-9593-99ac996d4548\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42136\n  w/ user/pass: admin / 45969f32-3fe1-41ec-8a5a-3e1207623e1d\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 00:07:56 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40136
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.301963
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 6b3b1105-5a4e-4426-bcc4-c3be7bd9b0b3
setting min_mon_release = octopus
epoch 0
fsid 6b3b1105-5a4e-4426-bcc4-c3be7bd9b0b3
last_changed 2021-05-17T00:07:41.976958-0700
created 2021-05-17T00:07:41.976958-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40136/0,v1:10.10.1.2:40137/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.301963 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 3e76ea3f-156d-46d0-a49d-dc28fa1ae78b
0
start osd.0
add osd1 b089ca4f-05fe-443e-bd55-1b687d4ce520
1
start osd.1
add osd2 1bc19a32-7c51-4d81-9593-99ac996d4548
2
start osd.2


restful urls: https://10.10.1.2:42136
  w/ user/pass: admin / 45969f32-3fe1-41ec-8a5a-3e1207623e1d


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T00:07:16.826-0700 7f802f3691c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T00:07:16.826-0700 7f802f3691c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T00:07:16.842-0700 7f0f6d5431c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T00:07:16.842-0700 7f0f6d5431c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40136,v1:10.10.1.2:40137] --print /tmp/ceph_monmap.301963 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.301963 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.301963 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42136 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.lvUC8zvO50 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 3e76ea3f-156d-46d0-a49d-dc28fa1ae78b -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBGFqJg+t6OKRAAQe+RXDt8mjYydpJYLkEYvA== --osd-uuid 3e76ea3f-156d-46d0-a49d-dc28fa1ae78b 
2021-05-17T00:07:51.034-0700 7f4e2b902f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T00:07:51.034-0700 7f4e2b902f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T00:07:51.034-0700 7f4e2b902f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T00:07:51.110-0700 7f4e2b902f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new b089ca4f-05fe-443e-bd55-1b687d4ce520 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T00:07:51.362-0700 7f0740a53f00 -1 Falling back to public interface
2021-05-17T00:07:51.374-0700 7f0740a53f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBHFqJgh46lFRAAP0/jUdvJcCboMT64lRXWJQ== --osd-uuid b089ca4f-05fe-443e-bd55-1b687d4ce520 
2021-05-17T00:07:51.702-0700 7f2ad5da5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T00:07:51.702-0700 7f2ad5da5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T00:07:51.702-0700 7f2ad5da5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T00:07:51.746-0700 7f2ad5da5f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 1bc19a32-7c51-4d81-9593-99ac996d4548 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T00:07:51.998-0700 7f5ac2e2af00 -1 Falling back to public interface
2021-05-17T00:07:52.010-0700 7f5ac2e2af00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBIFqJg8zkEABAAFUwD7piy/Qbm+KeDkxYBsA== --osd-uuid 1bc19a32-7c51-4d81-9593-99ac996d4548 
2021-05-17T00:07:52.598-0700 7f372c732f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T00:07:52.598-0700 7f372c732f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T00:07:52.598-0700 7f372c732f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T00:07:52.666-0700 7f372c732f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T00:07:53.010-0700 7fb14da83f00 -1 Falling back to public interface
2021-05-17T00:07:53.026-0700 7fb14da83f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T00:07:56,900693487-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T00:07:56,907137628-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T00:07:56,989080463-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:07:56,995750678-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e6855b6e-7a7d-4f6f-bcb5-a717d32a4661', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DsZtev:/tmp/ceph-asok.DsZtev', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e6855b6e-7a7d-4f6f-bcb5-a717d32a4661 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DsZtev:/tmp/ceph-asok.DsZtev -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T00:07:59,795276566-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:07:59,801985277-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e6855b6e-7a7d-4f6f-bcb5-a717d32a4661', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DsZtev:/tmp/ceph-asok.DsZtev', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e6855b6e-7a7d-4f6f-bcb5-a717d32a4661 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DsZtev:/tmp/ceph-asok.DsZtev -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T00:08:02,626794130-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:08:02,632972276-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e6855b6e-7a7d-4f6f-bcb5-a717d32a4661', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DsZtev:/tmp/ceph-asok.DsZtev', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e6855b6e-7a7d-4f6f-bcb5-a717d32a4661 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DsZtev:/tmp/ceph-asok.DsZtev -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T00:08:05,478925397-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:08:05,486134654-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e6855b6e-7a7d-4f6f-bcb5-a717d32a4661', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DsZtev:/tmp/ceph-asok.DsZtev', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e6855b6e-7a7d-4f6f-bcb5-a717d32a4661 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DsZtev:/tmp/ceph-asok.DsZtev -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T00:08:11,065867863-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:08:11,072380731-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e6855b6e-7a7d-4f6f-bcb5-a717d32a4661', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DsZtev:/tmp/ceph-asok.DsZtev', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e6855b6e-7a7d-4f6f-bcb5-a717d32a4661 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DsZtev:/tmp/ceph-asok.DsZtev -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T00:08:14,836423862-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:08:14,842671664-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e6855b6e-7a7d-4f6f-bcb5-a717d32a4661', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DsZtev:/tmp/ceph-asok.DsZtev', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e6855b6e-7a7d-4f6f-bcb5-a717d32a4661 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DsZtev:/tmp/ceph-asok.DsZtev -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T00:08:18,189885803-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:08:18,196390495-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e6855b6e-7a7d-4f6f-bcb5-a717d32a4661', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DsZtev:/tmp/ceph-asok.DsZtev', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e6855b6e-7a7d-4f6f-bcb5-a717d32a4661 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DsZtev:/tmp/ceph-asok.DsZtev -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T00:08:21,344207864-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:08:21,350697553-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e6855b6e-7a7d-4f6f-bcb5-a717d32a4661', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DsZtev:/tmp/ceph-asok.DsZtev', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e6855b6e-7a7d-4f6f-bcb5-a717d32a4661 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DsZtev:/tmp/ceph-asok.DsZtev -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T00:08:24,684336650-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:08:24,690890160-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e6855b6e-7a7d-4f6f-bcb5-a717d32a4661', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DsZtev:/tmp/ceph-asok.DsZtev', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e6855b6e-7a7d-4f6f-bcb5-a717d32a4661 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DsZtev:/tmp/ceph-asok.DsZtev -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T00:08:27,924463789-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:08:27,930653727-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e6855b6e-7a7d-4f6f-bcb5-a717d32a4661', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DsZtev:/tmp/ceph-asok.DsZtev', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e6855b6e-7a7d-4f6f-bcb5-a717d32a4661 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DsZtev:/tmp/ceph-asok.DsZtev -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T00:08:31,350784540-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:08:31,357168446-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e6855b6e-7a7d-4f6f-bcb5-a717d32a4661', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DsZtev:/tmp/ceph-asok.DsZtev', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e6855b6e-7a7d-4f6f-bcb5-a717d32a4661 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DsZtev:/tmp/ceph-asok.DsZtev -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T00:08:34,066209751-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:08:34,072667484-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e6855b6e-7a7d-4f6f-bcb5-a717d32a4661', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DsZtev:/tmp/ceph-asok.DsZtev', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e6855b6e-7a7d-4f6f-bcb5-a717d32a4661 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DsZtev:/tmp/ceph-asok.DsZtev -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T00:08:36,663611650-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:08:59,717119397-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   199 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:09:07,679189943-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:09:15,638147273-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (9s)
      [=======.....................] (remaining: 28s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:09:23,598234741-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [=======.....................] (remaining: 52s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:09:31,743172456-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [=======.....................] (remaining: 63s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:09:39,698988355-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [========....................] (remaining: 82s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:09:47,643968888-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (40s)
      [========....................] (remaining: 94s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:09:55,896079625-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   235 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [==============..............] (remaining: 44s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:10:03,829197517-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:10:03,848612020-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:10:11,743979150-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:10:11,758276924-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:10:19,798965831-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:10:19,813158691-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:10:27,876771257-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:10:27,891125140-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:10:36,008574087-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:10:36,022630035-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:10:36,033690360-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T00:10:36,040016881-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:10:36,054594167-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=3905184
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4KB_write.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-17T00:10:36,066973143-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4KB_write.log.2
[1;30mlocalhost.localdomain	[2021-05-17T00:10:36,106451985-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:10:36,112918435-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e6855b6e-7a7d-4f6f-bcb5-a717d32a4661', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DsZtev:/tmp/ceph-asok.DsZtev', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e6855b6e-7a7d-4f6f-bcb5-a717d32a4661 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DsZtev:/tmp/ceph-asok.DsZtev -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T07:10:37.625+0000 ffff992c5010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T07:10:37.633+0000 ffff992c5010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T07:10:37.633+0000 ffff992c5010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T07:10:37.651849+0000 Maintaining 256 concurrent writes of 4096 bytes to objects of size 4096 for up to 20 seconds or 0 objects
2021-05-17T07:10:37.651899+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T07:10:37.654311+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T07:10:37.654311+0000     0       0         0         0         0         0           -           0
2021-05-17T07:10:38.654510+0000     1     255     17288     17033   66.5311   66.5352   0.0172715   0.0145519
2021-05-17T07:10:39.654699+0000     2     255     34638     34383   67.1459   67.7734  0.00164058    0.014476
2021-05-17T07:10:40.654924+0000     3     255     51606     51351   66.8527   66.2812  0.00832188   0.0147226
2021-05-17T07:10:41.655092+0000     4     255     68734     68479   66.8633   66.9062  0.00269997   0.0148228
2021-05-17T07:10:42.655274+0000     5     255     86032     85777   67.0022   67.5703  0.00139587   0.0147391
2021-05-17T07:10:43.655467+0000     6     255    103316    103061   67.0856   67.5156 0.000947052   0.0147617
2021-05-17T07:10:44.655647+0000     7     255    120909    120654   67.3177   68.7227  0.00636178   0.0147795
2021-05-17T07:10:45.655988+0000     8     255    138835    138580    67.653   70.0234  0.00260539   0.0146805
2021-05-17T07:10:46.656148+0000     9     255    156286    156031    67.709    68.168  0.00394006   0.0147111
2021-05-17T07:10:47.656387+0000    10     255    174019    173764   67.8634   69.2695  0.00208992     0.01465
2021-05-17T07:10:48.656600+0000    11     255    188857    188602    66.962   57.9609   0.0102269   0.0149008
2021-05-17T07:10:49.656771+0000    12     255    204952    204697   66.6202   62.8711  0.00290484    0.014976
2021-05-17T07:10:50.656982+0000    13     255    221810    221555     66.56   65.8516  0.00745134   0.0149966
2021-05-17T07:10:51.657195+0000    14     255    237488    237233   66.1793   61.2422   0.0163837   0.0150917
2021-05-17T07:10:52.657388+0000    15     255    255581    255326   66.4781   70.6758  0.00377029   0.0149933
2021-05-17T07:10:53.657562+0000    16     255    267556    267301   65.2463   46.7773   0.0143079   0.0152839
2021-05-17T07:10:54.657747+0000    17     255    282081    281826   64.7452   56.7383  0.00198044   0.0153985
2021-05-17T07:10:55.657910+0000    18     256    297527    297271   64.4995    60.332   0.0108814   0.0154531
2021-05-17T07:10:56.658071+0000    19     255    314391    314136   64.5716   65.8789  0.00190595   0.0154344
2021-05-17T07:10:57.658246+0000 min lat: 0.000677411 max lat: 0.235438 avg lat: 0.0155303
2021-05-17T07:10:57.658246+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T07:10:57.658246+0000    20     149    329542    329393   64.3224   59.5977  0.00640029   0.0155303
2021-05-17T07:10:58.658488+0000 Total time run:         20.0401
Total writes made:      329542
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     64.2348
Stddev Bandwidth:       5.80368
Max bandwidth (MB/sec): 70.6758
Min bandwidth (MB/sec): 46.7773
Average IOPS:           16444
Stddev IOPS:            1485.74
Max IOPS:               18093
Min IOPS:               11975
Average Latency(s):     0.0155364
Stddev Latency(s):      0.0298254
Max latency(s):         0.235438
Min latency(s):         0.000677411

[1;32mlocalhost.localdomain	[2021-05-17T00:10:58,951805763-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 3905184


[1;33mlocalhost.localdomain	[2021-05-17T00:10:58,958885038-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:11:22,301150181-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 329.54k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:11:22,315359454-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:11:30,714779853-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 329.54k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:11:30,729003304-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:11:38,874955276-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 329.54k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:11:38,888940171-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:11:46,722580288-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 329.54k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:11:46,736760921-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:11:54,738454789-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 329.54k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:11:54,752415995-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:11:54,763398246-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T00:11:54,769763789-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:11:54,784328383-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=3908563
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4KB_seq.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-17T00:11:54,796264723-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4KB_seq.log.2
[1;30mlocalhost.localdomain	[2021-05-17T00:11:54,834718665-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:11:54,840910883-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'e6855b6e-7a7d-4f6f-bcb5-a717d32a4661', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DsZtev:/tmp/ceph-asok.DsZtev', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid e6855b6e-7a7d-4f6f-bcb5-a717d32a4661 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DsZtev:/tmp/ceph-asok.DsZtev -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T07:11:56.422+0000 ffffb133d010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T07:11:56.430+0000 ffffb133d010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T07:11:56.430+0000 ffffb133d010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T07:11:56.451616+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T07:11:56.451616+0000     0       0         0         0         0         0           -           0
2021-05-17T07:11:57.451817+0000     1     255     30003     29748   116.165   116.203  0.00687114  0.00852136
2021-05-17T07:11:58.452405+0000     2     256     58396     58140   113.503   110.906   0.0188552  0.00876753
2021-05-17T07:11:59.452583+0000     3     255     91880     91625    119.26   130.801   0.0140318  0.00835485
2021-05-17T07:12:00.453858+0000     4     255    124726    124471   121.482   128.305   0.0168734  0.00820552
2021-05-17T07:12:01.454021+0000     5     255    155775    155520   121.438   121.285  0.00236424  0.00821222
2021-05-17T07:12:02.454191+0000     6     255    180424    180169   117.245   96.2852  0.00811196  0.00851156
2021-05-17T07:12:03.454416+0000     7     255    206993    206738   115.319   103.785   0.0140057   0.0086532
2021-05-17T07:12:04.454734+0000     8     255    234614    234359   114.387   107.895   0.0065156  0.00872649
2021-05-17T07:12:05.454947+0000     9     255    257531    257276   111.622   89.5195  0.00215345  0.00894038
2021-05-17T07:12:06.455174+0000    10     255    282962    282707   110.392   99.3398    0.004233  0.00904355
2021-05-17T07:12:07.455385+0000    11     255    310781    310526   110.233   108.668  0.00337841  0.00905512
2021-05-17T07:12:08.455614+0000 Total time run:       11.7337
Total reads made:     329542
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   109.707
Average IOPS:         28085
Stddev IOPS:          3337.43
Max IOPS:             33485
Min IOPS:             22917
Average Latency(s):   0.0091041
Max latency(s):       0.156308
Min latency(s):       0.000298101

[1;32mlocalhost.localdomain	[2021-05-17T00:12:08,763859111-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 3908563


[1;33mlocalhost.localdomain	[2021-05-17T00:12:08,770833248-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:12:32,079819359-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 329.54k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:12:32,094221865-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:12:40,197625820-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 329.54k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:12:40,211828688-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:12:48,546287425-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 329.54k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:12:48,560674344-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:12:56,516656334-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 329.54k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:12:56,530635273-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:13:04,625012986-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 329.54k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:13:04,639537973-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:13:04,650486950-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T00:13:04,654478667-07:00][RUNNING][ROUND 3/1/40] object_size=4KB[0m


[1;33mlocalhost.localdomain	[2021-05-17T00:13:04,660815975-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T00:13:04,676862327-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40272\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.303067\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid a80d8e18-8bdc-4d60-af57-bb3f77c05e49\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid a80d8e18-8bdc-4d60-af57-bb3f77c05e49\nlast_changed 2021-05-17T00:13:29.073560-0700\ncreated 2021-05-17T00:13:29.073560-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40272/0,v1:10.10.1.2:40273/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.303067 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 55853838-d0b4-4c93-96c7-2f658c203a05\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 de58d1a6-f80b-4a1e-8c4f-083ad298183e\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 2cfe4c82-ca71-461c-b1ce-995a3449c3a4\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42272\n  w/ user/pass: admin / 898123a2-e6eb-4e2a-a0ca-6fbfcbcbd3ec\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 00:13:43 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40272
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.303067
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid a80d8e18-8bdc-4d60-af57-bb3f77c05e49
setting min_mon_release = octopus
epoch 0
fsid a80d8e18-8bdc-4d60-af57-bb3f77c05e49
last_changed 2021-05-17T00:13:29.073560-0700
created 2021-05-17T00:13:29.073560-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40272/0,v1:10.10.1.2:40273/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.303067 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 55853838-d0b4-4c93-96c7-2f658c203a05
0
start osd.0
add osd1 de58d1a6-f80b-4a1e-8c4f-083ad298183e
1
start osd.1
add osd2 2cfe4c82-ca71-461c-b1ce-995a3449c3a4
2
start osd.2


restful urls: https://10.10.1.2:42272
  w/ user/pass: admin / 898123a2-e6eb-4e2a-a0ca-6fbfcbcbd3ec


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T00:13:05.689-0700 7f4a20b9c1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T00:13:05.689-0700 7f4a20b9c1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T00:13:05.705-0700 7fdf3add71c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T00:13:05.705-0700 7fdf3add71c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40272,v1:10.10.1.2:40273] --print /tmp/ceph_monmap.303067 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.303067 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.303067 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42272 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.jQwhsFb5fg 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 55853838-d0b4-4c93-96c7-2f658c203a05 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQChF6JgPU1CKRAA1lmEX1hCYT4WdOJ3QMMPXQ== --osd-uuid 55853838-d0b4-4c93-96c7-2f658c203a05 
2021-05-17T00:13:38.010-0700 7fdf1450cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T00:13:38.010-0700 7fdf1450cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T00:13:38.010-0700 7fdf1450cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T00:13:38.062-0700 7fdf1450cf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new de58d1a6-f80b-4a1e-8c4f-083ad298183e -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T00:13:38.318-0700 7f2475edef00 -1 Falling back to public interface
2021-05-17T00:13:38.330-0700 7f2475edef00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCiF6Jgi5PUEhAAnMd6N0gNPOvaMhiwO+CUDg== --osd-uuid de58d1a6-f80b-4a1e-8c4f-083ad298183e 
2021-05-17T00:13:38.662-0700 7fa0af270f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T00:13:38.662-0700 7fa0af270f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T00:13:38.662-0700 7fa0af270f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T00:13:38.706-0700 7fa0af270f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 2cfe4c82-ca71-461c-b1ce-995a3449c3a4 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T00:13:38.994-0700 7f9fd5552f00 -1 Falling back to public interface
2021-05-17T00:13:39.006-0700 7f9fd5552f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCiF6JgTCQqOxAAWe54JhdCQerzczxQDzbnvg== --osd-uuid 2cfe4c82-ca71-461c-b1ce-995a3449c3a4 
2021-05-17T00:13:39.306-0700 7f7b08207f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T00:13:39.306-0700 7f7b08207f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T00:13:39.306-0700 7f7b08207f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T00:13:39.362-0700 7f7b08207f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T00:13:39.658-0700 7f4ba945df00 -1 Falling back to public interface
2021-05-17T00:13:39.674-0700 7f4ba945df00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T00:13:43,632990958-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T00:13:43,639608028-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T00:13:43,720385330-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:13:43,727024424-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b13ad062-e2a5-40fd-b00d-fb8898c7004a', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cGWZbS:/tmp/ceph-asok.cGWZbS', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b13ad062-e2a5-40fd-b00d-fb8898c7004a --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cGWZbS:/tmp/ceph-asok.cGWZbS -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T00:13:46,519039938-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:13:46,526413948-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b13ad062-e2a5-40fd-b00d-fb8898c7004a', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cGWZbS:/tmp/ceph-asok.cGWZbS', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b13ad062-e2a5-40fd-b00d-fb8898c7004a --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cGWZbS:/tmp/ceph-asok.cGWZbS -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T00:13:49,291709993-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:13:49,298420484-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b13ad062-e2a5-40fd-b00d-fb8898c7004a', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cGWZbS:/tmp/ceph-asok.cGWZbS', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b13ad062-e2a5-40fd-b00d-fb8898c7004a --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cGWZbS:/tmp/ceph-asok.cGWZbS -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T00:13:52,020813893-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:13:52,027379549-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b13ad062-e2a5-40fd-b00d-fb8898c7004a', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cGWZbS:/tmp/ceph-asok.cGWZbS', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b13ad062-e2a5-40fd-b00d-fb8898c7004a --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cGWZbS:/tmp/ceph-asok.cGWZbS -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T00:13:57,630980587-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:13:57,637626566-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b13ad062-e2a5-40fd-b00d-fb8898c7004a', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cGWZbS:/tmp/ceph-asok.cGWZbS', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b13ad062-e2a5-40fd-b00d-fb8898c7004a --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cGWZbS:/tmp/ceph-asok.cGWZbS -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T00:14:01,391903245-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:14:01,398262870-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b13ad062-e2a5-40fd-b00d-fb8898c7004a', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cGWZbS:/tmp/ceph-asok.cGWZbS', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b13ad062-e2a5-40fd-b00d-fb8898c7004a --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cGWZbS:/tmp/ceph-asok.cGWZbS -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T00:14:04,682315052-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:14:04,688778885-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b13ad062-e2a5-40fd-b00d-fb8898c7004a', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cGWZbS:/tmp/ceph-asok.cGWZbS', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b13ad062-e2a5-40fd-b00d-fb8898c7004a --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cGWZbS:/tmp/ceph-asok.cGWZbS -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T00:14:08,395171024-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:14:08,401481020-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b13ad062-e2a5-40fd-b00d-fb8898c7004a', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cGWZbS:/tmp/ceph-asok.cGWZbS', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b13ad062-e2a5-40fd-b00d-fb8898c7004a --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cGWZbS:/tmp/ceph-asok.cGWZbS -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T00:14:11,683837828-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:14:11,690336017-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b13ad062-e2a5-40fd-b00d-fb8898c7004a', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cGWZbS:/tmp/ceph-asok.cGWZbS', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b13ad062-e2a5-40fd-b00d-fb8898c7004a --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cGWZbS:/tmp/ceph-asok.cGWZbS -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T00:14:14,889149884-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:14:14,895692315-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b13ad062-e2a5-40fd-b00d-fb8898c7004a', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cGWZbS:/tmp/ceph-asok.cGWZbS', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b13ad062-e2a5-40fd-b00d-fb8898c7004a --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cGWZbS:/tmp/ceph-asok.cGWZbS -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T00:14:18,349110279-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:14:18,355423321-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b13ad062-e2a5-40fd-b00d-fb8898c7004a', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cGWZbS:/tmp/ceph-asok.cGWZbS', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b13ad062-e2a5-40fd-b00d-fb8898c7004a --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cGWZbS:/tmp/ceph-asok.cGWZbS -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T00:14:21,127026463-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:14:21,133340299-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b13ad062-e2a5-40fd-b00d-fb8898c7004a', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cGWZbS:/tmp/ceph-asok.cGWZbS', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b13ad062-e2a5-40fd-b00d-fb8898c7004a --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cGWZbS:/tmp/ceph-asok.cGWZbS -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   47 KiB   0 B   0 B   0 B  100 GiB     0  0.95   70      up          osd.2  
                       TOTAL  300 GiB  151 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.95/1.03  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T00:14:23,836805940-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:14:46,738573496-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   188 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:14:54,704858236-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   209 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:15:02,883414850-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   209 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=======.....................] (remaining: 27s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:15:10,746056083-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   209 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [========....................] (remaining: 47s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:15:18,758728867-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   209 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [========....................] (remaining: 57s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:15:26,641283859-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   209 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [=========...................] (remaining: 71s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:15:34,732051398-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   209 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (40s)
      [=========...................] (remaining: 81s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:15:42,853958364-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [===============.............] (remaining: 41s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:15:50,796819692-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:15:50,811283529-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:15:58,826785326-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:15:58,841371255-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:16:06,977987350-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:16:06,992002204-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:16:14,936909492-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:16:14,951326101-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:16:22,976874196-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:16:22,991029774-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:16:23,001807417-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T00:16:23,008026166-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:16:23,022318238-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=3922168
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4KB_write.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-17T00:16:23,034678338-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4KB_write.log.3
[1;30mlocalhost.localdomain	[2021-05-17T00:16:23,074117103-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:16:23,080458828-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b13ad062-e2a5-40fd-b00d-fb8898c7004a', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cGWZbS:/tmp/ceph-asok.cGWZbS', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b13ad062-e2a5-40fd-b00d-fb8898c7004a --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cGWZbS:/tmp/ceph-asok.cGWZbS -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T07:16:24.681+0000 ffff919b0010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T07:16:24.689+0000 ffff919b0010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T07:16:24.689+0000 ffff919b0010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T07:16:24.705803+0000 Maintaining 256 concurrent writes of 4096 bytes to objects of size 4096 for up to 20 seconds or 0 objects
2021-05-17T07:16:24.705864+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T07:16:24.708217+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T07:16:24.708217+0000     0       0         0         0         0         0           -           0
2021-05-17T07:16:25.708390+0000     1     255     17440     17185    67.126   67.1289    0.104728   0.0141646
2021-05-17T07:16:26.708658+0000     2     256     34777     34521   67.4133   67.7188  0.00188966    0.014443
2021-05-17T07:16:27.708812+0000     3     255     51898     51643   67.2331   66.8828   0.0011372   0.0145709
2021-05-17T07:16:28.708976+0000     4     255     68644     68389   66.7756   65.4141  0.00480848   0.0147543
2021-05-17T07:16:29.709134+0000     5     255     85255     85000   66.3958   64.8867  0.00264171   0.0148846
2021-05-17T07:16:30.709305+0000     6     256    101810    101554   66.1053   64.6641  0.00129867   0.0149819
2021-05-17T07:16:31.709485+0000     7     255    119409    119154   66.4814     68.75   0.0876654   0.0149573
2021-05-17T07:16:32.709757+0000     8     255    137240    136985   66.8754   69.6523   0.0526407   0.0148985
2021-05-17T07:16:33.710160+0000     9     256    155277    155021   67.2699   70.4531  0.00514726   0.0148024
2021-05-17T07:16:34.710333+0000    10     256    171554    171298      66.9    63.582    0.113135   0.0148301
2021-05-17T07:16:35.710496+0000    11     255    188050    187795   66.6755   64.4414  0.00916859   0.0149586
2021-05-17T07:16:36.710653+0000    12     255    203690    203435   66.2096   61.0938  0.00485625   0.0150612
2021-05-17T07:16:37.710827+0000    13     255    219602    219347    65.897   62.1562   0.0259774   0.0151427
2021-05-17T07:16:38.710985+0000    14     255    234912    234657   65.4612   59.8047  0.00148182   0.0152319
2021-05-17T07:16:39.711165+0000    15     255    251496    251241   65.4151   64.7812  0.00269616   0.0152501
2021-05-17T07:16:40.711374+0000    16     255    268759    268504   65.5403   67.4336  0.00225609   0.0152107
2021-05-17T07:16:41.711646+0000    17     255    285101    284846    65.439   63.8359  0.00105999    0.015219
2021-05-17T07:16:42.711844+0000    18     255    300757    300502   65.2004   61.1562  0.00483221   0.0152884
2021-05-17T07:16:43.712032+0000    19     256    317008    316752   65.1091   63.4766  0.00141876   0.0153017
2021-05-17T07:16:44.712237+0000 min lat: 0.000645756 max lat: 0.219672 avg lat: 0.0153592
2021-05-17T07:16:44.712237+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T07:16:44.712237+0000    20     223    332525    332302   64.8901   60.7422    0.128849   0.0153592
2021-05-17T07:16:45.712512+0000 Total time run:         20.0919
Total writes made:      332525
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     64.6494
Stddev Bandwidth:       3.04553
Max bandwidth (MB/sec): 70.4531
Min bandwidth (MB/sec): 59.8047
Average IOPS:           16550
Stddev IOPS:            779.655
Max IOPS:               18036
Min IOPS:               15310
Average Latency(s):     0.0154215
Stddev Latency(s):      0.031725
Max latency(s):         0.219672
Min latency(s):         0.000645756

[1;32mlocalhost.localdomain	[2021-05-17T00:16:46,212758860-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 3922168


[1;33mlocalhost.localdomain	[2021-05-17T00:16:46,219829530-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:17:09,443111094-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 332.53k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:17:09,458458732-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:17:17,772128991-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 332.53k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:17:17,786251472-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:17:27,998403130-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 332.53k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:17:28,012878787-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:17:36,009658655-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 332.53k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:17:36,023787527-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:17:44,125397432-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 332.53k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:17:44,139371883-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:17:44,150806382-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T00:17:44,157549740-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:17:44,172567650-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=3925724
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4KB_seq.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-17T00:17:44,184859217-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4KB_seq.log.3
[1;30mlocalhost.localdomain	[2021-05-17T00:17:44,224260110-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:17:44,231182944-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'b13ad062-e2a5-40fd-b00d-fb8898c7004a', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cGWZbS:/tmp/ceph-asok.cGWZbS', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid b13ad062-e2a5-40fd-b00d-fb8898c7004a --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cGWZbS:/tmp/ceph-asok.cGWZbS -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T07:17:45.714+0000 ffff9be43010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T07:17:45.722+0000 ffff9be43010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T07:17:45.722+0000 ffff9be43010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T07:17:45.743691+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T07:17:45.743691+0000     0       0         0         0         0         0           -           0
2021-05-17T07:17:46.743857+0000     1     256     25893     25637   100.115   100.145   0.0111021  0.00990263
2021-05-17T07:17:47.744035+0000     2     255     57275     57020   111.341    122.59   0.0118001  0.00894276
2021-05-17T07:17:48.744203+0000     3     255     89329     89074   115.957   125.211  0.00365347  0.00859476
2021-05-17T07:17:49.744371+0000     4     255    123524    123269   120.356   133.574   0.0115212  0.00828723
2021-05-17T07:17:50.745401+0000     5     255    154428    154173   120.403   120.719   0.0592515  0.00821718
2021-05-17T07:17:51.745641+0000     6     255    185528    185273   120.579   121.484  0.00186742  0.00826661
2021-05-17T07:17:52.746409+0000     7     255    215071    214816   119.826   115.402   0.0114674  0.00832989
2021-05-17T07:17:53.746806+0000     8     255    235411    235156   114.776   79.4531   0.0127178  0.00869592
2021-05-17T07:17:54.746997+0000     9     255    260494    260239   112.908   97.9805    0.010055  0.00884142
2021-05-17T07:17:55.747180+0000    10     256    286232    285976   111.669   100.535 0.000428879   0.0088649
2021-05-17T07:17:56.747340+0000    11     255    310932    310677   110.288   96.4883  0.00784371    0.009052
2021-05-17T07:17:57.747634+0000 Total time run:       11.8969
Total reads made:     332525
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   109.182
Average IOPS:         27950
Stddev IOPS:          4180.95
Max IOPS:             34195
Min IOPS:             20340
Average Latency(s):   0.00914591
Max latency(s):       0.198563
Min latency(s):       0.000282979

[1;32mlocalhost.localdomain	[2021-05-17T00:17:58,067701848-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 3925724


[1;33mlocalhost.localdomain	[2021-05-17T00:17:58,074613130-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:18:21,322866134-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 332.53k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:18:21,337085741-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:18:29,486203723-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 332.53k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:18:29,500552757-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:18:37,672565796-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 332.53k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:18:37,686738865-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:18:46,145062893-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 332.53k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:18:46,159181727-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:18:54,083832059-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 332.53k objects, 1.3 GiB
    usage:   2.5 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:18:54,102818418-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:18:54,115813229-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T00:18:54,120788038-07:00][RUNNING][ROUND 4/1/40] object_size=4KB[0m


[1;33mlocalhost.localdomain	[2021-05-17T00:18:54,129252132-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T00:18:54,147503800-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40388\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.304185\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 7a60d798-d94b-4350-bcfe-e639b9168e4b\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 7a60d798-d94b-4350-bcfe-e639b9168e4b\nlast_changed 2021-05-17T00:19:19.204245-0700\ncreated 2021-05-17T00:19:19.204245-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40388/0,v1:10.10.1.2:40389/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.304185 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 c48c4e9f-b94e-435d-a868-77ee3c16a170\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 5dc73899-b36d-4ec1-9878-0b263f1f2ede\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 ec9578ee-736f-449a-bd29-8917673aa845\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42388\n  w/ user/pass: admin / 650a7c25-d457-4b0f-ac78-a88ca72a37f9\n'
10.10.1.2: b'\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 00:19:33 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40388
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.304185
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 7a60d798-d94b-4350-bcfe-e639b9168e4b
setting min_mon_release = octopus
epoch 0
fsid 7a60d798-d94b-4350-bcfe-e639b9168e4b
last_changed 2021-05-17T00:19:19.204245-0700
created 2021-05-17T00:19:19.204245-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40388/0,v1:10.10.1.2:40389/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.304185 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 c48c4e9f-b94e-435d-a868-77ee3c16a170
0
start osd.0
add osd1 5dc73899-b36d-4ec1-9878-0b263f1f2ede
1
start osd.1
add osd2 ec9578ee-736f-449a-bd29-8917673aa845
2
start osd.2


restful urls: https://10.10.1.2:42388
  w/ user/pass: admin / 650a7c25-d457-4b0f-ac78-a88ca72a37f9


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T00:18:55.161-0700 7f5d76cc81c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T00:18:55.161-0700 7f5d76cc81c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T00:18:55.181-0700 7fb7fdb281c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T00:18:55.181-0700 7fb7fdb281c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40388,v1:10.10.1.2:40389] --print /tmp/ceph_monmap.304185 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.304185 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.304185 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42388 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.xwxiSrD7Wg 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new c48c4e9f-b94e-435d-a868-77ee3c16a170 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQD/GKJgX+ZgNhAATL2iDI5qbnzFL2ObMb5GAg== --osd-uuid c48c4e9f-b94e-435d-a868-77ee3c16a170 
2021-05-17T00:19:28.245-0700 7fdb4030ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T00:19:28.245-0700 7fdb4030ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T00:19:28.245-0700 7fdb4030ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T00:19:28.309-0700 7fdb4030ff00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 5dc73899-b36d-4ec1-9878-0b263f1f2ede -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T00:19:28.585-0700 7f69be11ef00 -1 Falling back to public interface
2021-05-17T00:19:28.597-0700 7f69be11ef00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAAGaJg6sQEIxAAxHhAj6qE6yYKZzhYdb8j9A== --osd-uuid 5dc73899-b36d-4ec1-9878-0b263f1f2ede 
2021-05-17T00:19:28.917-0700 7f2e81e3cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T00:19:28.917-0700 7f2e81e3cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T00:19:28.917-0700 7f2e81e3cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T00:19:28.977-0700 7f2e81e3cf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new ec9578ee-736f-449a-bd29-8917673aa845 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T00:19:29.253-0700 7f3b384edf00 -1 Falling back to public interface
2021-05-17T00:19:29.269-0700 7f3b384edf00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQABGaJg8zhWDxAARS06jZ336/qFPfm3s+gmGQ== --osd-uuid ec9578ee-736f-449a-bd29-8917673aa845 
2021-05-17T00:19:29.569-0700 7fb353471f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T00:19:29.569-0700 7fb353471f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T00:19:29.569-0700 7fb353471f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T00:19:29.625-0700 7fb353471f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T00:19:29.981-0700 7f0e056b2f00 -1 Falling back to public interface
2021-05-17T00:19:29.997-0700 7f0e056b2f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T00:19:33,852628122-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T00:19:33,859507073-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T00:19:33,942144632-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:19:33,948661090-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '81a0fb48-29ed-4314-ac6f-ddc62acf52f3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XaX6si:/tmp/ceph-asok.XaX6si', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 81a0fb48-29ed-4314-ac6f-ddc62acf52f3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XaX6si:/tmp/ceph-asok.XaX6si -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T00:19:36,738388865-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:19:36,744853834-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '81a0fb48-29ed-4314-ac6f-ddc62acf52f3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XaX6si:/tmp/ceph-asok.XaX6si', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 81a0fb48-29ed-4314-ac6f-ddc62acf52f3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XaX6si:/tmp/ceph-asok.XaX6si -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T00:19:39,661077790-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:19:39,667270868-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '81a0fb48-29ed-4314-ac6f-ddc62acf52f3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XaX6si:/tmp/ceph-asok.XaX6si', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 81a0fb48-29ed-4314-ac6f-ddc62acf52f3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XaX6si:/tmp/ceph-asok.XaX6si -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T00:19:42,410358020-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:19:42,416801295-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '81a0fb48-29ed-4314-ac6f-ddc62acf52f3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XaX6si:/tmp/ceph-asok.XaX6si', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 81a0fb48-29ed-4314-ac6f-ddc62acf52f3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XaX6si:/tmp/ceph-asok.XaX6si -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T00:19:48,279556880-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:19:48,286065552-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '81a0fb48-29ed-4314-ac6f-ddc62acf52f3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XaX6si:/tmp/ceph-asok.XaX6si', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 81a0fb48-29ed-4314-ac6f-ddc62acf52f3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XaX6si:/tmp/ceph-asok.XaX6si -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T00:19:51,859551919-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:19:51,865881371-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '81a0fb48-29ed-4314-ac6f-ddc62acf52f3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XaX6si:/tmp/ceph-asok.XaX6si', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 81a0fb48-29ed-4314-ac6f-ddc62acf52f3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XaX6si:/tmp/ceph-asok.XaX6si -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T00:19:55,217978031-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:19:55,224876019-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '81a0fb48-29ed-4314-ac6f-ddc62acf52f3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XaX6si:/tmp/ceph-asok.XaX6si', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 81a0fb48-29ed-4314-ac6f-ddc62acf52f3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XaX6si:/tmp/ceph-asok.XaX6si -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T00:19:58,612492852-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:19:58,619021282-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '81a0fb48-29ed-4314-ac6f-ddc62acf52f3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XaX6si:/tmp/ceph-asok.XaX6si', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 81a0fb48-29ed-4314-ac6f-ddc62acf52f3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XaX6si:/tmp/ceph-asok.XaX6si -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T00:20:02,002286606-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:20:02,008552236-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '81a0fb48-29ed-4314-ac6f-ddc62acf52f3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XaX6si:/tmp/ceph-asok.XaX6si', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 81a0fb48-29ed-4314-ac6f-ddc62acf52f3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XaX6si:/tmp/ceph-asok.XaX6si -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T00:20:05,299459582-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:20:05,306070632-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '81a0fb48-29ed-4314-ac6f-ddc62acf52f3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XaX6si:/tmp/ceph-asok.XaX6si', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 81a0fb48-29ed-4314-ac6f-ddc62acf52f3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XaX6si:/tmp/ceph-asok.XaX6si -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T00:20:08,655948795-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:20:08,662597396-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '81a0fb48-29ed-4314-ac6f-ddc62acf52f3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XaX6si:/tmp/ceph-asok.XaX6si', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 81a0fb48-29ed-4314-ac6f-ddc62acf52f3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XaX6si:/tmp/ceph-asok.XaX6si -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T00:20:11,331244234-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:20:11,338158485-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '81a0fb48-29ed-4314-ac6f-ddc62acf52f3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XaX6si:/tmp/ceph-asok.XaX6si', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 81a0fb48-29ed-4314-ac6f-ddc62acf52f3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XaX6si:/tmp/ceph-asok.XaX6si -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   47 KiB   0 B   0 B   0 B  100 GiB     0  0.95   70      up          osd.2  
                       TOTAL  300 GiB  151 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.95/1.03  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T00:20:14,037205691-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:20:37,012046672-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   198 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:20:45,166532892-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:20:53,078431388-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:21:01,070582606-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:21:09,053967412-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:21:17,209893511-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:21:25,159083351-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:21:33,132600388-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:21:33,147063423-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:21:41,403870190-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:21:41,418147703-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:21:49,507647575-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:21:49,522290776-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:21:57,515368422-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:21:57,530036798-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:22:05,507898748-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:22:05,522252144-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:22:05,533705288-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T00:22:05,540321498-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:22:05,554972455-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=3938847
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4KB_write.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-17T00:22:05,567698222-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4KB_write.log.4
[1;30mlocalhost.localdomain	[2021-05-17T00:22:05,608502459-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:22:05,614831010-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '81a0fb48-29ed-4314-ac6f-ddc62acf52f3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XaX6si:/tmp/ceph-asok.XaX6si', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 81a0fb48-29ed-4314-ac6f-ddc62acf52f3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XaX6si:/tmp/ceph-asok.XaX6si -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T07:22:07.255+0000 ffffb148f010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T07:22:07.263+0000 ffffb148f010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T07:22:07.263+0000 ffffb148f010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T07:22:07.278266+0000 Maintaining 256 concurrent writes of 4096 bytes to objects of size 4096 for up to 20 seconds or 0 objects
2021-05-17T07:22:07.278312+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T07:22:07.280707+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T07:22:07.280707+0000     0       0         0         0         0         0           -           0
2021-05-17T07:22:08.280905+0000     1     255     18474     18219   71.1638    71.168  0.00528497   0.0134642
2021-05-17T07:22:09.281076+0000     2     255     36709     36454    71.191   71.2305    0.117998   0.0136557
2021-05-17T07:22:10.281238+0000     3     255     54643     54388   70.8085   70.0547    0.116497   0.0138466
2021-05-17T07:22:11.281399+0000     4     256     72928     72672   70.9589   71.4219  0.00165832   0.0139786
2021-05-17T07:22:12.281600+0000     5     255     90645     90390   70.6065   69.2109  0.00812247    0.014101
2021-05-17T07:22:13.281755+0000     6     255    108545    108290   70.4906   69.9219  0.00170798   0.0141193
2021-05-17T07:22:14.282040+0000     7     255    126328    126073   70.3412   69.4648  0.00321577   0.0141129
2021-05-17T07:22:15.282217+0000     8     255    144944    144689   70.6368   72.7188  0.00774701   0.0140925
2021-05-17T07:22:16.282370+0000     9     255    162698    162443   70.4928   69.3516   0.0324324    0.014116
2021-05-17T07:22:17.282595+0000    10     255    180145    179890   70.2572   68.1523  0.00168884   0.0141619
2021-05-17T07:22:18.282759+0000    11     255    197695    197440   70.1014   68.5547   0.0669685   0.0142258
2021-05-17T07:22:19.282921+0000    12     255    215297    215042   69.9885   68.7578  0.00193491   0.0142333
2021-05-17T07:22:20.283088+0000    13     256    230843    230587    69.275   60.7227   0.0175398   0.0143673
2021-05-17T07:22:21.283249+0000    14     255    247941    247686    69.097    66.793  0.00508719    0.014447
2021-05-17T07:22:22.283447+0000    15     255    263860    263605   68.6352   62.1836  0.00136096    0.014499
2021-05-17T07:22:23.283617+0000    16     255    279073    278818    68.059   59.4258  0.00956327   0.0146655
2021-05-17T07:22:24.283828+0000    17     255    295135    294880   67.7455   62.7422  0.00504296   0.0147323
2021-05-17T07:22:25.284039+0000    18     255    310165    309910   67.2428   58.7109  0.00208774    0.014837
2021-05-17T07:22:26.284207+0000    19     255    326872    326617    67.138   65.2617  0.00171109   0.0148609
2021-05-17T07:22:27.284391+0000 min lat: 0.00066982 max lat: 0.20004 avg lat: 0.0148801
2021-05-17T07:22:27.284391+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T07:22:27.284391+0000    20     256    343619    343363   67.0512   65.4141  0.00193823   0.0148801
2021-05-17T07:22:28.284640+0000 Total time run:         20.0497
Total writes made:      343619
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     66.9469
Stddev Bandwidth:       4.23663
Max bandwidth (MB/sec): 72.7188
Min bandwidth (MB/sec): 58.7109
Average IOPS:           17138
Stddev IOPS:            1084.58
Max IOPS:               18616
Min IOPS:               15030
Average Latency(s):     0.0149078
Stddev Latency(s):      0.0259963
Max latency(s):         0.20004
Min latency(s):         0.00066982

[1;32mlocalhost.localdomain	[2021-05-17T00:22:28,588362749-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 3938847


[1;33mlocalhost.localdomain	[2021-05-17T00:22:28,595188514-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:22:51,397857495-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 343.62k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:22:51,412060520-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:22:59,537955020-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 343.62k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:22:59,552564000-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:23:08,065242047-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 343.62k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:23:08,079490095-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:23:16,161733110-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 343.62k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:23:16,176231160-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:23:24,403622926-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 343.62k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:23:24,418065287-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:23:24,429379596-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T00:23:24,436090066-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:23:24,450753400-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=3942245
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4KB_seq.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-17T00:23:24,463315053-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4KB_seq.log.4
[1;30mlocalhost.localdomain	[2021-05-17T00:23:24,502572252-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:23:24,509023463-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '81a0fb48-29ed-4314-ac6f-ddc62acf52f3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XaX6si:/tmp/ceph-asok.XaX6si', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 81a0fb48-29ed-4314-ac6f-ddc62acf52f3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XaX6si:/tmp/ceph-asok.XaX6si -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T07:23:26.021+0000 ffff99c43010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T07:23:26.025+0000 ffff99c43010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T07:23:26.029+0000 ffff99c43010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T07:23:26.045327+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T07:23:26.045327+0000     0       0         0         0         0         0           -           0
2021-05-17T07:23:27.045522+0000     1     255     25966     25711     100.4   100.434   0.0108215  0.00987133
2021-05-17T07:23:28.045693+0000     2     255     54876     54621   106.655    112.93  0.00776808  0.00933906
2021-05-17T07:23:29.045886+0000     3     255     85317     85062   110.732    118.91   0.0515347  0.00889063
2021-05-17T07:23:30.046050+0000     4     255    117564    117309   114.535   125.965  0.00716487  0.00871001
2021-05-17T07:23:31.046226+0000     5     255    143296    143041   111.728   100.516  0.00326373  0.00892967
2021-05-17T07:23:32.046439+0000     6     255    171988    171733   111.782   112.078   0.0042243  0.00892808
2021-05-17T07:23:33.046695+0000     7     255    199614    199359   111.225   107.914   0.0112707  0.00897417
2021-05-17T07:23:34.046869+0000     8     256    224905    224649   109.669   98.7891  0.00046586  0.00904274
2021-05-17T07:23:35.047040+0000     9     256    249087    248831   107.977   94.4609 0.000337258  0.00914722
2021-05-17T07:23:36.047314+0000    10     256    274835    274579   107.235   100.578  0.00759378  0.00931154
2021-05-17T07:23:37.047493+0000    11     256    300698    300442   106.669   101.027 0.000313149  0.00930898
2021-05-17T07:23:38.047888+0000    12     255    326600    326345   106.208   101.184  0.00501623  0.00940216
2021-05-17T07:23:39.048222+0000 Total time run:       12.5552
Total reads made:     343619
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   106.909
Average IOPS:         27368
Stddev IOPS:          2412.36
Max IOPS:             32247
Min IOPS:             24182
Average Latency(s):   0.00934112
Max latency(s):       0.152035
Min latency(s):       0.00029072

[1;32mlocalhost.localdomain	[2021-05-17T00:23:39,441263927-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 3942245


[1;33mlocalhost.localdomain	[2021-05-17T00:23:39,448066888-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:24:02,675734034-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 343.62k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:24:02,690325745-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:24:10,653755378-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 343.62k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:24:10,668308007-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:24:18,631959920-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 343.62k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:24:18,646503202-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:24:26,769030543-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 343.62k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:24:26,783582197-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:24:34,933660230-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 343.62k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:24:34,948018951-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:24:34,959254256-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T00:24:34,963534490-07:00][RUNNING][ROUND 5/1/40] object_size=4KB[0m


[1;33mlocalhost.localdomain	[2021-05-17T00:24:34,970249775-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T00:24:34,986770726-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40888\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.305306\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 98690e3c-d41c-48aa-a87b-356c28204e2a\nsetting min_mon_release = octopus\nepoch 0\nfsid 98690e3c-d41c-48aa-a87b-356c28204e2a\nlast_changed 2021-05-17T00:24:59.439763-0700\ncreated 2021-05-17T00:24:59.439763-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40888/0,v1:10.10.1.2:40889/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.305306 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 732824ba-f965-472f-b93a-6238955ebdd3\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 0dcaef58-eb68-4044-9162-2302b957820a\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 7ab5b285-9478-41a1-824b-90298123520f\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42888\n  w/ user/pass: admin / 15e28e0f-dcf5-4bf9-a88e-806bcaf1a4de\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 00:25:14 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40888
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.305306
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 98690e3c-d41c-48aa-a87b-356c28204e2a
setting min_mon_release = octopus
epoch 0
fsid 98690e3c-d41c-48aa-a87b-356c28204e2a
last_changed 2021-05-17T00:24:59.439763-0700
created 2021-05-17T00:24:59.439763-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40888/0,v1:10.10.1.2:40889/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.305306 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 732824ba-f965-472f-b93a-6238955ebdd3
0
start osd.0
add osd1 0dcaef58-eb68-4044-9162-2302b957820a
1
start osd.1
add osd2 7ab5b285-9478-41a1-824b-90298123520f
2
start osd.2


restful urls: https://10.10.1.2:42888
  w/ user/pass: admin / 15e28e0f-dcf5-4bf9-a88e-806bcaf1a4de


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T00:24:36.004-0700 7fb62d8721c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T00:24:36.004-0700 7fb62d8721c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T00:24:36.024-0700 7f68896551c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T00:24:36.024-0700 7f68896551c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40888,v1:10.10.1.2:40889] --print /tmp/ceph_monmap.305306 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.305306 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.305306 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42888 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.vBRjijQQYl 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 732824ba-f965-472f-b93a-6238955ebdd3 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBUGqJgweR3ChAAF46Oc2rOs+bv9U89fdutxA== --osd-uuid 732824ba-f965-472f-b93a-6238955ebdd3 
2021-05-17T00:25:08.508-0700 7fb739addf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T00:25:08.508-0700 7fb739addf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T00:25:08.508-0700 7fb739addf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T00:25:08.572-0700 7fb739addf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 0dcaef58-eb68-4044-9162-2302b957820a -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T00:25:08.840-0700 7f6f966e9f00 -1 Falling back to public interface
2021-05-17T00:25:08.852-0700 7f6f966e9f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBUGqJgcnA+MhAApEGIyRrArv58QvVYbNf0bA== --osd-uuid 0dcaef58-eb68-4044-9162-2302b957820a 
2021-05-17T00:25:09.180-0700 7ff5b4197f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T00:25:09.180-0700 7ff5b4197f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T00:25:09.180-0700 7ff5b4197f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T00:25:09.224-0700 7ff5b4197f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 7ab5b285-9478-41a1-824b-90298123520f -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T00:25:09.500-0700 7f6529e7cf00 -1 Falling back to public interface
2021-05-17T00:25:09.516-0700 7f6529e7cf00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBVGqJgYwEIHhAADtoxDqOm1dKmCaEZsgBbIA== --osd-uuid 7ab5b285-9478-41a1-824b-90298123520f 
2021-05-17T00:25:09.812-0700 7efeb9adef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T00:25:09.816-0700 7efeb9adef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T00:25:09.816-0700 7efeb9adef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T00:25:09.868-0700 7efeb9adef00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T00:25:10.160-0700 7f0fb4dddf00 -1 Falling back to public interface
2021-05-17T00:25:10.176-0700 7f0fb4dddf00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T00:25:14,084512873-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T00:25:14,091654007-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T00:25:14,173884941-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:25:14,180471595-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '536f8ec4-f147-472c-9d29-6a77f7990776', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.je1aUz:/tmp/ceph-asok.je1aUz', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 536f8ec4-f147-472c-9d29-6a77f7990776 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.je1aUz:/tmp/ceph-asok.je1aUz -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T00:25:17,182849304-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:25:17,189112712-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '536f8ec4-f147-472c-9d29-6a77f7990776', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.je1aUz:/tmp/ceph-asok.je1aUz', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 536f8ec4-f147-472c-9d29-6a77f7990776 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.je1aUz:/tmp/ceph-asok.je1aUz -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T00:25:20,122377309-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:25:20,128862621-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '536f8ec4-f147-472c-9d29-6a77f7990776', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.je1aUz:/tmp/ceph-asok.je1aUz', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 536f8ec4-f147-472c-9d29-6a77f7990776 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.je1aUz:/tmp/ceph-asok.je1aUz -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T00:25:22,968885487-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:25:22,975216287-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '536f8ec4-f147-472c-9d29-6a77f7990776', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.je1aUz:/tmp/ceph-asok.je1aUz', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 536f8ec4-f147-472c-9d29-6a77f7990776 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.je1aUz:/tmp/ceph-asok.je1aUz -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T00:25:28,541935273-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:25:28,549317033-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '536f8ec4-f147-472c-9d29-6a77f7990776', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.je1aUz:/tmp/ceph-asok.je1aUz', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 536f8ec4-f147-472c-9d29-6a77f7990776 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.je1aUz:/tmp/ceph-asok.je1aUz -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T00:25:32,352075845-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:25:32,358575454-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '536f8ec4-f147-472c-9d29-6a77f7990776', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.je1aUz:/tmp/ceph-asok.je1aUz', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 536f8ec4-f147-472c-9d29-6a77f7990776 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.je1aUz:/tmp/ceph-asok.je1aUz -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T00:25:35,866825655-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:25:35,873216810-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '536f8ec4-f147-472c-9d29-6a77f7990776', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.je1aUz:/tmp/ceph-asok.je1aUz', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 536f8ec4-f147-472c-9d29-6a77f7990776 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.je1aUz:/tmp/ceph-asok.je1aUz -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T00:25:38,882256522-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:25:38,890522773-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '536f8ec4-f147-472c-9d29-6a77f7990776', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.je1aUz:/tmp/ceph-asok.je1aUz', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 536f8ec4-f147-472c-9d29-6a77f7990776 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.je1aUz:/tmp/ceph-asok.je1aUz -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T00:25:42,244813944-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:25:42,252369328-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '536f8ec4-f147-472c-9d29-6a77f7990776', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.je1aUz:/tmp/ceph-asok.je1aUz', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 536f8ec4-f147-472c-9d29-6a77f7990776 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.je1aUz:/tmp/ceph-asok.je1aUz -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T00:25:45,662473595-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:25:45,669628591-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '536f8ec4-f147-472c-9d29-6a77f7990776', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.je1aUz:/tmp/ceph-asok.je1aUz', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 536f8ec4-f147-472c-9d29-6a77f7990776 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.je1aUz:/tmp/ceph-asok.je1aUz -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T00:25:48,831854774-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:25:48,838166896-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '536f8ec4-f147-472c-9d29-6a77f7990776', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.je1aUz:/tmp/ceph-asok.je1aUz', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 536f8ec4-f147-472c-9d29-6a77f7990776 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.je1aUz:/tmp/ceph-asok.je1aUz -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T00:25:51,474355449-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:25:51,480954284-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '536f8ec4-f147-472c-9d29-6a77f7990776', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.je1aUz:/tmp/ceph-asok.je1aUz', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 536f8ec4-f147-472c-9d29-6a77f7990776 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.je1aUz:/tmp/ceph-asok.je1aUz -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T00:25:54,290228821-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:26:17,117702474-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   199 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:26:25,078844566-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:26:35,148738976-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=======.....................] (remaining: 28s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:26:43,234629655-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [=======.....................] (remaining: 52s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:26:51,190638088-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (30s)
      [========....................] (remaining: 71s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:26:59,173991253-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [========....................] (remaining: 78s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:27:07,244352959-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (45s)
      [========....................] (remaining: 96s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:27:15,187723477-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [==============..............] (remaining: 44s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:27:23,141959247-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:27:23,156461090-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:27:31,124969929-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:27:31,139493016-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:27:39,065904765-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:27:39,080577413-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:27:47,126691574-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:27:47,141162752-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:27:55,129499468-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:27:55,144345981-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:27:55,155911341-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T00:27:55,162588160-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:27:55,177854684-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=3955762
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4KB_write.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-17T00:27:55,190486623-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4KB_write.log.5
[1;30mlocalhost.localdomain	[2021-05-17T00:27:55,230802397-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:27:55,237132371-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '536f8ec4-f147-472c-9d29-6a77f7990776', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.je1aUz:/tmp/ceph-asok.je1aUz', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4096', '-O', '4096', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 536f8ec4-f147-472c-9d29-6a77f7990776 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.je1aUz:/tmp/ceph-asok.je1aUz -- rados bench 20 write --pool bench_rados -b 4096 -O 4096 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T07:27:56.767+0000 ffff917db010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T07:27:56.775+0000 ffff917db010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T07:27:56.775+0000 ffff917db010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T07:27:56.792558+0000 Maintaining 256 concurrent writes of 4096 bytes to objects of size 4096 for up to 20 seconds or 0 objects
2021-05-17T07:27:56.792601+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T07:27:56.794851+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T07:27:56.794851+0000     0       0         0         0         0         0           -           0
2021-05-17T07:27:57.795008+0000     1     255     18507     18252    71.293   71.2969  0.00179338   0.0135841
2021-05-17T07:27:58.795167+0000     2     255     36837     36582   71.4416   71.6016  0.00179666   0.0135893
2021-05-17T07:27:59.795325+0000     3     255     55612     55357   72.0705   73.3398  0.00222191   0.0136537
2021-05-17T07:28:00.795590+0000     4     255     73913     73658   71.9202   71.4883  0.00203844   0.0137009
2021-05-17T07:28:01.795854+0000     5     255     91892     91637   71.5785   70.2305  0.00172716   0.0138008
2021-05-17T07:28:02.796032+0000     6     255    109047    108792   70.8154   67.0117  0.00112195   0.0139951
2021-05-17T07:28:03.796332+0000     7     255    126418    126163   70.3896   67.8555   0.0841241   0.0141395
2021-05-17T07:28:04.796524+0000     8     255    143470    143215   69.9155   66.6094  0.00315061   0.0141893
2021-05-17T07:28:05.796808+0000     9     256    159870    159614   69.2626   64.0586   0.0126174   0.0143917
2021-05-17T07:28:06.796979+0000    10     255    175634    175379   68.4935    61.582  0.00288447   0.0145475
2021-05-17T07:28:07.797147+0000    11     256    193459    193203   68.5953    69.625  0.00321587   0.0144949
2021-05-17T07:28:08.797303+0000    12     255    208218    207963    67.683   57.6562  0.00176103   0.0147104
2021-05-17T07:28:09.797476+0000    13     256    224199    223943   67.2775   62.4219   0.0079178   0.0147987
2021-05-17T07:28:10.797635+0000    14     256    239009    238753   66.6036   57.8516   0.0954601   0.0149506
2021-05-17T07:28:11.797797+0000    15     256    256084    255828   66.6092   66.6992    0.119663   0.0149517
2021-05-17T07:28:12.798309+0000    16     255    272784    272529   66.5214   65.2383   0.0022626   0.0149872
2021-05-17T07:28:13.798495+0000    17     255    286704    286449   65.8064    54.375  0.00172384   0.0150931
2021-05-17T07:28:14.798676+0000    18     255    302801    302546   65.6431   62.8789  0.00700152   0.0152167
2021-05-17T07:28:15.798880+0000    19     255    319513    319258   65.6234   65.2812  0.00345001      0.0152
2021-05-17T07:28:16.799085+0000 min lat: 0.000697635 max lat: 0.270413 avg lat: 0.0152751
2021-05-17T07:28:16.799085+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T07:28:16.799085+0000    20     166    334860    334694   65.3564   60.2969   0.0194008   0.0152751
2021-05-17T07:28:17.799338+0000 Total time run:         20.0531
Total writes made:      334860
Write size:             4096
Object size:            4096
Bandwidth (MB/sec):     65.2291
Stddev Bandwidth:       5.24386
Max bandwidth (MB/sec): 73.3398
Min bandwidth (MB/sec): 54.375
Average IOPS:           16698
Stddev IOPS:            1342.43
Max IOPS:               18775
Min IOPS:               13920
Average Latency(s):     0.0152949
Stddev Latency(s):      0.0337229
Max latency(s):         0.270413
Min latency(s):         0.000697635

[1;32mlocalhost.localdomain	[2021-05-17T00:28:18,086194609-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 3955762


[1;33mlocalhost.localdomain	[2021-05-17T00:28:18,093415646-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:28:41,093882087-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 334.86k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:28:41,108841929-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:28:49,096525415-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 334.86k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:28:49,111097065-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:28:57,090347221-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 334.86k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:28:57,105566052-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:29:05,269836668-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 334.86k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:29:05,284696037-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:29:13,138234758-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 334.86k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:29:13,153618182-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:29:13,166591759-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T00:29:13,173927895-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:29:13,189999142-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=3959158
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4KB_seq.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-17T00:29:13,203026067-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4KB_seq.log.5
[1;30mlocalhost.localdomain	[2021-05-17T00:29:13,245590650-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:29:13,252663596-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '536f8ec4-f147-472c-9d29-6a77f7990776', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.je1aUz:/tmp/ceph-asok.je1aUz', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 536f8ec4-f147-472c-9d29-6a77f7990776 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.je1aUz:/tmp/ceph-asok.je1aUz -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T07:29:14.765+0000 ffff864c0010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T07:29:14.865+0000 ffff864c0010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T07:29:14.869+0000 ffff864c0010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T07:29:14.884594+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T07:29:14.884594+0000     0       0         0         0         0         0           -           0
2021-05-17T07:29:15.884772+0000     1     255     28110     27855   108.775   108.809   0.0116415  0.00910457
2021-05-17T07:29:16.884945+0000     2     256     57513     57257   111.803   114.852   0.0143264  0.00890381
2021-05-17T07:29:17.885124+0000     3     255     89578     89323   116.281   125.258  0.00434098  0.00856782
2021-05-17T07:29:18.885305+0000     4     255    117156    116901   114.137   107.727   0.0146938  0.00873652
2021-05-17T07:29:19.885519+0000     5     255    150165    149910   117.093   128.941   0.0190698  0.00851628
2021-05-17T07:29:20.885703+0000     6     255    180135    179880   117.085    117.07  0.00767774  0.00852459
2021-05-17T07:29:21.886390+0000     7     255    209184    208929   116.558   113.473  0.00240652  0.00856348
2021-05-17T07:29:22.886564+0000     8     255    235256    235001   114.717   101.844   0.0035177  0.00870158
2021-05-17T07:29:23.886784+0000     9     255    263189    262934   114.091   109.113  0.00506793  0.00875084
2021-05-17T07:29:24.887118+0000    10     255    289377    289122   112.908   102.297    0.003411  0.00884262
2021-05-17T07:29:25.887647+0000    11     255    318089    317834   112.835   112.156  0.00593292  0.00884999
2021-05-17T07:29:26.888296+0000 Total time run:       11.5048
Total reads made:     334860
Read size:            4096
Object size:          4096
Bandwidth (MB/sec):   113.695
Average IOPS:         29106
Stddev IOPS:          2179.19
Max IOPS:             33009
Min IOPS:             26072
Average Latency(s):   0.00878387
Max latency(s):       0.170444
Min latency(s):       0.000304192

[1;32mlocalhost.localdomain	[2021-05-17T00:29:27,206080884-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 3959158


[1;33mlocalhost.localdomain	[2021-05-17T00:29:27,213251212-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:29:50,045084352-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 334.86k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:29:50,063645665-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:29:57,986544221-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 334.86k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:29:58,001368379-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:30:06,469149129-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 334.86k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:30:06,484279024-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:30:14,312051604-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 334.86k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:30:14,326628385-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:30:22,475830402-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 334.86k objects, 1.3 GiB
    usage:   2.6 GiB used, 297 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:30:22,491185186-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:30:22,502744350-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T00:30:22,510111751-07:00][RUNNING][ROUND 1/2/40] object_size=16KB[0m


[1;33mlocalhost.localdomain	[2021-05-17T00:30:22,516966995-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T00:30:22,533732455-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40929\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.306423\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 54d1891b-b12e-4786-b230-a3e64029b661\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 54d1891b-b12e-4786-b230-a3e64029b661\nlast_changed 2021-05-17T00:30:48.049114-0700\ncreated 2021-05-17T00:30:48.049114-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40929/0,v1:10.10.1.2:40930/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.306423 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 c3b3fc0a-7310-4386-ab96-59644c09cd47\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 bba5c243-1b21-4046-a0fe-30115c48ace8\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 b286a68c-8933-4a5d-ae78-b80bb5ea53dc\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42929\n  w/ user/pass: admin / fac671d6-0029-4eec-9e51-fb9cb1895665\n\n'
10.10.1.2: b'\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\nCEPH_DEV=1\n'
[1] 00:31:02 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40929
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.306423
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 54d1891b-b12e-4786-b230-a3e64029b661
setting min_mon_release = octopus
epoch 0
fsid 54d1891b-b12e-4786-b230-a3e64029b661
last_changed 2021-05-17T00:30:48.049114-0700
created 2021-05-17T00:30:48.049114-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40929/0,v1:10.10.1.2:40930/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.306423 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 c3b3fc0a-7310-4386-ab96-59644c09cd47
0
start osd.0
add osd1 bba5c243-1b21-4046-a0fe-30115c48ace8
1
start osd.1
add osd2 b286a68c-8933-4a5d-ae78-b80bb5ea53dc
2
start osd.2


restful urls: https://10.10.1.2:42929
  w/ user/pass: admin / fac671d6-0029-4eec-9e51-fb9cb1895665


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T00:30:23.555-0700 7fbd073671c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T00:30:23.555-0700 7fbd073671c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T00:30:23.571-0700 7f30d63d11c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T00:30:23.571-0700 7f30d63d11c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40929,v1:10.10.1.2:40930] --print /tmp/ceph_monmap.306423 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.306423 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.306423 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42929 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.wWt5Z2eibR 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new c3b3fc0a-7310-4386-ab96-59644c09cd47 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCwG6JgABXxNBAAhcfzDbydQnMcQaRt7E9gsw== --osd-uuid c3b3fc0a-7310-4386-ab96-59644c09cd47 
2021-05-17T00:30:57.243-0700 7f78f494af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T00:30:57.243-0700 7f78f494af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T00:30:57.243-0700 7f78f494af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T00:30:57.287-0700 7f78f494af00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new bba5c243-1b21-4046-a0fe-30115c48ace8 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T00:30:57.575-0700 7f8d0df71f00 -1 Falling back to public interface
2021-05-17T00:30:57.587-0700 7f8d0df71f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCxG6JgWhx/IhAAqhhOkI+9uvoilMbiZ8sytA== --osd-uuid bba5c243-1b21-4046-a0fe-30115c48ace8 
2021-05-17T00:30:57.895-0700 7fda50d49f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T00:30:57.895-0700 7fda50d49f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T00:30:57.895-0700 7fda50d49f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T00:30:57.943-0700 7fda50d49f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new b286a68c-8933-4a5d-ae78-b80bb5ea53dc -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T00:30:58.319-0700 7f1c50063f00 -1 Falling back to public interface
2021-05-17T00:30:58.335-0700 7f1c50063f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCyG6Jg9vg2ExAAgJjZuoeeQiCc3X6lZQUAxA== --osd-uuid b286a68c-8933-4a5d-ae78-b80bb5ea53dc 
2021-05-17T00:30:58.651-0700 7f24b1838f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T00:30:58.651-0700 7f24b1838f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T00:30:58.651-0700 7f24b1838f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T00:30:58.719-0700 7f24b1838f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T00:30:59.051-0700 7f0e7ef22f00 -1 Falling back to public interface
2021-05-17T00:30:59.063-0700 7f0e7ef22f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T00:31:02,968907980-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T00:31:02,976014742-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T00:31:03,058044089-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:31:03,064789453-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '10aab132-bc8f-4b01-8ede-8589f50c496e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.USJBCU:/tmp/ceph-asok.USJBCU', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 10aab132-bc8f-4b01-8ede-8589f50c496e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.USJBCU:/tmp/ceph-asok.USJBCU -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T00:31:05,948354297-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:31:05,954760618-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '10aab132-bc8f-4b01-8ede-8589f50c496e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.USJBCU:/tmp/ceph-asok.USJBCU', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 10aab132-bc8f-4b01-8ede-8589f50c496e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.USJBCU:/tmp/ceph-asok.USJBCU -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T00:31:08,702970353-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:31:08,709162123-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '10aab132-bc8f-4b01-8ede-8589f50c496e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.USJBCU:/tmp/ceph-asok.USJBCU', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 10aab132-bc8f-4b01-8ede-8589f50c496e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.USJBCU:/tmp/ceph-asok.USJBCU -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T00:31:11,436395430-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:31:11,442752558-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '10aab132-bc8f-4b01-8ede-8589f50c496e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.USJBCU:/tmp/ceph-asok.USJBCU', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 10aab132-bc8f-4b01-8ede-8589f50c496e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.USJBCU:/tmp/ceph-asok.USJBCU -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T00:31:16,977329188-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:31:16,984152266-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '10aab132-bc8f-4b01-8ede-8589f50c496e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.USJBCU:/tmp/ceph-asok.USJBCU', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 10aab132-bc8f-4b01-8ede-8589f50c496e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.USJBCU:/tmp/ceph-asok.USJBCU -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T00:31:20,776478789-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:31:20,782874594-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '10aab132-bc8f-4b01-8ede-8589f50c496e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.USJBCU:/tmp/ceph-asok.USJBCU', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 10aab132-bc8f-4b01-8ede-8589f50c496e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.USJBCU:/tmp/ceph-asok.USJBCU -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T00:31:24,265927702-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:31:24,272705292-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '10aab132-bc8f-4b01-8ede-8589f50c496e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.USJBCU:/tmp/ceph-asok.USJBCU', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 10aab132-bc8f-4b01-8ede-8589f50c496e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.USJBCU:/tmp/ceph-asok.USJBCU -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T00:31:27,503936165-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:31:27,510185229-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '10aab132-bc8f-4b01-8ede-8589f50c496e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.USJBCU:/tmp/ceph-asok.USJBCU', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 10aab132-bc8f-4b01-8ede-8589f50c496e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.USJBCU:/tmp/ceph-asok.USJBCU -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T00:31:30,957101149-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:31:30,963632367-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '10aab132-bc8f-4b01-8ede-8589f50c496e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.USJBCU:/tmp/ceph-asok.USJBCU', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 10aab132-bc8f-4b01-8ede-8589f50c496e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.USJBCU:/tmp/ceph-asok.USJBCU -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T00:31:34,274911055-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:31:34,281247585-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '10aab132-bc8f-4b01-8ede-8589f50c496e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.USJBCU:/tmp/ceph-asok.USJBCU', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 10aab132-bc8f-4b01-8ede-8589f50c496e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.USJBCU:/tmp/ceph-asok.USJBCU -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T00:31:37,467871297-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:31:37,474235807-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '10aab132-bc8f-4b01-8ede-8589f50c496e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.USJBCU:/tmp/ceph-asok.USJBCU', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 10aab132-bc8f-4b01-8ede-8589f50c496e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.USJBCU:/tmp/ceph-asok.USJBCU -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T00:31:40,220327994-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:31:40,226593141-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '10aab132-bc8f-4b01-8ede-8589f50c496e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.USJBCU:/tmp/ceph-asok.USJBCU', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 10aab132-bc8f-4b01-8ede-8589f50c496e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.USJBCU:/tmp/ceph-asok.USJBCU -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T00:31:43,108368536-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:32:06,049659435-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   203 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:32:14,018925583-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:32:22,020722233-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=======.....................] (remaining: 26s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:32:30,006051356-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [========....................] (remaining: 49s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:32:38,007499042-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [========....................] (remaining: 59s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:32:45,962421288-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [========....................] (remaining: 78s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:32:53,929197272-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (40s)
      [========....................] (remaining: 90s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:33:01,861704271-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   235 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [===============.............] (remaining: 40s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:33:09,954982385-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:33:09,969604862-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:33:17,961689261-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:33:17,976606314-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:33:25,824786744-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:33:25,839124053-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:33:33,984593068-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:33:33,999590565-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:33:42,050342460-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:33:42,065355109-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:33:42,077051711-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T00:33:42,083897277-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:33:42,099181488-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=3972697
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16KB_write.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-17T00:33:42,112047795-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16KB_write.log.1
[1;30mlocalhost.localdomain	[2021-05-17T00:33:42,156670862-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:33:42,165036714-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '10aab132-bc8f-4b01-8ede-8589f50c496e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.USJBCU:/tmp/ceph-asok.USJBCU', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 10aab132-bc8f-4b01-8ede-8589f50c496e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.USJBCU:/tmp/ceph-asok.USJBCU -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T07:33:43.723+0000 ffffb1346010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T07:33:43.731+0000 ffffb1346010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T07:33:43.731+0000 ffffb1346010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T07:33:43.747334+0000 Maintaining 256 concurrent writes of 16384 bytes to objects of size 16384 for up to 20 seconds or 0 objects
2021-05-17T07:33:43.747380+0000 Object prefix: benchmark_data_localhost.localdomain_7
2021-05-17T07:33:43.751865+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T07:33:43.751865+0000     0       0         0         0         0         0           -           0
2021-05-17T07:33:44.752061+0000     1     255     18655     18400   287.482     287.5   0.0099974   0.0136024
2021-05-17T07:33:45.752237+0000     2     255     36755     36500   285.122   282.812  0.00529706   0.0138676
2021-05-17T07:33:46.752402+0000     3     255     55028     54773   285.238   285.516  0.00428994   0.0138354
2021-05-17T07:33:47.752568+0000     4     255     72453     72198   281.983   272.266  0.00252327   0.0140034
2021-05-17T07:33:48.752732+0000     5     255     90697     90442    282.59   285.062  0.00915671   0.0141089
2021-05-17T07:33:49.752900+0000     6     255    107227    106972   278.531   258.281   0.0120492   0.0143249
2021-05-17T07:33:50.753073+0000     7     255    124380    124125   277.022   268.016  0.00593809    0.014397
2021-05-17T07:33:51.753238+0000     8     256    140417    140161    273.71   250.562 0.000973262   0.0145485
2021-05-17T07:33:52.753434+0000     9     256    155256    155000   269.054   231.859   0.0122491   0.0148351
2021-05-17T07:33:53.753636+0000    10     255    170364    170109   265.752   236.078  0.00279565   0.0150168
2021-05-17T07:33:54.753872+0000    11     255    185539    185284   263.143   237.109  0.00964179   0.0151671
2021-05-17T07:33:55.754131+0000    12     256    200470    200214   260.649   233.281  0.00128752   0.0153104
2021-05-17T07:33:56.754363+0000    13     255    215072    214817   258.147   228.172   0.0344513   0.0154679
2021-05-17T07:33:57.754543+0000    14     255    228589    228334   254.791   211.203   0.0105505   0.0156828
2021-05-17T07:33:58.754837+0000    15     255    242622    242367   252.418   219.266   0.0159722   0.0158279
2021-05-17T07:33:59.755118+0000    16     256    255083    254827   248.806   194.688 0.000780958   0.0159642
2021-05-17T07:34:00.755309+0000    17     256    270057    269801   247.931   233.969   0.0143177   0.0161135
2021-05-17T07:34:01.755625+0000    18     255    285022    284767   247.144   233.844   0.0183408   0.0161642
2021-05-17T07:34:02.755858+0000    19     255    301020    300765   247.289   249.969   0.0119076   0.0161577
2021-05-17T07:34:03.756111+0000 min lat: 0.000780958 max lat: 0.170749 avg lat: 0.0162983
2021-05-17T07:34:03.756111+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T07:34:03.756111+0000    20     159    314048    313889   245.175   205.062   0.0210053   0.0162983
2021-05-17T07:34:04.756354+0000 Total time run:         20.0672
Total writes made:      314048
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     244.528
Stddev Bandwidth:       28.0026
Max bandwidth (MB/sec): 287.5
Min bandwidth (MB/sec): 194.688
Average IOPS:           15649
Stddev IOPS:            1792.16
Max IOPS:               18400
Min IOPS:               12460
Average Latency(s):     0.0163254
Stddev Latency(s):      0.0161411
Max latency(s):         0.170749
Min latency(s):         0.000780958

[1;32mlocalhost.localdomain	[2021-05-17T00:34:05,052969791-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 3972697


[1;33mlocalhost.localdomain	[2021-05-17T00:34:05,060267055-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:34:28,547187201-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 314.05k objects, 4.8 GiB
    usage:   9.6 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:34:28,562597343-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:34:37,000753450-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 314.05k objects, 4.8 GiB
    usage:   9.6 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:34:37,015756363-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:34:45,220353896-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 314.05k objects, 4.8 GiB
    usage:   9.6 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:34:45,235627575-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:34:53,448871344-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 314.05k objects, 4.8 GiB
    usage:   9.6 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:34:53,465645055-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:35:01,715959480-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 314.05k objects, 4.8 GiB
    usage:   9.6 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:35:01,731046121-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:35:01,742686673-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T00:35:01,749875679-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:35:01,764856687-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=3976092
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16KB_seq.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-17T00:35:01,777640975-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16KB_seq.log.1
[1;30mlocalhost.localdomain	[2021-05-17T00:35:01,817811564-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:35:01,824459442-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '10aab132-bc8f-4b01-8ede-8589f50c496e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.USJBCU:/tmp/ceph-asok.USJBCU', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 10aab132-bc8f-4b01-8ede-8589f50c496e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.USJBCU:/tmp/ceph-asok.USJBCU -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T07:35:03.276+0000 ffff9b2a8010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T07:35:03.284+0000 ffff9b2a8010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T07:35:03.284+0000 ffff9b2a8010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T07:35:03.302043+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T07:35:03.302043+0000     0       0         0         0         0         0           -           0
2021-05-17T07:35:04.302201+0000     1     255     19768     19513   304.803   304.891   0.0347802   0.0128614
2021-05-17T07:35:05.302374+0000     2     256     41164     40908    319.52   334.297  0.00088146   0.0124205
2021-05-17T07:35:06.302589+0000     3     255     60058     59803   311.404   295.234  0.00217221   0.0127677
2021-05-17T07:35:07.302784+0000     4     256     80971     80715   315.225    326.75  0.00411835   0.0126332
2021-05-17T07:35:08.302975+0000     5     256    102245    101989   318.648   332.406  0.00196831    0.012507
2021-05-17T07:35:09.303141+0000     6     255    125455    125200   325.975   362.672    0.010474   0.0122422
2021-05-17T07:35:10.303325+0000     7     255    147405    147150   328.394   342.969   0.0281265   0.0121413
2021-05-17T07:35:11.303515+0000     8     255    167296    167041   326.187   310.797  0.00113328   0.0122351
2021-05-17T07:35:12.303689+0000     9     256    183255    182999   317.644   249.344   0.0259192   0.0125597
2021-05-17T07:35:13.303869+0000    10     256    203371    203115   317.305   314.312  0.00126967    0.012576
2021-05-17T07:35:14.304050+0000    11     255    224224    223969   318.076   325.844   0.0136374   0.0125539
2021-05-17T07:35:15.304586+0000    12     255    244448    244193   317.889       316   0.0134561   0.0125563
2021-05-17T07:35:16.304782+0000    13     255    266255    266000   319.641   340.734  0.00341662   0.0124895
2021-05-17T07:35:17.305094+0000    14     256    285984    285728    318.82    308.25  0.00114324   0.0125177
2021-05-17T07:35:18.305427+0000    15     255    305356    305101   317.739   302.703  0.00965999   0.0125651
2021-05-17T07:35:19.305699+0000 Total time run:       15.4485
Total reads made:     314048
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   317.637
Average IOPS:         20328
Stddev IOPS:          1671.45
Max IOPS:             23211
Min IOPS:             15958
Average Latency(s):   0.0125711
Max latency(s):       0.147837
Min latency(s):       0.000327491

[1;32mlocalhost.localdomain	[2021-05-17T00:35:19,621235518-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 3976092


[1;33mlocalhost.localdomain	[2021-05-17T00:35:19,628573409-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:35:42,512117711-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 314.05k objects, 4.8 GiB
    usage:   9.6 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:35:42,527128680-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:35:50,550952408-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 314.05k objects, 4.8 GiB
    usage:   9.6 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:35:50,565840341-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:35:58,912894427-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 314.05k objects, 4.8 GiB
    usage:   9.6 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:35:58,927760006-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:36:06,955296006-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 314.05k objects, 4.8 GiB
    usage:   9.6 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:36:06,970179003-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:36:14,895863179-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 314.05k objects, 4.8 GiB
    usage:   9.6 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:36:14,910903013-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:36:14,922595248-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T00:36:14,926915298-07:00][RUNNING][ROUND 2/2/40] object_size=16KB[0m


[1;33mlocalhost.localdomain	[2021-05-17T00:36:14,933752973-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T00:36:14,950457386-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40725\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.307628\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 39b16651-67d3-40a8-a386-c56096d1b18a\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 39b16651-67d3-40a8-a386-c56096d1b18a\nlast_changed 2021-05-17T00:36:42.572372-0700\ncreated 2021-05-17T00:36:42.572372-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40725/0,v1:10.10.1.2:40726/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.307628 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 881ec457-dafc-4c6d-92b3-4b9a46e94bbc\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 d99e9201-3498-4a67-b092-1816503d2a50\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 4122f4ef-91f6-4925-82ef-af3f89efdbca\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\n'
10.10.1.2: b'restful urls: https://10.10.1.2:42725\n  w/ user/pass: admin / 77eccda5-8d75-4bcf-9e40-c0e1d0cfe3b3\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 00:36:57 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40725
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.307628
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 39b16651-67d3-40a8-a386-c56096d1b18a
setting min_mon_release = octopus
epoch 0
fsid 39b16651-67d3-40a8-a386-c56096d1b18a
last_changed 2021-05-17T00:36:42.572372-0700
created 2021-05-17T00:36:42.572372-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40725/0,v1:10.10.1.2:40726/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.307628 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 881ec457-dafc-4c6d-92b3-4b9a46e94bbc
0
start osd.0
add osd1 d99e9201-3498-4a67-b092-1816503d2a50
1
start osd.1
add osd2 4122f4ef-91f6-4925-82ef-af3f89efdbca
2
start osd.2


restful urls: https://10.10.1.2:42725
  w/ user/pass: admin / 77eccda5-8d75-4bcf-9e40-c0e1d0cfe3b3


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T00:36:15.950-0700 7f708a30c1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T00:36:15.950-0700 7f708a30c1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T00:36:15.966-0700 7f6cf7d5b1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T00:36:15.970-0700 7f6cf7d5b1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40725,v1:10.10.1.2:40726] --print /tmp/ceph_monmap.307628 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.307628 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.307628 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42725 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.MzBqcgHVDJ 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 881ec457-dafc-4c6d-92b3-4b9a46e94bbc -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQATHaJgNf7mHBAA/myDQ3ELbzHPw7psS+msLw== --osd-uuid 881ec457-dafc-4c6d-92b3-4b9a46e94bbc 
2021-05-17T00:36:51.827-0700 7f9a4e727f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T00:36:51.827-0700 7f9a4e727f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T00:36:51.827-0700 7f9a4e727f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T00:36:51.875-0700 7f9a4e727f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new d99e9201-3498-4a67-b092-1816503d2a50 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T00:36:52.159-0700 7f77c8535f00 -1 Falling back to public interface
2021-05-17T00:36:52.171-0700 7f77c8535f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAUHaJgDQiECRAAtBBXfKXpmd+gpq2iV4Xksw== --osd-uuid d99e9201-3498-4a67-b092-1816503d2a50 
2021-05-17T00:36:52.491-0700 7fa5b99adf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T00:36:52.491-0700 7fa5b99adf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T00:36:52.491-0700 7fa5b99adf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T00:36:52.535-0700 7fa5b99adf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 4122f4ef-91f6-4925-82ef-af3f89efdbca -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T00:36:52.887-0700 7fbc89644f00 -1 Falling back to public interface
2021-05-17T00:36:52.903-0700 7fbc89644f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAUHaJgiTTTNBAAUtLAe+1b4a0RIHYRsujcrQ== --osd-uuid 4122f4ef-91f6-4925-82ef-af3f89efdbca 
2021-05-17T00:36:53.227-0700 7fd69be35f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T00:36:53.227-0700 7fd69be35f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T00:36:53.227-0700 7fd69be35f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T00:36:53.315-0700 7fd69be35f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T00:36:53.615-0700 7fd32b148f00 -1 Falling back to public interface
2021-05-17T00:36:53.631-0700 7fd32b148f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T00:36:57,501054866-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T00:36:57,508364671-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T00:36:57,591153682-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:36:57,598654111-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '84d66a48-0ec8-4cc6-a47e-135cbe7fd6f4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aDvemU:/tmp/ceph-asok.aDvemU', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 84d66a48-0ec8-4cc6-a47e-135cbe7fd6f4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aDvemU:/tmp/ceph-asok.aDvemU -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T00:37:00,545299060-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:37:00,551561927-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '84d66a48-0ec8-4cc6-a47e-135cbe7fd6f4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aDvemU:/tmp/ceph-asok.aDvemU', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 84d66a48-0ec8-4cc6-a47e-135cbe7fd6f4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aDvemU:/tmp/ceph-asok.aDvemU -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T00:37:03,474168607-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:37:03,480618631-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '84d66a48-0ec8-4cc6-a47e-135cbe7fd6f4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aDvemU:/tmp/ceph-asok.aDvemU', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 84d66a48-0ec8-4cc6-a47e-135cbe7fd6f4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aDvemU:/tmp/ceph-asok.aDvemU -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T00:37:06,169714675-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:37:06,176538997-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '84d66a48-0ec8-4cc6-a47e-135cbe7fd6f4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aDvemU:/tmp/ceph-asok.aDvemU', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 84d66a48-0ec8-4cc6-a47e-135cbe7fd6f4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aDvemU:/tmp/ceph-asok.aDvemU -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T00:37:12,067412184-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:37:12,073891102-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '84d66a48-0ec8-4cc6-a47e-135cbe7fd6f4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aDvemU:/tmp/ceph-asok.aDvemU', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 84d66a48-0ec8-4cc6-a47e-135cbe7fd6f4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aDvemU:/tmp/ceph-asok.aDvemU -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T00:37:15,541057819-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:37:15,547514759-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '84d66a48-0ec8-4cc6-a47e-135cbe7fd6f4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aDvemU:/tmp/ceph-asok.aDvemU', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 84d66a48-0ec8-4cc6-a47e-135cbe7fd6f4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aDvemU:/tmp/ceph-asok.aDvemU -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T00:37:18,930155466-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:37:18,936663879-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '84d66a48-0ec8-4cc6-a47e-135cbe7fd6f4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aDvemU:/tmp/ceph-asok.aDvemU', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 84d66a48-0ec8-4cc6-a47e-135cbe7fd6f4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aDvemU:/tmp/ceph-asok.aDvemU -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T00:37:24,260520977-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:37:24,266890271-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '84d66a48-0ec8-4cc6-a47e-135cbe7fd6f4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aDvemU:/tmp/ceph-asok.aDvemU', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 84d66a48-0ec8-4cc6-a47e-135cbe7fd6f4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aDvemU:/tmp/ceph-asok.aDvemU -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T00:37:27,471824893-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:37:27,478291855-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '84d66a48-0ec8-4cc6-a47e-135cbe7fd6f4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aDvemU:/tmp/ceph-asok.aDvemU', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 84d66a48-0ec8-4cc6-a47e-135cbe7fd6f4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aDvemU:/tmp/ceph-asok.aDvemU -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T00:37:31,197868366-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:37:31,204310438-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '84d66a48-0ec8-4cc6-a47e-135cbe7fd6f4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aDvemU:/tmp/ceph-asok.aDvemU', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 84d66a48-0ec8-4cc6-a47e-135cbe7fd6f4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aDvemU:/tmp/ceph-asok.aDvemU -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T00:37:35,269020730-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:37:35,275393400-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '84d66a48-0ec8-4cc6-a47e-135cbe7fd6f4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aDvemU:/tmp/ceph-asok.aDvemU', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 84d66a48-0ec8-4cc6-a47e-135cbe7fd6f4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aDvemU:/tmp/ceph-asok.aDvemU -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T00:37:38,130667645-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:37:38,137059849-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '84d66a48-0ec8-4cc6-a47e-135cbe7fd6f4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aDvemU:/tmp/ceph-asok.aDvemU', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 84d66a48-0ec8-4cc6-a47e-135cbe7fd6f4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aDvemU:/tmp/ceph-asok.aDvemU -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  156 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T00:37:40,974324020-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:38:03,879021230-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (4s)
      [=========...................] (remaining: 9s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:38:11,864592378-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:38:19,823372773-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:38:27,670275269-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:38:35,874323176-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:38:43,896325488-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:38:52,047364254-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   214 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:38:52,062252892-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:38:59,984582950-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:38:59,999702057-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:39:07,939065647-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:39:07,953958470-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:39:16,023745021-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:39:16,038477467-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:39:24,009043725-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:39:24,024612383-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:39:24,037103115-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T00:39:24,045440582-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:39:24,063185108-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=3989165
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16KB_write.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-17T00:39:24,077598989-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16KB_write.log.2
[1;30mlocalhost.localdomain	[2021-05-17T00:39:24,121865772-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:39:24,128358898-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '84d66a48-0ec8-4cc6-a47e-135cbe7fd6f4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aDvemU:/tmp/ceph-asok.aDvemU', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 84d66a48-0ec8-4cc6-a47e-135cbe7fd6f4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aDvemU:/tmp/ceph-asok.aDvemU -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T07:39:25.753+0000 ffff8c588010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T07:39:25.761+0000 ffff8c588010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T07:39:25.761+0000 ffff8c588010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T07:39:25.778443+0000 Maintaining 256 concurrent writes of 16384 bytes to objects of size 16384 for up to 20 seconds or 0 objects
2021-05-17T07:39:25.778488+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T07:39:25.783086+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T07:39:25.783086+0000     0       0         0         0         0         0           -           0
2021-05-17T07:39:26.783352+0000     1     255     18769     18514   289.237   289.281  0.00791285    0.013431
2021-05-17T07:39:27.783611+0000     2     255     37540     37285   291.229   293.297  0.00388904   0.0135863
2021-05-17T07:39:28.783791+0000     3     255     57429     57174   297.723   310.766   0.0127196   0.0133759
2021-05-17T07:39:29.783983+0000     4     255     76374     76119   297.282   296.016  0.00818647   0.0134027
2021-05-17T07:39:30.784439+0000     5     255     95376     95121   297.179   296.906   0.0225863   0.0134081
2021-05-17T07:39:31.784667+0000     6     255    112705    112450   292.767   270.766   0.0103656   0.0135729
2021-05-17T07:39:32.784822+0000     7     255    129967    129712   289.469   269.719  0.00280071   0.0136685
2021-05-17T07:39:33.784987+0000     8     255    146697    146442   285.956   261.406  0.00321733   0.0138749
2021-05-17T07:39:34.785148+0000     9     255    164279    164024   284.702   274.719  0.00197923   0.0139794
2021-05-17T07:39:35.785319+0000    10     255    182116    181861   284.098   278.703  0.00490759    0.014049
2021-05-17T07:39:36.785485+0000    11     255    200351    200096   284.168   284.922   0.0206846   0.0140543
2021-05-17T07:39:37.785648+0000    12     255    217910    217655   283.347   274.359  0.00874017   0.0140988
2021-05-17T07:39:38.785820+0000    13     255    233578    233323    280.38   244.812   0.0103963   0.0142393
2021-05-17T07:39:39.786000+0000    14     255    251129    250874   279.937   274.234  0.00343607   0.0142482
2021-05-17T07:39:40.786222+0000    15     255    267054    266799    277.86   248.828   0.0202566   0.0143756
2021-05-17T07:39:41.786486+0000    16     255    282933    282678   275.996   248.109  0.00445755   0.0144704
2021-05-17T07:39:42.786665+0000    17     255    299376    299121   274.871   256.922   0.0156416   0.0145248
2021-05-17T07:39:43.786946+0000    18     255    317025    316770   274.917   275.766  0.00178025    0.014528
2021-05-17T07:39:44.787161+0000    19     255    334500    334245   274.815   273.047   0.0155521   0.0145393
2021-05-17T07:39:45.787349+0000 min lat: 0.000757029 max lat: 0.235802 avg lat: 0.0146098
2021-05-17T07:39:45.787349+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T07:39:45.787349+0000    20     131    349922    349791   273.218   242.906   0.0110201   0.0146098
2021-05-17T07:39:46.787597+0000 Total time run:         20.0439
Total writes made:      349922
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     272.777
Stddev Bandwidth:       18.7845
Max bandwidth (MB/sec): 310.766
Min bandwidth (MB/sec): 242.906
Average IOPS:           17457
Stddev IOPS:            1202.21
Max IOPS:               19889
Min IOPS:               15546
Average Latency(s):     0.014632
Stddev Latency(s):      0.0213464
Max latency(s):         0.235802
Min latency(s):         0.000757029

[1;32mlocalhost.localdomain	[2021-05-17T00:39:47,124888473-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 3989165


[1;33mlocalhost.localdomain	[2021-05-17T00:39:47,132304466-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:40:10,226124037-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 349.92k objects, 5.3 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:40:10,241073089-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:40:18,025748707-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 349.92k objects, 5.3 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:40:18,040962900-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:40:26,405092770-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 349.92k objects, 5.3 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:40:26,420233240-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:40:34,387353497-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 349.92k objects, 5.3 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:40:34,402512076-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:40:42,369599486-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 349.92k objects, 5.3 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:40:42,384752454-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:40:42,396834005-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T00:40:42,404012133-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:40:42,419861595-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=3992489
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16KB_seq.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-17T00:40:42,433221816-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16KB_seq.log.2
[1;30mlocalhost.localdomain	[2021-05-17T00:40:42,472635403-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:40:42,479220225-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '84d66a48-0ec8-4cc6-a47e-135cbe7fd6f4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aDvemU:/tmp/ceph-asok.aDvemU', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 84d66a48-0ec8-4cc6-a47e-135cbe7fd6f4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.aDvemU:/tmp/ceph-asok.aDvemU -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T07:40:43.987+0000 ffffa824e010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T07:40:44.099+0000 ffffa824e010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T07:40:44.099+0000 ffffa824e010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T07:40:44.118986+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T07:40:44.118986+0000     0       0         0         0         0         0           -           0
2021-05-17T07:40:45.119249+0000     1     255     18502     18247   284.995   285.109  0.00956429   0.0138174
2021-05-17T07:40:46.121039+0000     2     255     40199     39944   311.721   339.016  0.00469246   0.0127114
2021-05-17T07:40:47.121236+0000     3     255     65549     65294   339.802   396.094   0.0137674   0.0116984
2021-05-17T07:40:48.121413+0000     4     256     91015     90759     354.3   397.891 0.000448922   0.0111988
2021-05-17T07:40:49.121632+0000     5     255    107946    107691   336.347   264.562   0.0944899   0.0118336
2021-05-17T07:40:50.121835+0000     6     255    131783    131528    342.35   372.453   0.0213277   0.0116376
2021-05-17T07:40:51.122019+0000     7     255    157185    156930   350.132   396.906   0.0244076   0.0113813
2021-05-17T07:40:52.122207+0000     8     255    179537    179282   350.013    349.25   0.0698249   0.0113578
2021-05-17T07:40:53.122399+0000     9     255    201654    201399   349.513   345.578   0.0305895   0.0113999
2021-05-17T07:40:54.122591+0000    10     255    222995    222740   347.901   333.453  0.00192574   0.0114364
2021-05-17T07:40:55.122941+0000    11     256    241519    241263   342.576   289.422  0.00581016   0.0116484
2021-05-17T07:40:56.123199+0000    12     256    262819    262563   341.755   332.812  0.00500207   0.0116766
2021-05-17T07:40:57.123383+0000    13     256    280472    280216   336.681   275.828   0.0182735   0.0117875
2021-05-17T07:40:58.123578+0000    14     255    300983    300728   335.521     320.5  0.00275425   0.0118951
2021-05-17T07:40:59.123763+0000    15     256    320135    319879   333.098   299.234 0.000380946   0.0119212
2021-05-17T07:41:00.123966+0000    16     255    341416    341161   333.058   332.531  0.00237729   0.0119825
2021-05-17T07:41:01.124203+0000 Total time run:       16.4538
Total reads made:     349922
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   332.295
Average IOPS:         21266
Stddev IOPS:          2751.85
Max IOPS:             25465
Min IOPS:             16932
Average Latency(s):   0.0120108
Max latency(s):       0.15632
Min latency(s):       0.000326516

[1;32mlocalhost.localdomain	[2021-05-17T00:41:01,431332311-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 3992489


[1;33mlocalhost.localdomain	[2021-05-17T00:41:01,439279965-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:41:24,536906624-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 349.92k objects, 5.3 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:41:24,551907376-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:41:32,504693296-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 349.92k objects, 5.3 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:41:32,522717560-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:41:40,528426213-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 349.92k objects, 5.3 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:41:40,543439867-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:41:48,606558341-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 349.92k objects, 5.3 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:41:48,621820662-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:41:56,650671558-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 349.92k objects, 5.3 GiB
    usage:   11 GiB used, 289 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:41:56,665930683-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:41:56,677990692-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T00:41:56,682215279-07:00][RUNNING][ROUND 3/2/40] object_size=16KB[0m


[1;33mlocalhost.localdomain	[2021-05-17T00:41:56,689104878-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T00:41:56,705876832-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40627\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.308734\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 63f25911-f37f-4439-aab8-d21195210838\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 63f25911-f37f-4439-aab8-d21195210838\nlast_changed 2021-05-17T00:42:23.267510-0700\ncreated 2021-05-17T00:42:23.267510-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40627/0,v1:10.10.1.2:40628/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.308734 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 6632da39-99d9-4520-98b8-d8bc6e810767\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 2d2b3799-6322-4955-9296-bed678dcb491\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 c5de542e-5d10-48b2-a473-ba0a2f5b3564\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42627\n'
10.10.1.2: b'  w/ user/pass: admin / dfd8d2ef-c808-4727-aa1a-d42190f5150e\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 00:42:38 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40627
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.308734
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 63f25911-f37f-4439-aab8-d21195210838
setting min_mon_release = octopus
epoch 0
fsid 63f25911-f37f-4439-aab8-d21195210838
last_changed 2021-05-17T00:42:23.267510-0700
created 2021-05-17T00:42:23.267510-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40627/0,v1:10.10.1.2:40628/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.308734 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 6632da39-99d9-4520-98b8-d8bc6e810767
0
start osd.0
add osd1 2d2b3799-6322-4955-9296-bed678dcb491
1
start osd.1
add osd2 c5de542e-5d10-48b2-a473-ba0a2f5b3564
2
start osd.2


restful urls: https://10.10.1.2:42627
  w/ user/pass: admin / dfd8d2ef-c808-4727-aa1a-d42190f5150e


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T00:41:57.694-0700 7fb1804921c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T00:41:57.694-0700 7fb1804921c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T00:41:57.710-0700 7f39fd03d1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T00:41:57.710-0700 7f39fd03d1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40627,v1:10.10.1.2:40628] --print /tmp/ceph_monmap.308734 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.308734 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.308734 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42627 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.GIkA8UwiZe 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 6632da39-99d9-4520-98b8-d8bc6e810767 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBnHqJgDmjpORAAB/cntpzAGIG9abgNoazeZA== --osd-uuid 6632da39-99d9-4520-98b8-d8bc6e810767 
2021-05-17T00:42:32.298-0700 7f150b73cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T00:42:32.298-0700 7f150b73cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T00:42:32.298-0700 7f150b73cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T00:42:32.362-0700 7f150b73cf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 2d2b3799-6322-4955-9296-bed678dcb491 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T00:42:32.606-0700 7f2957435f00 -1 Falling back to public interface
2021-05-17T00:42:32.618-0700 7f2957435f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBoHqJg8n4yJBAAuJNGKQJDWxSTNwHvpj7AAg== --osd-uuid 2d2b3799-6322-4955-9296-bed678dcb491 
2021-05-17T00:42:32.986-0700 7fab45170f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T00:42:32.986-0700 7fab45170f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T00:42:32.986-0700 7fab45170f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T00:42:33.030-0700 7fab45170f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new c5de542e-5d10-48b2-a473-ba0a2f5b3564 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T00:42:33.458-0700 7fb7b8017f00 -1 Falling back to public interface
2021-05-17T00:42:33.474-0700 7fb7b8017f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBpHqJgfPBXGxAAO7Jv0C8yrY0jTx6yFtyegg== --osd-uuid c5de542e-5d10-48b2-a473-ba0a2f5b3564 
2021-05-17T00:42:33.786-0700 7fdb20ba8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T00:42:33.786-0700 7fdb20ba8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T00:42:33.786-0700 7fdb20ba8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T00:42:33.866-0700 7fdb20ba8f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T00:42:34.230-0700 7fc898784f00 -1 Falling back to public interface
2021-05-17T00:42:34.242-0700 7fc898784f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T00:42:38,120687408-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T00:42:38,128084678-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T00:42:38,211254346-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:42:38,217954927-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7c78e7cb-66ea-4967-b645-24786f35b29e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.crhZJG:/tmp/ceph-asok.crhZJG', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7c78e7cb-66ea-4967-b645-24786f35b29e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.crhZJG:/tmp/ceph-asok.crhZJG -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T00:42:41,145872600-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:42:41,152545052-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7c78e7cb-66ea-4967-b645-24786f35b29e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.crhZJG:/tmp/ceph-asok.crhZJG', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7c78e7cb-66ea-4967-b645-24786f35b29e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.crhZJG:/tmp/ceph-asok.crhZJG -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T00:42:43,940198780-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:42:43,946685184-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7c78e7cb-66ea-4967-b645-24786f35b29e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.crhZJG:/tmp/ceph-asok.crhZJG', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7c78e7cb-66ea-4967-b645-24786f35b29e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.crhZJG:/tmp/ceph-asok.crhZJG -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T00:42:46,830213338-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:42:46,836918210-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7c78e7cb-66ea-4967-b645-24786f35b29e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.crhZJG:/tmp/ceph-asok.crhZJG', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7c78e7cb-66ea-4967-b645-24786f35b29e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.crhZJG:/tmp/ceph-asok.crhZJG -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T00:42:52,247295818-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:42:52,253825220-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7c78e7cb-66ea-4967-b645-24786f35b29e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.crhZJG:/tmp/ceph-asok.crhZJG', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7c78e7cb-66ea-4967-b645-24786f35b29e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.crhZJG:/tmp/ceph-asok.crhZJG -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T00:42:56,169397080-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:42:56,175904998-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7c78e7cb-66ea-4967-b645-24786f35b29e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.crhZJG:/tmp/ceph-asok.crhZJG', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7c78e7cb-66ea-4967-b645-24786f35b29e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.crhZJG:/tmp/ceph-asok.crhZJG -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T00:42:59,440890999-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:42:59,447144443-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7c78e7cb-66ea-4967-b645-24786f35b29e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.crhZJG:/tmp/ceph-asok.crhZJG', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7c78e7cb-66ea-4967-b645-24786f35b29e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.crhZJG:/tmp/ceph-asok.crhZJG -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T00:43:02,700160195-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:43:02,706590760-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7c78e7cb-66ea-4967-b645-24786f35b29e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.crhZJG:/tmp/ceph-asok.crhZJG', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7c78e7cb-66ea-4967-b645-24786f35b29e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.crhZJG:/tmp/ceph-asok.crhZJG -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T00:43:06,115240885-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:43:06,121819420-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7c78e7cb-66ea-4967-b645-24786f35b29e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.crhZJG:/tmp/ceph-asok.crhZJG', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7c78e7cb-66ea-4967-b645-24786f35b29e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.crhZJG:/tmp/ceph-asok.crhZJG -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T00:43:09,441252432-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:43:09,447978577-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7c78e7cb-66ea-4967-b645-24786f35b29e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.crhZJG:/tmp/ceph-asok.crhZJG', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7c78e7cb-66ea-4967-b645-24786f35b29e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.crhZJG:/tmp/ceph-asok.crhZJG -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T00:43:12,600489014-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:43:12,606907622-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7c78e7cb-66ea-4967-b645-24786f35b29e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.crhZJG:/tmp/ceph-asok.crhZJG', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7c78e7cb-66ea-4967-b645-24786f35b29e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.crhZJG:/tmp/ceph-asok.crhZJG -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T00:43:15,243464721-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:43:15,250270260-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7c78e7cb-66ea-4967-b645-24786f35b29e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.crhZJG:/tmp/ceph-asok.crhZJG', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7c78e7cb-66ea-4967-b645-24786f35b29e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.crhZJG:/tmp/ceph-asok.crhZJG -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T00:43:18,022811745-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:43:41,009861309-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   193 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:43:48,944780917-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:43:56,925200349-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (9s)
      [=======.....................] (remaining: 26s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:44:05,087571216-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [========....................] (remaining: 47s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:44:13,143962645-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [========....................] (remaining: 53s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:44:21,129136888-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [=========...................] (remaining: 73s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:44:29,117940006-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (40s)
      [=========...................] (remaining: 83s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:44:37,395161023-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   228 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [==============..............] (remaining: 44s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:44:45,528094943-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:44:45,543380998-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:44:53,554759651-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:44:53,570041176-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:45:01,653530530-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:45:01,668762576-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:45:09,606532345-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:45:09,621577984-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:45:17,602562296-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:45:17,617651592-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:45:17,629494828-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T00:45:17,636436291-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:45:17,652312306-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=4006201
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16KB_write.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-17T00:45:17,665886673-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16KB_write.log.3
[1;30mlocalhost.localdomain	[2021-05-17T00:45:17,706566612-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:45:17,712932246-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7c78e7cb-66ea-4967-b645-24786f35b29e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.crhZJG:/tmp/ceph-asok.crhZJG', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7c78e7cb-66ea-4967-b645-24786f35b29e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.crhZJG:/tmp/ceph-asok.crhZJG -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T07:45:19.402+0000 ffffae0d0010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T07:45:19.410+0000 ffffae0d0010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T07:45:19.410+0000 ffffae0d0010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T07:45:19.429808+0000 Maintaining 256 concurrent writes of 16384 bytes to objects of size 16384 for up to 20 seconds or 0 objects
2021-05-17T07:45:19.429864+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T07:45:19.434310+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T07:45:19.434310+0000     0       0         0         0         0         0           -           0
2021-05-17T07:45:20.434515+0000     1     255     18226     17971   280.773   280.797  0.00983934   0.0140402
2021-05-17T07:45:21.434804+0000     2     255     37159     36904   288.258   295.828   0.0195872    0.013763
2021-05-17T07:45:22.435023+0000     3     255     54969     54714   284.912   278.281    0.010237   0.0139234
2021-05-17T07:45:23.435238+0000     4     255     73072     72817   284.384   282.859   0.0136312   0.0140176
2021-05-17T07:45:24.435422+0000     5     256     90656     90400   282.444   274.734   0.0116623   0.0141156
2021-05-17T07:45:25.435605+0000     6     255    108368    108113   281.489   276.766  0.00536595   0.0141614
2021-05-17T07:45:26.435783+0000     7     256    123896    123640   275.929   242.609  0.00660146   0.0144637
2021-05-17T07:45:27.435960+0000     8     255    139714    139459   272.329   247.172    0.010574   0.0146505
2021-05-17T07:45:28.436129+0000     9     255    155193    154938   268.939   241.859  0.00700524   0.0148336
2021-05-17T07:45:29.436304+0000    10     256    169705    169449   264.714   226.734  0.00144844   0.0150274
2021-05-17T07:45:30.436499+0000    11     255    182687    182432   259.088   202.859   0.0210394   0.0154168
2021-05-17T07:45:31.436681+0000    12     255    198348    198093   257.885   244.703  0.00762312   0.0154818
2021-05-17T07:45:32.436908+0000    13     256    212126    211870   254.603   215.266    0.011816   0.0156891
2021-05-17T07:45:33.437133+0000    14     256    228071    227815   254.209   249.141 0.000947457   0.0157017
2021-05-17T07:45:34.437316+0000    15     255    242474    242219   252.263   225.062   0.0242825   0.0158386
2021-05-17T07:45:35.437708+0000    16     255    257514    257259   251.178       235   0.0222495   0.0159058
2021-05-17T07:45:36.437875+0000    17     255    272858    272603   250.503    239.75  0.00694136   0.0159471
2021-05-17T07:45:37.438202+0000    18     255    288579    288324   250.229   245.641   0.0283472   0.0159691
2021-05-17T07:45:38.438438+0000    19     255    302039    301784   248.125   210.312   0.0238429   0.0161033
2021-05-17T07:45:39.438604+0000 min lat: 0.000749753 max lat: 0.160639 avg lat: 0.0161719
2021-05-17T07:45:39.438604+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T07:45:39.438604+0000    20      53    316494    316441   247.168   229.016  0.00493038   0.0161719
2021-05-17T07:45:40.438899+0000 Total time run:         20.0118
Total writes made:      316494
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     247.115
Stddev Bandwidth:       26.3737
Max bandwidth (MB/sec): 295.828
Min bandwidth (MB/sec): 202.859
Average IOPS:           15815
Stddev IOPS:            1687.92
Max IOPS:               18933
Min IOPS:               12983
Average Latency(s):     0.0161716
Stddev Latency(s):      0.0138733
Max latency(s):         0.160639
Min latency(s):         0.000749753

[1;32mlocalhost.localdomain	[2021-05-17T00:45:40,759597646-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 4006201


[1;33mlocalhost.localdomain	[2021-05-17T00:45:40,767287932-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:46:03,720157499-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 316.50k objects, 4.8 GiB
    usage:   9.7 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:46:03,735442128-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:46:11,876556578-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 316.50k objects, 4.8 GiB
    usage:   9.7 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:46:11,891836542-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:46:19,840846472-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 316.50k objects, 4.8 GiB
    usage:   9.7 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:46:19,858001502-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:46:27,980541087-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 316.50k objects, 4.8 GiB
    usage:   9.7 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:46:27,995645221-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:46:36,203028681-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 316.50k objects, 4.8 GiB
    usage:   9.7 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:46:36,218956215-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:46:36,230986083-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T00:46:36,238141497-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:46:36,253842568-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=4009527
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16KB_seq.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-17T00:46:36,266966482-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16KB_seq.log.3
[1;30mlocalhost.localdomain	[2021-05-17T00:46:36,306175689-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:46:36,312592782-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7c78e7cb-66ea-4967-b645-24786f35b29e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.crhZJG:/tmp/ceph-asok.crhZJG', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7c78e7cb-66ea-4967-b645-24786f35b29e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.crhZJG:/tmp/ceph-asok.crhZJG -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T07:46:37.831+0000 ffffa0569010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T07:46:37.839+0000 ffffa0569010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T07:46:37.839+0000 ffffa0569010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T07:46:37.859845+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T07:46:37.859845+0000     0       0         0         0         0         0           -           0
2021-05-17T07:46:38.860023+0000     1     255     18220     17965   280.613   280.703   0.0280881   0.0140113
2021-05-17T07:46:39.862958+0000     2     256     34370     34114   266.082   252.328  0.00101006   0.0148625
2021-05-17T07:46:40.863161+0000     3     255     52143     51888   269.939   277.719  0.00636665    0.014768
2021-05-17T07:46:41.863357+0000     4     256     70381     70125   273.676   284.953 0.000396503   0.0145167
2021-05-17T07:46:42.863590+0000     5     255     90078     89823   280.479   307.781  0.00964191   0.0142173
2021-05-17T07:46:43.863814+0000     6     256    108953    108697   282.871   294.906  0.00480649   0.0140995
2021-05-17T07:46:44.864029+0000     7     255    128021    127766   285.016   297.953   0.0023991    0.013995
2021-05-17T07:46:45.864266+0000     8     255    147042    146787    286.53   297.203  0.00674019   0.0139212
2021-05-17T07:46:46.864447+0000     9     256    163703    163447   283.613   260.312  0.00040501    0.013985
2021-05-17T07:46:47.864678+0000    10     255    180002    179747   280.715   254.688  0.00936315   0.0142242
2021-05-17T07:46:48.864974+0000    11     255    201991    201736    286.42   343.578  0.00478244    0.013938
2021-05-17T07:46:49.865781+0000    12     255    221720    221465    288.22   308.266 0.000881055    0.013846
2021-05-17T07:46:50.866016+0000    13     255    240116    239861   288.155   287.438  0.00274158   0.0138548
2021-05-17T07:46:51.866291+0000    14     256    255801    255545   285.072   245.062 0.000428503   0.0139963
2021-05-17T07:46:52.866482+0000    15     255    273394    273139   284.391   274.906  0.00251968   0.0140346
2021-05-17T07:46:53.866683+0000    16     256    292884    292628   285.645   304.516   0.0043651   0.0139808
2021-05-17T07:46:54.866870+0000    17     255    313987    313732   288.235    329.75   0.0270072   0.0138547
2021-05-17T07:46:55.867106+0000 Total time run:       17.1279
Total reads made:     316494
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   288.723
Average IOPS:         18478
Stddev IOPS:          1713.46
Max IOPS:             21989
Min IOPS:             15684
Average Latency(s):   0.0138329
Max latency(s):       0.173201
Min latency(s):       0.000309518

[1;32mlocalhost.localdomain	[2021-05-17T00:46:56,191101818-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 4009527


[1;33mlocalhost.localdomain	[2021-05-17T00:46:56,198756247-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:47:19,470226538-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 316.50k objects, 4.8 GiB
    usage:   9.7 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:47:19,486280619-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:47:27,420835859-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 316.50k objects, 4.8 GiB
    usage:   9.7 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:47:27,436291109-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:47:35,663637824-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 316.50k objects, 4.8 GiB
    usage:   9.7 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:47:35,679394865-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:47:43,666746441-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 316.50k objects, 4.8 GiB
    usage:   9.7 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:47:43,682493027-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:47:51,658953759-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 316.50k objects, 4.8 GiB
    usage:   9.7 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:47:51,674376277-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:47:51,686614407-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T00:47:51,690959853-07:00][RUNNING][ROUND 4/2/40] object_size=16KB[0m


[1;33mlocalhost.localdomain	[2021-05-17T00:47:51,697863512-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T00:47:51,714436302-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40541\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.309851\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 7314dc7b-8fca-43cc-a317-00b078648055\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 7314dc7b-8fca-43cc-a317-00b078648055\nlast_changed 2021-05-17T00:48:18.795084-0700\ncreated 2021-05-17T00:48:18.795084-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40541/0,v1:10.10.1.2:40542/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.309851 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 0a76d0fb-6ecb-4307-a957-31f9e14518f0\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 9d9dc3d2-7b10-4699-8dee-28d5696cad61\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 5481672c-53ff-457a-bd0e-f4051cc7a9f0\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42541\n  w/ user/pass: admin / b3012106-61cf-40be-8261-2b2c0237b8e8\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 00:48:33 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40541
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.309851
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 7314dc7b-8fca-43cc-a317-00b078648055
setting min_mon_release = octopus
epoch 0
fsid 7314dc7b-8fca-43cc-a317-00b078648055
last_changed 2021-05-17T00:48:18.795084-0700
created 2021-05-17T00:48:18.795084-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40541/0,v1:10.10.1.2:40542/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.309851 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 0a76d0fb-6ecb-4307-a957-31f9e14518f0
0
start osd.0
add osd1 9d9dc3d2-7b10-4699-8dee-28d5696cad61
1
start osd.1
add osd2 5481672c-53ff-457a-bd0e-f4051cc7a9f0
2
start osd.2


restful urls: https://10.10.1.2:42541
  w/ user/pass: admin / b3012106-61cf-40be-8261-2b2c0237b8e8


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T00:47:52.701-0700 7f107115c1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T00:47:52.701-0700 7f107115c1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T00:47:52.717-0700 7f448784b1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T00:47:52.717-0700 7f448784b1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40541,v1:10.10.1.2:40542] --print /tmp/ceph_monmap.309851 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.309851 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.309851 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42541 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.e7QExYpf7c 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 0a76d0fb-6ecb-4307-a957-31f9e14518f0 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDLH6Jg045+JRAAkovk0+0h6DNJKEtbRbLEpQ== --osd-uuid 0a76d0fb-6ecb-4307-a957-31f9e14518f0 
2021-05-17T00:48:27.953-0700 7fda889d5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T00:48:27.953-0700 7fda889d5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T00:48:27.953-0700 7fda889d5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T00:48:28.025-0700 7fda889d5f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 9d9dc3d2-7b10-4699-8dee-28d5696cad61 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T00:48:28.369-0700 7f5a036d9f00 -1 Falling back to public interface
2021-05-17T00:48:28.381-0700 7f5a036d9f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDMH6JgkQj3FRAAze/k03HlC9gKJy+G60ue2w== --osd-uuid 9d9dc3d2-7b10-4699-8dee-28d5696cad61 
2021-05-17T00:48:28.705-0700 7f7c1c7cbf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T00:48:28.705-0700 7f7c1c7cbf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T00:48:28.705-0700 7f7c1c7cbf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T00:48:28.753-0700 7f7c1c7cbf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 5481672c-53ff-457a-bd0e-f4051cc7a9f0 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T00:48:29.045-0700 7fae9ccb1f00 -1 Falling back to public interface
2021-05-17T00:48:29.061-0700 7fae9ccb1f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDNH6JgGpTMAhAAuO3FIgr9nuZwR6/veT7SJA== --osd-uuid 5481672c-53ff-457a-bd0e-f4051cc7a9f0 
2021-05-17T00:48:29.377-0700 7f26638c3f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T00:48:29.377-0700 7f26638c3f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T00:48:29.377-0700 7f26638c3f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T00:48:29.437-0700 7f26638c3f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T00:48:29.813-0700 7fce9bb26f00 -1 Falling back to public interface
2021-05-17T00:48:29.829-0700 7fce9bb26f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T00:48:33,708750264-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T00:48:33,716055152-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T00:48:33,799711797-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:48:33,806279731-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '61159097-25da-4d69-a27d-3f1209f0d3c4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sM9Sir:/tmp/ceph-asok.sM9Sir', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 61159097-25da-4d69-a27d-3f1209f0d3c4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sM9Sir:/tmp/ceph-asok.sM9Sir -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T00:48:36,610252249-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:48:36,616924211-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '61159097-25da-4d69-a27d-3f1209f0d3c4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sM9Sir:/tmp/ceph-asok.sM9Sir', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 61159097-25da-4d69-a27d-3f1209f0d3c4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sM9Sir:/tmp/ceph-asok.sM9Sir -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T00:48:39,419395970-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:48:39,426040838-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '61159097-25da-4d69-a27d-3f1209f0d3c4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sM9Sir:/tmp/ceph-asok.sM9Sir', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 61159097-25da-4d69-a27d-3f1209f0d3c4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sM9Sir:/tmp/ceph-asok.sM9Sir -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T00:48:42,295033935-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:48:42,301517421-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '61159097-25da-4d69-a27d-3f1209f0d3c4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sM9Sir:/tmp/ceph-asok.sM9Sir', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 61159097-25da-4d69-a27d-3f1209f0d3c4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sM9Sir:/tmp/ceph-asok.sM9Sir -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T00:48:47,903013350-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:48:47,909630478-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '61159097-25da-4d69-a27d-3f1209f0d3c4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sM9Sir:/tmp/ceph-asok.sM9Sir', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 61159097-25da-4d69-a27d-3f1209f0d3c4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sM9Sir:/tmp/ceph-asok.sM9Sir -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T00:48:51,622458256-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:48:51,628961035-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '61159097-25da-4d69-a27d-3f1209f0d3c4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sM9Sir:/tmp/ceph-asok.sM9Sir', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 61159097-25da-4d69-a27d-3f1209f0d3c4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sM9Sir:/tmp/ceph-asok.sM9Sir -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T00:48:55,017508546-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:48:55,024462121-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '61159097-25da-4d69-a27d-3f1209f0d3c4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sM9Sir:/tmp/ceph-asok.sM9Sir', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 61159097-25da-4d69-a27d-3f1209f0d3c4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sM9Sir:/tmp/ceph-asok.sM9Sir -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T00:48:58,280662356-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:48:58,287791971-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '61159097-25da-4d69-a27d-3f1209f0d3c4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sM9Sir:/tmp/ceph-asok.sM9Sir', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 61159097-25da-4d69-a27d-3f1209f0d3c4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sM9Sir:/tmp/ceph-asok.sM9Sir -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T00:49:01,582812173-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:49:01,590193200-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '61159097-25da-4d69-a27d-3f1209f0d3c4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sM9Sir:/tmp/ceph-asok.sM9Sir', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 61159097-25da-4d69-a27d-3f1209f0d3c4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sM9Sir:/tmp/ceph-asok.sM9Sir -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T00:49:04,777336176-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:49:04,783709949-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '61159097-25da-4d69-a27d-3f1209f0d3c4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sM9Sir:/tmp/ceph-asok.sM9Sir', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 61159097-25da-4d69-a27d-3f1209f0d3c4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sM9Sir:/tmp/ceph-asok.sM9Sir -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T00:49:08,266050909-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:49:08,272509042-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '61159097-25da-4d69-a27d-3f1209f0d3c4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sM9Sir:/tmp/ceph-asok.sM9Sir', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 61159097-25da-4d69-a27d-3f1209f0d3c4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sM9Sir:/tmp/ceph-asok.sM9Sir -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T00:49:11,067934963-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:49:11,074833269-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '61159097-25da-4d69-a27d-3f1209f0d3c4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sM9Sir:/tmp/ceph-asok.sM9Sir', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 61159097-25da-4d69-a27d-3f1209f0d3c4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sM9Sir:/tmp/ceph-asok.sM9Sir -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T00:49:13,782071527-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:49:36,714229470-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   189 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:49:44,727610278-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:49:52,751206316-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:50:00,930858358-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:50:08,928371040-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:50:17,088873126-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:50:25,076709871-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:50:33,113726625-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:50:33,129030622-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:50:41,076081604-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:50:41,091584820-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:50:49,011963002-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:50:49,027784783-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:50:57,001175189-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:50:57,017059006-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:51:05,181595425-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:51:05,197410365-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:51:05,209872107-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T00:51:05,217215842-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:51:05,233241403-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=4022933
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16KB_write.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-17T00:51:05,247132750-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16KB_write.log.4
[1;30mlocalhost.localdomain	[2021-05-17T00:51:05,287269856-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:51:05,293761343-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '61159097-25da-4d69-a27d-3f1209f0d3c4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sM9Sir:/tmp/ceph-asok.sM9Sir', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 61159097-25da-4d69-a27d-3f1209f0d3c4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sM9Sir:/tmp/ceph-asok.sM9Sir -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T07:51:06.940+0000 ffff81ccf010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T07:51:06.948+0000 ffff81ccf010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T07:51:06.948+0000 ffff81ccf010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T07:51:06.962758+0000 Maintaining 256 concurrent writes of 16384 bytes to objects of size 16384 for up to 20 seconds or 0 objects
2021-05-17T07:51:06.962803+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T07:51:06.967282+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T07:51:06.967282+0000     0       0         0         0         0         0           -           0
2021-05-17T07:51:07.967526+0000     1     255     17434     17179   268.386   268.422  0.00434162   0.0145077
2021-05-17T07:51:08.967793+0000     2     255     35278     35023   273.562   278.812   0.0123136   0.0144666
2021-05-17T07:51:09.968112+0000     3     255     53285     53030   276.132   281.359   0.0227714   0.0144096
2021-05-17T07:51:10.968359+0000     4     255     72985     72730   284.033   307.812  0.00284577    0.013986
2021-05-17T07:51:11.968551+0000     5     255     91658     91403   285.568   291.766  0.00236801   0.0138874
2021-05-17T07:51:12.969001+0000     6     256    110724    110468     287.6   297.891  0.00813666    0.013847
2021-05-17T07:51:13.969378+0000     7     255    128484    128229   286.144   277.516   0.0119312   0.0139477
2021-05-17T07:51:14.969658+0000     8     255    146668    146413   285.882   284.125   0.0625943   0.0139364
2021-05-17T07:51:15.969850+0000     9     255    165343    165088   286.533   291.797  0.00679113   0.0139161
2021-05-17T07:51:16.970118+0000    10     255    181273    181018   282.764   248.906   0.0545948   0.0141111
2021-05-17T07:51:17.970355+0000    11     255    196206    195951   278.265   233.328  0.00942769   0.0143543
2021-05-17T07:51:18.970730+0000    12     255    212010    211755   275.646   246.938   0.0251214   0.0144791
2021-05-17T07:51:19.971020+0000    13     256    228606    228350   274.383   259.297   0.0235708   0.0145603
2021-05-17T07:51:20.971202+0000    14     256    244472    244216   272.488   247.906   0.0302645   0.0146547
2021-05-17T07:51:21.971774+0000    15     255    261909    261654   272.477   272.469   0.0101048   0.0146512
2021-05-17T07:51:22.972007+0000    16     256    279072    278816   272.203   268.156 0.000794399   0.0146566
2021-05-17T07:51:23.972172+0000    17     255    295378    295123   271.177   254.797   0.0140251   0.0147295
2021-05-17T07:51:24.972335+0000    18     255    312480    312225   270.954   267.219    0.039666    0.014735
2021-05-17T07:51:25.972507+0000    19     256    326204    325948   267.977   214.422  0.00246414   0.0148941
2021-05-17T07:51:26.972674+0000 min lat: 0.000735034 max lat: 0.20644 avg lat: 0.0149709
2021-05-17T07:51:26.972674+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T07:51:26.972674+0000    20      45    341860    341815   266.972   247.922   0.0216219   0.0149709
2021-05-17T07:51:27.972985+0000 Total time run:         20.0185
Total writes made:      341860
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     266.831
Stddev Bandwidth:       23.1652
Max bandwidth (MB/sec): 307.812
Min bandwidth (MB/sec): 214.422
Average IOPS:           17077
Stddev IOPS:            1482.57
Max IOPS:               19700
Min IOPS:               13723
Average Latency(s):     0.0149715
Stddev Latency(s):      0.0202188
Max latency(s):         0.20644
Min latency(s):         0.000735034

[1;32mlocalhost.localdomain	[2021-05-17T00:51:28,289322202-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 4022933


[1;33mlocalhost.localdomain	[2021-05-17T00:51:28,296974720-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:51:51,133157048-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 341.86k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:51:51,149070618-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:51:59,355408831-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 341.86k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:51:59,371479028-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:52:07,332742362-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 341.86k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:52:07,350591005-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:52:15,317866306-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 341.86k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:52:15,333878175-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:52:23,667356111-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 341.86k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:52:23,683035749-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:52:23,695267836-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T00:52:23,702445315-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:52:23,718725339-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=4026249
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16KB_seq.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-17T00:52:23,732932314-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16KB_seq.log.4
[1;30mlocalhost.localdomain	[2021-05-17T00:52:23,771819063-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:52:23,778298325-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '61159097-25da-4d69-a27d-3f1209f0d3c4', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sM9Sir:/tmp/ceph-asok.sM9Sir', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 61159097-25da-4d69-a27d-3f1209f0d3c4 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sM9Sir:/tmp/ceph-asok.sM9Sir -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T07:52:25.490+0000 ffff8fc5b010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T07:52:25.850+0000 ffff8fc5b010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T07:52:25.850+0000 ffff8fc5b010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T07:52:25.868858+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T07:52:25.868858+0000     0       0         0         0         0         0           -           0
2021-05-17T07:52:26.869019+0000     1     255     17604     17349       271   271.078  0.00875183   0.0144653
2021-05-17T07:52:27.869351+0000     2     255     39584     39329   307.163   343.438   0.0294079   0.0129173
2021-05-17T07:52:28.869573+0000     3     255     59585     59330   308.924   312.516   0.0154839   0.0128942
2021-05-17T07:52:29.869851+0000     4     256     80531     80275   313.487   327.266  0.00183181   0.0126952
2021-05-17T07:52:30.870111+0000     5     256     99832     99576   311.089   301.578 0.000350189   0.0126372
2021-05-17T07:52:31.870289+0000     6     255    120160    119905   312.172   317.641  0.00864083   0.0127748
2021-05-17T07:52:32.870591+0000     7     255    142462    142207   317.342   348.469   0.0263131   0.0125648
2021-05-17T07:52:33.870908+0000     8     256    158896    158640    309.76   256.766   0.0117291   0.0128847
2021-05-17T07:52:34.871191+0000     9     255    180894    180639   313.524   343.734   0.0171731   0.0127296
2021-05-17T07:52:35.871428+0000    10     255    202033    201778   315.193   330.297   0.0261136   0.0126583
2021-05-17T07:52:36.871662+0000    11     256    225133    224877   319.342   360.922  0.00139792   0.0124945
2021-05-17T07:52:37.871972+0000    12     256    247941    247685   322.419   356.375   0.0010677   0.0123803
2021-05-17T07:52:38.872156+0000    13     255    268323    268068   322.112   318.484  0.00226655   0.0123953
2021-05-17T07:52:39.872613+0000    14     255    284047    283792   316.644   245.688     0.01319   0.0126117
2021-05-17T07:52:40.872964+0000    15     255    303471    303216   315.761     303.5 0.000386135    0.012637
2021-05-17T07:52:41.873187+0000    16     255    326266    326011   318.281   356.172   0.0106457   0.0125466
2021-05-17T07:52:42.873422+0000 Total time run:       16.831
Total reads made:     341860
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   317.364
Average IOPS:         20311
Stddev IOPS:          2273.9
Max IOPS:             23099
Min IOPS:             15724
Average Latency(s):   0.0125828
Max latency(s):       0.210262
Min latency(s):       0.000327866

[1;32mlocalhost.localdomain	[2021-05-17T00:52:43,181587612-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 4026249


[1;33mlocalhost.localdomain	[2021-05-17T00:52:43,189361908-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:53:06,333503976-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 341.86k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:53:06,349097757-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:53:14,335628489-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 341.86k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:53:14,351365680-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:53:22,407712664-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 341.86k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:53:22,423782236-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:53:30,608896694-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 341.86k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:53:30,625116007-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:53:38,456219952-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 341.86k objects, 5.2 GiB
    usage:   10 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:53:38,471846575-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:53:38,484235367-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T00:53:38,488577235-07:00][RUNNING][ROUND 5/2/40] object_size=16KB[0m


[1;33mlocalhost.localdomain	[2021-05-17T00:53:38,495836600-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T00:53:38,512823098-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40493\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.310963\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 1a0b69d6-605b-44ea-80c3-e2fc725681cc\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 1a0b69d6-605b-44ea-80c3-e2fc725681cc\nlast_changed 2021-05-17T00:54:05.019996-0700\ncreated 2021-05-17T00:54:05.019996-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40493/0,v1:10.10.1.2:40494/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.310963 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 87f32a04-7ab3-4ada-8e84-43cf82fbd672\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 f1899a54-a7ea-4e51-bed3-1c90db5a7fe2\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 cf9d20be-dfe6-4dcd-8aa6-7e548dce37ac\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\n'
10.10.1.2: b'restful urls: https://10.10.1.2:42493\n  w/ user/pass: admin / 8eb86ef5-649a-4eaa-a7c1-d6c4f86d38ce\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 00:54:19 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40493
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.310963
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 1a0b69d6-605b-44ea-80c3-e2fc725681cc
setting min_mon_release = octopus
epoch 0
fsid 1a0b69d6-605b-44ea-80c3-e2fc725681cc
last_changed 2021-05-17T00:54:05.019996-0700
created 2021-05-17T00:54:05.019996-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40493/0,v1:10.10.1.2:40494/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.310963 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 87f32a04-7ab3-4ada-8e84-43cf82fbd672
0
start osd.0
add osd1 f1899a54-a7ea-4e51-bed3-1c90db5a7fe2
1
start osd.1
add osd2 cf9d20be-dfe6-4dcd-8aa6-7e548dce37ac
2
start osd.2


restful urls: https://10.10.1.2:42493
  w/ user/pass: admin / 8eb86ef5-649a-4eaa-a7c1-d6c4f86d38ce


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T00:53:39.492-0700 7f73d9d971c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T00:53:39.492-0700 7f73d9d971c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T00:53:39.508-0700 7f434eb731c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T00:53:39.508-0700 7f434eb731c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40493,v1:10.10.1.2:40494] --print /tmp/ceph_monmap.310963 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.310963 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.310963 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42493 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.7c4L167fm5 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 87f32a04-7ab3-4ada-8e84-43cf82fbd672 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAlIaJgYfqzMhAAmbafFXaQX2JAIMmTMOJLVQ== --osd-uuid 87f32a04-7ab3-4ada-8e84-43cf82fbd672 
2021-05-17T00:54:14.181-0700 7f9c493baf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T00:54:14.181-0700 7f9c493baf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T00:54:14.181-0700 7f9c493baf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T00:54:14.253-0700 7f9c493baf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new f1899a54-a7ea-4e51-bed3-1c90db5a7fe2 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T00:54:14.533-0700 7fbaedc7df00 -1 Falling back to public interface
2021-05-17T00:54:14.545-0700 7fbaedc7df00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAmIaJgRZC6HxAAfXyxV3+L19Uj6i0oIqEOvw== --osd-uuid f1899a54-a7ea-4e51-bed3-1c90db5a7fe2 
2021-05-17T00:54:14.877-0700 7fbb6592bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T00:54:14.877-0700 7fbb6592bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T00:54:14.877-0700 7fbb6592bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T00:54:14.921-0700 7fbb6592bf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new cf9d20be-dfe6-4dcd-8aa6-7e548dce37ac -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T00:54:15.225-0700 7f987713ef00 -1 Falling back to public interface
2021-05-17T00:54:15.241-0700 7f987713ef00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAnIaJgTiKEDRAAoBKM/P296PB3sumyUY0ngQ== --osd-uuid cf9d20be-dfe6-4dcd-8aa6-7e548dce37ac 
2021-05-17T00:54:15.573-0700 7efc55a62f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T00:54:15.573-0700 7efc55a62f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T00:54:15.573-0700 7efc55a62f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T00:54:15.617-0700 7efc55a62f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T00:54:15.997-0700 7efe018f1f00 -1 Falling back to public interface
2021-05-17T00:54:16.013-0700 7efe018f1f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T00:54:19,932742290-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T00:54:19,940254045-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T00:54:20,026283335-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:54:20,032831417-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c0eb0aa9-bb16-4f38-a531-ad0fa62fc5e1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.abOS51:/tmp/ceph-asok.abOS51', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c0eb0aa9-bb16-4f38-a531-ad0fa62fc5e1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.abOS51:/tmp/ceph-asok.abOS51 -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T00:54:22,968606820-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:54:22,975009286-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c0eb0aa9-bb16-4f38-a531-ad0fa62fc5e1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.abOS51:/tmp/ceph-asok.abOS51', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c0eb0aa9-bb16-4f38-a531-ad0fa62fc5e1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.abOS51:/tmp/ceph-asok.abOS51 -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T00:54:25,643559110-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:54:25,651171997-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c0eb0aa9-bb16-4f38-a531-ad0fa62fc5e1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.abOS51:/tmp/ceph-asok.abOS51', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c0eb0aa9-bb16-4f38-a531-ad0fa62fc5e1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.abOS51:/tmp/ceph-asok.abOS51 -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T00:54:28,449366056-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:54:28,456055762-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c0eb0aa9-bb16-4f38-a531-ad0fa62fc5e1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.abOS51:/tmp/ceph-asok.abOS51', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c0eb0aa9-bb16-4f38-a531-ad0fa62fc5e1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.abOS51:/tmp/ceph-asok.abOS51 -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T00:54:34,239898894-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:54:34,246635963-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c0eb0aa9-bb16-4f38-a531-ad0fa62fc5e1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.abOS51:/tmp/ceph-asok.abOS51', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c0eb0aa9-bb16-4f38-a531-ad0fa62fc5e1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.abOS51:/tmp/ceph-asok.abOS51 -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T00:54:37,718225061-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:54:37,724758831-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c0eb0aa9-bb16-4f38-a531-ad0fa62fc5e1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.abOS51:/tmp/ceph-asok.abOS51', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c0eb0aa9-bb16-4f38-a531-ad0fa62fc5e1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.abOS51:/tmp/ceph-asok.abOS51 -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T00:54:41,215365285-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:54:41,221966581-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c0eb0aa9-bb16-4f38-a531-ad0fa62fc5e1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.abOS51:/tmp/ceph-asok.abOS51', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c0eb0aa9-bb16-4f38-a531-ad0fa62fc5e1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.abOS51:/tmp/ceph-asok.abOS51 -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T00:54:44,604900787-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:54:44,611304305-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c0eb0aa9-bb16-4f38-a531-ad0fa62fc5e1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.abOS51:/tmp/ceph-asok.abOS51', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c0eb0aa9-bb16-4f38-a531-ad0fa62fc5e1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.abOS51:/tmp/ceph-asok.abOS51 -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T00:54:47,886299033-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:54:47,892797636-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c0eb0aa9-bb16-4f38-a531-ad0fa62fc5e1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.abOS51:/tmp/ceph-asok.abOS51', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c0eb0aa9-bb16-4f38-a531-ad0fa62fc5e1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.abOS51:/tmp/ceph-asok.abOS51 -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T00:54:51,128763598-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:54:51,135062788-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c0eb0aa9-bb16-4f38-a531-ad0fa62fc5e1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.abOS51:/tmp/ceph-asok.abOS51', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c0eb0aa9-bb16-4f38-a531-ad0fa62fc5e1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.abOS51:/tmp/ceph-asok.abOS51 -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T00:54:54,511026076-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:54:54,517617785-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c0eb0aa9-bb16-4f38-a531-ad0fa62fc5e1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.abOS51:/tmp/ceph-asok.abOS51', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c0eb0aa9-bb16-4f38-a531-ad0fa62fc5e1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.abOS51:/tmp/ceph-asok.abOS51 -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T00:54:57,223309808-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:54:57,229786088-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c0eb0aa9-bb16-4f38-a531-ad0fa62fc5e1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.abOS51:/tmp/ceph-asok.abOS51', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c0eb0aa9-bb16-4f38-a531-ad0fa62fc5e1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.abOS51:/tmp/ceph-asok.abOS51 -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T00:55:00,114913677-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:55:23,037238589-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   189 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:55:31,018951004-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:55:39,146557608-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=======.....................] (remaining: 30s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:55:47,148657893-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [=======.....................] (remaining: 53s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:55:55,083614004-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [========....................] (remaining: 62s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:56:03,094610215-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [========....................] (remaining: 85s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:56:11,165729269-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (40s)
      [========....................] (remaining: 97s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:56:19,154416582-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [==============..............] (remaining: 46s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:56:27,057169174-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:56:27,074676403-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:56:35,060013963-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:56:35,075921787-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:56:43,163825932-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:56:43,179698515-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:56:51,124273024-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:56:51,140048301-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:56:59,301267306-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:56:59,317487184-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:56:59,330524272-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T00:56:59,337927396-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:56:59,354459537-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=4040117
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16KB_write.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-17T00:56:59,368229409-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16KB_write.log.5
[1;30mlocalhost.localdomain	[2021-05-17T00:56:59,408658978-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:56:59,415214833-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c0eb0aa9-bb16-4f38-a531-ad0fa62fc5e1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.abOS51:/tmp/ceph-asok.abOS51', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16384', '-O', '16384', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c0eb0aa9-bb16-4f38-a531-ad0fa62fc5e1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.abOS51:/tmp/ceph-asok.abOS51 -- rados bench 20 write --pool bench_rados -b 16384 -O 16384 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T07:57:00.908+0000 ffffa47eb010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T07:57:00.916+0000 ffffa47eb010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T07:57:00.920+0000 ffffa47eb010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T07:57:00.935666+0000 Maintaining 256 concurrent writes of 16384 bytes to objects of size 16384 for up to 20 seconds or 0 objects
2021-05-17T07:57:00.935733+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T07:57:00.941008+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T07:57:00.941008+0000     0       0         0         0         0         0           -           0
2021-05-17T07:57:01.941196+0000     1     255     18214     17959   280.595   280.609  0.00603629   0.0138263
2021-05-17T07:57:02.941363+0000     2     255     35695     35440   276.845   273.141   0.0351189   0.0143305
2021-05-17T07:57:03.941552+0000     3     255     54335     54080   281.628    291.25   0.0535304   0.0141054
2021-05-17T07:57:04.941710+0000     4     255     71420     71165   277.949   266.953     0.01335    0.014331
2021-05-17T07:57:05.941883+0000     5     255     88313     88058   275.141   263.953  0.00632654   0.0144845
2021-05-17T07:57:06.942051+0000     6     255    105228    104973   273.326   264.297   0.0057167   0.0145293
2021-05-17T07:57:07.942219+0000     7     255    121911    121656   271.512   260.672  0.00862769   0.0146975
2021-05-17T07:57:08.942394+0000     8     255    138561    138306   270.087   260.156  0.00681321    0.014777
2021-05-17T07:57:09.942562+0000     9     255    154541    154286   267.815   249.688   0.0152767   0.0149101
2021-05-17T07:57:10.946247+0000    10     255    171806    171551   267.912   269.766     0.03493   0.0148966
2021-05-17T07:57:11.947812+0000    11     255    188737    188482   267.568   264.547  0.00389217   0.0149279
2021-05-17T07:57:12.948235+0000    12     255    202657    202402   263.389     217.5  0.00951647   0.0151646
2021-05-17T07:57:13.948577+0000    13     255    216721    216466   260.027    219.75  0.00514157   0.0153626
2021-05-17T07:57:14.948955+0000    14     256    232512    232256    259.07   246.719   0.0152231   0.0154232
2021-05-17T07:57:15.949604+0000    15     255    249038    248783   259.003   258.234   0.0102727   0.0154208
2021-05-17T07:57:16.949884+0000    16     255    263712    263457   257.142   229.281   0.0429118   0.0155308
2021-05-17T07:57:17.950079+0000    17     255    278337    278082   255.456   228.516   0.0558869   0.0156297
2021-05-17T07:57:18.950354+0000    18     255    291434    291179    252.63   204.641    0.008487   0.0158085
2021-05-17T07:57:19.950567+0000    19     255    307930    307675   252.897    257.75  0.00712711   0.0157999
2021-05-17T07:57:20.950934+0000 Total time run:         20.0074
Total writes made:      322897
Write size:             16384
Object size:            16384
Bandwidth (MB/sec):     252.169
Stddev Bandwidth:       23.0514
Max bandwidth (MB/sec): 291.25
Min bandwidth (MB/sec): 204.641
Average IOPS:           16138
Stddev IOPS:            1475.29
Max IOPS:               18640
Min IOPS:               13097
Average Latency(s):     0.0158496
Stddev Latency(s):      0.016456
Max latency(s):         0.202267
Min latency(s):         0.000723738

[1;32mlocalhost.localdomain	[2021-05-17T00:57:21,272539729-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 4040117


[1;33mlocalhost.localdomain	[2021-05-17T00:57:21,280403836-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:57:44,629931289-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 322.90k objects, 4.9 GiB
    usage:   9.9 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:57:44,645707573-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:57:52,985537348-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 322.90k objects, 4.9 GiB
    usage:   9.9 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:57:53,001276216-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:58:01,044635085-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 322.90k objects, 4.9 GiB
    usage:   9.9 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:58:01,060385941-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:58:09,372828374-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 322.90k objects, 4.9 GiB
    usage:   9.9 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:58:09,388780144-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:58:17,244657710-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 322.90k objects, 4.9 GiB
    usage:   9.9 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:58:17,260074039-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:58:17,272446337-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T00:58:17,279519121-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:58:17,296045159-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=4043428
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16KB_seq.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-17T00:58:17,310338832-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16KB_seq.log.5
[1;30mlocalhost.localdomain	[2021-05-17T00:58:17,349484160-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T00:58:17,355877134-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'c0eb0aa9-bb16-4f38-a531-ad0fa62fc5e1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.abOS51:/tmp/ceph-asok.abOS51', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid c0eb0aa9-bb16-4f38-a531-ad0fa62fc5e1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.abOS51:/tmp/ceph-asok.abOS51 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T07:58:19.113+0000 ffffa9960010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T07:58:19.357+0000 ffffa9960010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T07:58:19.357+0000 ffffa9960010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T07:58:19.380159+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T07:58:19.380159+0000     0       0         0         0         0         0           -           0
2021-05-17T07:58:20.380315+0000     1     256     20784     20528   320.661    320.75  0.00453764   0.0122311
2021-05-17T07:58:21.380521+0000     2     255     40459     40204   314.018   307.438   0.0140266   0.0126762
2021-05-17T07:58:22.380720+0000     3     256     59389     59133   307.914   295.766  0.00528256    0.012924
2021-05-17T07:58:23.380922+0000     4     255     77846     77591   303.023   288.406  0.00419345   0.0131421
2021-05-17T07:58:24.381599+0000     5     255     95580     95325   297.798   277.094  0.00295808   0.0133751
2021-05-17T07:58:25.381803+0000     6     255    114962    114707   298.628   302.844   0.0063014   0.0133595
2021-05-17T07:58:26.382033+0000     7     255    137639    137384   306.573   354.328   0.0272436   0.0130103
2021-05-17T07:58:27.382203+0000     8     256    155704    155448   303.527    282.25   0.0300026   0.0129614
2021-05-17T07:58:28.382711+0000     9     256    175588    175332   304.305   310.688  0.00183817   0.0129606
2021-05-17T07:58:29.383037+0000    10     255    193271    193016   301.497   276.312  0.00204866   0.0132372
2021-05-17T07:58:30.383339+0000    11     255    209708    209453   297.429   256.828  0.00366856   0.0134166
2021-05-17T07:58:31.383715+0000    12     256    229233    228977   298.056   305.062    0.027886   0.0133913
2021-05-17T07:58:32.385038+0000    13     255    247958    247703   297.605   292.594  0.00269735   0.0134111
2021-05-17T07:58:33.385352+0000    14     256    263853    263597   294.081   248.344  0.00156549    0.013448
2021-05-17T07:58:34.385539+0000    15     256    283046    282790   294.465   299.891  0.00148887   0.0135506
2021-05-17T07:58:35.385719+0000    16     255    305410    305155   297.897   349.453   0.0033812   0.0134046
2021-05-17T07:58:36.385976+0000 Total time run:       16.8425
Total reads made:     322897
Read size:            16384
Object size:          16384
Bandwidth (MB/sec):   299.556
Average IOPS:         19171
Stddev IOPS:          1818.28
Max IOPS:             22677
Min IOPS:             15894
Average Latency(s):   0.0133318
Max latency(s):       0.203137
Min latency(s):       0.000300276

[1;32mlocalhost.localdomain	[2021-05-17T00:58:36,706671143-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 4043428


[1;33mlocalhost.localdomain	[2021-05-17T00:58:36,714682051-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T00:58:59,910506248-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 322.90k objects, 4.9 GiB
    usage:   9.9 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:58:59,926520385-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:59:08,380022246-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 322.90k objects, 4.9 GiB
    usage:   9.9 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:59:08,395764912-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:59:16,294589023-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 322.90k objects, 4.9 GiB
    usage:   9.9 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:59:16,310481489-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:59:24,127861871-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 322.90k objects, 4.9 GiB
    usage:   9.9 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:59:24,143496804-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:59:32,386527229-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 322.90k objects, 4.9 GiB
    usage:   9.9 GiB used, 290 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T00:59:32,402773651-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T00:59:32,415544853-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T00:59:32,423337971-07:00][RUNNING][ROUND 1/3/40] object_size=64KB[0m


[1;33mlocalhost.localdomain	[2021-05-17T00:59:32,430695789-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T00:59:32,447495595-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40028\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.312076\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 79b19233-e3d1-4e02-a50f-f5c71e196278\nsetting min_mon_release = octopus\nepoch 0\nfsid 79b19233-e3d1-4e02-a50f-f5c71e196278\nlast_changed 2021-05-17T00:59:58.872615-0700\ncreated 2021-05-17T00:59:58.872615-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40028/0,v1:10.10.1.2:40029/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.312076 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 a94e5e9f-d37c-406d-80ac-d09fee9cd5e3\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 dd7b0b04-d93f-4b93-9d26-0f43bcb24816\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 0905d078-ddec-4eb8-9a72-9f8e373ef8c8\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42028\n  w/ user/pass: admin / 75fa5656-167a-4a34-835f-7fa75f88441d\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 01:00:13 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40028
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.312076
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 79b19233-e3d1-4e02-a50f-f5c71e196278
setting min_mon_release = octopus
epoch 0
fsid 79b19233-e3d1-4e02-a50f-f5c71e196278
last_changed 2021-05-17T00:59:58.872615-0700
created 2021-05-17T00:59:58.872615-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40028/0,v1:10.10.1.2:40029/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.312076 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 a94e5e9f-d37c-406d-80ac-d09fee9cd5e3
0
start osd.0
add osd1 dd7b0b04-d93f-4b93-9d26-0f43bcb24816
1
start osd.1
add osd2 0905d078-ddec-4eb8-9a72-9f8e373ef8c8
2
start osd.2


restful urls: https://10.10.1.2:42028
  w/ user/pass: admin / 75fa5656-167a-4a34-835f-7fa75f88441d


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T00:59:33.464-0700 7f1e4c7ce1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T00:59:33.464-0700 7f1e4c7ce1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T00:59:33.484-0700 7f18e00401c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T00:59:33.484-0700 7f18e00401c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40028,v1:10.10.1.2:40029] --print /tmp/ceph_monmap.312076 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.312076 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.312076 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42028 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.AaLhKhoc1c 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new a94e5e9f-d37c-406d-80ac-d09fee9cd5e3 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCHIqJgeBWlKRAAVMijQeO/9So5bRup8Hx7kQ== --osd-uuid a94e5e9f-d37c-406d-80ac-d09fee9cd5e3 
2021-05-17T01:00:08.028-0700 7f80fb6bff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T01:00:08.032-0700 7f80fb6bff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T01:00:08.032-0700 7f80fb6bff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T01:00:08.092-0700 7f80fb6bff00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new dd7b0b04-d93f-4b93-9d26-0f43bcb24816 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T01:00:08.376-0700 7f57846e3f00 -1 Falling back to public interface
2021-05-17T01:00:08.388-0700 7f57846e3f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCIIqJgNnWoFhAAhgP+NbZ28huaYeVawWleQQ== --osd-uuid dd7b0b04-d93f-4b93-9d26-0f43bcb24816 
2021-05-17T01:00:08.708-0700 7faaadcd3f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T01:00:08.708-0700 7faaadcd3f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T01:00:08.708-0700 7faaadcd3f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T01:00:08.760-0700 7faaadcd3f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 0905d078-ddec-4eb8-9a72-9f8e373ef8c8 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T01:00:09.012-0700 7ff8d18c1f00 -1 Falling back to public interface
2021-05-17T01:00:09.028-0700 7ff8d18c1f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCJIqJgjPf8ABAA2vGzOHPgKmJJpIo67Sc46Q== --osd-uuid 0905d078-ddec-4eb8-9a72-9f8e373ef8c8 
2021-05-17T01:00:09.352-0700 7ff099789f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T01:00:09.352-0700 7ff099789f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T01:00:09.352-0700 7ff099789f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T01:00:09.408-0700 7ff099789f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T01:00:09.784-0700 7f329e43af00 -1 Falling back to public interface
2021-05-17T01:00:09.800-0700 7f329e43af00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T01:00:13,643062555-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T01:00:13,650913523-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T01:00:13,744880956-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:00:13,751428412-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '72b333a2-39e5-47cb-9e05-41d9cb02ea6c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DxVI6W:/tmp/ceph-asok.DxVI6W', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 72b333a2-39e5-47cb-9e05-41d9cb02ea6c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DxVI6W:/tmp/ceph-asok.DxVI6W -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T01:00:16,518690688-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:00:16,525278201-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '72b333a2-39e5-47cb-9e05-41d9cb02ea6c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DxVI6W:/tmp/ceph-asok.DxVI6W', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 72b333a2-39e5-47cb-9e05-41d9cb02ea6c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DxVI6W:/tmp/ceph-asok.DxVI6W -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T01:00:19,522378594-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:00:19,528595845-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '72b333a2-39e5-47cb-9e05-41d9cb02ea6c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DxVI6W:/tmp/ceph-asok.DxVI6W', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 72b333a2-39e5-47cb-9e05-41d9cb02ea6c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DxVI6W:/tmp/ceph-asok.DxVI6W -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T01:00:22,286070979-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:00:22,292569272-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '72b333a2-39e5-47cb-9e05-41d9cb02ea6c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DxVI6W:/tmp/ceph-asok.DxVI6W', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 72b333a2-39e5-47cb-9e05-41d9cb02ea6c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DxVI6W:/tmp/ceph-asok.DxVI6W -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T01:00:27,878409356-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:00:27,885171889-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '72b333a2-39e5-47cb-9e05-41d9cb02ea6c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DxVI6W:/tmp/ceph-asok.DxVI6W', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 72b333a2-39e5-47cb-9e05-41d9cb02ea6c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DxVI6W:/tmp/ceph-asok.DxVI6W -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T01:00:31,597947553-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:00:31,604551584-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '72b333a2-39e5-47cb-9e05-41d9cb02ea6c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DxVI6W:/tmp/ceph-asok.DxVI6W', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 72b333a2-39e5-47cb-9e05-41d9cb02ea6c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DxVI6W:/tmp/ceph-asok.DxVI6W -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T01:00:34,952994754-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:00:34,959538085-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '72b333a2-39e5-47cb-9e05-41d9cb02ea6c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DxVI6W:/tmp/ceph-asok.DxVI6W', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 72b333a2-39e5-47cb-9e05-41d9cb02ea6c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DxVI6W:/tmp/ceph-asok.DxVI6W -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T01:00:38,373303745-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:00:38,379397270-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '72b333a2-39e5-47cb-9e05-41d9cb02ea6c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DxVI6W:/tmp/ceph-asok.DxVI6W', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 72b333a2-39e5-47cb-9e05-41d9cb02ea6c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DxVI6W:/tmp/ceph-asok.DxVI6W -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T01:00:41,752878677-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:00:41,759436185-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '72b333a2-39e5-47cb-9e05-41d9cb02ea6c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DxVI6W:/tmp/ceph-asok.DxVI6W', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 72b333a2-39e5-47cb-9e05-41d9cb02ea6c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DxVI6W:/tmp/ceph-asok.DxVI6W -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T01:00:44,995971803-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:00:45,002597498-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '72b333a2-39e5-47cb-9e05-41d9cb02ea6c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DxVI6W:/tmp/ceph-asok.DxVI6W', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 72b333a2-39e5-47cb-9e05-41d9cb02ea6c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DxVI6W:/tmp/ceph-asok.DxVI6W -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T01:00:48,351211480-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:00:48,357715940-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '72b333a2-39e5-47cb-9e05-41d9cb02ea6c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DxVI6W:/tmp/ceph-asok.DxVI6W', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 72b333a2-39e5-47cb-9e05-41d9cb02ea6c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DxVI6W:/tmp/ceph-asok.DxVI6W -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T01:00:51,008823265-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:00:51,016139692-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '72b333a2-39e5-47cb-9e05-41d9cb02ea6c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DxVI6W:/tmp/ceph-asok.DxVI6W', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 72b333a2-39e5-47cb-9e05-41d9cb02ea6c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DxVI6W:/tmp/ceph-asok.DxVI6W -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   47 KiB   0 B   0 B   0 B  100 GiB     0  0.95   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.95/1.03  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T01:00:53,665002797-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:01:16,747514260-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   199 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
    Global Recovery Event (4s)
      [======================......] (remaining: 1s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:01:24,889983047-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (9s)
      [=========================...] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:01:32,872950619-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:01:40,806031472-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:01:48,808336164-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:01:56,923903975-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:02:04,900240712-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:02:12,985093461-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:02:13,001160741-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:02:21,008436587-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:02:21,024432261-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:02:28,865996983-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:02:28,882235623-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:02:37,045175711-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:02:37,061276418-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:02:44,903962716-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:02:44,919590318-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:02:44,932350752-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T01:02:44,939719209-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:02:44,955963310-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=4056822
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_64KB_write.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-17T01:02:44,970060141-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_64KB_write.log.1
[1;30mlocalhost.localdomain	[2021-05-17T01:02:45,010981922-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:02:45,017527414-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '72b333a2-39e5-47cb-9e05-41d9cb02ea6c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DxVI6W:/tmp/ceph-asok.DxVI6W', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 72b333a2-39e5-47cb-9e05-41d9cb02ea6c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DxVI6W:/tmp/ceph-asok.DxVI6W -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T08:02:46.715+0000 ffff8c721010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T08:02:46.723+0000 ffff8c721010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T08:02:46.723+0000 ffff8c721010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T08:02:46.740788+0000 Maintaining 256 concurrent writes of 65536 bytes to objects of size 65536 for up to 20 seconds or 0 objects
2021-05-17T08:02:46.740831+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T08:02:46.754548+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T08:02:46.754548+0000     0       0         0         0         0         0           -           0
2021-05-17T08:02:47.754759+0000     1     255     10113      9858   616.073   616.125   0.0140517   0.0251228
2021-05-17T08:02:48.754966+0000     2     255     20191     19936   622.909   629.875  0.00473669   0.0252102
2021-05-17T08:02:49.755164+0000     3     256     29641     29385   612.088   590.562   0.0102219   0.0259716
2021-05-17T08:02:50.755359+0000     4     255     38161     37906    592.18   532.562   0.0618832   0.0267701
2021-05-17T08:02:51.755556+0000     5     256     47897     47641   595.408   608.438   0.0133965   0.0267094
2021-05-17T08:02:52.755766+0000     6     256     56182     55926   582.457   517.812   0.0135235   0.0273482
2021-05-17T08:02:53.755968+0000     7     256     65233     64977   580.045   565.688   0.0135984   0.0274918
2021-05-17T08:02:54.756164+0000     8     255     73473     73218   571.909   515.062   0.0212586   0.0278628
2021-05-17T08:02:55.756348+0000     9     256     81225     80969    562.18   484.438  0.00118111    0.028317
2021-05-17T08:02:56.756532+0000    10     255     89107     88852   555.222   492.688   0.0172564   0.0287511
2021-05-17T08:02:57.756750+0000    11     256     97118     96862   550.248   500.625  0.00146033   0.0289056
2021-05-17T08:02:58.756938+0000    12     256    104082    103826   540.658    435.25  0.00135523   0.0294507
2021-05-17T08:02:59.757153+0000    13     256    111003    110747   532.336   432.562   0.0297991   0.0299773
2021-05-17T08:03:00.757518+0000    14     256    118705    118449   528.683   481.375  0.00423348   0.0301605
2021-05-17T08:03:01.758212+0000    15     256    126634    126378   526.451   495.562  0.00549402   0.0303252
2021-05-17T08:03:02.758669+0000    16     255    135164    134909   526.857   533.188   0.0143134   0.0303172
2021-05-17T08:03:03.759187+0000    17     256    142778    142522   523.839   475.812   0.0077207   0.0304917
2021-05-17T08:03:04.759546+0000    18     255    150748    150493   522.404   498.188   0.0363569   0.0305951
2021-05-17T08:03:05.759812+0000    19     255    158149    157894   519.248   462.562    0.037422   0.0307619
2021-05-17T08:03:06.760110+0000 min lat: 0.00102187 max lat: 0.17827 avg lat: 0.0310785
2021-05-17T08:03:06.760110+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T08:03:06.760110+0000    20     140    164742    164602   514.241    419.25  0.00674312   0.0310785
2021-05-17T08:03:07.760501+0000 Total time run:         20.0204
Total writes made:      164742
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     514.294
Stddev Bandwidth:       61.1773
Max bandwidth (MB/sec): 629.875
Min bandwidth (MB/sec): 419.25
Average IOPS:           8228
Stddev IOPS:            978.837
Max IOPS:               10078
Min IOPS:               6708
Average Latency(s):     0.0310787
Stddev Latency(s):      0.0227332
Max latency(s):         0.17827
Min latency(s):         0.00102187

[1;32mlocalhost.localdomain	[2021-05-17T01:03:08,074806214-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 4056822


[1;33mlocalhost.localdomain	[2021-05-17T01:03:08,082939682-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:03:31,016090328-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 164.74k objects, 10 GiB
    usage:   20 GiB used, 280 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:03:31,032372641-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:03:39,323414445-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 164.74k objects, 10 GiB
    usage:   20 GiB used, 280 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:03:39,339750363-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:03:47,230587082-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 164.74k objects, 10 GiB
    usage:   20 GiB used, 280 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:03:47,246902131-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:03:55,416261068-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 164.74k objects, 10 GiB
    usage:   20 GiB used, 280 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:03:55,432294040-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:04:03,400103085-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 164.74k objects, 10 GiB
    usage:   20 GiB used, 280 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:04:03,417433642-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:04:03,431861071-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T01:04:03,440772046-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:04:03,461070528-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=4060219
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_64KB_seq.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-17T01:04:03,475771648-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_64KB_seq.log.1
[1;30mlocalhost.localdomain	[2021-05-17T01:04:03,516426597-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:04:03,523203414-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '72b333a2-39e5-47cb-9e05-41d9cb02ea6c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DxVI6W:/tmp/ceph-asok.DxVI6W', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 72b333a2-39e5-47cb-9e05-41d9cb02ea6c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.DxVI6W:/tmp/ceph-asok.DxVI6W -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T08:04:04.980+0000 ffffb59f0010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T08:04:04.988+0000 ffffb59f0010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T08:04:04.988+0000 ffffb59f0010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T08:04:05.006145+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T08:04:05.006145+0000     0       0         0         0         0         0           -           0
2021-05-17T08:04:06.006347+0000     1     255     12822     12567   785.171   785.438  0.00760911   0.0200179
2021-05-17T08:04:07.006552+0000     2     255     25207     24952   779.538   774.062    0.006523   0.0202246
2021-05-17T08:04:08.006772+0000     3     256     37072     36816   766.805     741.5   0.0341106   0.0207052
2021-05-17T08:04:09.006997+0000     4     255     50112     49857   778.823   815.062  0.00976327   0.0204107
2021-05-17T08:04:10.007272+0000     5     255     63507     63252    790.45   837.188   0.0102333   0.0201458
2021-05-17T08:04:11.007491+0000     6     255     75760     75505   786.316   765.812   0.0370272   0.0202697
2021-05-17T08:04:12.007696+0000     7     255     88062     87807   783.802   768.875   0.0310979   0.0203521
2021-05-17T08:04:13.007945+0000     8     255     99769     99514   777.265   731.688   0.0603731   0.0205005
2021-05-17T08:04:14.008710+0000     9     255    110961    110706   768.561     699.5   0.0163435   0.0207487
2021-05-17T08:04:15.008947+0000    10     256    123543    123287   770.317   786.312   0.0554961   0.0206819
2021-05-17T08:04:16.009175+0000    11     255    134189    133934    760.77   665.438   0.0505605   0.0209646
2021-05-17T08:04:17.009371+0000    12     256    143638    143382   746.572     590.5 0.000781123    0.021309
2021-05-17T08:04:18.009566+0000    13     256    153065    152809   734.458   589.188   0.0057237   0.0217094
2021-05-17T08:04:19.009795+0000    14     255    162845    162590   725.652   611.312  0.00798062   0.0219936
2021-05-17T08:04:20.010053+0000 Total time run:       14.2138
Total reads made:     164742
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   724.391
Average IOPS:         11590
Stddev IOPS:          1316.94
Max IOPS:             13395
Min IOPS:             9427
Average Latency(s):   0.0220276
Max latency(s):       0.155756
Min latency(s):       0.000456258

[1;32mlocalhost.localdomain	[2021-05-17T01:04:20,338326403-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 4060219


[1;33mlocalhost.localdomain	[2021-05-17T01:04:20,346237414-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:04:43,295866486-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 164.74k objects, 10 GiB
    usage:   20 GiB used, 280 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:04:43,311832982-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:04:51,175825921-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 164.74k objects, 10 GiB
    usage:   20 GiB used, 280 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:04:51,191914328-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:04:59,461317764-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 164.74k objects, 10 GiB
    usage:   20 GiB used, 280 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:04:59,477339740-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:05:07,429311272-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 164.74k objects, 10 GiB
    usage:   20 GiB used, 280 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:05:07,445707036-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:05:15,290397593-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 164.74k objects, 10 GiB
    usage:   20 GiB used, 280 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:05:15,306542469-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:05:15,319130286-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T01:05:15,323795329-07:00][RUNNING][ROUND 2/3/40] object_size=64KB[0m


[1;33mlocalhost.localdomain	[2021-05-17T01:05:15,331216237-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T01:05:15,348100679-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40033\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.313196\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 7574e9b2-db0b-4b65-adee-9225f3ba2a65\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 7574e9b2-db0b-4b65-adee-9225f3ba2a65\nlast_changed 2021-05-17T01:05:44.801925-0700\ncreated 2021-05-17T01:05:44.801925-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40033/0,v1:10.10.1.2:40034/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.313196 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 372824a6-0f58-491c-abf0-57ac2e288f5b\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 05c70ce9-bba8-4100-84c7-fe6f4c94bf11\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 2bb5ba74-6d48-4006-a831-0fc951a5d685\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42033\n  w/ user/pass: admin / 72c04f77-0ebb-4a65-9a30-a1d2f3551764\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 01:05:59 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40033
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.313196
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 7574e9b2-db0b-4b65-adee-9225f3ba2a65
setting min_mon_release = octopus
epoch 0
fsid 7574e9b2-db0b-4b65-adee-9225f3ba2a65
last_changed 2021-05-17T01:05:44.801925-0700
created 2021-05-17T01:05:44.801925-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40033/0,v1:10.10.1.2:40034/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.313196 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 372824a6-0f58-491c-abf0-57ac2e288f5b
0
start osd.0
add osd1 05c70ce9-bba8-4100-84c7-fe6f4c94bf11
1
start osd.1
add osd2 2bb5ba74-6d48-4006-a831-0fc951a5d685
2
start osd.2


restful urls: https://10.10.1.2:42033
  w/ user/pass: admin / 72c04f77-0ebb-4a65-9a30-a1d2f3551764


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T01:05:16.343-0700 7f8c3dea81c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T01:05:16.343-0700 7f8c3dea81c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T01:05:16.359-0700 7f9310ea11c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T01:05:16.359-0700 7f9310ea11c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40033,v1:10.10.1.2:40034] --print /tmp/ceph_monmap.313196 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.313196 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.313196 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42033 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.XPzOmIm9G5 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 372824a6-0f58-491c-abf0-57ac2e288f5b -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDhI6JgQbOQKRAA4H5MxYqYQHqT0zOFgEWoQA== --osd-uuid 372824a6-0f58-491c-abf0-57ac2e288f5b 
2021-05-17T01:05:54.039-0700 7f7c9cf1ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T01:05:54.039-0700 7f7c9cf1ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T01:05:54.039-0700 7f7c9cf1ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T01:05:54.119-0700 7f7c9cf1ef00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 05c70ce9-bba8-4100-84c7-fe6f4c94bf11 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T01:05:54.415-0700 7fdd5fa3ef00 -1 Falling back to public interface
2021-05-17T01:05:54.427-0700 7fdd5fa3ef00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDiI6JgPxmiGBAA8XGmpmJBhC5a7QIccL4nHg== --osd-uuid 05c70ce9-bba8-4100-84c7-fe6f4c94bf11 
2021-05-17T01:05:54.743-0700 7f1a09ba9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T01:05:54.743-0700 7f1a09ba9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T01:05:54.743-0700 7f1a09ba9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T01:05:54.787-0700 7f1a09ba9f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 2bb5ba74-6d48-4006-a831-0fc951a5d685 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T01:05:55.083-0700 7fe6b05fcf00 -1 Falling back to public interface
2021-05-17T01:05:55.095-0700 7fe6b05fcf00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDjI6JgjaPdBBAAzIryA+5dX4YTfRO2+VZiGg== --osd-uuid 2bb5ba74-6d48-4006-a831-0fc951a5d685 
2021-05-17T01:05:55.415-0700 7f5bed61bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T01:05:55.415-0700 7f5bed61bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T01:05:55.415-0700 7f5bed61bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T01:05:55.475-0700 7f5bed61bf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T01:05:55.847-0700 7f5d30e53f00 -1 Falling back to public interface
2021-05-17T01:05:55.859-0700 7f5d30e53f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T01:05:59,759562872-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T01:05:59,766963271-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T01:05:59,848333412-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:05:59,854798118-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2fa9c5a1-5afb-47ad-8ec7-069b7a097538', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8rSirr:/tmp/ceph-asok.8rSirr', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2fa9c5a1-5afb-47ad-8ec7-069b7a097538 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8rSirr:/tmp/ceph-asok.8rSirr -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T01:06:02,837256995-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:06:02,843847797-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2fa9c5a1-5afb-47ad-8ec7-069b7a097538', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8rSirr:/tmp/ceph-asok.8rSirr', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2fa9c5a1-5afb-47ad-8ec7-069b7a097538 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8rSirr:/tmp/ceph-asok.8rSirr -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T01:06:05,681816689-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:06:05,688302128-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2fa9c5a1-5afb-47ad-8ec7-069b7a097538', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8rSirr:/tmp/ceph-asok.8rSirr', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2fa9c5a1-5afb-47ad-8ec7-069b7a097538 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8rSirr:/tmp/ceph-asok.8rSirr -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T01:06:08,474823387-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:06:08,481381408-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2fa9c5a1-5afb-47ad-8ec7-069b7a097538', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8rSirr:/tmp/ceph-asok.8rSirr', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2fa9c5a1-5afb-47ad-8ec7-069b7a097538 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8rSirr:/tmp/ceph-asok.8rSirr -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T01:06:14,122730868-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:06:14,129185476-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2fa9c5a1-5afb-47ad-8ec7-069b7a097538', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8rSirr:/tmp/ceph-asok.8rSirr', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2fa9c5a1-5afb-47ad-8ec7-069b7a097538 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8rSirr:/tmp/ceph-asok.8rSirr -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T01:06:17,605770787-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:06:17,612212538-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2fa9c5a1-5afb-47ad-8ec7-069b7a097538', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8rSirr:/tmp/ceph-asok.8rSirr', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2fa9c5a1-5afb-47ad-8ec7-069b7a097538 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8rSirr:/tmp/ceph-asok.8rSirr -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T01:06:20,914730060-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:06:20,921182868-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2fa9c5a1-5afb-47ad-8ec7-069b7a097538', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8rSirr:/tmp/ceph-asok.8rSirr', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2fa9c5a1-5afb-47ad-8ec7-069b7a097538 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8rSirr:/tmp/ceph-asok.8rSirr -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T01:06:24,302690326-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:06:24,309157641-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2fa9c5a1-5afb-47ad-8ec7-069b7a097538', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8rSirr:/tmp/ceph-asok.8rSirr', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2fa9c5a1-5afb-47ad-8ec7-069b7a097538 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8rSirr:/tmp/ceph-asok.8rSirr -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T01:06:27,641571803-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:06:27,648129554-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2fa9c5a1-5afb-47ad-8ec7-069b7a097538', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8rSirr:/tmp/ceph-asok.8rSirr', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2fa9c5a1-5afb-47ad-8ec7-069b7a097538 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8rSirr:/tmp/ceph-asok.8rSirr -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T01:06:30,853813841-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:06:30,860256433-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2fa9c5a1-5afb-47ad-8ec7-069b7a097538', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8rSirr:/tmp/ceph-asok.8rSirr', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2fa9c5a1-5afb-47ad-8ec7-069b7a097538 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8rSirr:/tmp/ceph-asok.8rSirr -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T01:06:34,315046753-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:06:34,321432905-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2fa9c5a1-5afb-47ad-8ec7-069b7a097538', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8rSirr:/tmp/ceph-asok.8rSirr', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2fa9c5a1-5afb-47ad-8ec7-069b7a097538 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8rSirr:/tmp/ceph-asok.8rSirr -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T01:06:37,262944463-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:06:37,269265924-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2fa9c5a1-5afb-47ad-8ec7-069b7a097538', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8rSirr:/tmp/ceph-asok.8rSirr', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2fa9c5a1-5afb-47ad-8ec7-069b7a097538 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8rSirr:/tmp/ceph-asok.8rSirr -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T01:06:39,891447856-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:07:02,815242827-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   199 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:07:11,099687691-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:07:18,976931652-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=======.....................] (remaining: 25s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:07:26,937971596-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [========....................] (remaining: 48s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:07:37,325379041-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [========....................] (remaining: 57s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:07:45,311929155-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [=========...................] (remaining: 71s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:07:53,309855718-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (45s)
      [=========...................] (remaining: 92s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:08:01,307799724-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [=========...................] (remaining: 102s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:08:09,562894798-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:08:09,579343673-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:08:17,706051249-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:08:17,722767771-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:08:25,750493919-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:08:25,767092341-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:08:33,896070063-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:08:33,912518264-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:08:41,875004097-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:08:41,891340813-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:08:41,904234129-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T01:08:41,911718768-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:08:41,928777993-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=4074022
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_64KB_write.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-17T01:08:41,943275816-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_64KB_write.log.2
[1;30mlocalhost.localdomain	[2021-05-17T01:08:41,984017639-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:08:41,990593739-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2fa9c5a1-5afb-47ad-8ec7-069b7a097538', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8rSirr:/tmp/ceph-asok.8rSirr', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2fa9c5a1-5afb-47ad-8ec7-069b7a097538 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8rSirr:/tmp/ceph-asok.8rSirr -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T08:08:43.515+0000 ffffbeb33010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T08:08:43.523+0000 ffffbeb33010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T08:08:43.523+0000 ffffbeb33010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T08:08:43.541907+0000 Maintaining 256 concurrent writes of 65536 bytes to objects of size 65536 for up to 20 seconds or 0 objects
2021-05-17T08:08:43.541971+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T08:08:43.555599+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T08:08:43.555599+0000     0       0         0         0         0         0           -           0
2021-05-17T08:08:44.555817+0000     1     255      9354      9099   568.636   568.688   0.0139323   0.0272521
2021-05-17T08:08:45.555994+0000     2     255     18281     18026   563.237   557.938   0.0269253   0.0279704
2021-05-17T08:08:46.556213+0000     3     256     27692     27436   571.491   588.125   0.0141475   0.0276903
2021-05-17T08:08:47.556392+0000     4     255     36698     36443   569.327   562.938    0.056107   0.0278557
2021-05-17T08:08:48.556561+0000     5     256     45697     45441   567.918   562.375  0.00354239     0.02784
2021-05-17T08:08:49.556761+0000     6     256     54118     53862   560.966   526.312   0.0121986    0.028372
2021-05-17T08:08:50.556972+0000     7     256     62192     61936   552.902   504.625   0.0389567    0.028822
2021-05-17T08:08:51.557186+0000     8     256     69994     69738   544.729   487.625   0.0099355   0.0292497
2021-05-17T08:08:52.557382+0000     9     255     78738     78483   544.921   546.562    0.026234   0.0292579
2021-05-17T08:08:53.557580+0000    10     255     86896     86641   541.406   509.875   0.0262721   0.0294744
2021-05-17T08:08:54.557760+0000    11     256     95118     94862   538.889   513.812   0.0206229   0.0296134
2021-05-17T08:08:55.557973+0000    12     255    102504    102249   532.447   461.688    0.113681   0.0299278
2021-05-17T08:08:56.558282+0000    13     256    110824    110568   531.472   519.938   0.0207975   0.0300409
2021-05-17T08:08:57.558482+0000    14     255    119104    118849   530.471   517.562   0.0809175   0.0300401
2021-05-17T08:08:58.558685+0000    15     256    127686    127430   530.854   536.312   0.0397221   0.0300491
2021-05-17T08:08:59.558893+0000    16     256    136028    135772   530.254   521.375  0.00897826   0.0301049
2021-05-17T08:09:00.559149+0000    17     256    144192    143936    529.07    510.25   0.0151803   0.0301794
2021-05-17T08:09:01.559324+0000    18     255    152181    151926   527.415   499.375   0.0726787   0.0302396
2021-05-17T08:09:02.559527+0000    19     255    160108    159853   525.727   495.438   0.0730734   0.0303412
2021-05-17T08:09:03.559789+0000 min lat: 0.00102618 max lat: 0.142965 avg lat: 0.0304991
2021-05-17T08:09:03.559789+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T08:09:03.559789+0000    20     181    167587    167406   523.037   472.062    0.107594   0.0304991
2021-05-17T08:09:04.560095+0000 Total time run:         20.0168
Total writes made:      167587
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     523.269
Stddev Bandwidth:       33.3712
Max bandwidth (MB/sec): 588.125
Min bandwidth (MB/sec): 461.688
Average IOPS:           8372
Stddev IOPS:            533.94
Max IOPS:               9410
Min IOPS:               7387
Average Latency(s):     0.0305355
Stddev Latency(s):      0.0220325
Max latency(s):         0.142965
Min latency(s):         0.00102618

[1;32mlocalhost.localdomain	[2021-05-17T01:09:04,885931402-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 4074022


[1;33mlocalhost.localdomain	[2021-05-17T01:09:04,894152262-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:09:27,977940821-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 167.59k objects, 10 GiB
    usage:   20 GiB used, 280 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:09:27,994429649-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:09:35,987742132-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 167.59k objects, 10 GiB
    usage:   20 GiB used, 280 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:09:36,004273252-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:09:44,089032538-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 167.59k objects, 10 GiB
    usage:   20 GiB used, 280 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:09:44,105752630-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:09:52,058571559-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 167.59k objects, 10 GiB
    usage:   20 GiB used, 280 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:09:52,075190399-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:10:00,058823243-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 167.59k objects, 10 GiB
    usage:   20 GiB used, 280 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:10:00,075126805-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:10:00,088205777-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T01:10:00,095909019-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:10:00,113328486-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=4077345
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_64KB_seq.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-17T01:10:00,127130132-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_64KB_seq.log.2
[1;30mlocalhost.localdomain	[2021-05-17T01:10:00,166591473-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:10:00,172941814-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '2fa9c5a1-5afb-47ad-8ec7-069b7a097538', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8rSirr:/tmp/ceph-asok.8rSirr', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 2fa9c5a1-5afb-47ad-8ec7-069b7a097538 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.8rSirr:/tmp/ceph-asok.8rSirr -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T08:10:01.657+0000 ffffb6e95010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T08:10:01.665+0000 ffffb6e95010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T08:10:01.665+0000 ffffb6e95010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T08:10:01.680971+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T08:10:01.680971+0000     0       0         0         0         0         0           -           0
2021-05-17T08:10:02.681166+0000     1     256     12438     12182   761.125   761.375   0.0109578   0.0204931
2021-05-17T08:10:03.681357+0000     2     255     25676     25421     794.2   827.438  0.00339839   0.0198977
2021-05-17T08:10:04.681712+0000     3     256     38738     38482   801.475   816.312  0.00228273   0.0197547
2021-05-17T08:10:05.682355+0000     4     255     52049     51794   808.974       832   0.0214328   0.0196474
2021-05-17T08:10:06.682673+0000     5     256     63142     62886   785.786    693.25   0.0578654   0.0201778
2021-05-17T08:10:07.682919+0000     6     256     75959     75703   788.299   801.062   0.0539752   0.0202032
2021-05-17T08:10:08.683117+0000     7     255     88614     88359   788.663       791  0.00173089    0.020187
2021-05-17T08:10:09.683725+0000     8     255    100189     99934   780.453   723.438   0.0627813   0.0203991
2021-05-17T08:10:10.683997+0000     9     255    113635    113380   787.085   840.375   0.0325051   0.0202647
2021-05-17T08:10:11.684839+0000    10     256    126398    126142   788.072   797.625  0.00345006   0.0202491
2021-05-17T08:10:12.685132+0000    11     255    138815    138560   786.965   776.125   0.0243282   0.0202745
2021-05-17T08:10:13.685507+0000    12     256    150296    150040   781.154     717.5   0.0123996   0.0204274
2021-05-17T08:10:14.685699+0000    13     255    160112    159857   768.256   613.562   0.0438529   0.0207723
2021-05-17T08:10:15.685968+0000 Total time run:       13.7091
Total reads made:     167587
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   764.034
Average IOPS:         12224
Stddev IOPS:          1051.03
Max IOPS:             13446
Min IOPS:             9817
Average Latency(s):   0.0208812
Max latency(s):       0.151682
Min latency(s):       0.00044205

[1;32mlocalhost.localdomain	[2021-05-17T01:10:16,037288733-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 4077345


[1;33mlocalhost.localdomain	[2021-05-17T01:10:16,045093812-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:10:39,170121842-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 167.59k objects, 10 GiB
    usage:   20 GiB used, 280 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:10:39,186787986-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:10:47,309927990-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 167.59k objects, 10 GiB
    usage:   20 GiB used, 280 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:10:47,326487825-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:10:55,245801759-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 167.59k objects, 10 GiB
    usage:   20 GiB used, 280 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:10:55,262640702-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:11:03,277110533-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 167.59k objects, 10 GiB
    usage:   20 GiB used, 280 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:11:03,293944106-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:11:11,119745111-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 167.59k objects, 10 GiB
    usage:   20 GiB used, 280 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:11:11,136227728-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:11:11,149556854-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T01:11:11,154362788-07:00][RUNNING][ROUND 3/3/40] object_size=64KB[0m


[1;33mlocalhost.localdomain	[2021-05-17T01:11:11,162093995-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T01:11:11,179016142-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40369\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.314323\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 78b9a10e-d08e-42f4-8ea3-0c6c65639787\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 78b9a10e-d08e-42f4-8ea3-0c6c65639787\nlast_changed 2021-05-17T01:11:39.955083-0700\ncreated 2021-05-17T01:11:39.955083-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40369/0,v1:10.10.1.2:40370/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.314323 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 b6d3d9b2-fe10-424d-a5b3-0554b524a984\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 ca3af25a-9819-49b6-9058-5a95e8af6fe7\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 dea56eed-2a61-4a9a-856f-7373dc8a1873\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\n'
10.10.1.2: b'restful urls: https://10.10.1.2:42369\n  w/ user/pass: admin / 700ef752-678b-456e-a4b3-d7f703d9845d\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 01:11:54 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40369
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.314323
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 78b9a10e-d08e-42f4-8ea3-0c6c65639787
setting min_mon_release = octopus
epoch 0
fsid 78b9a10e-d08e-42f4-8ea3-0c6c65639787
last_changed 2021-05-17T01:11:39.955083-0700
created 2021-05-17T01:11:39.955083-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40369/0,v1:10.10.1.2:40370/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.314323 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 b6d3d9b2-fe10-424d-a5b3-0554b524a984
0
start osd.0
add osd1 ca3af25a-9819-49b6-9058-5a95e8af6fe7
1
start osd.1
add osd2 dea56eed-2a61-4a9a-856f-7373dc8a1873
2
start osd.2


restful urls: https://10.10.1.2:42369
  w/ user/pass: admin / 700ef752-678b-456e-a4b3-d7f703d9845d


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T01:11:12.166-0700 7f25dba801c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T01:11:12.166-0700 7f25dba801c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T01:11:12.182-0700 7f870a33a1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T01:11:12.182-0700 7f870a33a1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40369,v1:10.10.1.2:40370] --print /tmp/ceph_monmap.314323 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.314323 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.314323 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42369 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.OBx31mHmKm 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new b6d3d9b2-fe10-424d-a5b3-0554b524a984 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBEJaJgnKOuOBAAoxCrZadT9wKJuOBrp1kHiw== --osd-uuid b6d3d9b2-fe10-424d-a5b3-0554b524a984 
2021-05-17T01:11:49.302-0700 7fb75d1aef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T01:11:49.302-0700 7fb75d1aef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T01:11:49.302-0700 7fb75d1aef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T01:11:49.358-0700 7fb75d1aef00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new ca3af25a-9819-49b6-9058-5a95e8af6fe7 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T01:11:49.626-0700 7f1dabad7f00 -1 Falling back to public interface
2021-05-17T01:11:49.638-0700 7f1dabad7f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBFJaJgniRbJRAAhzQhSc4Y2KwyiD2LgJyXTA== --osd-uuid ca3af25a-9819-49b6-9058-5a95e8af6fe7 
2021-05-17T01:11:49.962-0700 7f4374b9df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T01:11:49.966-0700 7f4374b9df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T01:11:49.966-0700 7f4374b9df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T01:11:50.018-0700 7f4374b9df00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new dea56eed-2a61-4a9a-856f-7373dc8a1873 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T01:11:50.346-0700 7f9559d34f00 -1 Falling back to public interface
2021-05-17T01:11:50.358-0700 7f9559d34f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBGJaJgMKqbFBAAZsx5vG0DLrxY5QDK4ESO/Q== --osd-uuid dea56eed-2a61-4a9a-856f-7373dc8a1873 
2021-05-17T01:11:50.674-0700 7fb61a56bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T01:11:50.674-0700 7fb61a56bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T01:11:50.674-0700 7fb61a56bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T01:11:50.770-0700 7fb61a56bf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T01:11:51.110-0700 7fa38cacef00 -1 Falling back to public interface
2021-05-17T01:11:51.126-0700 7fa38cacef00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T01:11:54,979488189-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T01:11:54,987071411-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T01:11:55,070845804-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:11:55,077491006-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '667edaba-f08c-4ecf-9a6f-741864fee3f5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UsFCjW:/tmp/ceph-asok.UsFCjW', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 667edaba-f08c-4ecf-9a6f-741864fee3f5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UsFCjW:/tmp/ceph-asok.UsFCjW -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T01:11:58,035604466-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:11:58,042237486-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '667edaba-f08c-4ecf-9a6f-741864fee3f5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UsFCjW:/tmp/ceph-asok.UsFCjW', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 667edaba-f08c-4ecf-9a6f-741864fee3f5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UsFCjW:/tmp/ceph-asok.UsFCjW -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T01:12:00,793868866-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:12:00,801035617-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '667edaba-f08c-4ecf-9a6f-741864fee3f5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UsFCjW:/tmp/ceph-asok.UsFCjW', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 667edaba-f08c-4ecf-9a6f-741864fee3f5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UsFCjW:/tmp/ceph-asok.UsFCjW -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T01:12:03,487211763-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:12:03,493629392-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '667edaba-f08c-4ecf-9a6f-741864fee3f5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UsFCjW:/tmp/ceph-asok.UsFCjW', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 667edaba-f08c-4ecf-9a6f-741864fee3f5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UsFCjW:/tmp/ceph-asok.UsFCjW -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T01:12:09,640089881-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:12:09,646562748-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '667edaba-f08c-4ecf-9a6f-741864fee3f5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UsFCjW:/tmp/ceph-asok.UsFCjW', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 667edaba-f08c-4ecf-9a6f-741864fee3f5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UsFCjW:/tmp/ceph-asok.UsFCjW -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T01:12:13,063301694-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:12:13,069807972-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '667edaba-f08c-4ecf-9a6f-741864fee3f5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UsFCjW:/tmp/ceph-asok.UsFCjW', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 667edaba-f08c-4ecf-9a6f-741864fee3f5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UsFCjW:/tmp/ceph-asok.UsFCjW -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T01:12:16,429415913-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:12:16,436087040-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '667edaba-f08c-4ecf-9a6f-741864fee3f5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UsFCjW:/tmp/ceph-asok.UsFCjW', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 667edaba-f08c-4ecf-9a6f-741864fee3f5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UsFCjW:/tmp/ceph-asok.UsFCjW -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T01:12:19,632602108-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:12:19,639097479-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '667edaba-f08c-4ecf-9a6f-741864fee3f5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UsFCjW:/tmp/ceph-asok.UsFCjW', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 667edaba-f08c-4ecf-9a6f-741864fee3f5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UsFCjW:/tmp/ceph-asok.UsFCjW -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T01:12:22,950825281-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:12:22,957131575-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '667edaba-f08c-4ecf-9a6f-741864fee3f5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UsFCjW:/tmp/ceph-asok.UsFCjW', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 667edaba-f08c-4ecf-9a6f-741864fee3f5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UsFCjW:/tmp/ceph-asok.UsFCjW -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T01:12:26,185817923-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:12:26,192316850-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '667edaba-f08c-4ecf-9a6f-741864fee3f5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UsFCjW:/tmp/ceph-asok.UsFCjW', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 667edaba-f08c-4ecf-9a6f-741864fee3f5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UsFCjW:/tmp/ceph-asok.UsFCjW -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T01:12:29,477809060-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:12:29,484221362-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '667edaba-f08c-4ecf-9a6f-741864fee3f5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UsFCjW:/tmp/ceph-asok.UsFCjW', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 667edaba-f08c-4ecf-9a6f-741864fee3f5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UsFCjW:/tmp/ceph-asok.UsFCjW -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T01:12:32,290863488-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:12:32,297230918-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '667edaba-f08c-4ecf-9a6f-741864fee3f5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UsFCjW:/tmp/ceph-asok.UsFCjW', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 667edaba-f08c-4ecf-9a6f-741864fee3f5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UsFCjW:/tmp/ceph-asok.UsFCjW -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   47 KiB   0 B   0 B   0 B  100 GiB     0  0.95   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.95/1.03  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T01:12:35,103643748-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:12:57,941920554-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   192 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:13:06,021516617-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:13:14,204046323-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (9s)
      [=======.....................] (remaining: 26s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:13:22,248081452-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [========....................] (remaining: 48s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:13:30,233782642-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [========....................] (remaining: 59s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:13:38,225644729-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [========....................] (remaining: 78s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:13:46,213585258-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (40s)
      [========....................] (remaining: 90s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:13:54,431152178-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   224 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [===============.............] (remaining: 42s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:14:02,539485325-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:14:02,557497529-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:14:10,617177646-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:14:10,633238033-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:14:18,616438060-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:14:18,632985985-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:14:26,778008050-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:14:26,794678126-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:14:34,879348000-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:14:34,895715249-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:14:34,908667182-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T01:14:34,916366614-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:14:34,933846618-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=4091057
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_64KB_write.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-17T01:14:34,948258494-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_64KB_write.log.3
[1;30mlocalhost.localdomain	[2021-05-17T01:14:34,989039359-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:14:34,995475951-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '667edaba-f08c-4ecf-9a6f-741864fee3f5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UsFCjW:/tmp/ceph-asok.UsFCjW', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 667edaba-f08c-4ecf-9a6f-741864fee3f5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UsFCjW:/tmp/ceph-asok.UsFCjW -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T08:14:36.744+0000 ffffac026010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T08:14:36.760+0000 ffffac026010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T08:14:36.760+0000 ffffac026010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T08:14:36.775702+0000 Maintaining 256 concurrent writes of 65536 bytes to objects of size 65536 for up to 20 seconds or 0 objects
2021-05-17T08:14:36.775811+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T08:14:36.789745+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T08:14:36.789745+0000     0       0         0         0         0         0           -           0
2021-05-17T08:14:37.790019+0000     1     255      9589      9334   583.287   583.375   0.0329839   0.0263116
2021-05-17T08:14:38.790443+0000     2     255     20006     19751   617.041   651.062   0.0382115   0.0253386
2021-05-17T08:14:39.790824+0000     3     255     30172     29917   623.072   635.375    0.019613   0.0253004
2021-05-17T08:14:40.791360+0000     4     256     39910     39654   619.363   608.562   0.0153336   0.0255642
2021-05-17T08:14:41.792014+0000     5     255     50285     50030   625.107     648.5  0.00837611   0.0253571
2021-05-17T08:14:42.792516+0000     6     256     60466     60210   626.911    636.25   0.0224311   0.0253726
2021-05-17T08:14:43.793761+0000     7     255     70001     69746   622.386       596   0.0185559   0.0255696
2021-05-17T08:14:44.794727+0000     8     255     80027     79772    622.84   626.625   0.0158291   0.0255536
2021-05-17T08:14:45.795146+0000     9     256     90156     89900    623.94       633   0.0116122   0.0255206
2021-05-17T08:14:46.795632+0000    10     255     99091     98836   617.369     558.5    0.036204   0.0257982
2021-05-17T08:14:47.795907+0000    11     256    108027    107771   611.999   558.438   0.0135023   0.0260365
2021-05-17T08:14:48.796278+0000    12     255    116655    116400   605.926   539.312   0.0241329   0.0263123
2021-05-17T08:14:49.796461+0000    13     256    125649    125393   602.545   562.062  0.00107648   0.0264137
2021-05-17T08:14:50.796631+0000    14     255    135300    135045   602.588    603.25   0.0025111   0.0264551
2021-05-17T08:14:51.796982+0000    15     255    143964    143709   598.504     541.5  0.00431222   0.0266243
2021-05-17T08:14:52.797472+0000    16     255    151214    150959   589.403   453.125  0.00161755   0.0270182
2021-05-17T08:14:53.797714+0000    17     256    159663    159407   585.785       528  0.00150354   0.0271891
2021-05-17T08:14:54.797890+0000    18     256    167685    167429   581.092   501.375  0.00521843    0.027449
2021-05-17T08:14:55.798138+0000    19     256    174690    174434   573.546   437.812    0.020664   0.0278372
2021-05-17T08:14:56.798473+0000 min lat: 0.00105279 max lat: 0.206507 avg lat: 0.0278963
2021-05-17T08:14:56.798473+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T08:14:56.798473+0000    20      16    183316    183300   572.566   554.125   0.0372207   0.0278963
2021-05-17T08:14:57.798775+0000 Total time run:         20.0132
Total writes made:      183316
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     572.484
Stddev Bandwidth:       61.4743
Max bandwidth (MB/sec): 651.062
Min bandwidth (MB/sec): 437.812
Average IOPS:           9159
Stddev IOPS:            983.589
Max IOPS:               10417
Min IOPS:               7005
Average Latency(s):     0.0278952
Stddev Latency(s):      0.0215112
Max latency(s):         0.206507
Min latency(s):         0.00105279

[1;32mlocalhost.localdomain	[2021-05-17T01:14:58,114032322-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 4091057


[1;33mlocalhost.localdomain	[2021-05-17T01:14:58,122329666-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:15:20,986977887-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 183.32k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:15:21,003630215-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:15:29,126957106-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 183.32k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:15:29,143786674-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:15:37,150593492-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 183.32k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:15:37,167376508-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:15:45,110814897-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 183.32k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:15:45,127821855-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:15:53,444714879-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 183.32k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:15:53,461255633-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:15:53,474169699-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T01:15:53,481891890-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:15:53,499518095-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=4094375
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_64KB_seq.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-17T01:15:53,514180064-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_64KB_seq.log.3
[1;30mlocalhost.localdomain	[2021-05-17T01:15:53,553727194-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:15:53,560086073-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '667edaba-f08c-4ecf-9a6f-741864fee3f5', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UsFCjW:/tmp/ceph-asok.UsFCjW', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 667edaba-f08c-4ecf-9a6f-741864fee3f5 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.UsFCjW:/tmp/ceph-asok.UsFCjW -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T08:15:55.089+0000 ffff90f89010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T08:15:55.105+0000 ffff90f89010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T08:15:55.105+0000 ffff90f89010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T08:15:55.125043+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T08:15:55.125043+0000     0       0         0         0         0         0           -           0
2021-05-17T08:15:56.125317+0000     1     255     12537     12282   767.305   767.625   0.0364754   0.0204484
2021-05-17T08:15:57.125640+0000     2     255     25163     24908   778.087   789.125  0.00203733   0.0202271
2021-05-17T08:15:58.125836+0000     3     255     37698     37443   779.819   783.438   0.0284762   0.0203967
2021-05-17T08:15:59.126045+0000     4     255     50409     50154   783.432   794.438  0.00757272   0.0202932
2021-05-17T08:16:00.126243+0000     5     255     62817     62562   781.815     775.5   0.0241009   0.0203593
2021-05-17T08:16:01.126484+0000     6     256     74308     74052   771.171   718.125  0.00685728   0.0206181
2021-05-17T08:16:02.126707+0000     7     255     85643     85388   762.196     708.5  0.00363641   0.0208541
2021-05-17T08:16:03.126913+0000     8     256     97562     97306   760.012   744.875  0.00366281   0.0209516
2021-05-17T08:16:04.127368+0000     9     255    110635    110380   766.318   817.125  0.00222378   0.0207834
2021-05-17T08:16:05.127555+0000    10     256    123232    122976   768.396    787.25  0.00854528   0.0207543
2021-05-17T08:16:06.134564+0000    11     255    134845    134590   764.045   725.875  0.00243636    0.020853
2021-05-17T08:16:07.134788+0000    12     255    147710    147455   767.362   804.062    0.057663   0.0207642
2021-05-17T08:16:08.135038+0000    13     255    160118    159863   767.973     775.5  0.00120208   0.0207628
2021-05-17T08:16:09.135435+0000    14     256    171519    171263   763.992     712.5  0.00176508   0.0208603
2021-05-17T08:16:10.135618+0000    15     256    183218    182962   761.797   731.188   0.0626184   0.0209288
2021-05-17T08:16:11.135869+0000 Total time run:       15.0649
Total reads made:     183316
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   760.525
Average IOPS:         12168
Stddev IOPS:          571.677
Max IOPS:             13074
Min IOPS:             11336
Average Latency(s):   0.0209742
Max latency(s):       0.0940081
Min latency(s):       0.000453347

[1;32mlocalhost.localdomain	[2021-05-17T01:16:11,434314324-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 4094375


[1;33mlocalhost.localdomain	[2021-05-17T01:16:11,442748086-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:16:34,545785028-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 183.32k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:16:34,562512805-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:16:42,574345953-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 183.32k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:16:42,593788772-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:16:50,577123287-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 183.32k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:16:50,594086004-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:16:58,566114201-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 183.32k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:16:58,582677969-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:17:06,587522774-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 183.32k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:17:06,603544215-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:17:06,616890115-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T01:17:06,621944777-07:00][RUNNING][ROUND 4/3/40] object_size=64KB[0m


[1;33mlocalhost.localdomain	[2021-05-17T01:17:06,629852235-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T01:17:06,646658370-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40858\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.315572\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 43ced654-c9e0-4b96-b199-fee3386fa43e\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 43ced654-c9e0-4b96-b199-fee3386fa43e\nlast_changed 2021-05-17T01:17:33.988488-0700\ncreated 2021-05-17T01:17:33.988488-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40858/0,v1:10.10.1.2:40859/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.315572 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 25191280-0882-4ae1-a246-e975e8b06ec4\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 8f733230-4236-45c1-b1a4-ded5c87e1534\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 0dcb2d06-9c81-4227-8dcd-e2f63821d9e2\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42858\n  w/ user/pass: admin / bfffe209-96c9-4c1b-8f6e-763969c4d52f\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 01:17:48 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40858
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.315572
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 43ced654-c9e0-4b96-b199-fee3386fa43e
setting min_mon_release = octopus
epoch 0
fsid 43ced654-c9e0-4b96-b199-fee3386fa43e
last_changed 2021-05-17T01:17:33.988488-0700
created 2021-05-17T01:17:33.988488-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40858/0,v1:10.10.1.2:40859/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.315572 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 25191280-0882-4ae1-a246-e975e8b06ec4
0
start osd.0
add osd1 8f733230-4236-45c1-b1a4-ded5c87e1534
1
start osd.1
add osd2 0dcb2d06-9c81-4227-8dcd-e2f63821d9e2
2
start osd.2


restful urls: https://10.10.1.2:42858
  w/ user/pass: admin / bfffe209-96c9-4c1b-8f6e-763969c4d52f


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T01:17:07.677-0700 7f65961a41c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T01:17:07.677-0700 7f65961a41c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T01:17:07.697-0700 7f73a06631c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T01:17:07.697-0700 7f73a06631c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40858,v1:10.10.1.2:40859] --print /tmp/ceph_monmap.315572 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.315572 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.315572 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42858 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.kQemNI6rhR 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 25191280-0882-4ae1-a246-e975e8b06ec4 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCmJqJgMUeBMxAAxFClploAHjys3yEelEpQgQ== --osd-uuid 25191280-0882-4ae1-a246-e975e8b06ec4 
2021-05-17T01:17:43.214-0700 7f69de4b2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T01:17:43.214-0700 7f69de4b2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T01:17:43.214-0700 7f69de4b2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T01:17:43.290-0700 7f69de4b2f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 8f733230-4236-45c1-b1a4-ded5c87e1534 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T01:17:43.586-0700 7f849269ff00 -1 Falling back to public interface
2021-05-17T01:17:43.598-0700 7f849269ff00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCnJqJgdyAFIxAAuH4Jq4SkkbTs8uQmp3RNYw== --osd-uuid 8f733230-4236-45c1-b1a4-ded5c87e1534 
2021-05-17T01:17:43.918-0700 7fdeeb613f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T01:17:43.918-0700 7fdeeb613f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T01:17:43.918-0700 7fdeeb613f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T01:17:43.994-0700 7fdeeb613f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 0dcb2d06-9c81-4227-8dcd-e2f63821d9e2 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T01:17:44.314-0700 7fe7b4557f00 -1 Falling back to public interface
2021-05-17T01:17:44.326-0700 7fe7b4557f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCoJqJg5gGPEhAAltUTfxZ3jUVlY6bzYqjpKA== --osd-uuid 0dcb2d06-9c81-4227-8dcd-e2f63821d9e2 
2021-05-17T01:17:44.674-0700 7f826e35df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T01:17:44.674-0700 7f826e35df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T01:17:44.674-0700 7f826e35df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T01:17:44.782-0700 7f826e35df00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T01:17:45.134-0700 7f80b2713f00 -1 Falling back to public interface
2021-05-17T01:17:45.150-0700 7f80b2713f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T01:17:48,958643370-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T01:17:48,966679565-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T01:17:49,050203798-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:17:49,056694745-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7947150d-ddd0-472b-980d-e3418675cc91', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.eKiZcD:/tmp/ceph-asok.eKiZcD', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7947150d-ddd0-472b-980d-e3418675cc91 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.eKiZcD:/tmp/ceph-asok.eKiZcD -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T01:17:51,836468079-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:17:51,842873406-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7947150d-ddd0-472b-980d-e3418675cc91', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.eKiZcD:/tmp/ceph-asok.eKiZcD', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7947150d-ddd0-472b-980d-e3418675cc91 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.eKiZcD:/tmp/ceph-asok.eKiZcD -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T01:17:54,814016440-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:17:54,820548179-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7947150d-ddd0-472b-980d-e3418675cc91', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.eKiZcD:/tmp/ceph-asok.eKiZcD', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7947150d-ddd0-472b-980d-e3418675cc91 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.eKiZcD:/tmp/ceph-asok.eKiZcD -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T01:17:57,574214696-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:17:57,580976964-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7947150d-ddd0-472b-980d-e3418675cc91', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.eKiZcD:/tmp/ceph-asok.eKiZcD', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7947150d-ddd0-472b-980d-e3418675cc91 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.eKiZcD:/tmp/ceph-asok.eKiZcD -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 3          default
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T01:18:05,186648668-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:18:05,193332277-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7947150d-ddd0-472b-980d-e3418675cc91', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.eKiZcD:/tmp/ceph-asok.eKiZcD', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7947150d-ddd0-472b-980d-e3418675cc91 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.eKiZcD:/tmp/ceph-asok.eKiZcD -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T01:18:08,824248643-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:18:08,830871477-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7947150d-ddd0-472b-980d-e3418675cc91', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.eKiZcD:/tmp/ceph-asok.eKiZcD', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7947150d-ddd0-472b-980d-e3418675cc91 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.eKiZcD:/tmp/ceph-asok.eKiZcD -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T01:18:12,368837236-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:18:12,375537168-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7947150d-ddd0-472b-980d-e3418675cc91', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.eKiZcD:/tmp/ceph-asok.eKiZcD', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7947150d-ddd0-472b-980d-e3418675cc91 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.eKiZcD:/tmp/ceph-asok.eKiZcD -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T01:18:15,621041766-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:18:15,627500217-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7947150d-ddd0-472b-980d-e3418675cc91', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.eKiZcD:/tmp/ceph-asok.eKiZcD', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7947150d-ddd0-472b-980d-e3418675cc91 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.eKiZcD:/tmp/ceph-asok.eKiZcD -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T01:18:18,821580445-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:18:18,828147215-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7947150d-ddd0-472b-980d-e3418675cc91', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.eKiZcD:/tmp/ceph-asok.eKiZcD', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7947150d-ddd0-472b-980d-e3418675cc91 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.eKiZcD:/tmp/ceph-asok.eKiZcD -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T01:18:22,513806557-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:18:22,520355219-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7947150d-ddd0-472b-980d-e3418675cc91', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.eKiZcD:/tmp/ceph-asok.eKiZcD', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7947150d-ddd0-472b-980d-e3418675cc91 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.eKiZcD:/tmp/ceph-asok.eKiZcD -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T01:18:25,784928494-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:18:25,791386374-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7947150d-ddd0-472b-980d-e3418675cc91', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.eKiZcD:/tmp/ceph-asok.eKiZcD', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7947150d-ddd0-472b-980d-e3418675cc91 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.eKiZcD:/tmp/ceph-asok.eKiZcD -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T01:18:28,638682280-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:18:28,645277539-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7947150d-ddd0-472b-980d-e3418675cc91', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.eKiZcD:/tmp/ceph-asok.eKiZcD', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7947150d-ddd0-472b-980d-e3418675cc91 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.eKiZcD:/tmp/ceph-asok.eKiZcD -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  155 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T01:18:31,315746476-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:18:54,289000067-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   206 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
    Global Recovery Event (4s)
      [============................] (remaining: 5s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:19:02,354289143-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:19:10,763753393-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:19:18,736971466-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:19:26,892692003-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:19:35,035648286-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:19:43,020429102-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:19:50,966632051-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:19:50,983007313-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:19:59,096144738-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:19:59,112867804-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:20:07,365881947-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:20:07,382569322-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:20:15,393963906-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:20:15,410665549-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:20:23,579745442-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:20:23,596149329-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:20:23,609526840-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T01:20:23,617374258-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:20:23,635191251-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=4107738
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_64KB_write.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-17T01:20:23,650371949-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_64KB_write.log.4
[1;30mlocalhost.localdomain	[2021-05-17T01:20:23,691222999-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:20:23,697878283-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7947150d-ddd0-472b-980d-e3418675cc91', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.eKiZcD:/tmp/ceph-asok.eKiZcD', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7947150d-ddd0-472b-980d-e3418675cc91 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.eKiZcD:/tmp/ceph-asok.eKiZcD -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T08:20:25.219+0000 ffffb307c010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T08:20:25.227+0000 ffffb307c010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T08:20:25.227+0000 ffffb307c010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T08:20:25.245802+0000 Maintaining 256 concurrent writes of 65536 bytes to objects of size 65536 for up to 20 seconds or 0 objects
2021-05-17T08:20:25.245852+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T08:20:25.259567+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T08:20:25.259567+0000     0       0         0         0         0         0           -           0
2021-05-17T08:20:26.259763+0000     1     256     10092      9836   614.718    614.75  0.00104925   0.0247588
2021-05-17T08:20:27.259938+0000     2     255     20540     20285   633.834   653.062   0.0109173   0.0250521
2021-05-17T08:20:28.260133+0000     3     255     30871     30616   637.744   645.688  0.00646927   0.0249077
2021-05-17T08:20:29.260323+0000     4     256     41249     40993   640.418   648.562   0.0118097   0.0248174
2021-05-17T08:20:30.260521+0000     5     255     51694     51439   642.883   652.875   0.0131836    0.024779
2021-05-17T08:20:31.260744+0000     6     255     61745     61490   640.411   628.188   0.0159797   0.0248926
2021-05-17T08:20:32.260961+0000     7     255     71241     70986    633.69     593.5   0.0206366   0.0251671
2021-05-17T08:20:33.261190+0000     8     255     81432     81177   634.078   636.938   0.0187123   0.0251651
2021-05-17T08:20:34.261474+0000     9     255     91010     90755    630.12   598.625   0.0180849   0.0253395
2021-05-17T08:20:35.261645+0000    10     255    100202     99947   624.548     574.5   0.0114475   0.0255034
2021-05-17T08:20:36.261918+0000    11     255    108643    108388   615.717   527.562   0.0187096   0.0259316
2021-05-17T08:20:37.262308+0000    12     255    117595    117340   611.014     559.5   0.0294697   0.0261376
2021-05-17T08:20:38.262637+0000    13     255    125940    125685   604.119   521.562  0.00942708   0.0264031
2021-05-17T08:20:39.262832+0000    14     256    133425    133169   594.372    467.75   0.0101608    0.026856
2021-05-17T08:20:40.263077+0000    15     255    140807    140552   585.502   461.438  0.00909449   0.0272597
2021-05-17T08:20:41.263390+0000    16     256    148540    148284   579.101    483.25   0.0037511   0.0275351
2021-05-17T08:20:42.263624+0000    17     255    156517    156262    574.36   498.625   0.0232122   0.0278298
2021-05-17T08:20:43.263836+0000    18     255    164100    163845   568.776   473.938   0.0160924   0.0280717
2021-05-17T08:20:44.264061+0000    19     255    171970    171715   564.723   491.875   0.0146883   0.0282812
2021-05-17T08:20:45.264251+0000 min lat: 0.00102067 max lat: 0.208078 avg lat: 0.0284892
2021-05-17T08:20:45.264251+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T08:20:45.264251+0000    20     121    179628    179507   560.832       487   0.0666663   0.0284892
2021-05-17T08:20:46.264513+0000 Total time run:         20.0099
Total writes made:      179628
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     561.06
Stddev Bandwidth:       71.3159
Max bandwidth (MB/sec): 653.062
Min bandwidth (MB/sec): 461.438
Average IOPS:           8976
Stddev IOPS:            1141.05
Max IOPS:               10449
Min IOPS:               7383
Average Latency(s):     0.0284958
Stddev Latency(s):      0.0214353
Max latency(s):         0.208078
Min latency(s):         0.00102067

[1;32mlocalhost.localdomain	[2021-05-17T01:20:46,580881525-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 4107738


[1;33mlocalhost.localdomain	[2021-05-17T01:20:46,589175884-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:21:09,501515516-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 179.63k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:21:09,519845582-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:21:17,879575149-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 179.63k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:21:17,896006671-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:21:25,940115088-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 179.63k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:21:25,956457134-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:21:33,951239438-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 179.63k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:21:33,967807488-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:21:42,130867502-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 179.63k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:21:42,147179846-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:21:42,160030094-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T01:21:42,167832527-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:21:42,185167241-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=4111034
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_64KB_seq.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-17T01:21:42,200181528-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_64KB_seq.log.4
[1;30mlocalhost.localdomain	[2021-05-17T01:21:42,239936142-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:21:42,246016753-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7947150d-ddd0-472b-980d-e3418675cc91', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.eKiZcD:/tmp/ceph-asok.eKiZcD', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7947150d-ddd0-472b-980d-e3418675cc91 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.eKiZcD:/tmp/ceph-asok.eKiZcD -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T08:21:43.789+0000 ffffa1a91010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T08:21:43.797+0000 ffffa1a91010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T08:21:43.797+0000 ffffa1a91010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T08:21:43.813967+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T08:21:43.813967+0000     0       0         0         0         0         0           -           0
2021-05-17T08:21:44.814144+0000     1     255     13778     13523   844.916   845.188  0.00339856   0.0184885
2021-05-17T08:21:45.814342+0000     2     255     25618     25363   792.388       740   0.0523269   0.0198915
2021-05-17T08:21:46.818814+0000     3     255     37793     37538   780.743   760.938   0.0132874    0.020332
2021-05-17T08:21:47.819034+0000     4     255     50624     50369   785.991   801.938  0.00829342   0.0202169
2021-05-17T08:21:48.819262+0000     5     255     63700     63445   792.201    817.25  0.00742615   0.0200782
2021-05-17T08:21:49.819760+0000     6     255     77200     76945   800.718    843.75   0.0164654   0.0198928
2021-05-17T08:21:50.820001+0000     7     255     90514     90259   805.173   832.125   0.0536354   0.0197847
2021-05-17T08:21:51.820304+0000     8     255    102812    102557   800.578   768.625  0.00216189   0.0198847
2021-05-17T08:21:52.820552+0000     9     255    114144    113889   790.305    708.25  0.00799474   0.0201707
2021-05-17T08:21:53.822851+0000    10     255    125946    125691    784.86   737.625   0.0430075   0.0203189
2021-05-17T08:21:54.823099+0000    11     255    138923    138668   787.222   811.062  0.00748193   0.0202544
2021-05-17T08:21:55.823335+0000    12     255    151755    151500   788.437       802   0.0497024    0.020231
2021-05-17T08:21:56.824345+0000    13     256    163492    163236   784.154     733.5  0.00419578    0.020324
2021-05-17T08:21:57.824561+0000    14     255    174407    174152   776.868    682.25   0.0576569   0.0205345
2021-05-17T08:21:58.824806+0000 Total time run:       14.5199
Total reads made:     179628
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   773.2
Average IOPS:         12371
Stddev IOPS:          830.341
Max IOPS:             13523
Min IOPS:             10916
Average Latency(s):   0.020643
Max latency(s):       0.103226
Min latency(s):       0.000488921

[1;32mlocalhost.localdomain	[2021-05-17T01:21:59,126364155-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 4111034


[1;33mlocalhost.localdomain	[2021-05-17T01:21:59,134701100-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:22:22,094092435-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 179.63k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:22:22,110502337-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:22:30,114405539-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 179.63k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:22:30,130886762-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:22:37,958389139-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 179.63k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:22:37,975119075-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:22:46,134438434-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 179.63k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:22:46,150708373-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:22:54,143197455-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 179.63k objects, 11 GiB
    usage:   22 GiB used, 278 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:22:54,161849691-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:22:54,177077772-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T01:22:54,184514092-07:00][RUNNING][ROUND 5/3/40] object_size=64KB[0m


[1;33mlocalhost.localdomain	[2021-05-17T01:22:54,194817893-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T01:22:54,213334234-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40030\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.316847\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid c5ff4ef1-d6ca-4107-84f8-0733dc6a96dc\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid c5ff4ef1-d6ca-4107-84f8-0733dc6a96dc\nlast_changed 2021-05-17T01:23:21.900867-0700\ncreated 2021-05-17T01:23:21.900867-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40030/0,v1:10.10.1.2:40031/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.316847 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 6c8e94ab-b0db-4331-9628-553254dfed5d\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 e9f4bf94-f089-476e-b720-a2a33df31067\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 18c0388d-328e-4a18-aa63-9f80ddb50788\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42030\n'
10.10.1.2: b'  w/ user/pass: admin / 5c75c1d3-4a22-4127-a238-8b2854c1442c\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 01:23:37 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40030
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.316847
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid c5ff4ef1-d6ca-4107-84f8-0733dc6a96dc
setting min_mon_release = octopus
epoch 0
fsid c5ff4ef1-d6ca-4107-84f8-0733dc6a96dc
last_changed 2021-05-17T01:23:21.900867-0700
created 2021-05-17T01:23:21.900867-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40030/0,v1:10.10.1.2:40031/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.316847 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 6c8e94ab-b0db-4331-9628-553254dfed5d
0
start osd.0
add osd1 e9f4bf94-f089-476e-b720-a2a33df31067
1
start osd.1
add osd2 18c0388d-328e-4a18-aa63-9f80ddb50788
2
start osd.2


restful urls: https://10.10.1.2:42030
  w/ user/pass: admin / 5c75c1d3-4a22-4127-a238-8b2854c1442c


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T01:22:55.221-0700 7fdeea1851c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T01:22:55.221-0700 7fdeea1851c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T01:22:55.237-0700 7f46051531c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T01:22:55.237-0700 7f46051531c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40030,v1:10.10.1.2:40031] --print /tmp/ceph_monmap.316847 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.316847 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.316847 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42030 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.VS3UqH0ian 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 6c8e94ab-b0db-4331-9628-553254dfed5d -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQADKKJg6YzfIBAAAXz645oktOs0xt3uTjH2Tw== --osd-uuid 6c8e94ab-b0db-4331-9628-553254dfed5d 
2021-05-17T01:23:31.885-0700 7f763a978f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T01:23:31.885-0700 7f763a978f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T01:23:31.885-0700 7f763a978f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T01:23:31.945-0700 7f763a978f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new e9f4bf94-f089-476e-b720-a2a33df31067 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T01:23:32.257-0700 7f7e1dcc4f00 -1 Falling back to public interface
2021-05-17T01:23:32.269-0700 7f7e1dcc4f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAEKKJgkkRyDxAA6iAETY/am1aapngT4JQQrw== --osd-uuid e9f4bf94-f089-476e-b720-a2a33df31067 
2021-05-17T01:23:32.605-0700 7f1defd42f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T01:23:32.605-0700 7f1defd42f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T01:23:32.605-0700 7f1defd42f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T01:23:32.677-0700 7f1defd42f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 18c0388d-328e-4a18-aa63-9f80ddb50788 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T01:23:33.037-0700 7fc48a79ff00 -1 Falling back to public interface
2021-05-17T01:23:33.049-0700 7fc48a79ff00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAFKKJgEaJKAhAAeK1cVio2oyfcKpSt1jVhSA== --osd-uuid 18c0388d-328e-4a18-aa63-9f80ddb50788 
2021-05-17T01:23:33.373-0700 7fe6c9530f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T01:23:33.373-0700 7fe6c9530f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T01:23:33.373-0700 7fe6c9530f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T01:23:33.429-0700 7fe6c9530f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T01:23:33.741-0700 7f38f5233f00 -1 Falling back to public interface
2021-05-17T01:23:33.753-0700 7f38f5233f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T01:23:37,670884354-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T01:23:37,678942980-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T01:23:37,770406471-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:23:37,777893692-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '85878e3c-44b7-441c-8170-3f2a246a3afe', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.502hma:/tmp/ceph-asok.502hma', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 85878e3c-44b7-441c-8170-3f2a246a3afe --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.502hma:/tmp/ceph-asok.502hma -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T01:23:40,648094624-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:23:40,654605055-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '85878e3c-44b7-441c-8170-3f2a246a3afe', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.502hma:/tmp/ceph-asok.502hma', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 85878e3c-44b7-441c-8170-3f2a246a3afe --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.502hma:/tmp/ceph-asok.502hma -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T01:23:43,776464432-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:23:43,782633058-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '85878e3c-44b7-441c-8170-3f2a246a3afe', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.502hma:/tmp/ceph-asok.502hma', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 85878e3c-44b7-441c-8170-3f2a246a3afe --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.502hma:/tmp/ceph-asok.502hma -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T01:23:46,650184092-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:23:46,656937190-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '85878e3c-44b7-441c-8170-3f2a246a3afe', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.502hma:/tmp/ceph-asok.502hma', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 85878e3c-44b7-441c-8170-3f2a246a3afe --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.502hma:/tmp/ceph-asok.502hma -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T01:23:52,257182852-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:23:52,263646879-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '85878e3c-44b7-441c-8170-3f2a246a3afe', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.502hma:/tmp/ceph-asok.502hma', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 85878e3c-44b7-441c-8170-3f2a246a3afe --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.502hma:/tmp/ceph-asok.502hma -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T01:23:55,728704448-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:23:55,735194400-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '85878e3c-44b7-441c-8170-3f2a246a3afe', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.502hma:/tmp/ceph-asok.502hma', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 85878e3c-44b7-441c-8170-3f2a246a3afe --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.502hma:/tmp/ceph-asok.502hma -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T01:23:59,376502945-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:23:59,383061128-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '85878e3c-44b7-441c-8170-3f2a246a3afe', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.502hma:/tmp/ceph-asok.502hma', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 85878e3c-44b7-441c-8170-3f2a246a3afe --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.502hma:/tmp/ceph-asok.502hma -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T01:24:02,531577625-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:24:02,538093786-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '85878e3c-44b7-441c-8170-3f2a246a3afe', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.502hma:/tmp/ceph-asok.502hma', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 85878e3c-44b7-441c-8170-3f2a246a3afe --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.502hma:/tmp/ceph-asok.502hma -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T01:24:06,378176232-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:24:06,384669274-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '85878e3c-44b7-441c-8170-3f2a246a3afe', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.502hma:/tmp/ceph-asok.502hma', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 85878e3c-44b7-441c-8170-3f2a246a3afe --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.502hma:/tmp/ceph-asok.502hma -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T01:24:09,591699966-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:24:09,598202655-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '85878e3c-44b7-441c-8170-3f2a246a3afe', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.502hma:/tmp/ceph-asok.502hma', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 85878e3c-44b7-441c-8170-3f2a246a3afe --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.502hma:/tmp/ceph-asok.502hma -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T01:24:12,838465669-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:24:12,845109052-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '85878e3c-44b7-441c-8170-3f2a246a3afe', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.502hma:/tmp/ceph-asok.502hma', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 85878e3c-44b7-441c-8170-3f2a246a3afe --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.502hma:/tmp/ceph-asok.502hma -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T01:24:15,606391421-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:24:15,612827167-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '85878e3c-44b7-441c-8170-3f2a246a3afe', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.502hma:/tmp/ceph-asok.502hma', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 85878e3c-44b7-441c-8170-3f2a246a3afe --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.502hma:/tmp/ceph-asok.502hma -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   47 KiB   0 B   0 B   0 B  100 GiB     0  0.95   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.95/1.03  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T01:24:18,391422854-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:24:41,519364817-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   188 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
    Global Recovery Event (5s)
      [==========..................] (remaining: 7s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:24:49,454917408-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (9s)
      [==================..........] (remaining: 4s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:24:57,379377135-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:25:05,511818408-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:25:13,521250401-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:25:21,565870596-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:25:29,562251945-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:25:37,743237577-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:25:37,759966685-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:25:45,750964624-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:25:45,767821658-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:25:53,765409963-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:25:53,782026311-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:26:01,819579664-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:26:01,836566155-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:26:09,974421422-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:26:09,990989940-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:26:10,005585675-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T01:26:10,014611498-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:26:10,034766358-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=4124585
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_64KB_write.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-17T01:26:10,052363905-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_64KB_write.log.5
[1;30mlocalhost.localdomain	[2021-05-17T01:26:10,094482445-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:26:10,100897709-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '85878e3c-44b7-441c-8170-3f2a246a3afe', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.502hma:/tmp/ceph-asok.502hma', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '65536', '-O', '65536', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 85878e3c-44b7-441c-8170-3f2a246a3afe --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.502hma:/tmp/ceph-asok.502hma -- rados bench 20 write --pool bench_rados -b 65536 -O 65536 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T08:26:11.631+0000 ffffa1268010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T08:26:11.639+0000 ffffa1268010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T08:26:11.639+0000 ffffa1268010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T08:26:11.655564+0000 Maintaining 256 concurrent writes of 65536 bytes to objects of size 65536 for up to 20 seconds or 0 objects
2021-05-17T08:26:11.655604+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T08:26:11.669317+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T08:26:11.669317+0000     0       0         0         0         0         0           -           0
2021-05-17T08:26:12.669491+0000     1     255     10336     10081   630.065   630.062   0.0237524    0.024921
2021-05-17T08:26:13.670262+0000     2     255     20390     20135   628.978   628.375   0.0182285   0.0251124
2021-05-17T08:26:14.670547+0000     3     256     30587     30331   631.674    637.25   0.0110025   0.0251382
2021-05-17T08:26:15.670743+0000     4     255     40958     40703   635.786    648.25    0.031896   0.0250293
2021-05-17T08:26:16.670994+0000     5     255     51012     50757   634.272   628.375   0.0352983   0.0250718
2021-05-17T08:26:17.671214+0000     6     256     60746     60490   629.924   608.312   0.0215362   0.0253066
2021-05-17T08:26:18.671718+0000     7     256     70979     70723   631.255   639.562   0.0153621   0.0252674
2021-05-17T08:26:19.672040+0000     8     255     81375     81120   633.549   649.812   0.0121171   0.0251836
2021-05-17T08:26:20.672243+0000     9     255     91066     90811    630.44   605.688   0.0307724    0.025311
2021-05-17T08:26:21.672500+0000    10     255     99395     99140   619.439   520.562  0.00136222   0.0257157
2021-05-17T08:26:22.672686+0000    11     255    107806    107551   610.908   525.688   0.0156661   0.0261392
2021-05-17T08:26:23.673538+0000    12     255    116485    116230   605.161   542.438   0.0192158   0.0263493
2021-05-17T08:26:24.673703+0000    13     255    125745    125490   603.122    578.75   0.0306218   0.0264861
2021-05-17T08:26:25.673871+0000    14     256    135387    135131   603.075   602.562  0.00124868   0.0264664
2021-05-17T08:26:26.674034+0000    15     255    143909    143654   598.377   532.688   0.0577593   0.0266948
2021-05-17T08:26:27.674208+0000    16     256    153168    152912   597.137   578.625   0.0153922   0.0267635
2021-05-17T08:26:28.674401+0000    17     255    161686    161431   593.325   532.438   0.0350048   0.0269158
2021-05-17T08:26:29.674633+0000    18     255    170023    169768   589.304   521.062  0.00550168   0.0270166
2021-05-17T08:26:30.674822+0000    19     256    178894    178638    587.46   554.375  0.00998231   0.0271905
2021-05-17T08:26:31.675185+0000 min lat: 0.000999023 max lat: 0.199165 avg lat: 0.0270937
2021-05-17T08:26:31.675185+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T08:26:31.675185+0000    20     254    189070    188816   589.882   636.125   0.0213876   0.0270937
2021-05-17T08:26:32.675479+0000 Total time run:         20.0201
Total writes made:      189070
Write size:             65536
Object size:            65536
Bandwidth (MB/sec):     590.25
Stddev Bandwidth:       47.6678
Max bandwidth (MB/sec): 649.812
Min bandwidth (MB/sec): 520.562
Average IOPS:           9444
Stddev IOPS:            762.684
Max IOPS:               10397
Min IOPS:               8329
Average Latency(s):     0.0270858
Stddev Latency(s):      0.0211802
Max latency(s):         0.199165
Min latency(s):         0.000999023

[1;32mlocalhost.localdomain	[2021-05-17T01:26:33,015333563-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 4124585


[1;33mlocalhost.localdomain	[2021-05-17T01:26:33,023877276-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:26:56,160182765-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 189.07k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:26:56,177196841-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:27:04,671782952-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 189.07k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:27:04,688409481-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:27:12,762961212-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 189.07k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:27:12,779660983-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:27:20,748831043-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 189.07k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:27:20,765813148-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:27:30,256475225-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 189.07k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:27:30,273612590-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:27:30,286997625-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T01:27:30,294969032-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:27:30,312634324-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=4127943
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_64KB_seq.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-17T01:27:30,327637375-07:00] INFO: > Run rados bench[0m
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_64KB_seq.log.5
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
[1;30mlocalhost.localdomain	[2021-05-17T01:27:30,367467269-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:27:30,373832648-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '85878e3c-44b7-441c-8170-3f2a246a3afe', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.502hma:/tmp/ceph-asok.502hma', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 85878e3c-44b7-441c-8170-3f2a246a3afe --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.502hma:/tmp/ceph-asok.502hma -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T08:27:31.877+0000 ffff9ac3e010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T08:27:31.885+0000 ffff9ac3e010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T08:27:31.885+0000 ffff9ac3e010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T08:27:31.947169+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T08:27:31.947169+0000     0       3         3         0         0         0           -           0
2021-05-17T08:27:32.947481+0000     1     255     10876     10621   663.344   663.812   0.0607144    0.023118
2021-05-17T08:27:33.948554+0000     2     256     21616     21360   666.907   671.188  0.00672362   0.0236436
2021-05-17T08:27:34.948775+0000     3     256     32836     32580   678.298    701.25   0.0568015   0.0232782
2021-05-17T08:27:35.949044+0000     4     255     44646     44391   693.216   738.188   0.0623107   0.0228803
2021-05-17T08:27:36.949268+0000     5     255     56339     56084   700.701   730.812  0.00279877   0.0226745
2021-05-17T08:27:37.949481+0000     6     256     68168     67912   707.098    739.25 0.000774074   0.0224806
2021-05-17T08:27:38.949672+0000     7     255     79988     79733   711.607   738.812  0.00172522   0.0223559
2021-05-17T08:27:39.950527+0000     8     255     91366     91111   711.471   711.125 0.000617597   0.0223778
2021-05-17T08:27:40.950720+0000     9     256    101633    101377   703.698   641.625   0.0162218   0.0226368
2021-05-17T08:27:41.950944+0000    10     255    112063    111808   698.509   651.938   0.0668274   0.0228111
2021-05-17T08:27:42.951177+0000    11     255    122164    121909   692.388   631.312   0.0101925   0.0230152
2021-05-17T08:27:43.951420+0000    12     256    134631    134375   699.599   779.125  0.00395239   0.0227852
2021-05-17T08:27:44.951625+0000    13     256    147327    147071   706.808     793.5  0.00129985   0.0225655
2021-05-17T08:27:45.951828+0000    14     256    159296    159040   709.744   748.062   0.0598062   0.0224783
2021-05-17T08:27:46.952024+0000    15     255    170692    170437   709.906   712.312  0.00190683   0.0224709
2021-05-17T08:27:47.952338+0000    16     256    182757    182501   712.647       754   0.0488408   0.0223956
2021-05-17T08:27:48.952603+0000 Total time run:       16.5345
Total reads made:     189070
Read size:            65536
Object size:          65536
Bandwidth (MB/sec):   714.681
Average IOPS:         11434
Stddev IOPS:          782.481
Max IOPS:             12696
Min IOPS:             10101
Average Latency(s):   0.0223333
Max latency(s):       0.0899858
Min latency(s):       0.000488169

[1;32mlocalhost.localdomain	[2021-05-17T01:27:49,273409902-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 4127943


[1;33mlocalhost.localdomain	[2021-05-17T01:27:49,281599671-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:28:12,205184161-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 189.07k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:28:12,222385165-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:28:20,107608242-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 189.07k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:28:20,125116588-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:28:28,260959153-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 189.07k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:28:28,278237614-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:28:36,457161928-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 189.07k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:28:36,474438274-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:28:44,467485826-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 189.07k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:28:44,484337043-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:28:44,498337316-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T01:28:44,506845288-07:00][RUNNING][ROUND 1/4/40] object_size=256KB[0m


[1;33mlocalhost.localdomain	[2021-05-17T01:28:44,514779231-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T01:28:44,531839044-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40670\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.318090\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 43a76dfe-1086-4823-9bc3-a703e64210ef\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 43a76dfe-1086-4823-9bc3-a703e64210ef\nlast_changed 2021-05-17T01:29:13.278010-0700\ncreated 2021-05-17T01:29:13.278010-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40670/0,v1:10.10.1.2:40671/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.318090 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 44b843bd-dfd7-4f55-acaf-12f243857e27\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 1b4449c1-9151-4f9f-ada4-8e73d6f0bc05\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 95ba57f8-4726-47b3-aff0-a0f518d25a9f\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\n'
10.10.1.2: b'restful urls: https://10.10.1.2:42670\n  w/ user/pass: admin / 1e8f035b-2b00-4522-a00a-427c85fb2b3e\n\n'
10.10.1.2: b'\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 01:29:28 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40670
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.318090
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 43a76dfe-1086-4823-9bc3-a703e64210ef
setting min_mon_release = octopus
epoch 0
fsid 43a76dfe-1086-4823-9bc3-a703e64210ef
last_changed 2021-05-17T01:29:13.278010-0700
created 2021-05-17T01:29:13.278010-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40670/0,v1:10.10.1.2:40671/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.318090 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 44b843bd-dfd7-4f55-acaf-12f243857e27
0
start osd.0
add osd1 1b4449c1-9151-4f9f-ada4-8e73d6f0bc05
1
start osd.1
add osd2 95ba57f8-4726-47b3-aff0-a0f518d25a9f
2
start osd.2


restful urls: https://10.10.1.2:42670
  w/ user/pass: admin / 1e8f035b-2b00-4522-a00a-427c85fb2b3e


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T01:28:45.524-0700 7f40526811c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T01:28:45.524-0700 7f40526811c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T01:28:45.540-0700 7fb0117de1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T01:28:45.540-0700 7fb0117de1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40670,v1:10.10.1.2:40671] --print /tmp/ceph_monmap.318090 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.318090 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.318090 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42670 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.xY0ux2Q77o 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 44b843bd-dfd7-4f55-acaf-12f243857e27 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBiKaJgNb8WBBAAqOshkLak9Jrm4WIw+x7QCw== --osd-uuid 44b843bd-dfd7-4f55-acaf-12f243857e27 
2021-05-17T01:29:22.424-0700 7f4f56ba6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T01:29:22.424-0700 7f4f56ba6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T01:29:22.424-0700 7f4f56ba6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T01:29:22.512-0700 7f4f56ba6f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 1b4449c1-9151-4f9f-ada4-8e73d6f0bc05 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T01:29:22.788-0700 7fd4c92def00 -1 Falling back to public interface
2021-05-17T01:29:22.804-0700 7fd4c92def00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBiKaJg+m9ALxAAdQaqjhkp13asoYz3N72qFA== --osd-uuid 1b4449c1-9151-4f9f-ada4-8e73d6f0bc05 
2021-05-17T01:29:23.168-0700 7f73dafd6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T01:29:23.168-0700 7f73dafd6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T01:29:23.168-0700 7f73dafd6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T01:29:23.216-0700 7f73dafd6f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 95ba57f8-4726-47b3-aff0-a0f518d25a9f -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T01:29:23.476-0700 7f2c60cb3f00 -1 Falling back to public interface
2021-05-17T01:29:23.488-0700 7f2c60cb3f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBjKaJgpsh0HBAA4M64pPOmKk/MVwxGtjtgAQ== --osd-uuid 95ba57f8-4726-47b3-aff0-a0f518d25a9f 
2021-05-17T01:29:23.840-0700 7f9dd1be9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T01:29:23.840-0700 7f9dd1be9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T01:29:23.840-0700 7f9dd1be9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T01:29:23.904-0700 7f9dd1be9f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T01:29:24.264-0700 7fcbf5b44f00 -1 Falling back to public interface
2021-05-17T01:29:24.276-0700 7fcbf5b44f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T01:29:28,196394691-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T01:29:28,204475134-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T01:29:28,287333697-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:29:28,293689306-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '055cee91-d1c9-4871-a251-2a84bf3d4895', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5roaQV:/tmp/ceph-asok.5roaQV', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 055cee91-d1c9-4871-a251-2a84bf3d4895 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5roaQV:/tmp/ceph-asok.5roaQV -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T01:29:31,121051756-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:29:31,127540093-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '055cee91-d1c9-4871-a251-2a84bf3d4895', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5roaQV:/tmp/ceph-asok.5roaQV', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 055cee91-d1c9-4871-a251-2a84bf3d4895 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5roaQV:/tmp/ceph-asok.5roaQV -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T01:29:33,881613195-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:29:33,888131882-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '055cee91-d1c9-4871-a251-2a84bf3d4895', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5roaQV:/tmp/ceph-asok.5roaQV', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 055cee91-d1c9-4871-a251-2a84bf3d4895 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5roaQV:/tmp/ceph-asok.5roaQV -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T01:29:36,697369934-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:29:36,705264884-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '055cee91-d1c9-4871-a251-2a84bf3d4895', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5roaQV:/tmp/ceph-asok.5roaQV', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 055cee91-d1c9-4871-a251-2a84bf3d4895 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5roaQV:/tmp/ceph-asok.5roaQV -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T01:29:42,467793164-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:29:42,474230012-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '055cee91-d1c9-4871-a251-2a84bf3d4895', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5roaQV:/tmp/ceph-asok.5roaQV', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 055cee91-d1c9-4871-a251-2a84bf3d4895 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5roaQV:/tmp/ceph-asok.5roaQV -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T01:29:46,110017687-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:29:46,116360138-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '055cee91-d1c9-4871-a251-2a84bf3d4895', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5roaQV:/tmp/ceph-asok.5roaQV', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 055cee91-d1c9-4871-a251-2a84bf3d4895 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5roaQV:/tmp/ceph-asok.5roaQV -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T01:29:49,530563698-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:29:49,537076504-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '055cee91-d1c9-4871-a251-2a84bf3d4895', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5roaQV:/tmp/ceph-asok.5roaQV', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 055cee91-d1c9-4871-a251-2a84bf3d4895 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5roaQV:/tmp/ceph-asok.5roaQV -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T01:29:52,736230652-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:29:52,742737157-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '055cee91-d1c9-4871-a251-2a84bf3d4895', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5roaQV:/tmp/ceph-asok.5roaQV', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 055cee91-d1c9-4871-a251-2a84bf3d4895 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5roaQV:/tmp/ceph-asok.5roaQV -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T01:29:56,076921709-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:29:56,083141964-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '055cee91-d1c9-4871-a251-2a84bf3d4895', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5roaQV:/tmp/ceph-asok.5roaQV', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 055cee91-d1c9-4871-a251-2a84bf3d4895 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5roaQV:/tmp/ceph-asok.5roaQV -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T01:29:59,297844842-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:29:59,304238992-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '055cee91-d1c9-4871-a251-2a84bf3d4895', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5roaQV:/tmp/ceph-asok.5roaQV', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 055cee91-d1c9-4871-a251-2a84bf3d4895 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5roaQV:/tmp/ceph-asok.5roaQV -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T01:30:02,667380344-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:30:02,674049146-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '055cee91-d1c9-4871-a251-2a84bf3d4895', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5roaQV:/tmp/ceph-asok.5roaQV', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 055cee91-d1c9-4871-a251-2a84bf3d4895 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5roaQV:/tmp/ceph-asok.5roaQV -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T01:30:05,386109447-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:30:05,392358567-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '055cee91-d1c9-4871-a251-2a84bf3d4895', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5roaQV:/tmp/ceph-asok.5roaQV', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 055cee91-d1c9-4871-a251-2a84bf3d4895 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5roaQV:/tmp/ceph-asok.5roaQV -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T01:30:08,060390002-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:30:31,131438124-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   199 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
    Global Recovery Event (5s)
      [======================......] (remaining: 1s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:30:39,170738985-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=========================...] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:30:47,159858014-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:30:55,146201243-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:31:03,277904807-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:31:11,115016622-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:31:19,132219426-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:31:27,437811490-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:31:27,455066688-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:31:35,450084327-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:31:35,467039548-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:31:43,702768784-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:31:43,719758720-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:31:51,672221323-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:31:51,689437619-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:31:59,801807124-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:31:59,818838122-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:31:59,832354464-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T01:31:59,840430538-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:31:59,858703300-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=4141342
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_256KB_write.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-17T01:31:59,874038691-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_256KB_write.log.1
[1;30mlocalhost.localdomain	[2021-05-17T01:31:59,914866014-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:31:59,921265128-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '055cee91-d1c9-4871-a251-2a84bf3d4895', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5roaQV:/tmp/ceph-asok.5roaQV', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 055cee91-d1c9-4871-a251-2a84bf3d4895 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5roaQV:/tmp/ceph-asok.5roaQV -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T08:32:01.436+0000 ffff9b39f010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T08:32:01.444+0000 ffff9b39f010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T08:32:01.444+0000 ffff9b39f010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T08:32:01.461947+0000 Maintaining 256 concurrent writes of 262144 bytes to objects of size 262144 for up to 20 seconds or 0 objects
2021-05-17T08:32:01.462006+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T08:32:01.512247+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T08:32:01.512247+0000     0       0         0         0         0         0           -           0
2021-05-17T08:32:02.512448+0000     1     255      2220      1965    491.21    491.25   0.0916397    0.116478
2021-05-17T08:32:03.512643+0000     2     255      4281      4026   503.181    515.25   0.0416632    0.120962
2021-05-17T08:32:04.512859+0000     3     255      6152      5897   491.336    467.75   0.0697207    0.126418
2021-05-17T08:32:05.513084+0000     4     255      7794      7539   471.103     410.5    0.106297    0.131939
2021-05-17T08:32:06.513304+0000     5     255      9736      9481   473.961     485.5   0.0437871    0.133037
2021-05-17T08:32:07.513487+0000     6     255     11591     11336   472.245    463.75    0.130226    0.132314
2021-05-17T08:32:08.513665+0000     7     255     13437     13182   470.698     461.5    0.229444     0.13363
2021-05-17T08:32:09.513861+0000     8     255     15159     14904   465.663     430.5   0.0544409    0.135655
2021-05-17T08:32:10.514127+0000     9     255     16881     16626   461.743     430.5   0.0472532     0.13673
2021-05-17T08:32:11.514380+0000    10     255     18647     18392   459.707     441.5   0.0907209    0.137858
2021-05-17T08:32:12.514601+0000    11     255     20881     20626   468.678     558.5    0.113888    0.135608
2021-05-17T08:32:13.514886+0000    12     255     22710     22455   467.714    457.25    0.103897    0.135757
2021-05-17T08:32:14.515216+0000    13     255     25559     25304   486.509    712.25   0.0599953    0.130869
2021-05-17T08:32:15.515425+0000    14     255     28803     28548   509.674       811    0.094384    0.124933
2021-05-17T08:32:16.515650+0000    15     255     32045     31790   529.717     810.5   0.0515369    0.120224
2021-05-17T08:32:17.515952+0000    16     255     35342     35087   548.112    824.25   0.0654898    0.116192
2021-05-17T08:32:18.516203+0000    17     255     38673     38418   564.843    832.75    0.128893    0.112806
2021-05-17T08:32:19.516436+0000    18     255     41788     41533   576.717    778.75   0.0790576    0.110448
2021-05-17T08:32:20.516679+0000    19     255     45070     44815   589.537     820.5   0.0776537    0.108149
2021-05-17T08:32:21.516855+0000 min lat: 0.0115198 max lat: 0.343411 avg lat: 0.105697
2021-05-17T08:32:21.516855+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T08:32:21.516855+0000    20     116     48450     48334   604.039    879.75   0.0514071    0.105697
2021-05-17T08:32:22.517105+0000 Total time run:         20.0099
Total writes made:      48450
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     605.325
Stddev Bandwidth:       176.564
Max bandwidth (MB/sec): 879.75
Min bandwidth (MB/sec): 410.5
Average IOPS:           2421
Stddev IOPS:            706.257
Max IOPS:               3519
Min IOPS:               1642
Average Latency(s):     0.105549
Stddev Latency(s):      0.0497126
Max latency(s):         0.343411
Min latency(s):         0.0115198

[1;32mlocalhost.localdomain	[2021-05-17T01:32:22,867902815-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 4141342


[1;33mlocalhost.localdomain	[2021-05-17T01:32:22,877223623-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:32:45,869296764-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 48.45k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:32:45,887537359-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:32:53,886433975-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 48.45k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:32:53,903670972-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:33:02,101558622-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 48.45k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:33:02,118878898-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:33:10,211877825-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 48.45k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:33:10,231032197-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:33:18,211054484-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 48.45k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:33:18,228770093-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:33:18,242799341-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T01:33:18,250641222-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:33:18,268983894-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=4144642
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_256KB_seq.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-17T01:33:18,284549466-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_256KB_seq.log.1
[1;30mlocalhost.localdomain	[2021-05-17T01:33:18,324264236-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:33:18,330749179-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '055cee91-d1c9-4871-a251-2a84bf3d4895', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5roaQV:/tmp/ceph-asok.5roaQV', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 055cee91-d1c9-4871-a251-2a84bf3d4895 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.5roaQV:/tmp/ceph-asok.5roaQV -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T08:33:19.793+0000 ffff87a16010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T08:33:19.801+0000 ffff87a16010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T08:33:19.801+0000 ffff87a16010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T08:33:19.823578+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T08:33:19.823578+0000     0       0         0         0         0         0           -           0
2021-05-17T08:33:20.823763+0000     1     256      3547      3291   822.467    822.75    0.201104   0.0705404
2021-05-17T08:33:21.824053+0000     2     255      7199      6944   867.725    913.25    0.130776   0.0716239
2021-05-17T08:33:22.824566+0000     3     255     11742     11487   956.884   1135.75    0.107559   0.0650936
2021-05-17T08:33:23.824780+0000     4     255     15980     15725   982.478    1059.5  0.00961798   0.0635244
2021-05-17T08:33:24.825091+0000     5     255     20253     19998   999.566   1068.25  0.00520876   0.0628303
2021-05-17T08:33:25.825459+0000     6     256     24626     24370   1015.07      1093   0.0334956   0.0623795
2021-05-17T08:33:26.825754+0000     7     256     29033     28777   1027.41   1101.75    0.118376   0.0617306
2021-05-17T08:33:27.825987+0000     8     255     33078     32823   1025.39    1011.5  0.00704011   0.0618596
2021-05-17T08:33:28.827097+0000     9     256     37477     37221   1033.49    1099.5    0.123925   0.0614625
2021-05-17T08:33:29.827356+0000    10     255     41848     41593   1039.42      1093   0.0119429   0.0610783
2021-05-17T08:33:30.828289+0000    11     255     45918     45663   1037.34    1017.5    0.148689   0.0612025
2021-05-17T08:33:31.828646+0000 Total time run:       11.7484
Total reads made:     48450
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   1031
Average IOPS:         4123
Stddev IOPS:          374.904
Max IOPS:             4543
Min IOPS:             3291
Average Latency(s):   0.0616176
Max latency(s):       0.240118
Min latency(s):       0.00092464

[1;32mlocalhost.localdomain	[2021-05-17T01:33:32,181143452-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 4144642


[1;33mlocalhost.localdomain	[2021-05-17T01:33:32,189872601-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:33:54,983616724-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 48.45k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:33:55,001074872-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:34:03,143528900-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 48.45k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:34:03,160627045-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:34:11,139161080-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 48.45k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:34:11,159312026-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:34:19,180009224-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 48.45k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:34:19,197285345-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:34:27,283702643-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 48.45k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:34:27,301223845-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:34:27,315215090-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T01:34:27,320172214-07:00][RUNNING][ROUND 2/4/40] object_size=256KB[0m


[1;33mlocalhost.localdomain	[2021-05-17T01:34:27,328045660-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T01:34:27,345387761-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40312\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.319333\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid aa1df20b-f3ab-4ff0-a5c1-1a07a66ad0ef\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid aa1df20b-f3ab-4ff0-a5c1-1a07a66ad0ef\nlast_changed 2021-05-17T01:34:56.423120-0700\ncreated 2021-05-17T01:34:56.423120-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40312/0,v1:10.10.1.2:40313/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.319333 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 4844487d-b9fa-497b-a8b7-aece784f446c\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 5652c254-e765-4b4b-a65b-a070f5d42132\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 84640667-6227-4bfc-a3de-566c736bfad9\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\n'
10.10.1.2: b'restful urls: https://10.10.1.2:42312\n  w/ user/pass: admin / 76cda307-b883-4306-9179-3442a637bfe1\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 01:35:12 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40312
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.319333
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid aa1df20b-f3ab-4ff0-a5c1-1a07a66ad0ef
setting min_mon_release = octopus
epoch 0
fsid aa1df20b-f3ab-4ff0-a5c1-1a07a66ad0ef
last_changed 2021-05-17T01:34:56.423120-0700
created 2021-05-17T01:34:56.423120-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40312/0,v1:10.10.1.2:40313/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.319333 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 4844487d-b9fa-497b-a8b7-aece784f446c
0
start osd.0
add osd1 5652c254-e765-4b4b-a65b-a070f5d42132
1
start osd.1
add osd2 84640667-6227-4bfc-a3de-566c736bfad9
2
start osd.2


restful urls: https://10.10.1.2:42312
  w/ user/pass: admin / 76cda307-b883-4306-9179-3442a637bfe1


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T01:34:28.339-0700 7efc67ce91c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T01:34:28.339-0700 7efc67ce91c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T01:34:28.355-0700 7f0b70b151c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T01:34:28.355-0700 7f0b70b151c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40312,v1:10.10.1.2:40313] --print /tmp/ceph_monmap.319333 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.319333 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.319333 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42312 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.HaIJ2WrxI2 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 4844487d-b9fa-497b-a8b7-aece784f446c -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQC6KqJgAk38CBAAphnSycdme5MwiXf4+4CiVw== --osd-uuid 4844487d-b9fa-497b-a8b7-aece784f446c 
2021-05-17T01:35:06.471-0700 7f11ee377f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T01:35:06.471-0700 7f11ee377f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T01:35:06.471-0700 7f11ee377f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T01:35:06.535-0700 7f11ee377f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 5652c254-e765-4b4b-a65b-a070f5d42132 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T01:35:06.799-0700 7fe2d9f42f00 -1 Falling back to public interface
2021-05-17T01:35:06.811-0700 7fe2d9f42f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQC6KqJghFGzLxAAGyOxD5fRVO02ot5eqx1Y0g== --osd-uuid 5652c254-e765-4b4b-a65b-a070f5d42132 
2021-05-17T01:35:07.143-0700 7f2f0a6e6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T01:35:07.143-0700 7f2f0a6e6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T01:35:07.143-0700 7f2f0a6e6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T01:35:07.191-0700 7f2f0a6e6f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 84640667-6227-4bfc-a3de-566c736bfad9 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T01:35:07.571-0700 7f411b7c7f00 -1 Falling back to public interface
2021-05-17T01:35:07.583-0700 7f411b7c7f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQC7KqJgx0oFIhAAGZXkAB7p1hTPugLF0tZ63w== --osd-uuid 84640667-6227-4bfc-a3de-566c736bfad9 
2021-05-17T01:35:07.915-0700 7f8463352f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T01:35:07.915-0700 7f8463352f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T01:35:07.915-0700 7f8463352f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T01:35:07.967-0700 7f8463352f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T01:35:08.371-0700 7f934d727f00 -1 Falling back to public interface
2021-05-17T01:35:08.387-0700 7f934d727f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T01:35:12,250346713-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T01:35:12,258359772-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T01:35:12,341774207-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:35:12,348092154-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '476df56e-b80e-4846-833e-ab8f730c2795', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ceSSDM:/tmp/ceph-asok.ceSSDM', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 476df56e-b80e-4846-833e-ab8f730c2795 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ceSSDM:/tmp/ceph-asok.ceSSDM -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T01:35:15,186601313-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:35:15,193090125-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '476df56e-b80e-4846-833e-ab8f730c2795', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ceSSDM:/tmp/ceph-asok.ceSSDM', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 476df56e-b80e-4846-833e-ab8f730c2795 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ceSSDM:/tmp/ceph-asok.ceSSDM -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T01:35:17,963040199-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:35:17,969677971-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '476df56e-b80e-4846-833e-ab8f730c2795', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ceSSDM:/tmp/ceph-asok.ceSSDM', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 476df56e-b80e-4846-833e-ab8f730c2795 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ceSSDM:/tmp/ceph-asok.ceSSDM -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T01:35:20,778209439-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:35:20,784699946-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '476df56e-b80e-4846-833e-ab8f730c2795', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ceSSDM:/tmp/ceph-asok.ceSSDM', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 476df56e-b80e-4846-833e-ab8f730c2795 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ceSSDM:/tmp/ceph-asok.ceSSDM -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T01:35:26,437641730-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:35:26,444309568-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '476df56e-b80e-4846-833e-ab8f730c2795', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ceSSDM:/tmp/ceph-asok.ceSSDM', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 476df56e-b80e-4846-833e-ab8f730c2795 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ceSSDM:/tmp/ceph-asok.ceSSDM -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T01:35:30,103649096-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:35:30,110294835-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '476df56e-b80e-4846-833e-ab8f730c2795', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ceSSDM:/tmp/ceph-asok.ceSSDM', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 476df56e-b80e-4846-833e-ab8f730c2795 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ceSSDM:/tmp/ceph-asok.ceSSDM -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T01:35:33,468220329-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:35:33,474580209-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '476df56e-b80e-4846-833e-ab8f730c2795', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ceSSDM:/tmp/ceph-asok.ceSSDM', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 476df56e-b80e-4846-833e-ab8f730c2795 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ceSSDM:/tmp/ceph-asok.ceSSDM -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T01:35:36,848013720-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:35:36,854462475-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '476df56e-b80e-4846-833e-ab8f730c2795', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ceSSDM:/tmp/ceph-asok.ceSSDM', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 476df56e-b80e-4846-833e-ab8f730c2795 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ceSSDM:/tmp/ceph-asok.ceSSDM -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T01:35:40,215556825-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:35:40,222013246-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '476df56e-b80e-4846-833e-ab8f730c2795', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ceSSDM:/tmp/ceph-asok.ceSSDM', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 476df56e-b80e-4846-833e-ab8f730c2795 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ceSSDM:/tmp/ceph-asok.ceSSDM -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T01:35:43,544514892-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:35:43,550907252-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '476df56e-b80e-4846-833e-ab8f730c2795', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ceSSDM:/tmp/ceph-asok.ceSSDM', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 476df56e-b80e-4846-833e-ab8f730c2795 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ceSSDM:/tmp/ceph-asok.ceSSDM -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T01:35:46,859451350-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:35:46,865617126-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '476df56e-b80e-4846-833e-ab8f730c2795', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ceSSDM:/tmp/ceph-asok.ceSSDM', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 476df56e-b80e-4846-833e-ab8f730c2795 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ceSSDM:/tmp/ceph-asok.ceSSDM -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T01:35:49,644463176-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:35:49,650871589-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '476df56e-b80e-4846-833e-ab8f730c2795', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ceSSDM:/tmp/ceph-asok.ceSSDM', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 476df56e-b80e-4846-833e-ab8f730c2795 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ceSSDM:/tmp/ceph-asok.ceSSDM -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T01:35:52,369191827-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:36:15,322680942-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   199 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
    Global Recovery Event (4s)
      [======================......] (remaining: 1s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:36:23,360314298-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (9s)
      [=========================...] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:36:31,282955166-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:36:39,400785511-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:36:49,694502379-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:36:57,689378442-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:37:05,664542893-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:37:13,959302080-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:37:13,976294572-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:37:21,952367459-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:37:21,969845674-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:37:30,040461815-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:37:30,057732305-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:37:37,893164236-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:37:37,910684999-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:37:46,024959931-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:37:46,042895109-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:37:46,056949511-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T01:37:46,065163333-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:37:46,084366515-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=4157962
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_256KB_write.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-17T01:37:46,100023387-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_256KB_write.log.2
[1;30mlocalhost.localdomain	[2021-05-17T01:37:46,142402898-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:37:46,149038479-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '476df56e-b80e-4846-833e-ab8f730c2795', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ceSSDM:/tmp/ceph-asok.ceSSDM', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 476df56e-b80e-4846-833e-ab8f730c2795 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ceSSDM:/tmp/ceph-asok.ceSSDM -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T08:37:47.683+0000 ffffa821a010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T08:37:47.691+0000 ffffa821a010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T08:37:47.691+0000 ffffa821a010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T08:37:47.707445+0000 Maintaining 256 concurrent writes of 262144 bytes to objects of size 262144 for up to 20 seconds or 0 objects
2021-05-17T08:37:47.707503+0000 Object prefix: benchmark_data_localhost.localdomain_7
2021-05-17T08:37:47.757725+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T08:37:47.757725+0000     0       0         0         0         0         0           -           0
2021-05-17T08:37:48.757946+0000     1     255      3316      3061    765.19    765.25   0.0715443   0.0786291
2021-05-17T08:37:49.758185+0000     2     255      6544      6289       786       807   0.0682389    0.078202
2021-05-17T08:37:50.758458+0000     3     255      9813      9558   796.343    817.25   0.0685531   0.0784374
2021-05-17T08:37:51.758647+0000     4     255     12990     12735   795.783    794.25    0.068963    0.078805
2021-05-17T08:37:52.758833+0000     5     255     16203     15948   797.246    803.25     0.13425   0.0793006
2021-05-17T08:37:53.759082+0000     6     255     19483     19228   801.005       820   0.0447214   0.0788307
2021-05-17T08:37:54.759288+0000     7     255     22674     22419   800.516    797.75   0.0308661   0.0791037
2021-05-17T08:37:55.759542+0000     8     255     25952     25697   802.863     819.5   0.0334848   0.0790311
2021-05-17T08:37:56.759758+0000     9     255     29294     29039    806.47     835.5   0.0654995   0.0786886
2021-05-17T08:37:57.760031+0000    10     255     32574     32319     807.8       820   0.0600254   0.0785942
2021-05-17T08:37:58.760297+0000    11     255     35463     35208   800.005    722.25   0.0606778   0.0794758
2021-05-17T08:37:59.760612+0000    12     255     38339     38084   793.235       719   0.0378241    0.080225
2021-05-17T08:38:00.760914+0000    13     255     41071     40816   784.739       683   0.0643018    0.081095
2021-05-17T08:38:01.761232+0000    14     255     43940     43685   779.902    717.25   0.0528249   0.0816346
2021-05-17T08:38:02.761467+0000    15     255     46791     46536   775.414    712.75   0.0664004   0.0821097
2021-05-17T08:38:03.761657+0000    16     255     49905     49650   775.598     778.5   0.0820135   0.0820223
2021-05-17T08:38:04.761906+0000    17     255     52903     52648   774.051     749.5   0.0848363   0.0823197
2021-05-17T08:38:05.765125+0000    18     255     56022     55767    774.23    779.75   0.0811056   0.0822678
2021-05-17T08:38:06.765393+0000    19     255     58844     58589   770.603     705.5   0.0725905   0.0827236
2021-05-17T08:38:07.765956+0000 min lat: 0.0240298 max lat: 0.288515 avg lat: 0.0828343
2021-05-17T08:38:07.765956+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T08:38:07.765956+0000    20     255     61856     61601   769.701       753   0.0440506   0.0828343
2021-05-17T08:38:08.766281+0000 Total time run:         20.0365
Total writes made:      61857
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     771.805
Stddev Bandwidth:       46.6653
Max bandwidth (MB/sec): 835.5
Min bandwidth (MB/sec): 683
Average IOPS:           3087
Stddev IOPS:            186.661
Max IOPS:               3342
Min IOPS:               2732
Average Latency(s):     0.0827371
Stddev Latency(s):      0.0323768
Max latency(s):         0.288515
Min latency(s):         0.0240298

[1;32mlocalhost.localdomain	[2021-05-17T01:38:09,106013369-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 4157962


[1;33mlocalhost.localdomain	[2021-05-17T01:38:09,115783303-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:38:32,071006256-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 61.86k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:38:32,088863841-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:38:40,034241077-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 61.86k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:38:40,051531863-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:38:48,098099819-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 61.86k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:38:48,115746556-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:38:56,042337460-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 61.86k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:38:56,065333982-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:39:04,070060243-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 61.86k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:39:04,087768176-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:39:04,101893583-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T01:39:04,110263897-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:39:04,129279319-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=4161252
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_256KB_seq.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-17T01:39:04,144647496-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_256KB_seq.log.2
[1;30mlocalhost.localdomain	[2021-05-17T01:39:04,184542772-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:39:04,191124298-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '476df56e-b80e-4846-833e-ab8f730c2795', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ceSSDM:/tmp/ceph-asok.ceSSDM', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 476df56e-b80e-4846-833e-ab8f730c2795 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.ceSSDM:/tmp/ceph-asok.ceSSDM -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T08:39:05.781+0000 ffffa5f7d010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T08:39:05.789+0000 ffffa5f7d010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T08:39:05.789+0000 ffffa5f7d010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T08:39:05.808160+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T08:39:05.808160+0000     0       0         0         0         0         0           -           0
2021-05-17T08:39:06.808341+0000     1     256      3569      3313   827.973    828.25   0.0022088     0.07222
2021-05-17T08:39:07.808591+0000     2     255      6822      6567   820.635     813.5  0.00911944   0.0757262
2021-05-17T08:39:08.808834+0000     3     256     10107      9851    820.69       821   0.0156626   0.0755217
2021-05-17T08:39:09.809086+0000     4     256     13490     13234   826.902    845.75  0.00836266   0.0759178
2021-05-17T08:39:10.809431+0000     5     255     17004     16749   837.211    878.75   0.0124203   0.0753515
2021-05-17T08:39:11.809661+0000     6     255     20696     20441   851.473       923   0.0580502   0.0744343
2021-05-17T08:39:12.809883+0000     7     255     24450     24195   863.875     938.5   0.0129342   0.0734565
2021-05-17T08:39:13.810124+0000     8     256     28097     27841   869.801     911.5   0.0181861    0.072998
2021-05-17T08:39:14.810397+0000     9     256     31550     31294   869.047    863.25   0.0548304   0.0730217
2021-05-17T08:39:15.810812+0000    10     255     34605     34350   858.509       764  0.00500293   0.0738176
2021-05-17T08:39:16.813505+0000    11     256     38064     37808   858.843     864.5   0.0529518   0.0739673
2021-05-17T08:39:17.813829+0000    12     256     41655     41399   862.061    897.75    0.174566    0.073624
2021-05-17T08:39:18.814174+0000    13     255     44576     44321   851.923     730.5    0.170225   0.0745839
2021-05-17T08:39:19.814499+0000    14     256     47888     47632   850.177    827.75   0.0469186     0.07486
2021-05-17T08:39:20.814686+0000    15     256     51046     50790   846.123     789.5  0.00479404   0.0748828
2021-05-17T08:39:21.814976+0000    16     256     54728     54472   850.754     920.5   0.0341478   0.0747684
2021-05-17T08:39:22.815269+0000    17     255     58231     57976   852.224       876   0.0203344   0.0747353
2021-05-17T08:39:23.815592+0000    18     255     61819     61564   854.695       897  0.00727952   0.0745097
2021-05-17T08:39:24.815854+0000 Total time run:       18.1247
Total reads made:     61857
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   853.215
Average IOPS:         3412
Stddev IOPS:          230.247
Max IOPS:             3754
Min IOPS:             2922
Average Latency(s):   0.0746483
Max latency(s):       0.344627
Min latency(s):       0.000936461

[1;32mlocalhost.localdomain	[2021-05-17T01:39:25,149116871-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 4161252


[1;33mlocalhost.localdomain	[2021-05-17T01:39:25,157724900-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:39:48,102563200-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 61.86k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:39:48,120118511-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:39:56,216392555-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 61.86k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:39:56,233577693-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:40:04,035266604-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 61.86k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:40:04,054520882-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:40:12,160206796-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 61.86k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:40:12,177569279-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:40:20,204167684-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 61.86k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:40:20,221630834-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:40:20,235842145-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T01:40:20,241122302-07:00][RUNNING][ROUND 3/4/40] object_size=256KB[0m


[1;33mlocalhost.localdomain	[2021-05-17T01:40:20,249433325-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T01:40:20,266759411-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40820\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.320487\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid a0fc134d-8a60-4fb8-907e-710ac80f979f\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid a0fc134d-8a60-4fb8-907e-710ac80f979f\nlast_changed 2021-05-17T01:40:52.550807-0700\ncreated 2021-05-17T01:40:52.550807-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40820/0,v1:10.10.1.2:40821/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.320487 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 f2c526e0-a18a-4d18-8c2a-14f7191145d1\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 9b3b8c69-0f0d-43d2-bbe8-d6ef116597b9\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 29bc3f17-c63a-4de0-b192-ec44fed71e32\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42820\n  w/ user/pass: admin / 11c354ec-7bce-45ee-924e-7e86e25566b3\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 01:41:07 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40820
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.320487
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid a0fc134d-8a60-4fb8-907e-710ac80f979f
setting min_mon_release = octopus
epoch 0
fsid a0fc134d-8a60-4fb8-907e-710ac80f979f
last_changed 2021-05-17T01:40:52.550807-0700
created 2021-05-17T01:40:52.550807-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40820/0,v1:10.10.1.2:40821/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.320487 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 f2c526e0-a18a-4d18-8c2a-14f7191145d1
0
start osd.0
add osd1 9b3b8c69-0f0d-43d2-bbe8-d6ef116597b9
1
start osd.1
add osd2 29bc3f17-c63a-4de0-b192-ec44fed71e32
2
start osd.2


restful urls: https://10.10.1.2:42820
  w/ user/pass: admin / 11c354ec-7bce-45ee-924e-7e86e25566b3


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T01:40:21.262-0700 7f43ef1071c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T01:40:21.262-0700 7f43ef1071c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T01:40:21.278-0700 7f0925cc11c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T01:40:21.278-0700 7f0925cc11c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40820,v1:10.10.1.2:40821] --print /tmp/ceph_monmap.320487 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.320487 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.320487 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42820 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.EOpr6qzjOu 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new f2c526e0-a18a-4d18-8c2a-14f7191145d1 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAdLKJggIG2GxAA95hjxmVbA2NvleYZs7P61Q== --osd-uuid f2c526e0-a18a-4d18-8c2a-14f7191145d1 
2021-05-17T01:41:01.791-0700 7fdefdbcaf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T01:41:01.791-0700 7fdefdbcaf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T01:41:01.791-0700 7fdefdbcaf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T01:41:01.863-0700 7fdefdbcaf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 9b3b8c69-0f0d-43d2-bbe8-d6ef116597b9 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T01:41:02.155-0700 7f30df0fcf00 -1 Falling back to public interface
2021-05-17T01:41:02.167-0700 7f30df0fcf00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAeLKJgfuY0CRAArg1QAmGqlAqhesr0yMSCNg== --osd-uuid 9b3b8c69-0f0d-43d2-bbe8-d6ef116597b9 
2021-05-17T01:41:02.519-0700 7fc72aea9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T01:41:02.519-0700 7fc72aea9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T01:41:02.519-0700 7fc72aea9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T01:41:02.571-0700 7fc72aea9f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 29bc3f17-c63a-4de0-b192-ec44fed71e32 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T01:41:02.863-0700 7f877571ff00 -1 Falling back to public interface
2021-05-17T01:41:02.875-0700 7f877571ff00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAeLKJgYBMdMhAAhTH/LonBY9QySEleB3rRQg== --osd-uuid 29bc3f17-c63a-4de0-b192-ec44fed71e32 
2021-05-17T01:41:03.215-0700 7f9e101fdf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T01:41:03.215-0700 7f9e101fdf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T01:41:03.215-0700 7f9e101fdf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T01:41:03.267-0700 7f9e101fdf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T01:41:03.623-0700 7f634530af00 -1 Falling back to public interface
2021-05-17T01:41:03.635-0700 7f634530af00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T01:41:07,505086761-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T01:41:07,513601548-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T01:41:07,598674078-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:41:07,605895839-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '80e96513-faca-4b2d-8fd2-9d29c6bef184', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.D9J82O:/tmp/ceph-asok.D9J82O', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 80e96513-faca-4b2d-8fd2-9d29c6bef184 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.D9J82O:/tmp/ceph-asok.D9J82O -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T01:41:10,457122846-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:41:10,463382071-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '80e96513-faca-4b2d-8fd2-9d29c6bef184', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.D9J82O:/tmp/ceph-asok.D9J82O', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 80e96513-faca-4b2d-8fd2-9d29c6bef184 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.D9J82O:/tmp/ceph-asok.D9J82O -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T01:41:13,881086661-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:41:13,887542389-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '80e96513-faca-4b2d-8fd2-9d29c6bef184', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.D9J82O:/tmp/ceph-asok.D9J82O', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 80e96513-faca-4b2d-8fd2-9d29c6bef184 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.D9J82O:/tmp/ceph-asok.D9J82O -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T01:41:16,619200933-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:41:16,625677246-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '80e96513-faca-4b2d-8fd2-9d29c6bef184', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.D9J82O:/tmp/ceph-asok.D9J82O', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 80e96513-faca-4b2d-8fd2-9d29c6bef184 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.D9J82O:/tmp/ceph-asok.D9J82O -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T01:41:22,353822225-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:41:22,360399384-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '80e96513-faca-4b2d-8fd2-9d29c6bef184', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.D9J82O:/tmp/ceph-asok.D9J82O', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 80e96513-faca-4b2d-8fd2-9d29c6bef184 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.D9J82O:/tmp/ceph-asok.D9J82O -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T01:41:25,416172189-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:41:25,422511138-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '80e96513-faca-4b2d-8fd2-9d29c6bef184', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.D9J82O:/tmp/ceph-asok.D9J82O', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 80e96513-faca-4b2d-8fd2-9d29c6bef184 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.D9J82O:/tmp/ceph-asok.D9J82O -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T01:41:28,819389592-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:41:28,825695520-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '80e96513-faca-4b2d-8fd2-9d29c6bef184', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.D9J82O:/tmp/ceph-asok.D9J82O', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 80e96513-faca-4b2d-8fd2-9d29c6bef184 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.D9J82O:/tmp/ceph-asok.D9J82O -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T01:41:32,096864963-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:41:32,103210527-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '80e96513-faca-4b2d-8fd2-9d29c6bef184', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.D9J82O:/tmp/ceph-asok.D9J82O', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 80e96513-faca-4b2d-8fd2-9d29c6bef184 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.D9J82O:/tmp/ceph-asok.D9J82O -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T01:41:35,493290681-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:41:35,499829704-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '80e96513-faca-4b2d-8fd2-9d29c6bef184', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.D9J82O:/tmp/ceph-asok.D9J82O', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 80e96513-faca-4b2d-8fd2-9d29c6bef184 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.D9J82O:/tmp/ceph-asok.D9J82O -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T01:41:38,849672746-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:41:38,856170691-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '80e96513-faca-4b2d-8fd2-9d29c6bef184', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.D9J82O:/tmp/ceph-asok.D9J82O', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 80e96513-faca-4b2d-8fd2-9d29c6bef184 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.D9J82O:/tmp/ceph-asok.D9J82O -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T01:41:42,057209907-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:41:42,063607411-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '80e96513-faca-4b2d-8fd2-9d29c6bef184', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.D9J82O:/tmp/ceph-asok.D9J82O', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 80e96513-faca-4b2d-8fd2-9d29c6bef184 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.D9J82O:/tmp/ceph-asok.D9J82O -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T01:41:44,755188870-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:41:44,762745352-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '80e96513-faca-4b2d-8fd2-9d29c6bef184', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.D9J82O:/tmp/ceph-asok.D9J82O', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 80e96513-faca-4b2d-8fd2-9d29c6bef184 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.D9J82O:/tmp/ceph-asok.D9J82O -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   47 KiB   0 B   0 B   0 B  100 GiB     0  0.95   70      up          osd.2  
                       TOTAL  300 GiB  151 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.95/1.03  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T01:41:47,450798965-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:42:10,540227775-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   198 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:42:18,497878682-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:42:26,323515145-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=======.....................] (remaining: 27s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:42:34,575487213-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [========....................] (remaining: 49s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:42:42,836108745-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [========....................] (remaining: 57s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:42:50,982006438-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [=========...................] (remaining: 73s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:42:59,023112119-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (40s)
      [=========...................] (remaining: 83s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:43:06,988862479-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [=========...................] (remaining: 102s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:43:15,074297553-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:43:15,091690698-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:43:23,045036414-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:43:23,062584236-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:43:31,269968966-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:43:31,286846623-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:43:39,083806214-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:43:39,101176781-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:43:47,089684111-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:43:47,106768698-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:43:47,120550829-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T01:43:47,128554733-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:43:47,147298380-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=4175210
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_256KB_write.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-17T01:43:47,162946995-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_256KB_write.log.3
[1;30mlocalhost.localdomain	[2021-05-17T01:43:47,205016436-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:43:47,211434657-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '80e96513-faca-4b2d-8fd2-9d29c6bef184', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.D9J82O:/tmp/ceph-asok.D9J82O', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 80e96513-faca-4b2d-8fd2-9d29c6bef184 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.D9J82O:/tmp/ceph-asok.D9J82O -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T08:43:48.737+0000 ffff81755010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T08:43:48.745+0000 ffff81755010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T08:43:48.745+0000 ffff81755010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T08:43:48.764766+0000 Maintaining 256 concurrent writes of 262144 bytes to objects of size 262144 for up to 20 seconds or 0 objects
2021-05-17T08:43:48.764879+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T08:43:48.814518+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T08:43:48.814518+0000     0       0         0         0         0         0           -           0
2021-05-17T08:43:49.814740+0000     1     255      2527      2272   567.954       568    0.145177    0.102059
2021-05-17T08:43:50.814940+0000     2     255      5154      4899   612.289    656.75   0.0570028   0.0999596
2021-05-17T08:43:51.815197+0000     3     255      7863      7608   633.887    677.25    0.093484   0.0975371
2021-05-17T08:43:52.815471+0000     4     255     10492     10237   639.683    657.25    0.166209   0.0977625
2021-05-17T08:43:53.815651+0000     5     255     13144     12889   644.322       663   0.0804605   0.0975013
2021-05-17T08:43:54.815839+0000     6     255     15865     15610   650.289    680.25   0.0971443   0.0968576
2021-05-17T08:43:55.816094+0000     7     255     18535     18280   652.723     667.5    0.156917   0.0966231
2021-05-17T08:43:56.816291+0000     8     255     21284     21029   657.022    687.25   0.0803877   0.0964557
2021-05-17T08:43:57.816487+0000     9     255     24000     23745   659.449       679    0.075986   0.0960642
2021-05-17T08:43:58.816717+0000    10     255     26677     26422   660.414    669.25   0.0796236   0.0960493
2021-05-17T08:43:59.816940+0000    11     255     28728     28473    646.98    512.75    0.120131   0.0977077
2021-05-17T08:44:00.817121+0000    12     255     31069     30814   641.827    585.25   0.0572297   0.0988928
2021-05-17T08:44:01.817309+0000    13     255     33645     33390   641.985       644   0.0740809    0.098981
2021-05-17T08:44:02.817508+0000    14     255     36050     35795   639.066    601.25   0.0698426   0.0995985
2021-05-17T08:44:03.818024+0000    15     255     38238     37983   632.908       547   0.0892719     0.10049
2021-05-17T08:44:04.818323+0000    16     255     40797     40542   633.324    639.75    0.084459    0.100385
2021-05-17T08:44:05.818589+0000    17     255     43456     43201   635.162    664.75   0.0646222     0.10021
2021-05-17T08:44:06.818929+0000    18     255     45837     45582   632.933    595.25   0.0888837    0.100562
2021-05-17T08:44:07.819369+0000    19     255     48186     47931   630.515    587.25    0.153176    0.100566
2021-05-17T08:44:08.819660+0000 min lat: 0.0297769 max lat: 0.369347 avg lat: 0.101058
2021-05-17T08:44:08.819660+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T08:44:08.819660+0000    20     255     50667     50412   629.992    620.25    0.101245    0.101058
2021-05-17T08:44:09.819916+0000 Total time run:         20.0384
Total writes made:      50668
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     632.138
Stddev Bandwidth:       50.1934
Max bandwidth (MB/sec): 687.25
Min bandwidth (MB/sec): 512.75
Average IOPS:           2528
Stddev IOPS:            200.774
Max IOPS:               2749
Min IOPS:               2051
Average Latency(s):     0.100961
Stddev Latency(s):      0.0407859
Max latency(s):         0.369347
Min latency(s):         0.00698422

[1;32mlocalhost.localdomain	[2021-05-17T01:44:10,133427198-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 4175210


[1;33mlocalhost.localdomain	[2021-05-17T01:44:10,144179724-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:44:33,154377119-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 50.67k objects, 12 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:44:33,171890554-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:44:41,235418032-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 50.67k objects, 12 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:44:41,252810022-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:44:49,364359926-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 50.67k objects, 12 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:44:49,382014491-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:44:57,389238515-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 50.67k objects, 12 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:44:57,408969406-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:45:05,389569154-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 50.67k objects, 12 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:45:05,407014807-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:45:05,420653633-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T01:45:05,428700278-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:45:05,447491048-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=4178542
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_256KB_seq.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-17T01:45:05,463002299-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_256KB_seq.log.3
[1;30mlocalhost.localdomain	[2021-05-17T01:45:05,503365600-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:45:05,510050957-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '80e96513-faca-4b2d-8fd2-9d29c6bef184', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.D9J82O:/tmp/ceph-asok.D9J82O', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 80e96513-faca-4b2d-8fd2-9d29c6bef184 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.D9J82O:/tmp/ceph-asok.D9J82O -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T08:45:06.991+0000 ffff91c6f010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T08:45:06.999+0000 ffff91c6f010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T08:45:06.999+0000 ffff91c6f010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T08:45:07.019613+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T08:45:07.019613+0000     0      24        24         0         0         0           -           0
2021-05-17T08:45:08.019822+0000     1     255      3546      3291   822.033    822.75   0.0452243   0.0740741
2021-05-17T08:45:09.020079+0000     2     256      6765      6509   813.166     804.5   0.0362079   0.0764752
2021-05-17T08:45:10.020361+0000     3     255      9989      9734   810.785    806.25    0.127136   0.0776222
2021-05-17T08:45:11.020949+0000     4     256     13364     13108   818.841     843.5   0.0659982   0.0771509
2021-05-17T08:45:12.021192+0000     5     256     17191     16935   846.371    956.75   0.0751966   0.0749212
2021-05-17T08:45:13.021590+0000     6     256     21896     21640    901.27   1176.25    0.032807   0.0702953
2021-05-17T08:45:14.021829+0000     7     256     26629     26373   941.506   1183.25    0.133839   0.0672663
2021-05-17T08:45:15.022470+0000     8     255     30798     30543   954.049    1042.5    0.123228   0.0664445
2021-05-17T08:45:16.022861+0000     9     255     35003     34748   964.803   1051.25   0.0484049   0.0659072
2021-05-17T08:45:17.023117+0000    10     255     39478     39223   980.167   1118.75   0.0506065   0.0648832
2021-05-17T08:45:18.023610+0000    11     255     43897     43642   991.444   1104.75    0.142088   0.0640974
2021-05-17T08:45:19.023839+0000    12     256     48249     47993   999.447   1087.75  0.00344792   0.0634974
2021-05-17T08:45:20.024142+0000 Total time run:       12.6917
Total reads made:     50668
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   998.055
Average IOPS:         3992
Stddev IOPS:          585.138
Max IOPS:             4733
Min IOPS:             3218
Average Latency(s):   0.0636811
Max latency(s):       0.230163
Min latency(s):       0.000912831

[1;32mlocalhost.localdomain	[2021-05-17T01:45:20,357891356-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 4178542


[1;33mlocalhost.localdomain	[2021-05-17T01:45:20,366787837-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:45:43,194826262-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 50.67k objects, 12 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:45:43,212298844-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:45:51,231304496-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 50.67k objects, 12 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:45:51,248771377-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:45:59,285368192-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 50.67k objects, 12 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:45:59,303159205-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:46:07,362708398-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 50.67k objects, 12 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:46:07,381066417-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:46:15,162832486-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 50.67k objects, 12 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:46:15,185154426-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:46:15,203150868-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T01:46:15,209081476-07:00][RUNNING][ROUND 4/4/40] object_size=256KB[0m


[1;33mlocalhost.localdomain	[2021-05-17T01:46:15,218505942-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T01:46:15,237201025-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40353\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.321598\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 779eb93e-c1d0-47aa-847a-c4de06d9f483\nsetting min_mon_release = octopus\nepoch 0\nfsid 779eb93e-c1d0-47aa-847a-c4de06d9f483\nlast_changed 2021-05-17T01:46:43.200312-0700\ncreated 2021-05-17T01:46:43.200312-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40353/0,v1:10.10.1.2:40354/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.321598 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 55f4e260-11e6-429b-a193-8c5d0e23f8f6\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 c9bed962-3845-4b9a-ab8e-fe49d236afb4\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 efad0eb0-dfca-4434-a341-540d4e7e6871\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42353\n  w/ user/pass: admin / c4bc2551-d69e-4bfb-90e2-8d237a9fd569\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 01:46:58 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40353
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.321598
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 779eb93e-c1d0-47aa-847a-c4de06d9f483
setting min_mon_release = octopus
epoch 0
fsid 779eb93e-c1d0-47aa-847a-c4de06d9f483
last_changed 2021-05-17T01:46:43.200312-0700
created 2021-05-17T01:46:43.200312-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40353/0,v1:10.10.1.2:40354/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.321598 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 55f4e260-11e6-429b-a193-8c5d0e23f8f6
0
start osd.0
add osd1 c9bed962-3845-4b9a-ab8e-fe49d236afb4
1
start osd.1
add osd2 efad0eb0-dfca-4434-a341-540d4e7e6871
2
start osd.2


restful urls: https://10.10.1.2:42353
  w/ user/pass: admin / c4bc2551-d69e-4bfb-90e2-8d237a9fd569


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T01:46:16.250-0700 7f1c668e51c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T01:46:16.250-0700 7f1c668e51c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T01:46:16.266-0700 7fb685ea71c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T01:46:16.266-0700 7fb685ea71c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40353,v1:10.10.1.2:40354] --print /tmp/ceph_monmap.321598 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.321598 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.321598 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42353 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.TcYuGjGHkE 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 55f4e260-11e6-429b-a193-8c5d0e23f8f6 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQB8LaJgC83iBRAAILo6v+hOdktrG9PqQw+E3A== --osd-uuid 55f4e260-11e6-429b-a193-8c5d0e23f8f6 
2021-05-17T01:46:52.490-0700 7f4b3362bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T01:46:52.490-0700 7f4b3362bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T01:46:52.490-0700 7f4b3362bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T01:46:52.534-0700 7f4b3362bf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new c9bed962-3845-4b9a-ab8e-fe49d236afb4 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T01:46:52.838-0700 7f93539d0f00 -1 Falling back to public interface
2021-05-17T01:46:52.850-0700 7f93539d0f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQB8LaJg2SUHMhAAsYCsfZvM4VGGEsMsoRgx7g== --osd-uuid c9bed962-3845-4b9a-ab8e-fe49d236afb4 
2021-05-17T01:46:53.182-0700 7fb5fcc85f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T01:46:53.186-0700 7fb5fcc85f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T01:46:53.186-0700 7fb5fcc85f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T01:46:53.230-0700 7fb5fcc85f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new efad0eb0-dfca-4434-a341-540d4e7e6871 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T01:46:53.618-0700 7ff266098f00 -1 Falling back to public interface
2021-05-17T01:46:53.630-0700 7ff266098f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQB9LaJgp9u8JBAAP2l6dKbWIeqH0WWChMALYw== --osd-uuid efad0eb0-dfca-4434-a341-540d4e7e6871 
2021-05-17T01:46:53.946-0700 7fc1c356bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T01:46:53.946-0700 7fc1c356bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T01:46:53.946-0700 7fc1c356bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T01:46:54.010-0700 7fc1c356bf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T01:46:54.294-0700 7f9f01110f00 -1 Falling back to public interface
2021-05-17T01:46:54.310-0700 7f9f01110f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T01:46:58,317901574-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T01:46:58,326191891-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T01:46:58,409099983-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:46:58,415300982-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6792fd11-63ef-4c89-b9bf-f34c4b71d07e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Tfkl9X:/tmp/ceph-asok.Tfkl9X', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6792fd11-63ef-4c89-b9bf-f34c4b71d07e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Tfkl9X:/tmp/ceph-asok.Tfkl9X -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T01:47:01,366464642-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:47:01,372921825-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6792fd11-63ef-4c89-b9bf-f34c4b71d07e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Tfkl9X:/tmp/ceph-asok.Tfkl9X', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6792fd11-63ef-4c89-b9bf-f34c4b71d07e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Tfkl9X:/tmp/ceph-asok.Tfkl9X -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T01:47:04,669285831-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:47:04,676148128-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6792fd11-63ef-4c89-b9bf-f34c4b71d07e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Tfkl9X:/tmp/ceph-asok.Tfkl9X', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6792fd11-63ef-4c89-b9bf-f34c4b71d07e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Tfkl9X:/tmp/ceph-asok.Tfkl9X -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T01:47:07,571474559-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:47:07,578244965-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6792fd11-63ef-4c89-b9bf-f34c4b71d07e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Tfkl9X:/tmp/ceph-asok.Tfkl9X', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6792fd11-63ef-4c89-b9bf-f34c4b71d07e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Tfkl9X:/tmp/ceph-asok.Tfkl9X -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T01:47:13,220477101-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:47:13,226917976-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6792fd11-63ef-4c89-b9bf-f34c4b71d07e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Tfkl9X:/tmp/ceph-asok.Tfkl9X', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6792fd11-63ef-4c89-b9bf-f34c4b71d07e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Tfkl9X:/tmp/ceph-asok.Tfkl9X -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T01:47:17,100022986-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:47:17,106397940-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6792fd11-63ef-4c89-b9bf-f34c4b71d07e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Tfkl9X:/tmp/ceph-asok.Tfkl9X', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6792fd11-63ef-4c89-b9bf-f34c4b71d07e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Tfkl9X:/tmp/ceph-asok.Tfkl9X -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T01:47:20,632969258-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:47:20,639550242-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6792fd11-63ef-4c89-b9bf-f34c4b71d07e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Tfkl9X:/tmp/ceph-asok.Tfkl9X', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6792fd11-63ef-4c89-b9bf-f34c4b71d07e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Tfkl9X:/tmp/ceph-asok.Tfkl9X -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T01:47:23,983651151-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:47:23,990192964-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6792fd11-63ef-4c89-b9bf-f34c4b71d07e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Tfkl9X:/tmp/ceph-asok.Tfkl9X', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6792fd11-63ef-4c89-b9bf-f34c4b71d07e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Tfkl9X:/tmp/ceph-asok.Tfkl9X -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T01:47:29,889190146-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:47:29,895967709-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6792fd11-63ef-4c89-b9bf-f34c4b71d07e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Tfkl9X:/tmp/ceph-asok.Tfkl9X', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6792fd11-63ef-4c89-b9bf-f34c4b71d07e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Tfkl9X:/tmp/ceph-asok.Tfkl9X -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T01:47:33,190696638-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:47:33,197002115-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6792fd11-63ef-4c89-b9bf-f34c4b71d07e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Tfkl9X:/tmp/ceph-asok.Tfkl9X', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6792fd11-63ef-4c89-b9bf-f34c4b71d07e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Tfkl9X:/tmp/ceph-asok.Tfkl9X -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T01:47:36,701765671-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:47:36,708142545-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6792fd11-63ef-4c89-b9bf-f34c4b71d07e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Tfkl9X:/tmp/ceph-asok.Tfkl9X', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6792fd11-63ef-4c89-b9bf-f34c4b71d07e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Tfkl9X:/tmp/ceph-asok.Tfkl9X -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T01:47:39,729039487-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:47:39,735430584-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6792fd11-63ef-4c89-b9bf-f34c4b71d07e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Tfkl9X:/tmp/ceph-asok.Tfkl9X', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6792fd11-63ef-4c89-b9bf-f34c4b71d07e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Tfkl9X:/tmp/ceph-asok.Tfkl9X -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   47 KiB   0 B   0 B   0 B  100 GiB     0  0.95   70      up          osd.2  
                       TOTAL  300 GiB  151 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.95/1.03  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T01:47:42,552081021-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:48:05,562354820-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (4s)
      [===================.........] (remaining: 2s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:48:13,813386702-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:48:21,792388021-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:48:29,737476931-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:48:37,875036139-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:48:45,861891600-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:48:53,878431371-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:48:53,896074487-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:49:01,774174711-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:49:01,794158378-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:49:09,864814654-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:49:09,882821822-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:49:17,918521393-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:49:17,936468671-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:49:25,865546738-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:49:25,883465855-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:49:25,897718390-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T01:49:25,905865627-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:49:25,925124849-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=4191536
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_256KB_write.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-17T01:49:25,941087737-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_256KB_write.log.4
[1;30mlocalhost.localdomain	[2021-05-17T01:49:25,982226127-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:49:25,988659245-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6792fd11-63ef-4c89-b9bf-f34c4b71d07e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Tfkl9X:/tmp/ceph-asok.Tfkl9X', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6792fd11-63ef-4c89-b9bf-f34c4b71d07e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Tfkl9X:/tmp/ceph-asok.Tfkl9X -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T08:49:27.496+0000 ffffa89e7010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T08:49:27.504+0000 ffffa89e7010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T08:49:27.504+0000 ffffa89e7010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T08:49:27.520699+0000 Maintaining 256 concurrent writes of 262144 bytes to objects of size 262144 for up to 20 seconds or 0 objects
2021-05-17T08:49:27.520758+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T08:49:27.571984+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T08:49:27.571984+0000     0       0         0         0         0         0           -           0
2021-05-17T08:49:28.572187+0000     1     255      3141      2886   721.459     721.5   0.0597268   0.0823207
2021-05-17T08:49:29.572395+0000     2     255      6248      5993   749.026    776.75    0.123905   0.0823072
2021-05-17T08:49:30.572637+0000     3     255      9333      9078   756.372    771.25    0.127111   0.0826016
2021-05-17T08:49:31.572871+0000     4     255     12512     12257   765.921    794.75   0.0811427   0.0821574
2021-05-17T08:49:32.573374+0000     5     255     15708     15453   772.458       799   0.0470959   0.0817311
2021-05-17T08:49:33.573861+0000     6     255     19000     18745   780.817       823    0.070714   0.0810713
2021-05-17T08:49:34.574093+0000     7     255     22208     21953   783.816       802   0.0811435   0.0805296
2021-05-17T08:49:35.574348+0000     8     255     24968     24713   772.067       690   0.0875636   0.0819956
2021-05-17T08:49:36.574627+0000     9     255     28066     27811   772.314     774.5   0.0709346   0.0821427
2021-05-17T08:49:37.574855+0000    10     255     31230     30975   774.164       791   0.0444469   0.0821718
2021-05-17T08:49:38.575193+0000    11     255     34385     34130   775.466    788.75   0.0513169   0.0820294
2021-05-17T08:49:39.575554+0000    12     255     37530     37275   776.341    786.25   0.0929669   0.0819554
2021-05-17T08:49:40.575773+0000    13     255     40709     40454   777.744    794.75    0.128088   0.0817824
2021-05-17T08:49:41.576080+0000    14     255     43896     43641   779.084    796.75   0.0747762   0.0816789
2021-05-17T08:49:42.576334+0000    15     255     46781     46526   775.216    721.25    0.103366   0.0821871
2021-05-17T08:49:43.576559+0000    16     255     49416     49161   767.928    658.75    0.124056   0.0828837
2021-05-17T08:49:44.576767+0000    17     255     52031     51776   761.204    653.75    0.139797   0.0836731
2021-05-17T08:49:45.576982+0000    18     255     54746     54491   756.616    678.75    0.145857   0.0841997
2021-05-17T08:49:46.577199+0000    19     255     57263     57008   749.905    629.25   0.0564157   0.0850361
2021-05-17T08:49:47.577402+0000 min lat: 0.0261915 max lat: 0.237076 avg lat: 0.0857912
2021-05-17T08:49:47.577402+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T08:49:47.577402+0000    20     255     59723     59468   743.154       615   0.0532917   0.0857912
2021-05-17T08:49:48.577695+0000 Total time run:         20.0328
Total writes made:      59724
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     745.327
Stddev Bandwidth:       65.9808
Max bandwidth (MB/sec): 823
Min bandwidth (MB/sec): 615
Average IOPS:           2981
Stddev IOPS:            263.923
Max IOPS:               3292
Min IOPS:               2460
Average Latency(s):     0.085708
Stddev Latency(s):      0.0311814
Max latency(s):         0.237076
Min latency(s):         0.0261915

[1;32mlocalhost.localdomain	[2021-05-17T01:49:48,884741523-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 4191536


[1;33mlocalhost.localdomain	[2021-05-17T01:49:48,893555955-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:50:11,900402556-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 59.73k objects, 15 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:50:11,918499229-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:50:19,791246459-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 59.73k objects, 15 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:50:19,811363993-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:50:27,675649230-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 59.73k objects, 15 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:50:27,693922859-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:50:35,818048824-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 59.73k objects, 15 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:50:35,836054581-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:50:44,055604795-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 59.73k objects, 15 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:50:44,073853924-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:50:44,088209871-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T01:50:44,096666161-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:50:44,116233551-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=890
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_256KB_seq.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-17T01:50:44,132187767-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_256KB_seq.log.4
[1;30mlocalhost.localdomain	[2021-05-17T01:50:44,172281436-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:50:44,178660485-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '6792fd11-63ef-4c89-b9bf-f34c4b71d07e', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Tfkl9X:/tmp/ceph-asok.Tfkl9X', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 6792fd11-63ef-4c89-b9bf-f34c4b71d07e --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Tfkl9X:/tmp/ceph-asok.Tfkl9X -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T08:50:45.677+0000 ffff8a379010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T08:50:45.685+0000 ffff8a379010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T08:50:45.685+0000 ffff8a379010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T08:50:45.700875+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T08:50:45.700875+0000     0       0         0         0         0         0           -           0
2021-05-17T08:50:46.701061+0000     1     256      3521      3265    815.99    816.25  0.00156219    0.073129
2021-05-17T08:50:47.701322+0000     2     256      6738      6482   810.015    804.25    0.139889   0.0762734
2021-05-17T08:50:48.701528+0000     3     255      9985      9730   810.621       812    0.122581   0.0771079
2021-05-17T08:50:49.701774+0000     4     255     13333     13078   817.164       837  0.00365575   0.0760798
2021-05-17T08:50:50.701963+0000     5     255     16475     16220   810.802     785.5  0.00888268   0.0771361
2021-05-17T08:50:51.702239+0000     6     255     19926     19671   819.421    862.75    0.104283   0.0773869
2021-05-17T08:50:52.702502+0000     7     255     24794     24539   876.173      1217    0.123502   0.0723268
2021-05-17T08:50:53.702789+0000     8     256     29208     28952   904.519   1103.25    0.129496   0.0700358
2021-05-17T08:50:54.703208+0000     9     255     33354     33099   919.165   1036.75    0.177885   0.0690138
2021-05-17T08:50:55.703455+0000    10     256     37991     37735   943.119      1159  0.00429672   0.0673032
2021-05-17T08:50:56.703747+0000    11     255     42490     42235   959.624      1125  0.00772642   0.0660955
2021-05-17T08:50:57.703988+0000    12     255     46804     46549   969.509    1078.5    0.108207   0.0654987
2021-05-17T08:50:58.704315+0000    13     255     51187     50932   979.192   1095.75  0.00535268   0.0650206
2021-05-17T08:50:59.704663+0000    14     255     55746     55491   990.633   1139.75   0.0830439   0.0642848
2021-05-17T08:51:00.704925+0000 Total time run:       14.9498
Total reads made:     59724
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   998.74
Average IOPS:         3994
Stddev IOPS:          639.762
Max IOPS:             4868
Min IOPS:             3142
Average Latency(s):   0.0637822
Max latency(s):       0.248052
Min latency(s):       0.000915607

[1;32mlocalhost.localdomain	[2021-05-17T01:51:01,032202827-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 890


[1;33mlocalhost.localdomain	[2021-05-17T01:51:01,042360214-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:51:23,989370900-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 59.73k objects, 15 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:51:24,007648774-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:51:32,050181950-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 59.73k objects, 15 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:51:32,068388202-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:51:40,244406704-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 59.73k objects, 15 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:51:40,262710412-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:51:48,269084403-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 59.73k objects, 15 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:51:48,289825729-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:51:56,315091950-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 59.73k objects, 15 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:51:56,333021509-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:51:56,347732626-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T01:51:56,353214791-07:00][RUNNING][ROUND 5/4/40] object_size=256KB[0m


[1;33mlocalhost.localdomain	[2021-05-17T01:51:56,361560122-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T01:51:56,378991040-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40895\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.322705\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid a0220386-c8a6-4891-8105-090e84ee01db\nsetting min_mon_release = octopus\nepoch 0\nfsid a0220386-c8a6-4891-8105-090e84ee01db\nlast_changed 2021-05-17T01:52:25.351982-0700\ncreated 2021-05-17T01:52:25.351982-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40895/0,v1:10.10.1.2:40896/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.322705 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 8d89be1f-40d3-4d01-98f9-53b0220936e4\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 7f90b9f6-2155-49d9-92b9-88660a20e4fb\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 d8eae0e9-689c-4b13-b4d9-28c8f0c7b599\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42895\n  w/ user/pass: admin / ef1e9da4-29d5-49ef-92c2-5c007ff5d8bc\n'
10.10.1.2: b'\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 01:52:40 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40895
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.322705
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid a0220386-c8a6-4891-8105-090e84ee01db
setting min_mon_release = octopus
epoch 0
fsid a0220386-c8a6-4891-8105-090e84ee01db
last_changed 2021-05-17T01:52:25.351982-0700
created 2021-05-17T01:52:25.351982-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40895/0,v1:10.10.1.2:40896/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.322705 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 8d89be1f-40d3-4d01-98f9-53b0220936e4
0
start osd.0
add osd1 7f90b9f6-2155-49d9-92b9-88660a20e4fb
1
start osd.1
add osd2 d8eae0e9-689c-4b13-b4d9-28c8f0c7b599
2
start osd.2


restful urls: https://10.10.1.2:42895
  w/ user/pass: admin / ef1e9da4-29d5-49ef-92c2-5c007ff5d8bc


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T01:51:57.381-0700 7ff3ae7361c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T01:51:57.381-0700 7ff3ae7361c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T01:51:57.397-0700 7f558d09d1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T01:51:57.397-0700 7f558d09d1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40895,v1:10.10.1.2:40896] --print /tmp/ceph_monmap.322705 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.322705 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.322705 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42895 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.7GKCjxbrUI 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 8d89be1f-40d3-4d01-98f9-53b0220936e4 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDSLqJg/jJOBxAAOwubp6bkozuOy8DkomOJRA== --osd-uuid 8d89be1f-40d3-4d01-98f9-53b0220936e4 
2021-05-17T01:52:34.449-0700 7f351b0dbf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T01:52:34.449-0700 7f351b0dbf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T01:52:34.449-0700 7f351b0dbf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T01:52:34.517-0700 7f351b0dbf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 7f90b9f6-2155-49d9-92b9-88660a20e4fb -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T01:52:34.829-0700 7f09e7181f00 -1 Falling back to public interface
2021-05-17T01:52:34.841-0700 7f09e7181f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDSLqJgwEx6MRAAWVFi4b0jBd2OaqJgqVUl+Q== --osd-uuid 7f90b9f6-2155-49d9-92b9-88660a20e4fb 
2021-05-17T01:52:35.193-0700 7f9d5f064f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T01:52:35.193-0700 7f9d5f064f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T01:52:35.193-0700 7f9d5f064f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T01:52:35.245-0700 7f9d5f064f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new d8eae0e9-689c-4b13-b4d9-28c8f0c7b599 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T01:52:35.597-0700 7fdab0900f00 -1 Falling back to public interface
2021-05-17T01:52:35.613-0700 7fdab0900f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDTLqJgMoPSIxAA5bNaTEtI5Bc+rWIfKjxTHg== --osd-uuid d8eae0e9-689c-4b13-b4d9-28c8f0c7b599 
2021-05-17T01:52:35.929-0700 7f6efdc47f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T01:52:35.929-0700 7f6efdc47f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T01:52:35.929-0700 7f6efdc47f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T01:52:35.993-0700 7f6efdc47f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T01:52:36.369-0700 7ff987f71f00 -1 Falling back to public interface
2021-05-17T01:52:36.381-0700 7ff987f71f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T01:52:40,285647135-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T01:52:40,294479104-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T01:52:40,396186235-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:52:40,402614748-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '67b2f8bf-17b0-4247-8609-54bbdf933bcf', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Iq7sxX:/tmp/ceph-asok.Iq7sxX', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 67b2f8bf-17b0-4247-8609-54bbdf933bcf --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Iq7sxX:/tmp/ceph-asok.Iq7sxX -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T01:52:43,221087371-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:52:43,227655002-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '67b2f8bf-17b0-4247-8609-54bbdf933bcf', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Iq7sxX:/tmp/ceph-asok.Iq7sxX', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 67b2f8bf-17b0-4247-8609-54bbdf933bcf --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Iq7sxX:/tmp/ceph-asok.Iq7sxX -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T01:52:46,058427707-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:52:46,065206889-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '67b2f8bf-17b0-4247-8609-54bbdf933bcf', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Iq7sxX:/tmp/ceph-asok.Iq7sxX', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 67b2f8bf-17b0-4247-8609-54bbdf933bcf --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Iq7sxX:/tmp/ceph-asok.Iq7sxX -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T01:52:49,072109068-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:52:49,078759963-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '67b2f8bf-17b0-4247-8609-54bbdf933bcf', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Iq7sxX:/tmp/ceph-asok.Iq7sxX', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 67b2f8bf-17b0-4247-8609-54bbdf933bcf --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Iq7sxX:/tmp/ceph-asok.Iq7sxX -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T01:52:54,539683741-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:52:54,546277027-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '67b2f8bf-17b0-4247-8609-54bbdf933bcf', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Iq7sxX:/tmp/ceph-asok.Iq7sxX', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 67b2f8bf-17b0-4247-8609-54bbdf933bcf --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Iq7sxX:/tmp/ceph-asok.Iq7sxX -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T01:52:58,464740558-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:52:58,471266887-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '67b2f8bf-17b0-4247-8609-54bbdf933bcf', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Iq7sxX:/tmp/ceph-asok.Iq7sxX', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 67b2f8bf-17b0-4247-8609-54bbdf933bcf --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Iq7sxX:/tmp/ceph-asok.Iq7sxX -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T01:53:01,637962757-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:53:01,644404982-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '67b2f8bf-17b0-4247-8609-54bbdf933bcf', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Iq7sxX:/tmp/ceph-asok.Iq7sxX', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 67b2f8bf-17b0-4247-8609-54bbdf933bcf --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Iq7sxX:/tmp/ceph-asok.Iq7sxX -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T01:53:04,897761032-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:53:04,904327852-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '67b2f8bf-17b0-4247-8609-54bbdf933bcf', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Iq7sxX:/tmp/ceph-asok.Iq7sxX', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 67b2f8bf-17b0-4247-8609-54bbdf933bcf --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Iq7sxX:/tmp/ceph-asok.Iq7sxX -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T01:53:08,217538326-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:53:08,224122955-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '67b2f8bf-17b0-4247-8609-54bbdf933bcf', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Iq7sxX:/tmp/ceph-asok.Iq7sxX', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 67b2f8bf-17b0-4247-8609-54bbdf933bcf --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Iq7sxX:/tmp/ceph-asok.Iq7sxX -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T01:53:11,499859912-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:53:11,506296345-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '67b2f8bf-17b0-4247-8609-54bbdf933bcf', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Iq7sxX:/tmp/ceph-asok.Iq7sxX', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 67b2f8bf-17b0-4247-8609-54bbdf933bcf --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Iq7sxX:/tmp/ceph-asok.Iq7sxX -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T01:53:14,806489428-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:53:14,812936739-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '67b2f8bf-17b0-4247-8609-54bbdf933bcf', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Iq7sxX:/tmp/ceph-asok.Iq7sxX', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 67b2f8bf-17b0-4247-8609-54bbdf933bcf --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Iq7sxX:/tmp/ceph-asok.Iq7sxX -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T01:53:17,831359675-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:53:17,838204404-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '67b2f8bf-17b0-4247-8609-54bbdf933bcf', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Iq7sxX:/tmp/ceph-asok.Iq7sxX', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 67b2f8bf-17b0-4247-8609-54bbdf933bcf --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Iq7sxX:/tmp/ceph-asok.Iq7sxX -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   47 KiB   0 B   0 B   0 B  100 GiB     0  0.95   70      up          osd.2  
                       TOTAL  300 GiB  151 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.95/1.03  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T01:53:20,438835086-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:53:43,416690288-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   198 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:53:51,714393259-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:53:59,714487741-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (9s)
      [=======.....................] (remaining: 27s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:54:07,590783780-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [========....................] (remaining: 49s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:54:15,786296023-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [========....................] (remaining: 56s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:54:23,784108386-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [========....................] (remaining: 77s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:54:31,776566650-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (45s)
      [=========...................] (remaining: 92s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:54:39,803469067-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [=========...................] (remaining: 102s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:54:47,828798920-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:54:47,846963404-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:54:55,954628244-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:54:55,972515150-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:55:03,861557526-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:55:03,881589215-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:55:11,717771805-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:55:11,735647384-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:55:20,090612094-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:55:20,108664734-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:55:20,123041308-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T01:55:20,131578116-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:55:20,151100603-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=14705
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_256KB_write.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-17T01:55:20,167246865-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_256KB_write.log.5
[1;30mlocalhost.localdomain	[2021-05-17T01:55:20,208634340-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:55:20,215219568-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '67b2f8bf-17b0-4247-8609-54bbdf933bcf', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Iq7sxX:/tmp/ceph-asok.Iq7sxX', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '262144', '-O', '262144', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 67b2f8bf-17b0-4247-8609-54bbdf933bcf --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Iq7sxX:/tmp/ceph-asok.Iq7sxX -- rados bench 20 write --pool bench_rados -b 262144 -O 262144 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T08:55:21.756+0000 ffff86c3f010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T08:55:21.764+0000 ffff86c3f010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T08:55:21.764+0000 ffff86c3f010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T08:55:21.779896+0000 Maintaining 256 concurrent writes of 262144 bytes to objects of size 262144 for up to 20 seconds or 0 objects
2021-05-17T08:55:21.779952+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T08:55:21.830390+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T08:55:21.830390+0000     0       0         0         0         0         0           -           0
2021-05-17T08:55:22.830623+0000     1     255      2868      2613    653.19    653.25   0.0774278   0.0905627
2021-05-17T08:55:23.831053+0000     2     255      5865      5610   701.067    749.25   0.0842842   0.0882749
2021-05-17T08:55:24.831308+0000     3     255      8734      8479     706.4    717.25    0.115093    0.088132
2021-05-17T08:55:25.831650+0000     4     255     11649     11394   711.926    728.75    0.130204   0.0885333
2021-05-17T08:55:26.832039+0000     5     255     14582     14327   716.134    733.25    0.069464   0.0880809
2021-05-17T08:55:27.832520+0000     6     255     17572     17317   721.303     747.5   0.0829229   0.0877995
2021-05-17T08:55:28.832814+0000     7     255     20406     20151   719.444     708.5    0.146307   0.0879211
2021-05-17T08:55:29.833277+0000     8     255     23334     23079   720.971       732     0.07019   0.0879605
2021-05-17T08:55:30.833564+0000     9     255     25798     25543   709.289       616   0.0820495   0.0894666
2021-05-17T08:55:31.833854+0000    10     255     28114     27859   696.244       579    0.107104   0.0908287
2021-05-17T08:55:32.834042+0000    11     255     30947     30692   697.323    708.25   0.0848762   0.0910238
2021-05-17T08:55:33.834337+0000    12     255     33488     33233   692.135    635.25   0.0959135   0.0917751
2021-05-17T08:55:34.834582+0000    13     255     35795     35540   683.249    576.75   0.0658958   0.0930687
2021-05-17T08:55:35.834809+0000    14     255     37687     37432   668.224       473     0.11272   0.0949566
2021-05-17T08:55:36.835132+0000    15     255     39853     39598   659.764     541.5    0.124168   0.0964371
2021-05-17T08:55:37.835353+0000    16     255     41793     41538   648.836       485    0.106542    0.098174
2021-05-17T08:55:38.835574+0000    17     255     44339     44084   648.102     636.5    0.101342   0.0982483
2021-05-17T08:55:39.835856+0000    18     255     46666     46411   644.407    581.75   0.0614619    0.098869
2021-05-17T08:55:40.836222+0000    19     255     48935     48680   640.335    567.25    0.178957   0.0994329
2021-05-17T08:55:41.836486+0000 min lat: 0.0277532 max lat: 0.361323 avg lat: 0.10008
2021-05-17T08:55:41.836486+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T08:55:41.836486+0000    20     255     51228     50973   636.973    573.25   0.0658467     0.10008
2021-05-17T08:55:42.836758+0000 Total time run:         20.06
Total writes made:      51229
Write size:             262144
Object size:            262144
Bandwidth (MB/sec):     638.446
Stddev Bandwidth:       87.9444
Max bandwidth (MB/sec): 749.25
Min bandwidth (MB/sec): 473
Average IOPS:           2553
Stddev IOPS:            351.778
Max IOPS:               2997
Min IOPS:               1892
Average Latency(s):     0.100037
Stddev Latency(s):      0.0383589
Max latency(s):         0.361323
Min latency(s):         0.0277532

[1;32mlocalhost.localdomain	[2021-05-17T01:55:43,185382130-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 14705


[1;33mlocalhost.localdomain	[2021-05-17T01:55:43,194367599-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:56:06,142941631-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 51.23k objects, 13 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:56:06,161484617-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:56:14,327814014-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 51.23k objects, 13 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:56:14,346387351-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:56:22,376087241-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 51.23k objects, 13 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:56:22,395559051-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:56:30,332882874-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 51.23k objects, 13 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:56:30,351227514-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:56:40,373491427-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 51.23k objects, 13 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:56:40,392048549-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:56:40,406794320-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T01:56:40,415135957-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:56:40,434689097-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=18110
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_256KB_seq.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-17T01:56:40,450951219-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_256KB_seq.log.5
[1;30mlocalhost.localdomain	[2021-05-17T01:56:40,491193536-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:56:40,497632581-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '67b2f8bf-17b0-4247-8609-54bbdf933bcf', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Iq7sxX:/tmp/ceph-asok.Iq7sxX', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 67b2f8bf-17b0-4247-8609-54bbdf933bcf --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Iq7sxX:/tmp/ceph-asok.Iq7sxX -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T08:56:41.986+0000 ffff88f34010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T08:56:41.994+0000 ffff88f34010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T08:56:41.994+0000 ffff88f34010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T08:56:42.014397+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T08:56:42.014397+0000     0       0         0         0         0         0           -           0
2021-05-17T08:56:43.014601+0000     1     256      3422      3166   791.204     791.5  0.00159354   0.0725481
2021-05-17T08:56:44.014801+0000     2     255      6691      6436   804.269     817.5  0.00560126   0.0747467
2021-05-17T08:56:45.015104+0000     3     255     10054      9799   816.345    840.75  0.00132691   0.0768477
2021-05-17T08:56:46.015291+0000     4     256     13213     12957   809.597     789.5    0.244687   0.0766132
2021-05-17T08:56:47.015504+0000     5     256     16276     16020   800.796    765.75   0.0549796   0.0785131
2021-05-17T08:56:48.016618+0000     6     255     19601     19346   805.762     831.5  0.00128366   0.0785749
2021-05-17T08:56:49.016918+0000     7     255     22795     22540   804.691     798.5  0.00287396   0.0784705
2021-05-17T08:56:50.017189+0000     8     255     25938     25683   802.297    785.75   0.0645549    0.079019
2021-05-17T08:56:51.017800+0000     9     256     29246     28990   804.958    826.75    0.190505   0.0786582
2021-05-17T08:56:52.017981+0000    10     255     33274     33019   825.165   1007.25  0.00342252   0.0771728
2021-05-17T08:56:53.018207+0000    11     255     37918     37663   855.668      1161   0.0758423   0.0744158
2021-05-17T08:56:54.018649+0000    12     256     41882     41626   866.889    990.75   0.0611013   0.0734474
2021-05-17T08:56:55.019020+0000    13     255     46058     45803   880.502   1044.25   0.0942788   0.0723697
2021-05-17T08:56:56.019246+0000    14     256     50224     49968   891.966   1041.25   0.0768693   0.0714103
2021-05-17T08:56:57.019492+0000 Total time run:       14.37
Total reads made:     51229
Read size:            262144
Object size:          262144
Bandwidth (MB/sec):   891.247
Average IOPS:         3564
Stddev IOPS:          512.636
Max IOPS:             4644
Min IOPS:             3063
Average Latency(s):   0.0714904
Max latency(s):       0.25652
Min latency(s):       0.000976831

[1;32mlocalhost.localdomain	[2021-05-17T01:56:57,318181501-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 18110


[1;33mlocalhost.localdomain	[2021-05-17T01:56:57,327562845-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:57:20,284850060-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 51.23k objects, 13 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:57:20,303534855-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:57:28,703580521-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 51.23k objects, 13 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:57:28,722256016-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:57:36,763419207-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 51.23k objects, 13 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:57:36,782168769-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:57:44,788835633-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 51.23k objects, 13 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:57:44,807785945-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:57:53,061055571-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 51.23k objects, 13 GiB
    usage:   25 GiB used, 275 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:57:53,079573420-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T01:57:53,094727069-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T01:57:53,103923628-07:00][RUNNING][ROUND 1/5/40] object_size=1MB[0m


[1;33mlocalhost.localdomain	[2021-05-17T01:57:53,112590972-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T01:57:53,130374522-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40277\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.323814\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 74b04913-c8e7-440a-93bc-e3968213fd98\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 74b04913-c8e7-440a-93bc-e3968213fd98\nlast_changed 2021-05-17T01:58:26.352586-0700\ncreated 2021-05-17T01:58:26.352586-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40277/0,v1:10.10.1.2:40278/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.323814 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 51011272-cd22-4ba7-bc16-75111af672fe\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 1103b2ba-b6f8-4ff6-9a56-f2d6b88a5897\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 4b92639b-2e59-4645-8df5-70e1596dccd1\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42277\n  w/ user/pass: admin / 324cb329-35fa-47b6-bc1c-c8b0f1963bf0\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 01:58:41 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40277
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.323814
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 74b04913-c8e7-440a-93bc-e3968213fd98
setting min_mon_release = octopus
epoch 0
fsid 74b04913-c8e7-440a-93bc-e3968213fd98
last_changed 2021-05-17T01:58:26.352586-0700
created 2021-05-17T01:58:26.352586-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40277/0,v1:10.10.1.2:40278/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.323814 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 51011272-cd22-4ba7-bc16-75111af672fe
0
start osd.0
add osd1 1103b2ba-b6f8-4ff6-9a56-f2d6b88a5897
1
start osd.1
add osd2 4b92639b-2e59-4645-8df5-70e1596dccd1
2
start osd.2


restful urls: https://10.10.1.2:42277
  w/ user/pass: admin / 324cb329-35fa-47b6-bc1c-c8b0f1963bf0


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T01:57:54.116-0700 7f97a52d41c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T01:57:54.116-0700 7f97a52d41c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T01:57:54.132-0700 7f1574cf91c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T01:57:54.132-0700 7f1574cf91c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40277,v1:10.10.1.2:40278] --print /tmp/ceph_monmap.323814 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.323814 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.323814 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42277 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.fdQ2qxabqe 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 51011272-cd22-4ba7-bc16-75111af672fe -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQA7MKJghOBtCxAAMuSQNP9djh3qlYfET4zUbg== --osd-uuid 51011272-cd22-4ba7-bc16-75111af672fe 
2021-05-17T01:58:35.533-0700 7fe681b2ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T01:58:35.533-0700 7fe681b2ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T01:58:35.533-0700 7fe681b2ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T01:58:35.585-0700 7fe681b2ff00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 1103b2ba-b6f8-4ff6-9a56-f2d6b88a5897 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T01:58:35.865-0700 7f41c2570f00 -1 Falling back to public interface
2021-05-17T01:58:35.877-0700 7f41c2570f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQA7MKJgA0OgMxAAr/+46ddIZdlvNOL65adDfA== --osd-uuid 1103b2ba-b6f8-4ff6-9a56-f2d6b88a5897 
2021-05-17T01:58:36.209-0700 7fcd1b578f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T01:58:36.209-0700 7fcd1b578f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T01:58:36.209-0700 7fcd1b578f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T01:58:36.257-0700 7fcd1b578f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 4b92639b-2e59-4645-8df5-70e1596dccd1 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T01:58:36.709-0700 7fa59ad90f00 -1 Falling back to public interface
2021-05-17T01:58:36.725-0700 7fa59ad90f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQA8MKJgmt9dKhAAkuAxSLtR2eta1AbciU7LpQ== --osd-uuid 4b92639b-2e59-4645-8df5-70e1596dccd1 
2021-05-17T01:58:37.057-0700 7f4e513baf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T01:58:37.057-0700 7f4e513baf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T01:58:37.057-0700 7f4e513baf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T01:58:37.125-0700 7f4e513baf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T01:58:37.421-0700 7fd02b23ef00 -1 Falling back to public interface
2021-05-17T01:58:37.437-0700 7fd02b23ef00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T01:58:41,474264545-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T01:58:41,483157214-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T01:58:41,566104306-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:58:41,572856712-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '29436fd5-d52a-4d1a-a7ba-801ba7d3d31f', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LNsjng:/tmp/ceph-asok.LNsjng', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 29436fd5-d52a-4d1a-a7ba-801ba7d3d31f --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LNsjng:/tmp/ceph-asok.LNsjng -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T01:58:44,450693742-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:58:44,457355712-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '29436fd5-d52a-4d1a-a7ba-801ba7d3d31f', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LNsjng:/tmp/ceph-asok.LNsjng', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 29436fd5-d52a-4d1a-a7ba-801ba7d3d31f --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LNsjng:/tmp/ceph-asok.LNsjng -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T01:58:47,246495136-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:58:47,253034610-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '29436fd5-d52a-4d1a-a7ba-801ba7d3d31f', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LNsjng:/tmp/ceph-asok.LNsjng', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 29436fd5-d52a-4d1a-a7ba-801ba7d3d31f --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LNsjng:/tmp/ceph-asok.LNsjng -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T01:58:50,156575697-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:58:50,162708632-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '29436fd5-d52a-4d1a-a7ba-801ba7d3d31f', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LNsjng:/tmp/ceph-asok.LNsjng', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 29436fd5-d52a-4d1a-a7ba-801ba7d3d31f --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LNsjng:/tmp/ceph-asok.LNsjng -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T01:58:55,868062977-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:58:55,874581973-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '29436fd5-d52a-4d1a-a7ba-801ba7d3d31f', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LNsjng:/tmp/ceph-asok.LNsjng', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 29436fd5-d52a-4d1a-a7ba-801ba7d3d31f --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LNsjng:/tmp/ceph-asok.LNsjng -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T01:58:58,787059140-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:58:58,793566779-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '29436fd5-d52a-4d1a-a7ba-801ba7d3d31f', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LNsjng:/tmp/ceph-asok.LNsjng', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 29436fd5-d52a-4d1a-a7ba-801ba7d3d31f --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LNsjng:/tmp/ceph-asok.LNsjng -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T01:59:02,224817039-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:59:02,231603261-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '29436fd5-d52a-4d1a-a7ba-801ba7d3d31f', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LNsjng:/tmp/ceph-asok.LNsjng', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 29436fd5-d52a-4d1a-a7ba-801ba7d3d31f --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LNsjng:/tmp/ceph-asok.LNsjng -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T01:59:05,862855887-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:59:05,869315398-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '29436fd5-d52a-4d1a-a7ba-801ba7d3d31f', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LNsjng:/tmp/ceph-asok.LNsjng', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 29436fd5-d52a-4d1a-a7ba-801ba7d3d31f --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LNsjng:/tmp/ceph-asok.LNsjng -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T01:59:09,249306763-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:59:09,256035691-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '29436fd5-d52a-4d1a-a7ba-801ba7d3d31f', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LNsjng:/tmp/ceph-asok.LNsjng', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 29436fd5-d52a-4d1a-a7ba-801ba7d3d31f --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LNsjng:/tmp/ceph-asok.LNsjng -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T01:59:12,579799402-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:59:12,586510281-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '29436fd5-d52a-4d1a-a7ba-801ba7d3d31f', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LNsjng:/tmp/ceph-asok.LNsjng', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 29436fd5-d52a-4d1a-a7ba-801ba7d3d31f --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LNsjng:/tmp/ceph-asok.LNsjng -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T01:59:15,748626186-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:59:15,754843347-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '29436fd5-d52a-4d1a-a7ba-801ba7d3d31f', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LNsjng:/tmp/ceph-asok.LNsjng', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 29436fd5-d52a-4d1a-a7ba-801ba7d3d31f --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LNsjng:/tmp/ceph-asok.LNsjng -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T01:59:18,471052504-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T01:59:18,477624760-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '29436fd5-d52a-4d1a-a7ba-801ba7d3d31f', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LNsjng:/tmp/ceph-asok.LNsjng', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 29436fd5-d52a-4d1a-a7ba-801ba7d3d31f --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LNsjng:/tmp/ceph-asok.LNsjng -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T01:59:21,110853124-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T01:59:44,213419648-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   203 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T01:59:52,190255518-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:00:00,366609127-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=======.....................] (remaining: 26s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:00:08,294348116-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [========....................] (remaining: 46s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:00:16,448018724-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [=========...................] (remaining: 52s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:00:24,368029957-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [=========...................] (remaining: 73s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:00:32,376402224-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (40s)
      [=========...................] (remaining: 83s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:00:40,639450387-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [==============..............] (remaining: 44s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:00:48,770851880-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:00:48,790990763-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:00:56,763439046-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:00:56,781265016-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:01:04,916678819-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:01:04,934449610-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:01:12,885940517-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:01:12,903743789-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:01:21,014016815-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:01:21,031763918-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:01:21,046667362-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T02:01:21,055243240-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:01:21,074808712-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=32013
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_write.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-17T02:01:21,090666829-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_write.log.1
[1;30mlocalhost.localdomain	[2021-05-17T02:01:21,132097544-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:01:21,138628649-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '29436fd5-d52a-4d1a-a7ba-801ba7d3d31f', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LNsjng:/tmp/ceph-asok.LNsjng', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 29436fd5-d52a-4d1a-a7ba-801ba7d3d31f --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LNsjng:/tmp/ceph-asok.LNsjng -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T09:01:22.653+0000 ffffb6b7c010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:01:22.661+0000 ffffb6b7c010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:01:22.661+0000 ffffb6b7c010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T09:01:22.679139+0000 Maintaining 256 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 20 seconds or 0 objects
2021-05-17T09:01:22.679214+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T09:01:22.901127+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T09:01:22.901127+0000     0       0         0         0         0         0           -           0
2021-05-17T09:01:23.901346+0000     1     255       808       553   552.958       553    0.302203    0.317265
2021-05-17T09:01:24.901596+0000     2     255      1623      1368   683.889       815    0.325815    0.314275
2021-05-17T09:01:25.901810+0000     3     255      2458      2203   734.201       835    0.316198    0.312224
2021-05-17T09:01:26.902030+0000     4     255      3251      2996   748.858       793    0.303259    0.315793
2021-05-17T09:01:27.902275+0000     5     255      4058      3803   760.447       807    0.324488     0.31582
2021-05-17T09:01:28.902477+0000     6     255      4862      4607   767.679       804    0.310271    0.316193
2021-05-17T09:01:29.902694+0000     7     255      5653      5398   770.986       791    0.310302    0.316922
2021-05-17T09:01:30.902905+0000     8     255      6489      6234   779.091       836    0.322236    0.315648
2021-05-17T09:01:31.903110+0000     9     255      7228      6973    774.62       739    0.375663    0.317678
2021-05-17T09:01:32.903324+0000    10     255      7943      7688   768.642       715    0.398862    0.320817
2021-05-17T09:01:33.903524+0000    11     255      8664      8409   764.298       721    0.383109     0.32458
2021-05-17T09:01:34.903723+0000    12     255      9433      9178   764.677       769    0.313108    0.325963
2021-05-17T09:01:35.903941+0000    13     255     10211      9956   765.689       778    0.325777    0.326057
2021-05-17T09:01:36.904194+0000    14     255     10960     10705   764.483       749    0.356832    0.326992
2021-05-17T09:01:37.904662+0000    15     255     11726     11471    764.56       766    0.344169    0.327556
2021-05-17T09:01:38.904961+0000    16     255     12495     12240   764.824       769    0.318619    0.327881
2021-05-17T09:01:39.905247+0000    17     255     13319     13064   768.291       824    0.300313    0.327054
2021-05-17T09:01:40.905737+0000    18     255     14057     13802   766.588       738    0.355907    0.327768
2021-05-17T09:01:41.906052+0000    19     255     14828     14573   766.807       771    0.351698     0.32806
2021-05-17T09:01:42.906256+0000 min lat: 0.179786 max lat: 0.42545 avg lat: 0.328886
2021-05-17T09:01:42.906256+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T09:01:42.906256+0000    20     139     15552     15413   770.458       840    0.179786    0.328886
2021-05-17T09:01:43.906695+0000 Total time run:         20.0321
Total writes made:      15552
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     776.353
Stddev Bandwidth:       63.6324
Max bandwidth (MB/sec): 840
Min bandwidth (MB/sec): 553
Average IOPS:           776
Stddev IOPS:            63.6324
Max IOPS:               840
Min IOPS:               553
Average Latency(s):     0.326758
Stddev Latency(s):      0.0333272
Max latency(s):         0.42545
Min latency(s):         0.0101252

[1;32mlocalhost.localdomain	[2021-05-17T02:01:44,261408925-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 32013


[1;33mlocalhost.localdomain	[2021-05-17T02:01:44,270742102-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:02:07,334166677-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 15.55k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:02:07,352417567-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:02:15,481877499-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 15.55k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:02:15,500028292-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:02:23,470284001-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 15.55k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:02:23,488681752-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:02:31,573499051-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 15.55k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:02:31,591809232-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:02:39,784138634-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 15.55k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:02:39,802647869-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:02:39,817698175-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T02:02:39,826255077-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:02:39,845991386-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=35335
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_seq.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-17T02:02:39,862214772-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_seq.log.1
[1;30mlocalhost.localdomain	[2021-05-17T02:02:39,902358378-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:02:39,908772000-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '29436fd5-d52a-4d1a-a7ba-801ba7d3d31f', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LNsjng:/tmp/ceph-asok.LNsjng', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 29436fd5-d52a-4d1a-a7ba-801ba7d3d31f --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.LNsjng:/tmp/ceph-asok.LNsjng -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T09:02:41.459+0000 ffff91282010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:02:41.467+0000 ffff91282010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:02:41.467+0000 ffff91282010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T09:02:41.487565+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T09:02:41.487565+0000     0       0         0         0         0         0           -           0
2021-05-17T09:02:42.487768+0000     1     255       952       697   696.759       697    0.305857    0.290617
2021-05-17T09:02:43.487984+0000     2     255      1770      1515   757.287       818    0.311576    0.301827
2021-05-17T09:02:44.488201+0000     3     255      2558      2303   767.468       788      0.3325    0.308583
2021-05-17T09:02:45.488787+0000     4     256      3356      3100   774.736       797    0.321179     0.31199
2021-05-17T09:02:46.489014+0000     5     255      4129      3874   774.553       774    0.336362    0.315246
2021-05-17T09:02:47.489233+0000     6     255      4890      4635   772.267       761    0.245316    0.316867
2021-05-17T09:02:48.489441+0000     7     255      5618      5363   765.922       728    0.255977    0.321913
2021-05-17T09:02:49.489669+0000     8     255      6421      6166   770.534       803    0.243781    0.321919
2021-05-17T09:02:50.489893+0000     9     255      7226      6971   774.343       805    0.317099    0.322152
2021-05-17T09:02:51.490134+0000    10     255      7979      7724   772.191       753    0.356055    0.323382
2021-05-17T09:02:52.490351+0000    11     255      8691      8436   766.705       712    0.263268    0.325455
2021-05-17T09:02:53.490585+0000    12     255      9413      9158   762.966       722    0.524102    0.327808
2021-05-17T09:02:54.490832+0000    13     255     10156      9901   761.416       743    0.254138     0.32925
2021-05-17T09:02:55.491406+0000    14     255     10888     10633   759.284       732    0.250514     0.33068
2021-05-17T09:02:56.491630+0000    15     255     11606     11351   756.521       718    0.279376    0.331848
2021-05-17T09:02:57.491868+0000    16     255     12344     12089   755.353       738    0.257182    0.333317
2021-05-17T09:02:58.492092+0000    17     255     13090     12835   754.793       746    0.519081    0.333648
2021-05-17T09:02:59.492330+0000    18     255     13874     13619   756.405       784    0.249882    0.333359
2021-05-17T09:03:00.492653+0000    19     255     14648     14393   757.318       774    0.519582     0.33315
2021-05-17T09:03:01.492881+0000 min lat: 0.171046 max lat: 0.612329 avg lat: 0.33373
2021-05-17T09:03:01.492881+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T09:03:01.492881+0000    20     255     15402     15147   757.143       754    0.339506     0.33373
2021-05-17T09:03:02.493160+0000 Total time run:       20.4428
Total reads made:     15552
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   760.757
Average IOPS:         760
Stddev IOPS:          34.4602
Max IOPS:             818
Min IOPS:             697
Average Latency(s):   0.332534
Max latency(s):       0.612329
Min latency(s):       0.0880836

[1;32mlocalhost.localdomain	[2021-05-17T02:03:02,858915793-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 35335


[1;33mlocalhost.localdomain	[2021-05-17T02:03:02,868164837-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:03:25,976483482-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 15.55k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:03:25,995318499-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:03:33,903139992-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 15.55k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:03:33,921400561-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:03:41,748672461-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 15.55k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:03:41,767129234-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:03:49,752171292-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 15.55k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:03:49,770650674-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:03:58,064962626-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 15.55k objects, 15 GiB
    usage:   30 GiB used, 270 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:03:58,083477235-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:03:58,098588393-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T02:03:58,104229669-07:00][RUNNING][ROUND 2/5/40] object_size=1MB[0m


[1;33mlocalhost.localdomain	[2021-05-17T02:03:58,112705363-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T02:03:58,130643828-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40347\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.324926\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 9eca8e61-92c8-4ca4-8229-3c5693046fdc\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 9eca8e61-92c8-4ca4-8229-3c5693046fdc\nlast_changed 2021-05-17T02:04:27.026026-0700\ncreated 2021-05-17T02:04:27.026026-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40347/0,v1:10.10.1.2:40348/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.324926 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 11918978-d34f-41a6-a8ef-04dbb3ac395b\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 842c847d-5021-4fd8-a259-c5379b71fca8\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 395f0749-c2b5-4c68-8759-2ec8fe38c664\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42347\n  w/ user/pass: admin / aee416de-a262-444a-bfa1-fe6cd10db4f8\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 02:04:41 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40347
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.324926
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 9eca8e61-92c8-4ca4-8229-3c5693046fdc
setting min_mon_release = octopus
epoch 0
fsid 9eca8e61-92c8-4ca4-8229-3c5693046fdc
last_changed 2021-05-17T02:04:27.026026-0700
created 2021-05-17T02:04:27.026026-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40347/0,v1:10.10.1.2:40348/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.324926 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 11918978-d34f-41a6-a8ef-04dbb3ac395b
0
start osd.0
add osd1 842c847d-5021-4fd8-a259-c5379b71fca8
1
start osd.1
add osd2 395f0749-c2b5-4c68-8759-2ec8fe38c664
2
start osd.2


restful urls: https://10.10.1.2:42347
  w/ user/pass: admin / aee416de-a262-444a-bfa1-fe6cd10db4f8


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T02:03:59.136-0700 7f00a4e4a1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T02:03:59.136-0700 7f00a4e4a1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T02:03:59.152-0700 7f600f4be1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T02:03:59.152-0700 7f600f4be1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40347,v1:10.10.1.2:40348] --print /tmp/ceph_monmap.324926 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.324926 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.324926 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42347 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.k4gCkb9Ez8 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 11918978-d34f-41a6-a8ef-04dbb3ac395b -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCkMaJglJIUABAAC4t8K6sIwc1T/hw5Ej2PfA== --osd-uuid 11918978-d34f-41a6-a8ef-04dbb3ac395b 
2021-05-17T02:04:36.324-0700 7fb0a6921f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T02:04:36.324-0700 7fb0a6921f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T02:04:36.324-0700 7fb0a6921f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T02:04:36.380-0700 7fb0a6921f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 842c847d-5021-4fd8-a259-c5379b71fca8 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T02:04:36.688-0700 7fd1f2879f00 -1 Falling back to public interface
2021-05-17T02:04:36.704-0700 7fd1f2879f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCkMaJgvFJCKRAA2BZvEzC8F4J8dYr2k+QotQ== --osd-uuid 842c847d-5021-4fd8-a259-c5379b71fca8 
2021-05-17T02:04:37.020-0700 7f0f9cecaf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T02:04:37.020-0700 7f0f9cecaf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T02:04:37.020-0700 7f0f9cecaf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T02:04:37.072-0700 7f0f9cecaf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 395f0749-c2b5-4c68-8759-2ec8fe38c664 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T02:04:37.332-0700 7fcc7b7eaf00 -1 Falling back to public interface
2021-05-17T02:04:37.348-0700 7fcc7b7eaf00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQClMaJgxEYDFBAAWZSVF9YeG1hw/n1HBE2c3g== --osd-uuid 395f0749-c2b5-4c68-8759-2ec8fe38c664 
2021-05-17T02:04:37.688-0700 7f85c47cef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T02:04:37.688-0700 7f85c47cef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T02:04:37.688-0700 7f85c47cef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T02:04:37.728-0700 7f85c47cef00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T02:04:38.156-0700 7fc686d18f00 -1 Falling back to public interface
2021-05-17T02:04:38.168-0700 7fc686d18f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T02:04:42,001096267-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T02:04:42,009999669-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T02:04:42,094714902-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:04:42,100974389-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7df415e8-dae6-4cfa-9400-29ab32d0c585', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2wPBFH:/tmp/ceph-asok.2wPBFH', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7df415e8-dae6-4cfa-9400-29ab32d0c585 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2wPBFH:/tmp/ceph-asok.2wPBFH -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T02:04:44,950477079-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:04:44,956965789-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7df415e8-dae6-4cfa-9400-29ab32d0c585', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2wPBFH:/tmp/ceph-asok.2wPBFH', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7df415e8-dae6-4cfa-9400-29ab32d0c585 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2wPBFH:/tmp/ceph-asok.2wPBFH -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T02:04:48,000578854-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:04:48,006859374-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7df415e8-dae6-4cfa-9400-29ab32d0c585', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2wPBFH:/tmp/ceph-asok.2wPBFH', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7df415e8-dae6-4cfa-9400-29ab32d0c585 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2wPBFH:/tmp/ceph-asok.2wPBFH -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T02:04:51,099801591-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:04:51,106193550-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7df415e8-dae6-4cfa-9400-29ab32d0c585', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2wPBFH:/tmp/ceph-asok.2wPBFH', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7df415e8-dae6-4cfa-9400-29ab32d0c585 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2wPBFH:/tmp/ceph-asok.2wPBFH -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T02:04:56,678466086-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:04:56,685141188-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7df415e8-dae6-4cfa-9400-29ab32d0c585', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2wPBFH:/tmp/ceph-asok.2wPBFH', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7df415e8-dae6-4cfa-9400-29ab32d0c585 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2wPBFH:/tmp/ceph-asok.2wPBFH -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T02:04:59,697284278-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:04:59,704762527-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7df415e8-dae6-4cfa-9400-29ab32d0c585', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2wPBFH:/tmp/ceph-asok.2wPBFH', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7df415e8-dae6-4cfa-9400-29ab32d0c585 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2wPBFH:/tmp/ceph-asok.2wPBFH -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T02:05:03,016227665-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:05:03,023408128-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7df415e8-dae6-4cfa-9400-29ab32d0c585', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2wPBFH:/tmp/ceph-asok.2wPBFH', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7df415e8-dae6-4cfa-9400-29ab32d0c585 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2wPBFH:/tmp/ceph-asok.2wPBFH -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T02:05:06,856972840-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:05:06,863493281-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7df415e8-dae6-4cfa-9400-29ab32d0c585', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2wPBFH:/tmp/ceph-asok.2wPBFH', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7df415e8-dae6-4cfa-9400-29ab32d0c585 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2wPBFH:/tmp/ceph-asok.2wPBFH -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T02:05:10,196217298-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:05:10,202635016-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7df415e8-dae6-4cfa-9400-29ab32d0c585', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2wPBFH:/tmp/ceph-asok.2wPBFH', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7df415e8-dae6-4cfa-9400-29ab32d0c585 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2wPBFH:/tmp/ceph-asok.2wPBFH -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T02:05:13,459490488-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:05:13,466837555-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7df415e8-dae6-4cfa-9400-29ab32d0c585', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2wPBFH:/tmp/ceph-asok.2wPBFH', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7df415e8-dae6-4cfa-9400-29ab32d0c585 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2wPBFH:/tmp/ceph-asok.2wPBFH -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T02:05:16,657103172-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:05:16,663984380-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7df415e8-dae6-4cfa-9400-29ab32d0c585', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2wPBFH:/tmp/ceph-asok.2wPBFH', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7df415e8-dae6-4cfa-9400-29ab32d0c585 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2wPBFH:/tmp/ceph-asok.2wPBFH -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T02:05:19,393759461-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:05:19,400063175-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7df415e8-dae6-4cfa-9400-29ab32d0c585', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2wPBFH:/tmp/ceph-asok.2wPBFH', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7df415e8-dae6-4cfa-9400-29ab32d0c585 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2wPBFH:/tmp/ceph-asok.2wPBFH -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T02:05:22,406758461-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:05:45,427653384-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   189 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:05:53,473438589-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:06:01,533858357-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (9s)
      [=======.....................] (remaining: 27s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:06:09,564417950-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [=======.....................] (remaining: 53s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:06:17,508786618-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [=======.....................] (remaining: 63s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:06:25,480329507-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [========....................] (remaining: 78s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:06:33,708208575-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (45s)
      [========....................] (remaining: 96s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:06:41,692467536-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [===============.............] (remaining: 42s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:06:49,841004296-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:06:49,859450347-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:06:57,822563536-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:06:57,840471340-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:07:05,783782044-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:07:05,802165594-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:07:13,956199768-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:07:13,974518973-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:07:21,945543196-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:07:21,965704955-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:07:21,982323514-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T02:07:21,991946080-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:07:22,016270539-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=49217
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_write.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-17T02:07:22,034044761-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_write.log.2
[1;30mlocalhost.localdomain	[2021-05-17T02:07:22,075966771-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:07:22,082645280-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7df415e8-dae6-4cfa-9400-29ab32d0c585', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2wPBFH:/tmp/ceph-asok.2wPBFH', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7df415e8-dae6-4cfa-9400-29ab32d0c585 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2wPBFH:/tmp/ceph-asok.2wPBFH -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T09:07:23.723+0000 ffffb6a75010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:07:23.731+0000 ffffb6a75010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:07:23.731+0000 ffffb6a75010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T09:07:23.748292+0000 Maintaining 256 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 20 seconds or 0 objects
2021-05-17T09:07:23.748366+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T09:07:23.960046+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T09:07:23.960046+0000     0       0         0         0         0         0           -           0
2021-05-17T09:07:24.960283+0000     1     255       760       505   504.954       505    0.323923    0.326684
2021-05-17T09:07:25.960485+0000     2     255      1546      1291   645.405       786    0.307171    0.330026
2021-05-17T09:07:26.960727+0000     3     255      2344      2089   696.209       798    0.321181    0.326378
2021-05-17T09:07:27.960972+0000     4     255      3142      2887   721.609       798    0.303053    0.325341
2021-05-17T09:07:28.961197+0000     5     255      3882      3627   725.254       740    0.374773     0.32596
2021-05-17T09:07:29.961418+0000     6     255      4658      4403   733.683       776    0.311297    0.328696
2021-05-17T09:07:30.961676+0000     7     255      5435      5180   739.843       777    0.313542    0.328889
2021-05-17T09:07:31.961919+0000     8     255      6232      5977   746.964       797    0.311701    0.328238
2021-05-17T09:07:32.962163+0000     9     255      6975      6720   746.503       743    0.317083    0.329492
2021-05-17T09:07:33.962398+0000    10     255      7776      7521   751.934       801    0.320249    0.328505
2021-05-17T09:07:34.962667+0000    11     255      8555      8300   754.376       779    0.314044    0.328583
2021-05-17T09:07:35.962979+0000    12     255      9311      9056   754.491       756    0.353727     0.32892
2021-05-17T09:07:36.963321+0000    13     255     10151      9896   761.048       840    0.311846    0.327649
2021-05-17T09:07:37.963688+0000    14     255     10976     10721   765.594       825    0.301293    0.326319
2021-05-17T09:07:38.963974+0000    15     255     11815     11560   770.472       839    0.299223    0.324702
2021-05-17T09:07:39.964423+0000    16     255     12587     12332   770.546       772    0.321072    0.325093
2021-05-17T09:07:40.964678+0000    17     255     13423     13168   774.384       836    0.287908       0.324
2021-05-17T09:07:41.965493+0000    18     255     14237     13982   776.549       814    0.326735    0.323343
2021-05-17T09:07:42.966009+0000    19     255     14971     14716   774.289       734    0.360269    0.324092
2021-05-17T09:07:43.966416+0000 min lat: 0.270119 max lat: 0.40583 avg lat: 0.325471
2021-05-17T09:07:43.966416+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T09:07:43.966416+0000    20     255     15705     15450    772.26       734    0.326585    0.325471
2021-05-17T09:07:44.966876+0000 Total time run:         20.0287
Total writes made:      15706
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     784.176
Stddev Bandwidth:       71.517
Max bandwidth (MB/sec): 840
Min bandwidth (MB/sec): 505
Average IOPS:           784
Stddev IOPS:            71.517
Max IOPS:               840
Min IOPS:               505
Average Latency(s):     0.323154
Stddev Latency(s):      0.0297673
Max latency(s):         0.40583
Min latency(s):         0.02723

[1;32mlocalhost.localdomain	[2021-05-17T02:07:45,315003191-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 49217


[1;33mlocalhost.localdomain	[2021-05-17T02:07:45,324019174-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:08:08,394556276-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 15.71k objects, 15 GiB
    usage:   31 GiB used, 269 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:08:08,412966262-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:08:16,422735373-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 15.71k objects, 15 GiB
    usage:   31 GiB used, 269 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:08:16,441260639-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:08:24,525298915-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 15.71k objects, 15 GiB
    usage:   31 GiB used, 269 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:08:24,543988519-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:08:32,723362616-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 15.71k objects, 15 GiB
    usage:   31 GiB used, 269 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:08:32,741263326-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:08:40,755675423-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 15.71k objects, 15 GiB
    usage:   31 GiB used, 269 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:08:40,774602982-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:08:40,789883615-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T02:08:40,798633888-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:08:40,818852943-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=52556
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_seq.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-17T02:08:40,835803886-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_seq.log.2
[1;30mlocalhost.localdomain	[2021-05-17T02:08:40,876241411-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:08:40,882711175-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '7df415e8-dae6-4cfa-9400-29ab32d0c585', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2wPBFH:/tmp/ceph-asok.2wPBFH', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 7df415e8-dae6-4cfa-9400-29ab32d0c585 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.2wPBFH:/tmp/ceph-asok.2wPBFH -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T09:08:42.476+0000 ffff929e0010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:08:42.484+0000 ffff929e0010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:08:42.488+0000 ffff929e0010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T09:08:42.504698+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T09:08:42.504698+0000     0       0         0         0         0         0           -           0
2021-05-17T09:08:43.504888+0000     1     255       972       717    716.76       717    0.310371    0.282156
2021-05-17T09:08:44.505114+0000     2     255      1779      1524   761.786       807    0.308908    0.300703
2021-05-17T09:08:45.505317+0000     3     255      2582      2327   775.469       803    0.313504    0.306312
2021-05-17T09:08:46.505513+0000     4     255      3364      3109   777.063       782    0.325708    0.310536
2021-05-17T09:08:47.506292+0000     5     255      4074      3819   763.534       710    0.546573     0.31721
2021-05-17T09:08:48.506569+0000     6     255      4789      4534   755.413       715     0.26093    0.324199
2021-05-17T09:08:49.507035+0000     7     255      5524      5269   752.447       735    0.259258    0.327754
2021-05-17T09:08:50.507668+0000     8     255      6265      6010   750.958       741    0.347716    0.330776
2021-05-17T09:08:51.507929+0000     9     255      7004      6749   749.608       739    0.336112    0.332518
2021-05-17T09:08:52.508203+0000    10     255      7752      7497   749.426       748    0.341069    0.333405
2021-05-17T09:08:53.508594+0000    11     255      8514      8259   750.542       762    0.336812    0.333691
2021-05-17T09:08:54.508848+0000    12     255      9246      8991   748.982       732     0.35215    0.334636
2021-05-17T09:08:55.509132+0000    13     255      9987      9732   748.352       741    0.334195    0.335717
2021-05-17T09:08:56.509349+0000    14     255     10727     10472   747.744       740    0.351028    0.336288
2021-05-17T09:08:57.509594+0000    15     255     11469     11214   747.349       742    0.336417    0.336966
2021-05-17T09:08:58.509902+0000    16     255     12209     11954   746.875       740    0.344182    0.337415
2021-05-17T09:08:59.510127+0000    17     255     12967     12712    747.52       758    0.240615    0.337114
2021-05-17T09:09:00.510339+0000    18     255     13738     13483   748.815       771    0.334298    0.337179
2021-05-17T09:09:01.510781+0000    19     255     14490     14235   748.965       752    0.282689    0.336772
2021-05-17T09:09:02.511002+0000 min lat: 0.159587 max lat: 0.658191 avg lat: 0.337133
2021-05-17T09:09:02.511002+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T09:09:02.511002+0000    20     255     15230     14975   748.509       740    0.535183    0.337133
2021-05-17T09:09:03.511273+0000 Total time run:       20.8365
Total reads made:     15706
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   753.772
Average IOPS:         753
Stddev IOPS:          25.9734
Max IOPS:             807
Min IOPS:             710
Average Latency(s):   0.33593
Max latency(s):       0.658191
Min latency(s):       0.0837477

[1;32mlocalhost.localdomain	[2021-05-17T02:09:03,852899109-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 52556


[1;33mlocalhost.localdomain	[2021-05-17T02:09:03,861957610-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:09:26,894759796-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 15.71k objects, 15 GiB
    usage:   31 GiB used, 269 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:09:26,913418000-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:09:34,751961468-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 15.71k objects, 15 GiB
    usage:   31 GiB used, 269 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:09:34,770532958-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:09:42,749694051-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 15.71k objects, 15 GiB
    usage:   31 GiB used, 269 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:09:42,767607378-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:09:50,763948222-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 15.71k objects, 15 GiB
    usage:   31 GiB used, 269 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:09:50,782525323-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:09:58,718140124-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 15.71k objects, 15 GiB
    usage:   31 GiB used, 269 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:09:58,738243780-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:09:58,754250598-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T02:09:58,760224647-07:00][RUNNING][ROUND 3/5/40] object_size=1MB[0m


[1;33mlocalhost.localdomain	[2021-05-17T02:09:58,768870533-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T02:09:58,786899996-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40107\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.326088\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid eb0c34a7-0a2c-4418-b54f-374e7bc4c68a\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid eb0c34a7-0a2c-4418-b54f-374e7bc4c68a\nlast_changed 2021-05-17T02:10:28.573361-0700\ncreated 2021-05-17T02:10:28.573361-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40107/0,v1:10.10.1.2:40108/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.326088 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 270236c4-88f0-43f8-b686-51105fdcc00c\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 0b4d1bcf-9aba-4c05-817d-401d85b0546b\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 33030749-04a8-40d9-a048-4afc81acdb64\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42107\n  w/ user/pass: admin / c0553608-c9a0-4eb0-acfe-a95c4bd1e45b\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 02:10:43 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40107
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.326088
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid eb0c34a7-0a2c-4418-b54f-374e7bc4c68a
setting min_mon_release = octopus
epoch 0
fsid eb0c34a7-0a2c-4418-b54f-374e7bc4c68a
last_changed 2021-05-17T02:10:28.573361-0700
created 2021-05-17T02:10:28.573361-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40107/0,v1:10.10.1.2:40108/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.326088 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 270236c4-88f0-43f8-b686-51105fdcc00c
0
start osd.0
add osd1 0b4d1bcf-9aba-4c05-817d-401d85b0546b
1
start osd.1
add osd2 33030749-04a8-40d9-a048-4afc81acdb64
2
start osd.2


restful urls: https://10.10.1.2:42107
  w/ user/pass: admin / c0553608-c9a0-4eb0-acfe-a95c4bd1e45b


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T02:09:59.831-0700 7feae1ad01c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T02:09:59.831-0700 7feae1ad01c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T02:09:59.847-0700 7f4db3ebb1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T02:09:59.847-0700 7f4db3ebb1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40107,v1:10.10.1.2:40108] --print /tmp/ceph_monmap.326088 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.326088 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.326088 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42107 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.jenAZP3fW9 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 270236c4-88f0-43f8-b686-51105fdcc00c -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQANM6JgYSyJIRAA2QUa6aMTiivfeq7kijzoTw== --osd-uuid 270236c4-88f0-43f8-b686-51105fdcc00c 
2021-05-17T02:10:37.891-0700 7f6a5000af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T02:10:37.895-0700 7f6a5000af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T02:10:37.895-0700 7f6a5000af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T02:10:37.963-0700 7f6a5000af00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 0b4d1bcf-9aba-4c05-817d-401d85b0546b -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T02:10:38.347-0700 7f1cf4ef1f00 -1 Falling back to public interface
2021-05-17T02:10:38.363-0700 7f1cf4ef1f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAOM6JgQ3YNFRAAIIftCKdAPZgH0atDYzpSGQ== --osd-uuid 0b4d1bcf-9aba-4c05-817d-401d85b0546b 
2021-05-17T02:10:38.695-0700 7f8cb1ddaf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T02:10:38.695-0700 7f8cb1ddaf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T02:10:38.695-0700 7f8cb1ddaf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T02:10:38.763-0700 7f8cb1ddaf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 33030749-04a8-40d9-a048-4afc81acdb64 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T02:10:39.187-0700 7fda499b3f00 -1 Falling back to public interface
2021-05-17T02:10:39.199-0700 7fda499b3f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAPM6JgL7NWCxAA55XOfhSgCsZOnmFPn4SjPQ== --osd-uuid 33030749-04a8-40d9-a048-4afc81acdb64 
2021-05-17T02:10:39.515-0700 7f447491df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T02:10:39.519-0700 7f447491df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T02:10:39.519-0700 7f447491df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T02:10:39.571-0700 7f447491df00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T02:10:39.847-0700 7f237ab18f00 -1 Falling back to public interface
2021-05-17T02:10:39.863-0700 7f237ab18f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T02:10:43,914260156-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T02:10:43,923012305-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T02:10:44,006283287-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:10:44,012619529-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '69337986-5476-4cb1-8aaa-eddefb64d96d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kEfKxP:/tmp/ceph-asok.kEfKxP', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 69337986-5476-4cb1-8aaa-eddefb64d96d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kEfKxP:/tmp/ceph-asok.kEfKxP -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T02:10:46,746315461-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:10:46,752893875-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '69337986-5476-4cb1-8aaa-eddefb64d96d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kEfKxP:/tmp/ceph-asok.kEfKxP', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 69337986-5476-4cb1-8aaa-eddefb64d96d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kEfKxP:/tmp/ceph-asok.kEfKxP -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T02:10:49,675225749-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:10:49,681869618-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '69337986-5476-4cb1-8aaa-eddefb64d96d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kEfKxP:/tmp/ceph-asok.kEfKxP', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 69337986-5476-4cb1-8aaa-eddefb64d96d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kEfKxP:/tmp/ceph-asok.kEfKxP -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T02:10:52,356081755-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:10:52,363122877-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '69337986-5476-4cb1-8aaa-eddefb64d96d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kEfKxP:/tmp/ceph-asok.kEfKxP', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 69337986-5476-4cb1-8aaa-eddefb64d96d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kEfKxP:/tmp/ceph-asok.kEfKxP -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T02:10:58,241239860-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:10:58,248042216-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '69337986-5476-4cb1-8aaa-eddefb64d96d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kEfKxP:/tmp/ceph-asok.kEfKxP', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 69337986-5476-4cb1-8aaa-eddefb64d96d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kEfKxP:/tmp/ceph-asok.kEfKxP -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T02:11:01,654967609-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:11:01,661130106-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '69337986-5476-4cb1-8aaa-eddefb64d96d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kEfKxP:/tmp/ceph-asok.kEfKxP', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 69337986-5476-4cb1-8aaa-eddefb64d96d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kEfKxP:/tmp/ceph-asok.kEfKxP -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T02:11:05,020941612-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:11:05,027285805-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '69337986-5476-4cb1-8aaa-eddefb64d96d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kEfKxP:/tmp/ceph-asok.kEfKxP', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 69337986-5476-4cb1-8aaa-eddefb64d96d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kEfKxP:/tmp/ceph-asok.kEfKxP -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T02:11:08,384157206-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:11:08,390659362-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '69337986-5476-4cb1-8aaa-eddefb64d96d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kEfKxP:/tmp/ceph-asok.kEfKxP', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 69337986-5476-4cb1-8aaa-eddefb64d96d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kEfKxP:/tmp/ceph-asok.kEfKxP -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T02:11:12,214178370-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:11:12,220788739-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '69337986-5476-4cb1-8aaa-eddefb64d96d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kEfKxP:/tmp/ceph-asok.kEfKxP', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 69337986-5476-4cb1-8aaa-eddefb64d96d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kEfKxP:/tmp/ceph-asok.kEfKxP -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T02:11:16,055064369-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:11:16,061384303-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '69337986-5476-4cb1-8aaa-eddefb64d96d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kEfKxP:/tmp/ceph-asok.kEfKxP', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 69337986-5476-4cb1-8aaa-eddefb64d96d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kEfKxP:/tmp/ceph-asok.kEfKxP -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T02:11:19,395208478-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:11:19,401591333-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '69337986-5476-4cb1-8aaa-eddefb64d96d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kEfKxP:/tmp/ceph-asok.kEfKxP', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 69337986-5476-4cb1-8aaa-eddefb64d96d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kEfKxP:/tmp/ceph-asok.kEfKxP -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T02:11:22,114208114-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:11:22,120802550-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '69337986-5476-4cb1-8aaa-eddefb64d96d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kEfKxP:/tmp/ceph-asok.kEfKxP', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 69337986-5476-4cb1-8aaa-eddefb64d96d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kEfKxP:/tmp/ceph-asok.kEfKxP -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  156 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T02:11:24,974677757-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:11:47,968493306-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   206 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (4s)
      [............................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:11:56,177600214-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:12:04,047879306-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:12:12,218109837-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:12:20,311719964-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:12:28,253390462-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:12:36,301977792-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:12:44,380388166-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   238 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:12:44,399228054-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:12:52,364646603-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:12:52,383475374-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:13:00,342805647-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:13:00,361146370-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:13:08,300548831-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:13:08,319485021-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:13:16,495294809-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:13:16,514321839-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:13:16,529865394-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T02:13:16,538914355-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:13:16,559183942-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=66056
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_write.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-17T02:13:16,575547507-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_write.log.3
[1;30mlocalhost.localdomain	[2021-05-17T02:13:16,617061399-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:13:16,623595030-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '69337986-5476-4cb1-8aaa-eddefb64d96d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kEfKxP:/tmp/ceph-asok.kEfKxP', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 69337986-5476-4cb1-8aaa-eddefb64d96d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kEfKxP:/tmp/ceph-asok.kEfKxP -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T09:13:18.368+0000 ffffafa37010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:13:18.376+0000 ffffafa37010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:13:18.376+0000 ffffafa37010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T09:13:18.392661+0000 Maintaining 256 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 20 seconds or 0 objects
2021-05-17T09:13:18.392738+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T09:13:18.615870+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T09:13:18.615870+0000     0       0         0         0         0         0           -           0
2021-05-17T09:13:19.616104+0000     1     255       695       440   439.962       440    0.330336    0.377362
2021-05-17T09:13:20.616332+0000     2     255      1422      1167   583.408       727    0.336808    0.359248
2021-05-17T09:13:21.616535+0000     3     255      2161      1906   635.224       739    0.335743    0.354883
2021-05-17T09:13:22.616763+0000     4     255      2965      2710   677.374       804    0.329235    0.345056
2021-05-17T09:13:23.617048+0000     5     255      3749      3494   698.656       784    0.321307    0.340966
2021-05-17T09:13:24.617322+0000     6     255      4489      4234   705.513       740    0.356922    0.339984
2021-05-17T09:13:25.617716+0000     7     255      5237      4982   711.542       748    0.303216    0.342024
2021-05-17T09:13:26.618049+0000     8     255      6010      5755   719.192       773    0.342501    0.340228
2021-05-17T09:13:27.618371+0000     9     255      6805      6550   727.587       795    0.297457    0.338731
2021-05-17T09:13:28.618711+0000    10     255      7505      7250   724.805       700     0.36478    0.340023
2021-05-17T09:13:29.618937+0000    11     255      8215      7960   723.444       710    0.353251    0.341982
2021-05-17T09:13:30.619201+0000    12     255      8955      8700   724.808       740    0.356823    0.343133
2021-05-17T09:13:31.619542+0000    13     255      9638      9383   721.574       683    0.364446    0.344599
2021-05-17T09:13:32.619815+0000    14     255     10379     10124   722.947       741    0.319203    0.345509
2021-05-17T09:13:33.620212+0000    15     255     11137     10882   725.264       758    0.304398    0.344776
2021-05-17T09:13:34.620686+0000    16     255     11921     11666   728.912       784    0.318484    0.343603
2021-05-17T09:13:35.621197+0000    17     255     12682     12427   730.777       761    0.310475    0.343616
2021-05-17T09:13:36.621654+0000    18     255     13456     13201   733.159       774    0.304469    0.342571
2021-05-17T09:13:37.621956+0000    19     255     14206     13951   734.034       750    0.360467    0.342155
2021-05-17T09:13:38.622183+0000 min lat: 0.014435 max lat: 0.436651 avg lat: 0.339123
2021-05-17T09:13:38.622183+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T09:13:38.622183+0000    20      31     14991     14960   747.769      1009    0.014435    0.339123
2021-05-17T09:13:39.622472+0000 Total time run:         20.0208
Total writes made:      14991
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     748.77
Stddev Bandwidth:       97.6481
Max bandwidth (MB/sec): 1009
Min bandwidth (MB/sec): 440
Average IOPS:           748
Stddev IOPS:            97.6481
Max IOPS:               1009
Min IOPS:               440
Average Latency(s):     0.338528
Stddev Latency(s):      0.0325595
Max latency(s):         0.436651
Min latency(s):         0.014435

[1;32mlocalhost.localdomain	[2021-05-17T02:13:39,993358741-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 66056


[1;33mlocalhost.localdomain	[2021-05-17T02:13:40,002969638-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:14:02,946998722-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.99k objects, 15 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:14:02,965904217-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:14:11,105970373-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.99k objects, 15 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:14:11,124782897-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:14:19,074236131-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.99k objects, 15 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:14:19,095114508-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:14:26,945786708-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.99k objects, 15 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:14:26,964988549-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:14:34,968968086-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.99k objects, 15 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:14:34,988269513-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:14:35,003884571-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T02:14:35,012935738-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:14:35,033637235-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=69352
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_seq.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-17T02:14:35,050379945-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_seq.log.3
[1;30mlocalhost.localdomain	[2021-05-17T02:14:35,090573208-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:14:35,096930169-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '69337986-5476-4cb1-8aaa-eddefb64d96d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kEfKxP:/tmp/ceph-asok.kEfKxP', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 69337986-5476-4cb1-8aaa-eddefb64d96d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.kEfKxP:/tmp/ceph-asok.kEfKxP -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T09:14:36.705+0000 ffff80fd6010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:14:36.713+0000 ffff80fd6010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:14:36.713+0000 ffff80fd6010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T09:14:36.733266+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T09:14:36.733266+0000     0       0         0         0         0         0           -           0
2021-05-17T09:14:37.733478+0000     1     255       880       625    624.77       625    0.339304    0.316318
2021-05-17T09:14:38.733712+0000     2     255      1630      1375   687.293       750    0.347648     0.32891
2021-05-17T09:14:39.733925+0000     3     255      2392      2137    712.14       762    0.351206    0.331444
2021-05-17T09:14:40.734263+0000     4     255      3183      2928   731.789       791    0.317451    0.330561
2021-05-17T09:14:41.734562+0000     5     255      3975      3720   743.784       792     0.32057    0.328752
2021-05-17T09:14:42.734792+0000     6     255      4771      4516   752.456       796    0.325724    0.327181
2021-05-17T09:14:43.735044+0000     7     255      5541      5286   754.934       770    0.321292    0.328053
2021-05-17T09:14:44.735783+0000     8     255      6331      6076   759.246       790    0.323172    0.327451
2021-05-17T09:14:45.736013+0000     9     255      7126      6871   763.198       795    0.317267    0.326893
2021-05-17T09:14:46.736268+0000    10     255      7907      7652   764.958       781    0.336919    0.326654
2021-05-17T09:14:47.736519+0000    11     255      8696      8441   767.126       789     0.31361    0.326676
2021-05-17T09:14:48.736792+0000    12     255      9514      9259   771.347       818    0.311577    0.325453
2021-05-17T09:14:49.738242+0000    13     255     10304     10049   772.695       790    0.330304    0.325056
2021-05-17T09:14:50.738484+0000    14     255     11050     10795   770.776       746    0.347704    0.325932
2021-05-17T09:14:51.739316+0000    15     255     11841     11586   772.081       791    0.329974    0.326169
2021-05-17T09:14:52.739539+0000    16     255     12620     12365   772.502       779    0.318468    0.326394
2021-05-17T09:14:53.739794+0000    17     255     13376     13121    771.52       756    0.250288    0.326285
2021-05-17T09:14:54.740117+0000    18     255     14150     13895   771.644       774    0.319463    0.327231
2021-05-17T09:14:55.740350+0000    19     255     14934     14679   772.285       784    0.323672    0.327144
2021-05-17T09:14:56.740726+0000 Total time run:       19.2782
Total reads made:     14991
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   777.614
Average IOPS:         777
Stddev IOPS:          39.917
Max IOPS:             818
Min IOPS:             625
Average Latency(s):   0.325435
Max latency(s):       0.575847
Min latency(s):       0.0896728

[1;32mlocalhost.localdomain	[2021-05-17T02:14:57,113300535-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 69352


[1;33mlocalhost.localdomain	[2021-05-17T02:14:57,124003859-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:15:20,141625896-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.99k objects, 15 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:15:20,160508978-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:15:28,043630183-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.99k objects, 15 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:15:28,062732812-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:15:36,360409847-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.99k objects, 15 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:15:36,379443524-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:15:44,378876115-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.99k objects, 15 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:15:44,400508686-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:15:52,385713784-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.99k objects, 15 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:15:52,404870843-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:15:52,420293179-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T02:15:52,426133302-07:00][RUNNING][ROUND 4/5/40] object_size=1MB[0m


[1;33mlocalhost.localdomain	[2021-05-17T02:15:52,434818646-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T02:15:52,452893285-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40838\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.327197\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 0142bd35-8c49-45b6-9657-3e4f2e8dfb99\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 0142bd35-8c49-45b6-9657-3e4f2e8dfb99\nlast_changed 2021-05-17T02:16:25.932286-0700\ncreated 2021-05-17T02:16:25.932286-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40838/0,v1:10.10.1.2:40839/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.327197 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 bd5da989-0091-4028-81d0-869f021caf15\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 e506f9f2-c295-491b-a81a-97562d0bce68\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 71e3ac7c-fa52-4d21-ac2e-c7e6251c11a6\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\n'
10.10.1.2: b'restful urls: https://10.10.1.2:42838\n  w/ user/pass: admin / a1528424-0476-4f16-adc1-8eb62dddf312\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 02:16:40 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40838
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.327197
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 0142bd35-8c49-45b6-9657-3e4f2e8dfb99
setting min_mon_release = octopus
epoch 0
fsid 0142bd35-8c49-45b6-9657-3e4f2e8dfb99
last_changed 2021-05-17T02:16:25.932286-0700
created 2021-05-17T02:16:25.932286-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40838/0,v1:10.10.1.2:40839/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.327197 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 bd5da989-0091-4028-81d0-869f021caf15
0
start osd.0
add osd1 e506f9f2-c295-491b-a81a-97562d0bce68
1
start osd.1
add osd2 71e3ac7c-fa52-4d21-ac2e-c7e6251c11a6
2
start osd.2


restful urls: https://10.10.1.2:42838
  w/ user/pass: admin / a1528424-0476-4f16-adc1-8eb62dddf312


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T02:15:53.442-0700 7ffb497c61c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T02:15:53.442-0700 7ffb497c61c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T02:15:53.462-0700 7fec4b1491c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T02:15:53.462-0700 7fec4b1491c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40838,v1:10.10.1.2:40839] --print /tmp/ceph_monmap.327197 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.327197 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.327197 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42838 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.PuCnYwoDHu 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new bd5da989-0091-4028-81d0-869f021caf15 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQByNKJgP3MOMhAAAsPzMkL4zUr+q+0h6U1wUQ== --osd-uuid bd5da989-0091-4028-81d0-869f021caf15 
2021-05-17T02:16:35.214-0700 7f8c091f7f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T02:16:35.214-0700 7f8c091f7f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T02:16:35.214-0700 7f8c091f7f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T02:16:35.266-0700 7f8c091f7f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new e506f9f2-c295-491b-a81a-97562d0bce68 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T02:16:35.586-0700 7f333c28ef00 -1 Falling back to public interface
2021-05-17T02:16:35.602-0700 7f333c28ef00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBzNKJgrUIiIxAAIWvvAD2LVEU4tIPZWI8i1g== --osd-uuid e506f9f2-c295-491b-a81a-97562d0bce68 
2021-05-17T02:16:35.926-0700 7f0306553f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T02:16:35.926-0700 7f0306553f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T02:16:35.926-0700 7f0306553f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T02:16:35.970-0700 7f0306553f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 71e3ac7c-fa52-4d21-ac2e-c7e6251c11a6 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T02:16:36.374-0700 7fdc8ef73f00 -1 Falling back to public interface
2021-05-17T02:16:36.386-0700 7fdc8ef73f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQB0NKJgxUBHFhAAdwfoOBnsmkqNaM+nyDjk0w== --osd-uuid 71e3ac7c-fa52-4d21-ac2e-c7e6251c11a6 
2021-05-17T02:16:36.702-0700 7f7fca811f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T02:16:36.702-0700 7f7fca811f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T02:16:36.702-0700 7f7fca811f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T02:16:36.782-0700 7f7fca811f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T02:16:37.070-0700 7f41686e1f00 -1 Falling back to public interface
2021-05-17T02:16:37.082-0700 7f41686e1f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T02:16:41,021202714-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T02:16:41,030183624-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T02:16:41,113910113-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:16:41,120261673-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '35ce5a22-c6e6-4c77-b392-2a39312bf868', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NO2SUJ:/tmp/ceph-asok.NO2SUJ', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 35ce5a22-c6e6-4c77-b392-2a39312bf868 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NO2SUJ:/tmp/ceph-asok.NO2SUJ -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T02:16:43,811108613-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:16:43,817554989-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '35ce5a22-c6e6-4c77-b392-2a39312bf868', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NO2SUJ:/tmp/ceph-asok.NO2SUJ', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 35ce5a22-c6e6-4c77-b392-2a39312bf868 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NO2SUJ:/tmp/ceph-asok.NO2SUJ -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T02:16:46,806096010-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:16:46,812658611-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '35ce5a22-c6e6-4c77-b392-2a39312bf868', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NO2SUJ:/tmp/ceph-asok.NO2SUJ', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 35ce5a22-c6e6-4c77-b392-2a39312bf868 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NO2SUJ:/tmp/ceph-asok.NO2SUJ -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T02:16:49,603821790-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:16:49,610422844-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '35ce5a22-c6e6-4c77-b392-2a39312bf868', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NO2SUJ:/tmp/ceph-asok.NO2SUJ', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 35ce5a22-c6e6-4c77-b392-2a39312bf868 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NO2SUJ:/tmp/ceph-asok.NO2SUJ -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T02:16:55,405424704-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:16:55,412047691-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '35ce5a22-c6e6-4c77-b392-2a39312bf868', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NO2SUJ:/tmp/ceph-asok.NO2SUJ', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 35ce5a22-c6e6-4c77-b392-2a39312bf868 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NO2SUJ:/tmp/ceph-asok.NO2SUJ -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T02:16:58,960612237-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:16:58,966935458-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '35ce5a22-c6e6-4c77-b392-2a39312bf868', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NO2SUJ:/tmp/ceph-asok.NO2SUJ', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 35ce5a22-c6e6-4c77-b392-2a39312bf868 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NO2SUJ:/tmp/ceph-asok.NO2SUJ -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T02:17:02,490969496-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:17:02,497181728-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '35ce5a22-c6e6-4c77-b392-2a39312bf868', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NO2SUJ:/tmp/ceph-asok.NO2SUJ', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 35ce5a22-c6e6-4c77-b392-2a39312bf868 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NO2SUJ:/tmp/ceph-asok.NO2SUJ -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T02:17:05,770310846-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:17:05,776687086-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '35ce5a22-c6e6-4c77-b392-2a39312bf868', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NO2SUJ:/tmp/ceph-asok.NO2SUJ', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 35ce5a22-c6e6-4c77-b392-2a39312bf868 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NO2SUJ:/tmp/ceph-asok.NO2SUJ -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T02:17:08,935374743-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:17:08,941964875-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '35ce5a22-c6e6-4c77-b392-2a39312bf868', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NO2SUJ:/tmp/ceph-asok.NO2SUJ', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 35ce5a22-c6e6-4c77-b392-2a39312bf868 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NO2SUJ:/tmp/ceph-asok.NO2SUJ -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T02:17:12,035084683-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:17:12,043275918-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '35ce5a22-c6e6-4c77-b392-2a39312bf868', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NO2SUJ:/tmp/ceph-asok.NO2SUJ', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 35ce5a22-c6e6-4c77-b392-2a39312bf868 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NO2SUJ:/tmp/ceph-asok.NO2SUJ -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T02:17:15,476086820-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:17:15,483876811-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '35ce5a22-c6e6-4c77-b392-2a39312bf868', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NO2SUJ:/tmp/ceph-asok.NO2SUJ', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 35ce5a22-c6e6-4c77-b392-2a39312bf868 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NO2SUJ:/tmp/ceph-asok.NO2SUJ -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T02:17:18,248556260-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:17:18,255187018-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '35ce5a22-c6e6-4c77-b392-2a39312bf868', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NO2SUJ:/tmp/ceph-asok.NO2SUJ', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 35ce5a22-c6e6-4c77-b392-2a39312bf868 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NO2SUJ:/tmp/ceph-asok.NO2SUJ -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  153 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  156 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 1.00/1.00  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T02:17:21,081721351-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:17:44,309023276-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   203 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:17:52,165557025-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     191 active+clean
             1   active+clean+scrubbing
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:18:00,146780765-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=======.....................] (remaining: 27s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:18:08,277916179-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [========....................] (remaining: 49s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:18:16,198365693-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [========....................] (remaining: 57s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:18:24,287813187-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [=========...................] (remaining: 71s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:18:32,284219970-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (40s)
      [=========...................] (remaining: 81s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:18:40,333643449-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   235 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [===============.............] (remaining: 41s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:18:48,516974525-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:18:48,535848742-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:18:56,465133458-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:18:56,487179381-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:19:04,427084084-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:19:04,446110021-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:19:12,473123681-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:19:12,492271875-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:19:20,504930872-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:19:20,523845430-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:19:20,539273843-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T02:19:20,548129722-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:19:20,569084824-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=83375
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_write.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-17T02:19:20,586208706-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_write.log.4
[1;30mlocalhost.localdomain	[2021-05-17T02:19:20,628675611-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:19:20,635072780-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '35ce5a22-c6e6-4c77-b392-2a39312bf868', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NO2SUJ:/tmp/ceph-asok.NO2SUJ', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 35ce5a22-c6e6-4c77-b392-2a39312bf868 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NO2SUJ:/tmp/ceph-asok.NO2SUJ -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T09:19:22.210+0000 ffffb0e82010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:19:22.214+0000 ffffb0e82010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:19:22.218+0000 ffffb0e82010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T09:19:22.235232+0000 Maintaining 256 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 20 seconds or 0 objects
2021-05-17T09:19:22.235302+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T09:19:22.442300+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T09:19:22.442300+0000     0       0         0         0         0         0           -           0
2021-05-17T09:19:23.442538+0000     1     255       735       480   479.955       480    0.322735    0.339747
2021-05-17T09:19:24.442761+0000     2     255      1534      1279   639.399       799    0.306864    0.331639
2021-05-17T09:19:25.442984+0000     3     255      2330      2075   691.542       796    0.312341    0.327666
2021-05-17T09:19:26.443189+0000     4     255      3132      2877   719.116       802    0.321191    0.324648
2021-05-17T09:19:27.443373+0000     5     255      3888      3633   726.465       756    0.370629    0.326404
2021-05-17T09:19:28.443568+0000     6     255      4719      4464   743.861       831    0.310244    0.324655
2021-05-17T09:19:29.443804+0000     7     255      5572      5317   759.424       853    0.277272     0.32103
2021-05-17T09:19:30.444036+0000     8     255      6413      6158   769.597       841    0.322741    0.318111
2021-05-17T09:19:31.444285+0000     9     255      7231      6976   774.953       818    0.302871    0.317757
2021-05-17T09:19:32.444540+0000    10     255      8000      7745   774.338       769    0.301039    0.319258
2021-05-17T09:19:33.444759+0000    11     255      8785      8530   775.291       785      0.3207    0.319739
2021-05-17T09:19:34.444972+0000    12     255      9530      9275   772.754       745    0.346045    0.321408
2021-05-17T09:19:35.445213+0000    13     255     10322     10067    774.22       792    0.302354    0.322225
2021-05-17T09:19:36.445437+0000    14     255     11120     10865   775.906       798    0.301859    0.321821
2021-05-17T09:19:37.445674+0000    15     255     11928     11673   778.033       808    0.336223    0.321311
2021-05-17T09:19:38.445911+0000    16     255     12665     12410   775.457       737    0.351959    0.322822
2021-05-17T09:19:39.446125+0000    17     255     13464     13209   776.832       799    0.318671    0.322945
2021-05-17T09:19:40.446327+0000    18     255     14254     13999   777.555       790    0.317894    0.322842
2021-05-17T09:19:41.446552+0000    19     255     14982     14727   774.938       728    0.344222    0.324088
2021-05-17T09:19:42.446817+0000 min lat: 0.0089006 max lat: 0.413786 avg lat: 0.322482
2021-05-17T09:19:42.446817+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T09:19:42.446817+0000    20      32     15759     15727   786.178      1000    0.010698    0.322482
2021-05-17T09:19:43.447172+0000 Total time run:         20.0164
Total writes made:      15759
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     787.303
Stddev Bandwidth:       91.767
Max bandwidth (MB/sec): 1000
Min bandwidth (MB/sec): 480
Average IOPS:           787
Stddev IOPS:            91.767
Max IOPS:               1000
Min IOPS:               480
Average Latency(s):     0.321913
Stddev Latency(s):      0.0301181
Max latency(s):         0.413786
Min latency(s):         0.0089006

[1;32mlocalhost.localdomain	[2021-05-17T02:19:43,785996892-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 83375


[1;33mlocalhost.localdomain	[2021-05-17T02:19:43,795575220-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:20:06,942169584-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 15.76k objects, 15 GiB
    usage:   31 GiB used, 269 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:20:06,961556843-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:20:14,945184765-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 15.76k objects, 15 GiB
    usage:   31 GiB used, 269 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:20:14,964770029-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:20:23,025999527-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 15.76k objects, 15 GiB
    usage:   31 GiB used, 269 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:20:23,045381056-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:20:30,988176780-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 15.76k objects, 15 GiB
    usage:   31 GiB used, 269 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:20:31,007636082-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:20:39,009692537-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 15.76k objects, 15 GiB
    usage:   31 GiB used, 269 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:20:39,029085377-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:20:39,044819737-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T02:20:39,053990460-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:20:39,074726060-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=86747
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_seq.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-17T02:20:39,091872117-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_seq.log.4
[1;30mlocalhost.localdomain	[2021-05-17T02:20:39,132554542-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:20:39,138829095-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '35ce5a22-c6e6-4c77-b392-2a39312bf868', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NO2SUJ:/tmp/ceph-asok.NO2SUJ', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 35ce5a22-c6e6-4c77-b392-2a39312bf868 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.NO2SUJ:/tmp/ceph-asok.NO2SUJ -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T09:20:40.667+0000 ffffa3d35010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:20:40.675+0000 ffffa3d35010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:20:40.675+0000 ffffa3d35010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T09:20:40.698217+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T09:20:40.698217+0000     0       0         0         0         0         0           -           0
2021-05-17T09:20:41.698396+0000     1     255       915       660   659.792       660    0.354827    0.295908
2021-05-17T09:20:42.698632+0000     2     255      1723      1468   733.798       808    0.307268    0.312546
2021-05-17T09:20:43.698912+0000     3     255      2519      2264   754.458       796    0.319985    0.314835
2021-05-17T09:20:44.699483+0000     4     255      3313      3058   764.232       794    0.320116     0.31665
2021-05-17T09:20:45.699726+0000     5     255      4061      3806    760.95       748    0.346587     0.32069
2021-05-17T09:20:46.700056+0000     6     255      4793      4538   756.084       732    0.272254      0.3236
2021-05-17T09:20:47.700368+0000     7     255      5523      5268   752.326       730    0.256566    0.327938
2021-05-17T09:20:48.700743+0000     8     255      6259      6004    750.25       736    0.527094    0.329924
2021-05-17T09:20:49.701214+0000     9     255      6991      6736   748.184       732    0.258769    0.332295
2021-05-17T09:20:50.701865+0000    10     255      7768      7513   751.016       777    0.251161    0.332336
2021-05-17T09:20:51.702084+0000    11     255      8529      8274   751.908       761    0.508763    0.332338
2021-05-17T09:20:52.702380+0000    12     255      9257      9002   749.898       728    0.256582    0.334055
2021-05-17T09:20:53.702926+0000    13     255     10008      9753   749.951       751    0.259361     0.33464
2021-05-17T09:20:54.703160+0000    14     255     10710     10455   746.515       702    0.269403    0.335794
2021-05-17T09:20:55.703507+0000    15     255     11419     11164   743.998       709    0.264091    0.337743
2021-05-17T09:20:56.704368+0000    16     255     12112     11857   740.771       693    0.547911    0.339376
2021-05-17T09:20:57.704653+0000    17     255     12880     12625    742.36       768    0.246161    0.339486
2021-05-17T09:20:58.706026+0000    18     255     13656     13401   744.172       776    0.251329    0.338965
2021-05-17T09:20:59.707176+0000    19     255     14403     14148   744.275       747    0.242427     0.33902
2021-05-17T09:21:00.707574+0000 min lat: 0.157971 max lat: 0.572933 avg lat: 0.338603
2021-05-17T09:21:00.707574+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T09:21:00.707574+0000    20     255     15180     14925   745.896       777    0.246563    0.338603
2021-05-17T09:21:01.707899+0000    21       3     15759     15756   749.935       831    0.247295    0.337374
2021-05-17T09:21:02.708181+0000 Total time run:       21.0153
Total reads made:     15759
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   749.883
Average IOPS:         749
Stddev IOPS:          41.0063
Max IOPS:             831
Min IOPS:             660
Average Latency(s):   0.337355
Max latency(s):       0.572933
Min latency(s):       0.0625793

[1;32mlocalhost.localdomain	[2021-05-17T02:21:03,238419843-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 86747


[1;33mlocalhost.localdomain	[2021-05-17T02:21:03,248024591-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:21:26,206599955-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 15.76k objects, 15 GiB
    usage:   31 GiB used, 269 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:21:26,227495048-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:21:34,260598366-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 15.76k objects, 15 GiB
    usage:   31 GiB used, 269 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:21:34,279797583-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:21:42,166626893-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 15.76k objects, 15 GiB
    usage:   31 GiB used, 269 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:21:42,186081934-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:21:50,296226554-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 15.76k objects, 15 GiB
    usage:   31 GiB used, 269 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:21:50,315380674-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:21:58,612753449-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 15.76k objects, 15 GiB
    usage:   31 GiB used, 269 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:21:58,631980512-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:21:58,647856722-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T02:21:58,654015170-07:00][RUNNING][ROUND 5/5/40] object_size=1MB[0m


[1;33mlocalhost.localdomain	[2021-05-17T02:21:58,662891332-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T02:21:58,680703818-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40601\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.328320\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid c41737a7-5c4e-4d25-962c-b7e7b7fb4ba7\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid c41737a7-5c4e-4d25-962c-b7e7b7fb4ba7\nlast_changed 2021-05-17T02:22:26.448902-0700\ncreated 2021-05-17T02:22:26.448902-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40601/0,v1:10.10.1.2:40602/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.328320 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 b098015e-380d-46c8-91b4-6ba567a40a5a\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 9c976aa8-f87c-47b4-843d-f8b67c01b822\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 6cfaa0a3-cf7f-40d9-b1f7-7536820458a8\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42601\n  w/ user/pass: admin / 1c573a7d-850f-40e3-9ac3-0fbca57891b7\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 02:22:41 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40601
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.328320
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid c41737a7-5c4e-4d25-962c-b7e7b7fb4ba7
setting min_mon_release = octopus
epoch 0
fsid c41737a7-5c4e-4d25-962c-b7e7b7fb4ba7
last_changed 2021-05-17T02:22:26.448902-0700
created 2021-05-17T02:22:26.448902-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40601/0,v1:10.10.1.2:40602/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.328320 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 b098015e-380d-46c8-91b4-6ba567a40a5a
0
start osd.0
add osd1 9c976aa8-f87c-47b4-843d-f8b67c01b822
1
start osd.1
add osd2 6cfaa0a3-cf7f-40d9-b1f7-7536820458a8
2
start osd.2


restful urls: https://10.10.1.2:42601
  w/ user/pass: admin / 1c573a7d-850f-40e3-9ac3-0fbca57891b7


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T02:21:59.681-0700 7f8b044e91c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T02:21:59.681-0700 7f8b044e91c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T02:21:59.697-0700 7f87dad1d1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T02:21:59.697-0700 7f87dad1d1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40601,v1:10.10.1.2:40602] --print /tmp/ceph_monmap.328320 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.328320 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.328320 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42601 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.jF4gIYzuE5 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new b098015e-380d-46c8-91b4-6ba567a40a5a -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDbNaJgyLcEIBAAko6fOG8ZjlOu0DJ4WqQVJQ== --osd-uuid b098015e-380d-46c8-91b4-6ba567a40a5a 
2021-05-17T02:22:35.906-0700 7f5655e2cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T02:22:35.906-0700 7f5655e2cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T02:22:35.906-0700 7f5655e2cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T02:22:35.950-0700 7f5655e2cf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 9c976aa8-f87c-47b4-843d-f8b67c01b822 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T02:22:36.226-0700 7f62424f1f00 -1 Falling back to public interface
2021-05-17T02:22:36.242-0700 7f62424f1f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDcNaJg9XKgDRAAHwbhstx4KkEqpcuZpny9ZQ== --osd-uuid 9c976aa8-f87c-47b4-843d-f8b67c01b822 
2021-05-17T02:22:36.558-0700 7f101ae98f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T02:22:36.558-0700 7f101ae98f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T02:22:36.558-0700 7f101ae98f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T02:22:36.610-0700 7f101ae98f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 6cfaa0a3-cf7f-40d9-b1f7-7536820458a8 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T02:22:36.970-0700 7f3d14b5bf00 -1 Falling back to public interface
2021-05-17T02:22:36.986-0700 7f3d14b5bf00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDcNaJgkQb1ORAA14YIQuDdZVYiJeq88uWaPw== --osd-uuid 6cfaa0a3-cf7f-40d9-b1f7-7536820458a8 
2021-05-17T02:22:37.302-0700 7f69b709ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T02:22:37.302-0700 7f69b709ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T02:22:37.302-0700 7f69b709ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T02:22:37.382-0700 7f69b709ff00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T02:22:37.702-0700 7fe6c2141f00 -1 Falling back to public interface
2021-05-17T02:22:37.714-0700 7fe6c2141f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T02:22:41,686330122-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T02:22:41,695519538-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T02:22:41,779307651-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:22:41,785658102-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'aceebf12-fda6-43cc-b610-780d7f266672', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RugVn1:/tmp/ceph-asok.RugVn1', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid aceebf12-fda6-43cc-b610-780d7f266672 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RugVn1:/tmp/ceph-asok.RugVn1 -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T02:22:44,539120522-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:22:44,545574386-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'aceebf12-fda6-43cc-b610-780d7f266672', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RugVn1:/tmp/ceph-asok.RugVn1', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid aceebf12-fda6-43cc-b610-780d7f266672 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RugVn1:/tmp/ceph-asok.RugVn1 -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T02:22:47,351014800-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:22:47,357618195-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'aceebf12-fda6-43cc-b610-780d7f266672', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RugVn1:/tmp/ceph-asok.RugVn1', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid aceebf12-fda6-43cc-b610-780d7f266672 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RugVn1:/tmp/ceph-asok.RugVn1 -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T02:22:50,235203322-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:22:50,241537330-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'aceebf12-fda6-43cc-b610-780d7f266672', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RugVn1:/tmp/ceph-asok.RugVn1', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid aceebf12-fda6-43cc-b610-780d7f266672 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RugVn1:/tmp/ceph-asok.RugVn1 -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T02:22:55,898590856-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:22:55,905143632-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'aceebf12-fda6-43cc-b610-780d7f266672', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RugVn1:/tmp/ceph-asok.RugVn1', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid aceebf12-fda6-43cc-b610-780d7f266672 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RugVn1:/tmp/ceph-asok.RugVn1 -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T02:22:59,635759104-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:22:59,642159243-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'aceebf12-fda6-43cc-b610-780d7f266672', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RugVn1:/tmp/ceph-asok.RugVn1', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid aceebf12-fda6-43cc-b610-780d7f266672 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RugVn1:/tmp/ceph-asok.RugVn1 -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T02:23:02,987748263-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:23:02,994302824-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'aceebf12-fda6-43cc-b610-780d7f266672', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RugVn1:/tmp/ceph-asok.RugVn1', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid aceebf12-fda6-43cc-b610-780d7f266672 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RugVn1:/tmp/ceph-asok.RugVn1 -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T02:23:06,314811938-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:23:06,321113015-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'aceebf12-fda6-43cc-b610-780d7f266672', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RugVn1:/tmp/ceph-asok.RugVn1', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid aceebf12-fda6-43cc-b610-780d7f266672 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RugVn1:/tmp/ceph-asok.RugVn1 -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T02:23:10,110568435-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:23:10,117046138-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'aceebf12-fda6-43cc-b610-780d7f266672', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RugVn1:/tmp/ceph-asok.RugVn1', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid aceebf12-fda6-43cc-b610-780d7f266672 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RugVn1:/tmp/ceph-asok.RugVn1 -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T02:23:13,964124463-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:23:13,970660960-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'aceebf12-fda6-43cc-b610-780d7f266672', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RugVn1:/tmp/ceph-asok.RugVn1', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid aceebf12-fda6-43cc-b610-780d7f266672 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RugVn1:/tmp/ceph-asok.RugVn1 -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T02:23:17,158283011-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:23:17,164932373-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'aceebf12-fda6-43cc-b610-780d7f266672', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RugVn1:/tmp/ceph-asok.RugVn1', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid aceebf12-fda6-43cc-b610-780d7f266672 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RugVn1:/tmp/ceph-asok.RugVn1 -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T02:23:20,021320690-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:23:20,027932935-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'aceebf12-fda6-43cc-b610-780d7f266672', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RugVn1:/tmp/ceph-asok.RugVn1', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid aceebf12-fda6-43cc-b610-780d7f266672 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RugVn1:/tmp/ceph-asok.RugVn1 -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T02:23:22,729331235-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:23:45,685928285-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   203 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
    Global Recovery Event (4s)
      [=========...................] (remaining: 9s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:23:53,829115756-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:24:02,012557235-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:24:09,978863547-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:24:17,974879898-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:24:25,907441991-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:24:34,193484418-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:24:42,222620984-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:24:42,244684792-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:24:50,115916184-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:24:50,134879532-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:24:58,176246777-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:24:58,195471379-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:25:06,357479769-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:25:06,376614281-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:25:14,438657734-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:25:14,457385811-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:25:14,473078990-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T02:25:14,482115066-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:25:14,503064618-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=100246
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_write.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-17T02:25:14,519978796-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_write.log.5
[1;30mlocalhost.localdomain	[2021-05-17T02:25:14,561894644-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:25:14,568730548-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'aceebf12-fda6-43cc-b610-780d7f266672', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RugVn1:/tmp/ceph-asok.RugVn1', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '1048576', '-O', '1048576', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid aceebf12-fda6-43cc-b610-780d7f266672 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RugVn1:/tmp/ceph-asok.RugVn1 -- rados bench 20 write --pool bench_rados -b 1048576 -O 1048576 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T09:25:16.090+0000 ffffae7ae010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:25:16.102+0000 ffffae7ae010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:25:16.102+0000 ffffae7ae010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T09:25:16.121218+0000 Maintaining 256 concurrent writes of 1048576 bytes to objects of size 1048576 for up to 20 seconds or 0 objects
2021-05-17T09:25:16.121301+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T09:25:16.341226+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T09:25:16.341226+0000     0       0         0         0         0         0           -           0
2021-05-17T09:25:17.341481+0000     1     255       757       502   501.946       502    0.310185    0.334295
2021-05-17T09:25:18.341702+0000     2     255      1536      1281   640.395       779    0.322072    0.330546
2021-05-17T09:25:19.341936+0000     3     255      2334      2079    692.87       798    0.319581    0.328311
2021-05-17T09:25:20.342163+0000     4     255      3069      2814   703.361       735    0.322161    0.332615
2021-05-17T09:25:21.342375+0000     5     255      3830      3575   714.857       761    0.331229    0.332933
2021-05-17T09:25:22.342590+0000     6     255      4559      4304   717.188       729    0.343896      0.3362
2021-05-17T09:25:23.342841+0000     7     255      5254      4999   713.993       695    0.424199    0.338691
2021-05-17T09:25:24.344197+0000     8     255      5942      5687   710.624       688    0.365644    0.343519
2021-05-17T09:25:25.344494+0000     9     255      6703      6448   716.196       761    0.319432    0.343828
2021-05-17T09:25:26.344777+0000    10     255      7429      7174   717.156       726    0.352324    0.343854
2021-05-17T09:25:27.345064+0000    11     255      8181      7926   720.304       752    0.327688    0.344254
2021-05-17T09:25:28.345337+0000    12     255      8921      8666   721.928       740    0.358314    0.344098
2021-05-17T09:25:29.345617+0000    13     255      9603      9348   718.842       682    0.349655    0.346232
2021-05-17T09:25:30.345836+0000    14     255     10330     10075   719.414       727     0.37452    0.346438
2021-05-17T09:25:31.346111+0000    15     255     11074     10819   721.039       744    0.324988    0.347101
2021-05-17T09:25:32.346455+0000    16     255     11805     11550   721.646       731    0.299389    0.347239
2021-05-17T09:25:33.346773+0000    17     255     12561     12306   723.653       756     0.32421    0.346571
2021-05-17T09:25:34.347035+0000    18     255     13233     12978   720.773       672     0.34728    0.348092
2021-05-17T09:25:35.347347+0000    19     255     13942     13687   720.142       709    0.367981     0.34875
2021-05-17T09:25:36.347582+0000 min lat: 0.0202947 max lat: 0.463599 avg lat: 0.346965
2021-05-17T09:25:36.347582+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T09:25:36.347582+0000    20      21     14647     14626   731.073       939    0.024277    0.346965
2021-05-17T09:25:37.347942+0000 Total time run:         20.0184
Total writes made:      14647
Write size:             1048576
Object size:            1048576
Bandwidth (MB/sec):     731.676
Stddev Bandwidth:       77.8609
Max bandwidth (MB/sec): 939
Min bandwidth (MB/sec): 502
Average IOPS:           731
Stddev IOPS:            77.8609
Max IOPS:               939
Min IOPS:               502
Average Latency(s):     0.34654
Stddev Latency(s):      0.0346892
Max latency(s):         0.463599
Min latency(s):         0.011814

[1;32mlocalhost.localdomain	[2021-05-17T02:25:37,714813216-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 100246


[1;33mlocalhost.localdomain	[2021-05-17T02:25:37,724245149-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:26:00,813063597-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.65k objects, 14 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:26:00,832185447-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:26:08,900547642-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.65k objects, 14 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:26:08,920183045-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:26:16,937201081-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.65k objects, 14 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:26:16,956532652-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:26:25,090590827-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.65k objects, 14 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:26:25,110106810-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:26:33,367020868-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.65k objects, 14 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:26:33,386493014-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:26:33,402341874-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T02:26:33,411373929-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:26:33,432354732-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=103574
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_1MB_seq.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-17T02:26:33,449609498-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_1MB_seq.log.5
[1;30mlocalhost.localdomain	[2021-05-17T02:26:33,489953753-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:26:33,496463306-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'aceebf12-fda6-43cc-b610-780d7f266672', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RugVn1:/tmp/ceph-asok.RugVn1', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid aceebf12-fda6-43cc-b610-780d7f266672 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.RugVn1:/tmp/ceph-asok.RugVn1 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T09:26:35.012+0000 ffffb491a010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:26:35.020+0000 ffffb491a010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:26:35.020+0000 ffffb491a010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T09:26:35.039613+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T09:26:35.039613+0000     0       0         0         0         0         0           -           0
2021-05-17T09:26:36.039970+0000     1     255       900       645   644.676       645    0.338674    0.306945
2021-05-17T09:26:37.040473+0000     2     255      1648      1393    696.15       748     0.34286    0.325379
2021-05-17T09:26:38.040686+0000     3     255      2387      2132   710.378       739     0.36432    0.331516
2021-05-17T09:26:39.041757+0000     4     255      3166      2911   727.334       779    0.327695    0.332264
2021-05-17T09:26:40.041945+0000     5     255      3918      3663   732.237       752    0.333768    0.333704
2021-05-17T09:26:41.042146+0000     6     255      4659      4404   733.672       741    0.346695    0.335218
2021-05-17T09:26:42.042439+0000     7     255      5370      5115   730.404       711    0.342937    0.338608
2021-05-17T09:26:43.042750+0000     8     255      6127      5872   733.699       757    0.334255    0.338748
2021-05-17T09:26:44.043164+0000     9     255      6885      6630   736.364       758     0.34425    0.338351
2021-05-17T09:26:45.043376+0000    10     255      7608      7353   735.013       723    0.374186    0.339314
2021-05-17T09:26:46.043608+0000    11     255      8351      8096   735.723       743    0.346222    0.340275
2021-05-17T09:26:47.043863+0000    12     255      9122      8867   738.646       771     0.32762    0.339715
2021-05-17T09:26:48.044129+0000    13     255      9881      9626   740.196       759    0.335927    0.339441
2021-05-17T09:26:49.044498+0000    14     255     10633     10378   741.019       752    0.515613    0.338674
2021-05-17T09:26:50.045168+0000    15     255     11380     11125   741.385       747    0.248665    0.338976
2021-05-17T09:26:51.045388+0000    16     255     12114     11859   740.913       734    0.252976    0.339597
2021-05-17T09:26:52.049371+0000    17     255     12860     12605   741.039       746     0.26788    0.339752
2021-05-17T09:26:53.049594+0000    18     255     13622     13367   742.194       762    0.504448    0.339686
2021-05-17T09:26:54.050714+0000    19     255     14393     14138   743.665       771    0.346371    0.339549
2021-05-17T09:26:55.050992+0000 Total time run:       19.5488
Total reads made:     14647
Read size:            1048576
Object size:          1048576
Bandwidth (MB/sec):   749.254
Average IOPS:         749
Stddev IOPS:          29.0782
Max IOPS:             779
Min IOPS:             645
Average Latency(s):   0.338025
Max latency(s):       0.680112
Min latency(s):       0.112248

[1;32mlocalhost.localdomain	[2021-05-17T02:26:55,471956186-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 103574


[1;33mlocalhost.localdomain	[2021-05-17T02:26:55,482880304-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:27:18,450699275-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.65k objects, 14 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:27:18,470202281-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:27:26,466285089-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.65k objects, 14 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:27:26,485572387-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:27:34,505097581-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.65k objects, 14 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:27:34,524818274-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:27:42,566324213-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.65k objects, 14 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:27:42,588367738-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:27:50,572008367-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 14.65k objects, 14 GiB
    usage:   29 GiB used, 271 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:27:50,591096176-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:27:50,607109180-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T02:27:50,616694649-07:00][RUNNING][ROUND 1/6/40] object_size=4MB[0m


[1;33mlocalhost.localdomain	[2021-05-17T02:27:50,626119486-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T02:27:50,644106093-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40254\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.329425\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 0dbfadbd-3e7f-4078-9ba6-c57f01d284fa\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 0dbfadbd-3e7f-4078-9ba6-c57f01d284fa\nlast_changed 2021-05-17T02:28:26.597721-0700\ncreated 2021-05-17T02:28:26.597721-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40254/0,v1:10.10.1.2:40255/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.329425 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 bcb72969-d85d-45d2-a408-a0ed75631282\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 5f2ec833-2ad3-41a7-8670-c8e6a3a4a98c\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 275b9ade-8b16-4249-a637-506e16e7220f\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42254\n  w/ user/pass: admin / 5e6df9b2-e6bd-404c-a796-2786f84e6dfe\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 02:28:41 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40254
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.329425
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 0dbfadbd-3e7f-4078-9ba6-c57f01d284fa
setting min_mon_release = octopus
epoch 0
fsid 0dbfadbd-3e7f-4078-9ba6-c57f01d284fa
last_changed 2021-05-17T02:28:26.597721-0700
created 2021-05-17T02:28:26.597721-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40254/0,v1:10.10.1.2:40255/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.329425 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 bcb72969-d85d-45d2-a408-a0ed75631282
0
start osd.0
add osd1 5f2ec833-2ad3-41a7-8670-c8e6a3a4a98c
1
start osd.1
add osd2 275b9ade-8b16-4249-a637-506e16e7220f
2
start osd.2


restful urls: https://10.10.1.2:42254
  w/ user/pass: admin / 5e6df9b2-e6bd-404c-a796-2786f84e6dfe


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T02:27:51.649-0700 7f19cb5651c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T02:27:51.649-0700 7f19cb5651c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T02:27:51.665-0700 7fbf91f401c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T02:27:51.665-0700 7fbf91f401c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40254,v1:10.10.1.2:40255] --print /tmp/ceph_monmap.329425 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.329425 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.329425 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42254 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.ch86dzxW3w 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new bcb72969-d85d-45d2-a408-a0ed75631282 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBDN6Jg/oneIRAAeT3du7OD889Zbq/SiD0gzA== --osd-uuid bcb72969-d85d-45d2-a408-a0ed75631282 
2021-05-17T02:28:35.917-0700 7fd53e613f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T02:28:35.917-0700 7fd53e613f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T02:28:35.917-0700 7fd53e613f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T02:28:35.973-0700 7fd53e613f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 5f2ec833-2ad3-41a7-8670-c8e6a3a4a98c -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T02:28:36.257-0700 7f1e82c6df00 -1 Falling back to public interface
2021-05-17T02:28:36.269-0700 7f1e82c6df00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBEN6Jg3jyADxAAy38p9s0Y+BlsHcQFXrNniw== --osd-uuid 5f2ec833-2ad3-41a7-8670-c8e6a3a4a98c 
2021-05-17T02:28:36.597-0700 7fda146c1f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T02:28:36.597-0700 7fda146c1f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T02:28:36.597-0700 7fda146c1f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T02:28:36.649-0700 7fda146c1f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 275b9ade-8b16-4249-a637-506e16e7220f -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T02:28:37.037-0700 7f784afd4f00 -1 Falling back to public interface
2021-05-17T02:28:37.049-0700 7f784afd4f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBFN6JgqoU/AhAAwgOGDDiZ9ED+tZOL7Utgww== --osd-uuid 275b9ade-8b16-4249-a637-506e16e7220f 
2021-05-17T02:28:37.377-0700 7f01405aff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T02:28:37.377-0700 7f01405aff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T02:28:37.377-0700 7f01405aff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T02:28:37.445-0700 7f01405aff00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T02:28:37.741-0700 7f46dc7c8f00 -1 Falling back to public interface
2021-05-17T02:28:37.753-0700 7f46dc7c8f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T02:28:41,772117029-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T02:28:41,781814178-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T02:28:41,866424209-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:28:41,872517502-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bc2effa5-d49e-4224-99c4-e8d7d4a9042c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cpnpP5:/tmp/ceph-asok.cpnpP5', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bc2effa5-d49e-4224-99c4-e8d7d4a9042c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cpnpP5:/tmp/ceph-asok.cpnpP5 -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T02:28:44,743280108-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:28:44,749923619-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bc2effa5-d49e-4224-99c4-e8d7d4a9042c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cpnpP5:/tmp/ceph-asok.cpnpP5', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bc2effa5-d49e-4224-99c4-e8d7d4a9042c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cpnpP5:/tmp/ceph-asok.cpnpP5 -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T02:28:47,649143214-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:28:47,655331367-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bc2effa5-d49e-4224-99c4-e8d7d4a9042c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cpnpP5:/tmp/ceph-asok.cpnpP5', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bc2effa5-d49e-4224-99c4-e8d7d4a9042c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cpnpP5:/tmp/ceph-asok.cpnpP5 -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T02:28:50,477909454-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:28:50,484398769-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bc2effa5-d49e-4224-99c4-e8d7d4a9042c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cpnpP5:/tmp/ceph-asok.cpnpP5', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bc2effa5-d49e-4224-99c4-e8d7d4a9042c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cpnpP5:/tmp/ceph-asok.cpnpP5 -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T02:28:56,167397626-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:28:56,173827576-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bc2effa5-d49e-4224-99c4-e8d7d4a9042c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cpnpP5:/tmp/ceph-asok.cpnpP5', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bc2effa5-d49e-4224-99c4-e8d7d4a9042c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cpnpP5:/tmp/ceph-asok.cpnpP5 -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T02:28:59,153049256-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:28:59,159637138-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bc2effa5-d49e-4224-99c4-e8d7d4a9042c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cpnpP5:/tmp/ceph-asok.cpnpP5', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bc2effa5-d49e-4224-99c4-e8d7d4a9042c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cpnpP5:/tmp/ceph-asok.cpnpP5 -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T02:29:02,490480905-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:29:02,497074848-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bc2effa5-d49e-4224-99c4-e8d7d4a9042c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cpnpP5:/tmp/ceph-asok.cpnpP5', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bc2effa5-d49e-4224-99c4-e8d7d4a9042c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cpnpP5:/tmp/ceph-asok.cpnpP5 -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T02:29:06,117635179-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:29:06,123922754-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bc2effa5-d49e-4224-99c4-e8d7d4a9042c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cpnpP5:/tmp/ceph-asok.cpnpP5', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bc2effa5-d49e-4224-99c4-e8d7d4a9042c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cpnpP5:/tmp/ceph-asok.cpnpP5 -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T02:29:09,472620402-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:29:09,479422745-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bc2effa5-d49e-4224-99c4-e8d7d4a9042c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cpnpP5:/tmp/ceph-asok.cpnpP5', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bc2effa5-d49e-4224-99c4-e8d7d4a9042c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cpnpP5:/tmp/ceph-asok.cpnpP5 -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T02:29:12,571711328-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:29:12,578108197-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bc2effa5-d49e-4224-99c4-e8d7d4a9042c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cpnpP5:/tmp/ceph-asok.cpnpP5', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bc2effa5-d49e-4224-99c4-e8d7d4a9042c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cpnpP5:/tmp/ceph-asok.cpnpP5 -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T02:29:16,067645196-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:29:16,074216770-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bc2effa5-d49e-4224-99c4-e8d7d4a9042c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cpnpP5:/tmp/ceph-asok.cpnpP5', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bc2effa5-d49e-4224-99c4-e8d7d4a9042c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cpnpP5:/tmp/ceph-asok.cpnpP5 -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T02:29:19,058626432-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:29:19,064954274-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bc2effa5-d49e-4224-99c4-e8d7d4a9042c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cpnpP5:/tmp/ceph-asok.cpnpP5', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bc2effa5-d49e-4224-99c4-e8d7d4a9042c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cpnpP5:/tmp/ceph-asok.cpnpP5 -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   47 KiB   0 B   0 B   0 B  100 GiB     0  0.95   70      up          osd.2  
                       TOTAL  300 GiB  151 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.95/1.03  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T02:29:21,746006531-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:29:44,750877330-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   192 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:29:52,873206610-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   209 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:30:00,968223742-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   209 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (9s)
      [=======.....................] (remaining: 28s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:30:08,964877520-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   209 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [=======.....................] (remaining: 52s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:30:17,088192129-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   209 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [========....................] (remaining: 62s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:30:25,170113700-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   209 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [========....................] (remaining: 75s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:30:33,532625013-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   209 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (45s)
      [=========...................] (remaining: 92s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:30:41,494794433-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [===============.............] (remaining: 41s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:30:49,599486455-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:30:49,618567007-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:30:57,630695971-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:30:57,649624518-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:31:05,762725129-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:31:05,781963914-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:31:14,006642490-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:31:14,025962934-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:31:22,017049119-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:31:22,036585736-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:31:22,052655131-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T02:31:22,061963004-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:31:22,083476294-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=117634
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4MB_write.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-17T02:31:22,101001746-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4MB_write.log.1
[1;30mlocalhost.localdomain	[2021-05-17T02:31:22,143680807-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:31:22,150136443-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bc2effa5-d49e-4224-99c4-e8d7d4a9042c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cpnpP5:/tmp/ceph-asok.cpnpP5', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bc2effa5-d49e-4224-99c4-e8d7d4a9042c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cpnpP5:/tmp/ceph-asok.cpnpP5 -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T09:31:23.701+0000 ffffb51a0010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:31:23.709+0000 ffffb51a0010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:31:23.709+0000 ffffb51a0010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T09:31:23.728079+0000 Maintaining 256 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 20 seconds or 0 objects
2021-05-17T09:31:23.728180+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T09:31:24.696634+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T09:31:24.696634+0000     0       0         0         0         0         0           -           0
2021-05-17T09:31:25.697005+0000     1     149       149         0         0         0           -           0
2021-05-17T09:31:26.697326+0000     2     255       290        35   69.9818        70     1.77702     1.78974
2021-05-17T09:31:27.697686+0000     3     255       434       179   238.597       576      1.7846     1.78362
2021-05-17T09:31:28.698048+0000     4     255       592       337   336.897       632     1.70946     1.76171
2021-05-17T09:31:29.698427+0000     5     255       739       484   387.074       588     1.69127     1.73482
2021-05-17T09:31:30.698857+0000     6     255       892       637   424.521       612     1.68511     1.72682
2021-05-17T09:31:31.699180+0000     7     255      1041       786    448.99       596     1.69509     1.72366
2021-05-17T09:31:32.699396+0000     8     255      1194       939   469.348       612     1.68252     1.72101
2021-05-17T09:31:33.699675+0000     9     255      1348      1093   485.623       616     1.64624     1.71388
2021-05-17T09:31:34.699936+0000    10     255      1498      1243   497.044       600     1.67463     1.70638
2021-05-17T09:31:35.700234+0000    11     255      1649      1394   506.751       604     1.72194     1.70493
2021-05-17T09:31:36.700640+0000    12     255      1789      1534    511.17       560     1.79373     1.71031
2021-05-17T09:31:37.701071+0000    13     255      1950      1695   521.367       644     1.65075     1.71141
2021-05-17T09:31:38.701484+0000    14     255      2101      1846   527.252       604     1.65105     1.70542
2021-05-17T09:31:39.701725+0000    15     255      2256      2001   533.425       620     1.67455     1.70234
2021-05-17T09:31:40.701949+0000    16     255      2399      2144   535.828       572     1.72877     1.70294
2021-05-17T09:31:41.702167+0000    17     255      2558      2303   541.711       636     1.66046     1.70411
2021-05-17T09:31:42.702369+0000    18     255      2712      2457   545.831       616     1.63459     1.70058
2021-05-17T09:31:43.703199+0000    19     255      2862      2607   548.657       600     1.70614     1.69797
2021-05-17T09:31:44.703515+0000 min lat: 0.0509607 max lat: 1.8159 avg lat: 1.62946
2021-05-17T09:31:44.703515+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T09:31:44.703515+0000    20       8      3015      3007   601.198      1600   0.0509607     1.62946
2021-05-17T09:31:45.703838+0000 Total time run:         20.0365
Total writes made:      3015
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     601.9
Stddev Bandwidth:       294.638
Max bandwidth (MB/sec): 1600
Min bandwidth (MB/sec): 0
Average IOPS:           150
Stddev IOPS:            73.7067
Max IOPS:               400
Min IOPS:               0
Average Latency(s):     1.62528
Stddev Latency(s):      0.279651
Max latency(s):         1.8159
Min latency(s):         0.0382556

[1;32mlocalhost.localdomain	[2021-05-17T02:31:46,138667404-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 117634


[1;33mlocalhost.localdomain	[2021-05-17T02:31:46,148652687-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:32:09,272120202-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.02k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:32:09,292081153-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:32:17,257670039-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.02k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:32:17,277643128-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:32:25,230604896-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.02k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:32:25,250871585-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:32:33,406746070-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.02k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:32:33,426877542-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:32:41,599193426-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.02k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:32:41,619625384-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:32:41,635856256-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T02:32:41,645257717-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:32:41,666654083-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=121205
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4MB_seq.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-17T02:32:41,684585268-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4MB_seq.log.1
[1;30mlocalhost.localdomain	[2021-05-17T02:32:41,725606455-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:32:41,732038283-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'bc2effa5-d49e-4224-99c4-e8d7d4a9042c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cpnpP5:/tmp/ceph-asok.cpnpP5', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid bc2effa5-d49e-4224-99c4-e8d7d4a9042c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.cpnpP5:/tmp/ceph-asok.cpnpP5 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T09:32:43.222+0000 ffffa5141010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:32:43.230+0000 ffffa5141010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:32:43.230+0000 ffffa5141010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T09:32:43.255130+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T09:32:43.255130+0000     0       0         0         0         0         0           -           0
2021-05-17T09:32:44.255345+0000     1     255       260         5   19.9925        20    0.991949    0.984224
2021-05-17T09:32:45.255585+0000     2     255       444       189   377.883       736     1.31021     1.18018
2021-05-17T09:32:46.256432+0000     3     255       632       377   502.421       752     1.36452     1.27337
2021-05-17T09:32:47.256714+0000     4     255       814       559   558.756       728     1.39158      1.3078
2021-05-17T09:32:48.256934+0000     5     255       993       738   590.168       716     1.41713     1.33118
2021-05-17T09:32:49.257242+0000     6     255      1171       916   610.435       712     1.44404     1.35379
2021-05-17T09:32:50.257483+0000     7     255      1349      1094   624.918       712     1.41352     1.36431
2021-05-17T09:32:51.257687+0000     8     255      1525      1270   634.784       704     1.46265     1.37733
2021-05-17T09:32:52.257915+0000     9     255      1702      1447   642.901       708     1.42755     1.38594
2021-05-17T09:32:53.258134+0000    10     256      1884      1628   650.994       724     1.40109     1.39072
2021-05-17T09:32:54.258327+0000    11     255      2063      1808   657.254       720     1.41575     1.39236
2021-05-17T09:32:55.259753+0000    12     255      2241      1986   661.736       712     1.43012     1.39643
2021-05-17T09:32:56.260080+0000    13     255      2424      2169   667.122       732      1.3941     1.39909
2021-05-17T09:32:57.262295+0000    14     255      2606      2351   671.363       728     1.38965     1.39784
2021-05-17T09:32:58.263822+0000    15     255      2791      2536   675.868       740     1.40737     1.39773
2021-05-17T09:32:59.266561+0000    16     255      2976      2721   679.757       740      1.3896     1.39753
2021-05-17T09:33:00.266826+0000 Total time run:       16.681
Total reads made:     3015
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   722.977
Average IOPS:         180
Stddev IOPS:          44.1444
Max IOPS:             188
Min IOPS:             5
Average Latency(s):   1.35908
Max latency(s):       1.47676
Min latency(s):       0.470459

[1;32mlocalhost.localdomain	[2021-05-17T02:33:00,744985074-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 121205


[1;33mlocalhost.localdomain	[2021-05-17T02:33:00,755002915-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:33:23,707914507-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.02k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:33:23,727918670-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:33:31,946924526-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.02k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:33:31,966924788-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:33:39,989549378-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.02k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:33:40,009571800-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:33:48,157960535-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.02k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:33:48,177771227-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:33:56,245083536-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.02k objects, 12 GiB
    usage:   24 GiB used, 276 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:33:56,267058765-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:33:56,289279091-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T02:33:56,296507855-07:00][RUNNING][ROUND 2/6/40] object_size=4MB[0m


[1;33mlocalhost.localdomain	[2021-05-17T02:33:56,307090664-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T02:33:56,326853708-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40609\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.330534\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 8b33add1-3149-403a-a2f5-ca07868d3918\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 8b33add1-3149-403a-a2f5-ca07868d3918\nlast_changed 2021-05-17T02:34:25.763463-0700\ncreated 2021-05-17T02:34:25.763463-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40609/0,v1:10.10.1.2:40610/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.330534 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 800a8227-a8d4-4b5d-be7a-30bb47f86ce2\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 468080f6-2b91-4f8b-a8d6-41ab06d88cbd\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 1e0d0d43-a02c-4909-b24d-b06db83ea04d\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42609\n'
10.10.1.2: b'  w/ user/pass: admin / b5d3f3d4-78ea-4909-9b3a-23a6c4f5ff7f\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 02:34:41 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40609
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.330534
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 8b33add1-3149-403a-a2f5-ca07868d3918
setting min_mon_release = octopus
epoch 0
fsid 8b33add1-3149-403a-a2f5-ca07868d3918
last_changed 2021-05-17T02:34:25.763463-0700
created 2021-05-17T02:34:25.763463-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40609/0,v1:10.10.1.2:40610/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.330534 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 800a8227-a8d4-4b5d-be7a-30bb47f86ce2
0
start osd.0
add osd1 468080f6-2b91-4f8b-a8d6-41ab06d88cbd
1
start osd.1
add osd2 1e0d0d43-a02c-4909-b24d-b06db83ea04d
2
start osd.2


restful urls: https://10.10.1.2:42609
  w/ user/pass: admin / b5d3f3d4-78ea-4909-9b3a-23a6c4f5ff7f


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T02:33:57.356-0700 7fe014aae1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T02:33:57.356-0700 7fe014aae1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T02:33:57.376-0700 7fe96726d1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T02:33:57.376-0700 7fe96726d1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40609,v1:10.10.1.2:40610] --print /tmp/ceph_monmap.330534 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.330534 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.330534 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42609 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.a12MnpJLZW 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 800a8227-a8d4-4b5d-be7a-30bb47f86ce2 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCrOKJgeDIsIRAASXnR/LnPX2FDozHe9RzslQ== --osd-uuid 800a8227-a8d4-4b5d-be7a-30bb47f86ce2 
2021-05-17T02:34:35.888-0700 7f39566fef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T02:34:35.888-0700 7f39566fef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T02:34:35.888-0700 7f39566fef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T02:34:35.940-0700 7f39566fef00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 468080f6-2b91-4f8b-a8d6-41ab06d88cbd -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T02:34:36.252-0700 7ffb6d9fef00 -1 Falling back to public interface
2021-05-17T02:34:36.264-0700 7ffb6d9fef00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCsOKJgYf81DxAAr0jFvFn2kaG8HA2Oz9gvmQ== --osd-uuid 468080f6-2b91-4f8b-a8d6-41ab06d88cbd 
2021-05-17T02:34:36.608-0700 7f1000896f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T02:34:36.608-0700 7f1000896f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T02:34:36.608-0700 7f1000896f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T02:34:36.744-0700 7f1000896f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 1e0d0d43-a02c-4909-b24d-b06db83ea04d -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T02:34:37.140-0700 7fb302e33f00 -1 Falling back to public interface
2021-05-17T02:34:37.152-0700 7fb302e33f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCtOKJgi0t7CBAAu4s3H89aSHwMfWH75fmu9w== --osd-uuid 1e0d0d43-a02c-4909-b24d-b06db83ea04d 
2021-05-17T02:34:37.536-0700 7f0d4118ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T02:34:37.536-0700 7f0d4118ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T02:34:37.536-0700 7f0d4118ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T02:34:37.600-0700 7f0d4118ef00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T02:34:37.892-0700 7f3835133f00 -1 Falling back to public interface
2021-05-17T02:34:37.904-0700 7f3835133f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T02:34:41,917120708-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T02:34:41,926641968-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T02:34:42,013788535-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:34:42,020349643-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55bf2687-1b36-4e73-bfcb-fd62d704deb0', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.WclV4p:/tmp/ceph-asok.WclV4p', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55bf2687-1b36-4e73-bfcb-fd62d704deb0 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.WclV4p:/tmp/ceph-asok.WclV4p -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T02:34:44,856073078-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:34:44,862672592-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55bf2687-1b36-4e73-bfcb-fd62d704deb0', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.WclV4p:/tmp/ceph-asok.WclV4p', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55bf2687-1b36-4e73-bfcb-fd62d704deb0 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.WclV4p:/tmp/ceph-asok.WclV4p -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T02:34:47,664412283-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:34:47,670884636-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55bf2687-1b36-4e73-bfcb-fd62d704deb0', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.WclV4p:/tmp/ceph-asok.WclV4p', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55bf2687-1b36-4e73-bfcb-fd62d704deb0 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.WclV4p:/tmp/ceph-asok.WclV4p -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T02:34:50,616757975-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:34:50,623259867-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55bf2687-1b36-4e73-bfcb-fd62d704deb0', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.WclV4p:/tmp/ceph-asok.WclV4p', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55bf2687-1b36-4e73-bfcb-fd62d704deb0 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.WclV4p:/tmp/ceph-asok.WclV4p -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T02:34:56,170834479-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:34:56,177330431-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55bf2687-1b36-4e73-bfcb-fd62d704deb0', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.WclV4p:/tmp/ceph-asok.WclV4p', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55bf2687-1b36-4e73-bfcb-fd62d704deb0 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.WclV4p:/tmp/ceph-asok.WclV4p -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T02:34:59,628372314-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:34:59,634849588-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55bf2687-1b36-4e73-bfcb-fd62d704deb0', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.WclV4p:/tmp/ceph-asok.WclV4p', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55bf2687-1b36-4e73-bfcb-fd62d704deb0 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.WclV4p:/tmp/ceph-asok.WclV4p -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T02:35:03,329998279-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:35:03,336729846-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55bf2687-1b36-4e73-bfcb-fd62d704deb0', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.WclV4p:/tmp/ceph-asok.WclV4p', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55bf2687-1b36-4e73-bfcb-fd62d704deb0 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.WclV4p:/tmp/ceph-asok.WclV4p -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T02:35:06,648506689-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:35:06,654808703-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55bf2687-1b36-4e73-bfcb-fd62d704deb0', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.WclV4p:/tmp/ceph-asok.WclV4p', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55bf2687-1b36-4e73-bfcb-fd62d704deb0 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.WclV4p:/tmp/ceph-asok.WclV4p -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T02:35:10,092584550-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:35:10,098911933-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55bf2687-1b36-4e73-bfcb-fd62d704deb0', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.WclV4p:/tmp/ceph-asok.WclV4p', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55bf2687-1b36-4e73-bfcb-fd62d704deb0 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.WclV4p:/tmp/ceph-asok.WclV4p -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T02:35:13,618409701-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:35:13,624935808-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55bf2687-1b36-4e73-bfcb-fd62d704deb0', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.WclV4p:/tmp/ceph-asok.WclV4p', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55bf2687-1b36-4e73-bfcb-fd62d704deb0 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.WclV4p:/tmp/ceph-asok.WclV4p -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T02:35:16,879134176-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:35:16,885963980-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55bf2687-1b36-4e73-bfcb-fd62d704deb0', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.WclV4p:/tmp/ceph-asok.WclV4p', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55bf2687-1b36-4e73-bfcb-fd62d704deb0 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.WclV4p:/tmp/ceph-asok.WclV4p -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T02:35:19,557406070-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:35:19,563735808-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55bf2687-1b36-4e73-bfcb-fd62d704deb0', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.WclV4p:/tmp/ceph-asok.WclV4p', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55bf2687-1b36-4e73-bfcb-fd62d704deb0 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.WclV4p:/tmp/ceph-asok.WclV4p -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  144 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  144 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.06   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   45 KiB   0 B   0 B   0 B  100 GiB     0  0.94   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  1.00   70      up          osd.2  
                       TOTAL  300 GiB  145 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.94/1.06  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T02:35:22,374005452-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:35:45,455647211-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   199 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (4s)
      [============................] (remaining: 6s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:35:53,588816997-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:36:01,598757595-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:36:09,472318124-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:36:17,397520861-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:36:25,684156049-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:36:33,548436907-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:36:41,542448457-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   238 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:36:41,562206558-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:36:49,528153894-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:36:49,547486883-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:36:57,789408510-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:36:57,809294867-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:37:05,733372644-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:37:05,756669231-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:37:13,824483240-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:37:13,844351656-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:37:13,860448085-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T02:37:13,869842201-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:37:13,891506782-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=134634
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4MB_write.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-17T02:37:13,909207569-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4MB_write.log.2
[1;30mlocalhost.localdomain	[2021-05-17T02:37:13,951366876-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:37:13,957898731-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55bf2687-1b36-4e73-bfcb-fd62d704deb0', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.WclV4p:/tmp/ceph-asok.WclV4p', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55bf2687-1b36-4e73-bfcb-fd62d704deb0 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.WclV4p:/tmp/ceph-asok.WclV4p -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T09:37:15.468+0000 ffffa8bd5010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:37:15.476+0000 ffffa8bd5010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:37:15.476+0000 ffffa8bd5010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T09:37:15.495639+0000 Maintaining 256 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 20 seconds or 0 objects
2021-05-17T09:37:15.495743+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T09:37:16.479820+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T09:37:16.479820+0000     0       0         0         0         0         0           -           0
2021-05-17T09:37:17.480068+0000     1     152       152         0         0         0           -           0
2021-05-17T09:37:18.480285+0000     2     255       321        66    131.98       132      1.5705      1.6047
2021-05-17T09:37:19.480526+0000     3     255       486       231   307.945       660     1.55285     1.57429
2021-05-17T09:37:20.480762+0000     4     255       646       391   390.924       640     1.56477     1.57248
2021-05-17T09:37:21.481040+0000     5     255       819       564   451.105       692     1.48204     1.55721
2021-05-17T09:37:22.481293+0000     6     255       978       723   481.895       636     1.57757     1.54729
2021-05-17T09:37:23.481687+0000     7     255      1140       885   505.592       648     1.60611       1.559
2021-05-17T09:37:24.482158+0000     8     255      1312      1057   528.357       688     1.50456     1.55704
2021-05-17T09:37:25.482484+0000     9     255      1473      1218   541.183       644     1.54288     1.55156
2021-05-17T09:37:26.482701+0000    10     255      1637      1382    552.65       656     1.58107     1.55286
2021-05-17T09:37:27.482983+0000    11     255      1799      1544   561.302       648     1.60567     1.55764
2021-05-17T09:37:28.483233+0000    12     255      1964      1709   569.513       660     1.55765     1.56044
2021-05-17T09:37:29.483584+0000    13     255      2138      1883   579.224       696     1.46856     1.55604
2021-05-17T09:37:30.484007+0000    14     255      2299      2044   583.832       644     1.56466     1.55288
2021-05-17T09:37:31.484349+0000    15     255      2456      2201   586.763       628      1.5946     1.55599
2021-05-17T09:37:32.484637+0000    16     255      2622      2367   591.578       664      1.5786     1.55823
2021-05-17T09:37:33.484921+0000    17     255      2790      2535   596.298       672     1.54639     1.55687
2021-05-17T09:37:34.485290+0000    18     255      2943      2688   597.157       612     1.61786     1.55933
2021-05-17T09:37:35.485736+0000    19     255      3114      2859   601.713       684     1.52048     1.56205
2021-05-17T09:37:36.486112+0000 min lat: 0.0400111 max lat: 1.66386 avg lat: 1.4942
2021-05-17T09:37:36.486112+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T09:37:36.486112+0000    20       7      3294      3287   657.199      1712   0.0400111      1.4942
2021-05-17T09:37:37.486519+0000 Total time run:         20.0281
Total writes made:      3294
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     657.876
Stddev Bandwidth:       310.208
Max bandwidth (MB/sec): 1712
Min bandwidth (MB/sec): 0
Average IOPS:           164
Stddev IOPS:            77.5521
Max IOPS:               428
Min IOPS:               0
Average Latency(s):     1.49113
Stddev Latency(s):      0.258286
Max latency(s):         1.66386
Min latency(s):         0.0386751

[1;32mlocalhost.localdomain	[2021-05-17T02:37:37,960200690-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 134634


[1;33mlocalhost.localdomain	[2021-05-17T02:37:37,970193003-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:38:00,939291221-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.29k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:38:00,959038867-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:38:09,020220489-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.29k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:38:09,040825697-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:38:17,062089928-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.29k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:38:17,085364195-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:38:25,031644611-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.29k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:38:25,051646611-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:38:33,047004566-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.29k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:38:33,066933894-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:38:33,083465820-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T02:38:33,092890512-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:38:33,114527266-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=137929
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4MB_seq.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-17T02:38:33,132010653-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4MB_seq.log.2
[1;30mlocalhost.localdomain	[2021-05-17T02:38:33,173108415-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:38:33,179711158-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55bf2687-1b36-4e73-bfcb-fd62d704deb0', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.WclV4p:/tmp/ceph-asok.WclV4p', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55bf2687-1b36-4e73-bfcb-fd62d704deb0 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.WclV4p:/tmp/ceph-asok.WclV4p -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T09:38:34.914+0000 ffffa2208010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:38:34.922+0000 ffffa2208010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:38:34.926+0000 ffffa2208010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T09:38:34.946973+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T09:38:34.946973+0000     0       0         0         0         0         0           -           0
2021-05-17T09:38:35.947170+0000     1     253       253         0         0         0           -           0
2021-05-17T09:38:36.947373+0000     2     255       443       188   375.894       376     1.28449     1.18679
2021-05-17T09:38:37.947585+0000     3     255       614       359   478.543       684     1.44586     1.27972
2021-05-17T09:38:38.947797+0000     4     255       792       537   536.867       712     1.44791     1.34249
2021-05-17T09:38:39.948007+0000     5     255       968       713   570.263       704     1.45211     1.36666
2021-05-17T09:38:40.951663+0000     6     255      1138       883   588.191       680     1.49064     1.38746
2021-05-17T09:38:41.951913+0000     7     255      1312      1057    603.56       696     1.47889     1.40619
2021-05-17T09:38:42.952172+0000     8     255      1494      1239   619.085       728     1.40315     1.41147
2021-05-17T09:38:43.952388+0000     9     255      1683      1428   634.274       756     1.36215     1.40775
2021-05-17T09:38:44.952646+0000    10     255      1856      1601   640.026       692     1.44319     1.40617
2021-05-17T09:38:45.954106+0000    11     255      2041      1786   649.024       740     1.40717     1.41005
2021-05-17T09:38:46.954366+0000    12     255      2220      1965   654.588       716     1.41452     1.40882
2021-05-17T09:38:47.954627+0000    13     255      2404      2149   660.833       736     1.41923      1.4091
2021-05-17T09:38:48.954878+0000    14     255      2580      2325   663.903       704     1.43743     1.41015
2021-05-17T09:38:49.955323+0000    15     255      2762      2507   668.154       728     1.41271     1.41257
2021-05-17T09:38:50.955929+0000    16     255      2946      2691   672.367       736     1.37147     1.41143
2021-05-17T09:38:51.956200+0000    17     255      3118      2863   673.275       688     1.45532     1.41137
2021-05-17T09:38:52.957036+0000    18     246      3294      3048   676.949       740     1.42043     1.41422
2021-05-17T09:38:53.957320+0000 Total time run:       18.4013
Total reads made:     3294
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   716.038
Average IOPS:         179
Stddev IOPS:          45.8833
Max IOPS:             189
Min IOPS:             0
Average Latency(s):   1.37763
Max latency(s):       1.52872
Min latency(s):       0.408745

[1;32mlocalhost.localdomain	[2021-05-17T02:38:54,457928772-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 137929


[1;33mlocalhost.localdomain	[2021-05-17T02:38:54,467932203-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:39:17,706941919-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.29k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:39:17,727161428-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:39:25,721692297-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.29k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:39:25,742071419-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:39:33,995180213-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.29k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:39:34,015560234-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:39:42,112113649-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.29k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:39:42,132215434-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:39:50,114313670-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.29k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:39:50,134881029-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:39:50,151406460-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T02:39:50,157842315-07:00][RUNNING][ROUND 3/6/40] object_size=4MB[0m


[1;33mlocalhost.localdomain	[2021-05-17T02:39:50,166991532-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T02:39:50,185131283-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40860\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.331648\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 891ae4e3-26d1-4ced-9b42-5d40e02f44fd\nsetting min_mon_release = octopus\nepoch 0\nfsid 891ae4e3-26d1-4ced-9b42-5d40e02f44fd\nlast_changed 2021-05-17T02:40:19.547697-0700\ncreated 2021-05-17T02:40:19.547697-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40860/0,v1:10.10.1.2:40861/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.331648 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 501269c0-f9df-424d-8fac-a084a7434fa3\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 7c2bdde4-d148-4f68-9675-714d9edb3795\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 f1242cdd-3096-4650-ae43-044b97d7faa0\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42860\n  w/ user/pass: admin / 77a801a1-b899-4c90-a191-5a360a498cdb\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\nCEPH_DEV=1\n'
[1] 02:40:34 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40860
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.331648
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 891ae4e3-26d1-4ced-9b42-5d40e02f44fd
setting min_mon_release = octopus
epoch 0
fsid 891ae4e3-26d1-4ced-9b42-5d40e02f44fd
last_changed 2021-05-17T02:40:19.547697-0700
created 2021-05-17T02:40:19.547697-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40860/0,v1:10.10.1.2:40861/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.331648 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 501269c0-f9df-424d-8fac-a084a7434fa3
0
start osd.0
add osd1 7c2bdde4-d148-4f68-9675-714d9edb3795
1
start osd.1
add osd2 f1242cdd-3096-4650-ae43-044b97d7faa0
2
start osd.2


restful urls: https://10.10.1.2:42860
  w/ user/pass: admin / 77a801a1-b899-4c90-a191-5a360a498cdb


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T02:39:51.187-0700 7fb4235621c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T02:39:51.187-0700 7fb4235621c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T02:39:51.203-0700 7f6c919401c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T02:39:51.203-0700 7f6c919401c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40860,v1:10.10.1.2:40861] --print /tmp/ceph_monmap.331648 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.331648 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.331648 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42860 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.8qwDKJ8Yq4 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 501269c0-f9df-424d-8fac-a084a7434fa3 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAMOqJgo476HxAAt5MkTtfgi03B/ZH9jXWRFg== --osd-uuid 501269c0-f9df-424d-8fac-a084a7434fa3 
2021-05-17T02:40:28.916-0700 7f808efb9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T02:40:28.916-0700 7f808efb9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T02:40:28.916-0700 7f808efb9f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T02:40:28.972-0700 7f808efb9f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 7c2bdde4-d148-4f68-9675-714d9edb3795 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T02:40:29.284-0700 7f63f17bef00 -1 Falling back to public interface
2021-05-17T02:40:29.296-0700 7f63f17bef00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQANOqJgXvT0EBAAwMstpH7s8zzCeMW51/gdMg== --osd-uuid 7c2bdde4-d148-4f68-9675-714d9edb3795 
2021-05-17T02:40:29.604-0700 7f628c732f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T02:40:29.604-0700 7f628c732f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T02:40:29.604-0700 7f628c732f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T02:40:29.648-0700 7f628c732f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new f1242cdd-3096-4650-ae43-044b97d7faa0 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T02:40:30.036-0700 7f97ce834f00 -1 Falling back to public interface
2021-05-17T02:40:30.048-0700 7f97ce834f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAOOqJg3487AhAAbBAKaXt5mHw8abf/TDffAQ== --osd-uuid f1242cdd-3096-4650-ae43-044b97d7faa0 
2021-05-17T02:40:30.456-0700 7f8a15b8ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T02:40:30.456-0700 7f8a15b8ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T02:40:30.456-0700 7f8a15b8ef00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T02:40:30.552-0700 7f8a15b8ef00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T02:40:30.864-0700 7f837e6f4f00 -1 Falling back to public interface
2021-05-17T02:40:30.880-0700 7f837e6f4f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T02:40:34,768996558-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T02:40:34,778550604-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T02:40:34,863119967-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:40:34,869767223-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fe0c19be-ddfe-4d5c-8e48-b36adf3ec43c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sDMRVf:/tmp/ceph-asok.sDMRVf', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fe0c19be-ddfe-4d5c-8e48-b36adf3ec43c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sDMRVf:/tmp/ceph-asok.sDMRVf -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T02:40:37,885886132-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:40:37,892573820-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fe0c19be-ddfe-4d5c-8e48-b36adf3ec43c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sDMRVf:/tmp/ceph-asok.sDMRVf', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fe0c19be-ddfe-4d5c-8e48-b36adf3ec43c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sDMRVf:/tmp/ceph-asok.sDMRVf -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T02:40:40,802394324-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:40:40,808704368-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fe0c19be-ddfe-4d5c-8e48-b36adf3ec43c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sDMRVf:/tmp/ceph-asok.sDMRVf', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fe0c19be-ddfe-4d5c-8e48-b36adf3ec43c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sDMRVf:/tmp/ceph-asok.sDMRVf -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T02:40:43,619461373-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:40:43,625993814-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fe0c19be-ddfe-4d5c-8e48-b36adf3ec43c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sDMRVf:/tmp/ceph-asok.sDMRVf', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fe0c19be-ddfe-4d5c-8e48-b36adf3ec43c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sDMRVf:/tmp/ceph-asok.sDMRVf -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T02:40:49,490958622-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:40:49,498419392-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fe0c19be-ddfe-4d5c-8e48-b36adf3ec43c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sDMRVf:/tmp/ceph-asok.sDMRVf', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fe0c19be-ddfe-4d5c-8e48-b36adf3ec43c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sDMRVf:/tmp/ceph-asok.sDMRVf -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T02:40:52,451054943-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:40:52,457549744-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fe0c19be-ddfe-4d5c-8e48-b36adf3ec43c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sDMRVf:/tmp/ceph-asok.sDMRVf', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fe0c19be-ddfe-4d5c-8e48-b36adf3ec43c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sDMRVf:/tmp/ceph-asok.sDMRVf -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T02:40:56,069672386-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:40:56,076028173-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fe0c19be-ddfe-4d5c-8e48-b36adf3ec43c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sDMRVf:/tmp/ceph-asok.sDMRVf', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fe0c19be-ddfe-4d5c-8e48-b36adf3ec43c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sDMRVf:/tmp/ceph-asok.sDMRVf -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T02:40:59,256675227-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:40:59,263082293-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fe0c19be-ddfe-4d5c-8e48-b36adf3ec43c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sDMRVf:/tmp/ceph-asok.sDMRVf', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fe0c19be-ddfe-4d5c-8e48-b36adf3ec43c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sDMRVf:/tmp/ceph-asok.sDMRVf -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T02:41:02,835282703-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:41:02,841794411-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fe0c19be-ddfe-4d5c-8e48-b36adf3ec43c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sDMRVf:/tmp/ceph-asok.sDMRVf', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fe0c19be-ddfe-4d5c-8e48-b36adf3ec43c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sDMRVf:/tmp/ceph-asok.sDMRVf -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T02:41:05,904480296-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:41:05,910780814-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fe0c19be-ddfe-4d5c-8e48-b36adf3ec43c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sDMRVf:/tmp/ceph-asok.sDMRVf', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fe0c19be-ddfe-4d5c-8e48-b36adf3ec43c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sDMRVf:/tmp/ceph-asok.sDMRVf -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T02:41:09,229379687-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:41:09,235666702-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fe0c19be-ddfe-4d5c-8e48-b36adf3ec43c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sDMRVf:/tmp/ceph-asok.sDMRVf', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fe0c19be-ddfe-4d5c-8e48-b36adf3ec43c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sDMRVf:/tmp/ceph-asok.sDMRVf -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T02:41:11,944774506-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:41:11,951167590-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fe0c19be-ddfe-4d5c-8e48-b36adf3ec43c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sDMRVf:/tmp/ceph-asok.sDMRVf', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fe0c19be-ddfe-4d5c-8e48-b36adf3ec43c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sDMRVf:/tmp/ceph-asok.sDMRVf -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T02:41:14,655148459-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:41:37,589172197-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   203 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:41:45,899065106-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:41:53,878917746-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (9s)
      [=======.....................] (remaining: 29s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:42:01,902816752-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [=======.....................] (remaining: 53s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:42:10,065725115-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [========....................] (remaining: 62s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:42:18,089282810-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [========....................] (remaining: 87s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:42:26,215447363-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (45s)
      [========....................] (remaining: 103s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:42:34,193258930-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [==============..............] (remaining: 46s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:42:42,283581020-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:42:42,303553247-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:42:50,515809552-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:42:50,535727350-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:42:58,650998229-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:42:58,671330352-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:43:06,861431788-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:43:06,882005528-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:43:14,932530943-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:43:14,952619245-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:43:14,969105764-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T02:43:14,978641193-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:43:15,000197675-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=151866
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4MB_write.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-17T02:43:15,018165231-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4MB_write.log.3
[1;30mlocalhost.localdomain	[2021-05-17T02:43:15,060335345-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:43:15,066919636-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fe0c19be-ddfe-4d5c-8e48-b36adf3ec43c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sDMRVf:/tmp/ceph-asok.sDMRVf', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fe0c19be-ddfe-4d5c-8e48-b36adf3ec43c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sDMRVf:/tmp/ceph-asok.sDMRVf -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T09:43:16.622+0000 ffffa12d8010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:43:16.630+0000 ffffa12d8010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:43:16.630+0000 ffffa12d8010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T09:43:16.653042+0000 Maintaining 256 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 20 seconds or 0 objects
2021-05-17T09:43:16.653140+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T09:43:17.582497+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T09:43:17.582497+0000     0       0         0         0         0         0           -           0
2021-05-17T09:43:18.582748+0000     1     167       167         0         0         0           -           0
2021-05-17T09:43:19.583006+0000     2     255       346        91   181.969       182     1.43479     1.49199
2021-05-17T09:43:20.583251+0000     3     255       520       265   353.264       696     1.43957      1.4689
2021-05-17T09:43:21.583519+0000     4     255       696       441   440.905       704     1.42618     1.46023
2021-05-17T09:43:22.583733+0000     5     255       859       604   483.096       652     1.52561     1.46265
2021-05-17T09:43:23.584013+0000     6     255      1032       777   517.883       692     1.56134     1.48261
2021-05-17T09:43:24.584404+0000     7     255      1213       958   547.292       724     1.41562     1.48106
2021-05-17T09:43:25.584710+0000     8     255      1385      1130   564.855       688      1.4621     1.47087
2021-05-17T09:43:26.585096+0000     9     255      1557      1302    578.51       688     1.52817     1.47317
2021-05-17T09:43:27.585411+0000    10     255      1728      1473   589.038       684     1.48764     1.47626
2021-05-17T09:43:28.585679+0000    11     255      1897      1642   596.927       676     1.52173     1.47913
2021-05-17T09:43:29.585918+0000    12     255      2068      1813   604.169       684     1.47479     1.48112
2021-05-17T09:43:30.586208+0000    13     255      2226      1971   606.296       632     1.58868      1.4863
2021-05-17T09:43:31.586613+0000    14     255      2381      2126   607.257       620     1.62719      1.4961
2021-05-17T09:43:32.586958+0000    15     255      2534      2279   607.559       612     1.66285     1.50695
2021-05-17T09:43:33.587260+0000    16     255      2698      2443   610.574       656      1.6018     1.51463
2021-05-17T09:43:34.587582+0000    17     255      2859      2604   612.529       644     1.59004      1.5166
2021-05-17T09:43:35.587900+0000    18     255      3012      2757   612.488       612     1.67405     1.52263
2021-05-17T09:43:36.588202+0000    19     255      3172      2917   613.926       640     1.62522     1.52989
2021-05-17T09:43:37.588472+0000 min lat: 1.37785 max lat: 1.7086 avg lat: 1.53352
2021-05-17T09:43:37.588472+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T09:43:37.588472+0000    20     255      3338      3083   616.421       664     1.60384     1.53352
2021-05-17T09:43:38.588762+0000 Total time run:         20.0533
Total writes made:      3339
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     666.024
Stddev Bandwidth:       181.808
Max bandwidth (MB/sec): 724
Min bandwidth (MB/sec): 0
Average IOPS:           166
Stddev IOPS:            45.5138
Max IOPS:               181
Min IOPS:               0
Average Latency(s):     1.47684
Stddev Latency(s):      0.244863
Max latency(s):         1.7086
Min latency(s):         0.0351911

[1;32mlocalhost.localdomain	[2021-05-17T02:43:39,050893671-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 151866


[1;33mlocalhost.localdomain	[2021-05-17T02:43:39,060470327-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:44:02,329399447-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.34k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:44:02,349602804-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:44:10,369783332-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.34k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:44:10,389685498-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:44:18,453522196-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.34k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:44:18,473615405-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:44:26,609621053-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.34k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:44:26,629621306-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:44:34,644225086-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.34k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:44:34,664491020-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:44:34,680769604-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T02:44:34,690434026-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:44:34,712912582-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=155244
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4MB_seq.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-17T02:44:34,731025243-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4MB_seq.log.3
[1;30mlocalhost.localdomain	[2021-05-17T02:44:34,772203286-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:44:34,778804396-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fe0c19be-ddfe-4d5c-8e48-b36adf3ec43c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sDMRVf:/tmp/ceph-asok.sDMRVf', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fe0c19be-ddfe-4d5c-8e48-b36adf3ec43c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.sDMRVf:/tmp/ceph-asok.sDMRVf -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T09:44:36.252+0000 ffff9b0fc010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:44:36.260+0000 ffff9b0fc010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:44:36.260+0000 ffff9b0fc010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T09:44:36.281432+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T09:44:36.281432+0000     0       0         0         0         0         0           -           0
2021-05-17T09:44:37.281672+0000     1     234       234         0         0         0           -           0
2021-05-17T09:44:38.283072+0000     2     255       415       160    319.71       320     1.36404     1.26265
2021-05-17T09:44:39.284669+0000     3     255       589       334   444.828       696     1.48375     1.36079
2021-05-17T09:44:40.285621+0000     4     256       775       519   518.435       740      1.3767     1.38896
2021-05-17T09:44:41.287041+0000     5     255       949       694   554.559       700     1.42516     1.39396
2021-05-17T09:44:42.287316+0000     6     255      1128       873   581.413       716     1.43343     1.40839
2021-05-17T09:44:43.289576+0000     7     255      1317      1062   606.137       756     1.33973     1.40275
2021-05-17T09:44:44.289835+0000     8     255      1497      1242   620.335       720     1.41905     1.39897
2021-05-17T09:44:45.290154+0000     9     255      1685      1430   634.928       752     1.37914      1.3986
2021-05-17T09:44:46.290821+0000    10     255      1871      1616   645.783       744     1.37683     1.39631
2021-05-17T09:44:47.291103+0000    11     255      2059      1804   655.414       752     1.35191     1.39336
2021-05-17T09:44:48.291375+0000    12     255      2247      1992   663.441       752     1.36023     1.39034
2021-05-17T09:44:49.291623+0000    13     255      2442      2187   672.387       780     1.31896     1.38586
2021-05-17T09:44:50.292657+0000    14     255      2633      2378   678.876       764     1.33452     1.38115
2021-05-17T09:44:51.295701+0000    15     255      2818      2563   682.809       740     1.37231     1.37828
2021-05-17T09:44:52.296213+0000    16     255      2994      2739   684.111       704     1.44792      1.3801
2021-05-17T09:44:53.296429+0000    17     255      3170      2915   685.271       704     1.44624     1.38422
2021-05-17T09:44:54.296976+0000    18     254      3339      3085   684.958       680     1.48968     1.38955
2021-05-17T09:44:55.297381+0000 Total time run:       18.4189
Total reads made:     3339
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   725.125
Average IOPS:         181
Stddev IOPS:          48.647
Max IOPS:             195
Min IOPS:             0
Average Latency(s):   1.3571
Max latency(s):       1.50582
Min latency(s):       0.414506

[1;32mlocalhost.localdomain	[2021-05-17T02:44:55,765893317-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 155244


[1;33mlocalhost.localdomain	[2021-05-17T02:44:55,775920651-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:45:18,769160657-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.34k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:45:18,789857435-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:45:26,888613428-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.34k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:45:26,908783196-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:45:35,192936277-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.34k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:45:35,216852963-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:45:43,320791770-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.34k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:45:43,341695914-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:45:51,336828386-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.34k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:45:51,357953458-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:45:51,374753636-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T02:45:51,381154133-07:00][RUNNING][ROUND 4/6/40] object_size=4MB[0m


[1;33mlocalhost.localdomain	[2021-05-17T02:45:51,390819515-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T02:45:51,409366562-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40556\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.332761\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid d840a52d-1f4c-42ae-976f-04c9a927194c\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid d840a52d-1f4c-42ae-976f-04c9a927194c\nlast_changed 2021-05-17T02:46:21.223971-0700\ncreated 2021-05-17T02:46:21.223971-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40556/0,v1:10.10.1.2:40557/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.332761 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 80e8e213-ef3f-4d88-b60f-f7440cbc27ae\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 ab9b012a-fcff-4c4e-af7a-a56a47078f82\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 36287fc7-4d03-4053-99ff-ac71e2106487\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\n'
10.10.1.2: b'restful urls: https://10.10.1.2:42556\n  w/ user/pass: admin / 46fcf216-fc5c-4fb6-abb8-e63dc7224547\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 02:46:36 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40556
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.332761
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid d840a52d-1f4c-42ae-976f-04c9a927194c
setting min_mon_release = octopus
epoch 0
fsid d840a52d-1f4c-42ae-976f-04c9a927194c
last_changed 2021-05-17T02:46:21.223971-0700
created 2021-05-17T02:46:21.223971-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40556/0,v1:10.10.1.2:40557/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.332761 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 80e8e213-ef3f-4d88-b60f-f7440cbc27ae
0
start osd.0
add osd1 ab9b012a-fcff-4c4e-af7a-a56a47078f82
1
start osd.1
add osd2 36287fc7-4d03-4053-99ff-ac71e2106487
2
start osd.2


restful urls: https://10.10.1.2:42556
  w/ user/pass: admin / 46fcf216-fc5c-4fb6-abb8-e63dc7224547


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T02:45:52.426-0700 7f2e6604f1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T02:45:52.426-0700 7f2e6604f1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T02:45:52.446-0700 7f9b542071c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T02:45:52.446-0700 7f9b542071c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40556,v1:10.10.1.2:40557] --print /tmp/ceph_monmap.332761 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.332761 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.332761 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42556 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.dU2KhyrKDN 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 80e8e213-ef3f-4d88-b60f-f7440cbc27ae -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQB2O6JghUg/EBAAWVspRrN6ZCq1xBdZg/8+Hw== --osd-uuid 80e8e213-ef3f-4d88-b60f-f7440cbc27ae 
2021-05-17T02:46:30.631-0700 7fe5ea3a3f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T02:46:30.631-0700 7fe5ea3a3f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T02:46:30.631-0700 7fe5ea3a3f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T02:46:30.679-0700 7fe5ea3a3f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new ab9b012a-fcff-4c4e-af7a-a56a47078f82 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T02:46:30.971-0700 7f6aba56cf00 -1 Falling back to public interface
2021-05-17T02:46:30.983-0700 7f6aba56cf00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQB2O6JgrGDsORAARvP13TllwU9xEdwECw/ACg== --osd-uuid ab9b012a-fcff-4c4e-af7a-a56a47078f82 
2021-05-17T02:46:31.319-0700 7fa4ed139f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T02:46:31.319-0700 7fa4ed139f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T02:46:31.319-0700 7fa4ed139f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T02:46:31.363-0700 7fa4ed139f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 36287fc7-4d03-4053-99ff-ac71e2106487 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T02:46:31.735-0700 7fbcd4975f00 -1 Falling back to public interface
2021-05-17T02:46:31.747-0700 7fbcd4975f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQB3O6JgCVKwKxAAH5XKdTZWty+PSv/xmc5SNA== --osd-uuid 36287fc7-4d03-4053-99ff-ac71e2106487 
2021-05-17T02:46:32.079-0700 7f2ac82e6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T02:46:32.079-0700 7f2ac82e6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T02:46:32.079-0700 7f2ac82e6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T02:46:32.151-0700 7f2ac82e6f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T02:46:32.443-0700 7f58fdc3af00 -1 Falling back to public interface
2021-05-17T02:46:32.459-0700 7f58fdc3af00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T02:46:36,457055993-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T02:46:36,466862444-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T02:46:36,551199534-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:46:36,557571375-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55d728d4-c915-4fda-84a4-72a4b6799b3d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wNcjwi:/tmp/ceph-asok.wNcjwi', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55d728d4-c915-4fda-84a4-72a4b6799b3d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wNcjwi:/tmp/ceph-asok.wNcjwi -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T02:46:39,301667281-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:46:39,307893912-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55d728d4-c915-4fda-84a4-72a4b6799b3d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wNcjwi:/tmp/ceph-asok.wNcjwi', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55d728d4-c915-4fda-84a4-72a4b6799b3d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wNcjwi:/tmp/ceph-asok.wNcjwi -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T02:46:42,199438937-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:46:42,207636920-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55d728d4-c915-4fda-84a4-72a4b6799b3d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wNcjwi:/tmp/ceph-asok.wNcjwi', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55d728d4-c915-4fda-84a4-72a4b6799b3d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wNcjwi:/tmp/ceph-asok.wNcjwi -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T02:46:44,955497756-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:46:44,961918941-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55d728d4-c915-4fda-84a4-72a4b6799b3d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wNcjwi:/tmp/ceph-asok.wNcjwi', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55d728d4-c915-4fda-84a4-72a4b6799b3d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wNcjwi:/tmp/ceph-asok.wNcjwi -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T02:46:50,915984543-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:46:50,922534930-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55d728d4-c915-4fda-84a4-72a4b6799b3d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wNcjwi:/tmp/ceph-asok.wNcjwi', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55d728d4-c915-4fda-84a4-72a4b6799b3d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wNcjwi:/tmp/ceph-asok.wNcjwi -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T02:46:54,843865732-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:46:54,850381853-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55d728d4-c915-4fda-84a4-72a4b6799b3d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wNcjwi:/tmp/ceph-asok.wNcjwi', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55d728d4-c915-4fda-84a4-72a4b6799b3d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wNcjwi:/tmp/ceph-asok.wNcjwi -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T02:46:58,702885053-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:46:58,709249378-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55d728d4-c915-4fda-84a4-72a4b6799b3d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wNcjwi:/tmp/ceph-asok.wNcjwi', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55d728d4-c915-4fda-84a4-72a4b6799b3d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wNcjwi:/tmp/ceph-asok.wNcjwi -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T02:47:02,028290049-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:47:02,034706298-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55d728d4-c915-4fda-84a4-72a4b6799b3d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wNcjwi:/tmp/ceph-asok.wNcjwi', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55d728d4-c915-4fda-84a4-72a4b6799b3d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wNcjwi:/tmp/ceph-asok.wNcjwi -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T02:47:05,306799381-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:47:05,313194296-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55d728d4-c915-4fda-84a4-72a4b6799b3d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wNcjwi:/tmp/ceph-asok.wNcjwi', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55d728d4-c915-4fda-84a4-72a4b6799b3d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wNcjwi:/tmp/ceph-asok.wNcjwi -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T02:47:08,826155916-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:47:08,832567935-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55d728d4-c915-4fda-84a4-72a4b6799b3d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wNcjwi:/tmp/ceph-asok.wNcjwi', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55d728d4-c915-4fda-84a4-72a4b6799b3d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wNcjwi:/tmp/ceph-asok.wNcjwi -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T02:47:12,194251611-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:47:12,200776674-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55d728d4-c915-4fda-84a4-72a4b6799b3d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wNcjwi:/tmp/ceph-asok.wNcjwi', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55d728d4-c915-4fda-84a4-72a4b6799b3d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wNcjwi:/tmp/ceph-asok.wNcjwi -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T02:47:15,083180005-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:47:15,089704767-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55d728d4-c915-4fda-84a4-72a4b6799b3d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wNcjwi:/tmp/ceph-asok.wNcjwi', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55d728d4-c915-4fda-84a4-72a4b6799b3d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wNcjwi:/tmp/ceph-asok.wNcjwi -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  146 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  146 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   47 KiB   0 B   0 B   0 B  100 GiB     0  0.97   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.05   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.99   70      up          osd.2  
                       TOTAL  300 GiB  148 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.97/1.05  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T02:47:17,761188201-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:47:40,841577714-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   192 KiB used, 300 GiB / 300 GiB avail
    pgs:     10.938% pgs not active
             171 active+clean
             21  peering
 
  progress:
    Global Recovery Event (4s)
      [=...........................] (remaining: 90s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:47:48,953565848-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:47:56,836590562-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:48:04,824522275-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:48:12,922573641-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:48:21,017214703-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:48:29,035419501-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:48:37,085203830-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:48:37,105020800-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:48:45,096801246-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:48:45,116967116-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:48:53,264465903-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:48:53,284629522-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:49:01,101154875-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:49:01,121431478-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:49:09,094031106-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:49:09,113948743-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:49:09,130799093-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T02:49:09,140268123-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:49:09,162619280-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=168758
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4MB_write.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-17T02:49:09,180401604-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4MB_write.log.4
[1;30mlocalhost.localdomain	[2021-05-17T02:49:09,223068252-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:49:09,229921361-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55d728d4-c915-4fda-84a4-72a4b6799b3d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wNcjwi:/tmp/ceph-asok.wNcjwi', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55d728d4-c915-4fda-84a4-72a4b6799b3d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wNcjwi:/tmp/ceph-asok.wNcjwi -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T09:49:10.714+0000 ffff93847010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:49:10.722+0000 ffff93847010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:49:10.722+0000 ffff93847010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T09:49:10.743812+0000 Maintaining 256 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 20 seconds or 0 objects
2021-05-17T09:49:10.743945+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T09:49:11.724196+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T09:49:11.724196+0000     0       0         0         0         0         0           -           0
2021-05-17T09:49:12.724442+0000     1     144       144         0         0         0           -           0
2021-05-17T09:49:13.724679+0000     2     255       291        36   71.9886        72      1.7805     1.74658
2021-05-17T09:49:14.724878+0000     3     255       443       188   250.624       608     1.71291     1.72255
2021-05-17T09:49:15.725117+0000     4     255       599       344   343.935       624     1.67556     1.70996
2021-05-17T09:49:16.725351+0000     5     255       760       505    403.92       644     1.60519     1.69254
2021-05-17T09:49:17.725630+0000     6     255       910       655   436.574       600     1.63564      1.6742
2021-05-17T09:49:18.725856+0000     7     255      1049       794   453.617       556     1.79121     1.67893
2021-05-17T09:49:19.726064+0000     8     255      1197       942     470.9       592     1.80669     1.69698
2021-05-17T09:49:20.726298+0000     9     255      1346      1091   484.785       596     1.73598     1.70371
2021-05-17T09:49:21.726524+0000    10     255      1495      1240   495.893       596     1.73342      1.7066
2021-05-17T09:49:22.726858+0000    11     255      1639      1384   503.159       576     1.72638     1.70885
2021-05-17T09:49:23.727139+0000    12     255      1782      1527   508.882       572     1.79753     1.71312
2021-05-17T09:49:24.727595+0000    13     255      1938      1683   517.717       624     1.70261     1.71824
2021-05-17T09:49:25.727839+0000    14     255      2088      1833   523.584       600      1.6532     1.71468
2021-05-17T09:49:26.728076+0000    15     255      2243      1988   530.002       620     1.69903     1.71171
2021-05-17T09:49:27.728770+0000    16     255      2388      2133   533.103       580     1.72126      1.7118
2021-05-17T09:49:28.729663+0000    17     255      2541      2286   537.715       612     1.70506     1.71192
2021-05-17T09:49:29.729969+0000    18     255      2687      2432   540.276       584     1.71832     1.71148
2021-05-17T09:49:30.730462+0000    19     255      2839      2584   543.825       608     1.73986     1.71181
2021-05-17T09:49:31.730696+0000 min lat: 0.0549943 max lat: 1.82948 avg lat: 1.64045
2021-05-17T09:49:31.730696+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T09:49:31.730696+0000    20       8      2993      2985   596.811      1604   0.0549943     1.64045
2021-05-17T09:49:32.731238+0000 Total time run:         20.0537
Total writes made:      2993
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     596.998
Stddev Bandwidth:       295.023
Max bandwidth (MB/sec): 1604
Min bandwidth (MB/sec): 0
Average IOPS:           149
Stddev IOPS:            73.7558
Max IOPS:               401
Min IOPS:               0
Average Latency(s):     1.63621
Stddev Latency(s):      0.285586
Max latency(s):         1.82948
Min latency(s):         0.025415

[1;32mlocalhost.localdomain	[2021-05-17T02:49:33,314679604-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 168758


[1;33mlocalhost.localdomain	[2021-05-17T02:49:33,324798680-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:49:56,308416792-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.99k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:49:56,328267804-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:50:04,480061488-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.99k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:50:04,500394742-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:50:12,310007271-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.99k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:50:12,332357229-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:50:20,335197108-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.99k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:50:20,355428014-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:50:28,344972965-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.99k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:50:28,365399220-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:50:28,381935161-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T02:50:28,391430011-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:50:28,413859828-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=172084
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4MB_seq.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-17T02:50:28,432088508-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4MB_seq.log.4
[1;30mlocalhost.localdomain	[2021-05-17T02:50:28,473719890-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:50:28,480177532-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '55d728d4-c915-4fda-84a4-72a4b6799b3d', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wNcjwi:/tmp/ceph-asok.wNcjwi', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 55d728d4-c915-4fda-84a4-72a4b6799b3d --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.wNcjwi:/tmp/ceph-asok.wNcjwi -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T09:50:29.984+0000 ffff8bf48010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:50:29.992+0000 ffff8bf48010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:50:29.992+0000 ffff8bf48010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T09:50:30.012943+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T09:50:30.012943+0000     0       0         0         0         0         0           -           0
2021-05-17T09:50:31.013166+0000     1     256       296        40   159.939       160    0.938559    0.891793
2021-05-17T09:50:32.014187+0000     2     255       494       239   477.665       796     1.24033     1.05064
2021-05-17T09:50:33.014440+0000     3     255       683       428   570.352       756     1.33338     1.16702
2021-05-17T09:50:34.014757+0000     4     255       877       622   621.694       776     1.32303     1.21955
2021-05-17T09:50:35.015170+0000     5     255      1074       819   654.888       788     1.29703     1.24165
2021-05-17T09:50:36.017237+0000     6     255      1263      1008   671.502       756     1.33401     1.25503
2021-05-17T09:50:37.018306+0000     7     255      1434      1179   673.183       684     1.45837     1.27776
2021-05-17T09:50:38.022068+0000     8     256      1611      1355   676.715       704     1.44797     1.30173
2021-05-17T09:50:39.022327+0000     9     255      1796      1541   684.164       744     1.40854     1.31657
2021-05-17T09:50:40.023482+0000    10     255      1973      1718   686.466       708     1.43349     1.32621
2021-05-17T09:50:41.025317+0000    11     255      2146      1891   686.854       692     1.46866     1.33754
2021-05-17T09:50:42.025783+0000    12     255      2333      2078   691.917       748     1.41982     1.34727
2021-05-17T09:50:43.026059+0000    13     255      2510      2255   693.138       708     1.41013     1.35002
2021-05-17T09:50:44.026354+0000    14     255      2684      2429   693.328       696     1.48798     1.35802
2021-05-17T09:50:45.027917+0000    15     255      2865      2610   695.299       724     1.40964     1.36522
2021-05-17T09:50:46.030092+0000    16      87      2993      2906   725.715      1184     0.78711     1.35223
2021-05-17T09:50:47.030390+0000 Total time run:       16.138
Total reads made:     2993
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   741.85
Average IOPS:         185
Stddev IOPS:          47.8245
Max IOPS:             296
Min IOPS:             40
Average Latency(s):   1.33036
Max latency(s):       1.50226
Min latency(s):       0.41574

[1;32mlocalhost.localdomain	[2021-05-17T02:50:47,531601271-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 172084


[1;33mlocalhost.localdomain	[2021-05-17T02:50:47,542069127-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:51:10,660607042-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.99k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:51:10,683043806-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:51:18,982793682-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.99k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:51:19,003508482-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:51:26,968156359-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.99k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:51:26,988100987-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:51:35,032957012-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.99k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:51:35,053300543-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:51:43,070954728-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 2.99k objects, 12 GiB
    usage:   23 GiB used, 277 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:51:43,091388244-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:51:43,108165337-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T02:51:43,114531673-07:00][RUNNING][ROUND 5/6/40] object_size=4MB[0m


[1;33mlocalhost.localdomain	[2021-05-17T02:51:43,124114649-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T02:51:43,142462945-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40311\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.333874\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid ea0057c5-848b-4dfc-b09c-2c03247d4ba7\nsetting min_mon_release = octopus\nepoch 0\nfsid ea0057c5-848b-4dfc-b09c-2c03247d4ba7\nlast_changed 2021-05-17T02:52:10.820598-0700\ncreated 2021-05-17T02:52:10.820598-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40311/0,v1:10.10.1.2:40312/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.333874 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 8c9a4069-0d08-4c76-bec3-1415d445ad9b\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 f5c81b57-36cb-4720-ad22-9c61541fa4af\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 a7ffc2a8-3c7f-4049-ac1c-09466b1ce4ab\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42311\n  w/ user/pass: admin / efa00809-be7f-420a-a3dd-aa77f9990b2d\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\nCEPH_DEV=1\n'
[1] 02:52:25 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40311
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.333874
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid ea0057c5-848b-4dfc-b09c-2c03247d4ba7
setting min_mon_release = octopus
epoch 0
fsid ea0057c5-848b-4dfc-b09c-2c03247d4ba7
last_changed 2021-05-17T02:52:10.820598-0700
created 2021-05-17T02:52:10.820598-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40311/0,v1:10.10.1.2:40312/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.333874 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 8c9a4069-0d08-4c76-bec3-1415d445ad9b
0
start osd.0
add osd1 f5c81b57-36cb-4720-ad22-9c61541fa4af
1
start osd.1
add osd2 a7ffc2a8-3c7f-4049-ac1c-09466b1ce4ab
2
start osd.2


restful urls: https://10.10.1.2:42311
  w/ user/pass: admin / efa00809-be7f-420a-a3dd-aa77f9990b2d


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T02:51:44.154-0700 7eff4a1481c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T02:51:44.154-0700 7eff4a1481c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T02:51:44.170-0700 7f2a1ea571c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T02:51:44.170-0700 7f2a1ea571c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40311,v1:10.10.1.2:40312] --print /tmp/ceph_monmap.333874 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.333874 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.333874 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42311 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.xjwCWNl9fP 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 8c9a4069-0d08-4c76-bec3-1415d445ad9b -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDTPKJg8alqKRAAloyfOHVypTXz5KjJ8qlClQ== --osd-uuid 8c9a4069-0d08-4c76-bec3-1415d445ad9b 
2021-05-17T02:52:20.046-0700 7f06a5d4ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T02:52:20.046-0700 7f06a5d4ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T02:52:20.046-0700 7f06a5d4ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T02:52:20.134-0700 7f06a5d4ff00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new f5c81b57-36cb-4720-ad22-9c61541fa4af -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T02:52:20.454-0700 7fed71bf0f00 -1 Falling back to public interface
2021-05-17T02:52:20.466-0700 7fed71bf0f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDUPKJgAQIGGxAAPEEQ63pYgXgfrj0XOOdOYQ== --osd-uuid f5c81b57-36cb-4720-ad22-9c61541fa4af 
2021-05-17T02:52:20.806-0700 7fbb4ea80f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T02:52:20.806-0700 7fbb4ea80f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T02:52:20.806-0700 7fbb4ea80f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T02:52:20.878-0700 7fbb4ea80f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new a7ffc2a8-3c7f-4049-ac1c-09466b1ce4ab -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T02:52:21.282-0700 7ff3a40e9f00 -1 Falling back to public interface
2021-05-17T02:52:21.294-0700 7ff3a40e9f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQDVPKJgqvWeEBAAkNhVSmrJ4AiKYmjAmNC6Kw== --osd-uuid a7ffc2a8-3c7f-4049-ac1c-09466b1ce4ab 
2021-05-17T02:52:21.610-0700 7fc6c20e0f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T02:52:21.610-0700 7fc6c20e0f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T02:52:21.610-0700 7fc6c20e0f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T02:52:21.690-0700 7fc6c20e0f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T02:52:21.974-0700 7ffad204bf00 -1 Falling back to public interface
2021-05-17T02:52:21.990-0700 7ffad204bf00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T02:52:25,945141270-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T02:52:25,954950709-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T02:52:26,040433632-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:52:26,046928570-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4e948809-93e5-4747-926c-fe9738d110c3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.VLWVds:/tmp/ceph-asok.VLWVds', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4e948809-93e5-4747-926c-fe9738d110c3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.VLWVds:/tmp/ceph-asok.VLWVds -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T02:52:28,822843085-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:52:28,829195739-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4e948809-93e5-4747-926c-fe9738d110c3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.VLWVds:/tmp/ceph-asok.VLWVds', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4e948809-93e5-4747-926c-fe9738d110c3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.VLWVds:/tmp/ceph-asok.VLWVds -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T02:52:31,738728428-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:52:31,745061564-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4e948809-93e5-4747-926c-fe9738d110c3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.VLWVds:/tmp/ceph-asok.VLWVds', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4e948809-93e5-4747-926c-fe9738d110c3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.VLWVds:/tmp/ceph-asok.VLWVds -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T02:52:34,441663803-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:52:34,452214519-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4e948809-93e5-4747-926c-fe9738d110c3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.VLWVds:/tmp/ceph-asok.VLWVds', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4e948809-93e5-4747-926c-fe9738d110c3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.VLWVds:/tmp/ceph-asok.VLWVds -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T02:52:40,247538118-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:52:40,254120251-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4e948809-93e5-4747-926c-fe9738d110c3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.VLWVds:/tmp/ceph-asok.VLWVds', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4e948809-93e5-4747-926c-fe9738d110c3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.VLWVds:/tmp/ceph-asok.VLWVds -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T02:52:43,785147271-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:52:43,791612174-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4e948809-93e5-4747-926c-fe9738d110c3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.VLWVds:/tmp/ceph-asok.VLWVds', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4e948809-93e5-4747-926c-fe9738d110c3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.VLWVds:/tmp/ceph-asok.VLWVds -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T02:52:47,207977975-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:52:47,214487781-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4e948809-93e5-4747-926c-fe9738d110c3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.VLWVds:/tmp/ceph-asok.VLWVds', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4e948809-93e5-4747-926c-fe9738d110c3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.VLWVds:/tmp/ceph-asok.VLWVds -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T02:52:50,605324343-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:52:50,611980874-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4e948809-93e5-4747-926c-fe9738d110c3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.VLWVds:/tmp/ceph-asok.VLWVds', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4e948809-93e5-4747-926c-fe9738d110c3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.VLWVds:/tmp/ceph-asok.VLWVds -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T02:52:53,810925663-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:52:53,817427112-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4e948809-93e5-4747-926c-fe9738d110c3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.VLWVds:/tmp/ceph-asok.VLWVds', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4e948809-93e5-4747-926c-fe9738d110c3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.VLWVds:/tmp/ceph-asok.VLWVds -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T02:52:57,149311135-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:52:57,155839724-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4e948809-93e5-4747-926c-fe9738d110c3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.VLWVds:/tmp/ceph-asok.VLWVds', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4e948809-93e5-4747-926c-fe9738d110c3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.VLWVds:/tmp/ceph-asok.VLWVds -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T02:53:00,463110730-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:53:00,469681492-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4e948809-93e5-4747-926c-fe9738d110c3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.VLWVds:/tmp/ceph-asok.VLWVds', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4e948809-93e5-4747-926c-fe9738d110c3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.VLWVds:/tmp/ceph-asok.VLWVds -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T02:53:03,372265562-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:53:03,378572773-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4e948809-93e5-4747-926c-fe9738d110c3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.VLWVds:/tmp/ceph-asok.VLWVds', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4e948809-93e5-4747-926c-fe9738d110c3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.VLWVds:/tmp/ceph-asok.VLWVds -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T02:53:06,008779751-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:53:29,186413216-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   202 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:53:37,351587835-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:53:45,358514308-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (9s)
      [=======.....................] (remaining: 27s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:53:53,256255897-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [=======.....................] (remaining: 51s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:54:01,528412565-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (30s)
      [========....................] (remaining: 71s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:54:09,479732531-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [========....................] (remaining: 75s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:54:17,400782176-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   220 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (40s)
      [========....................] (remaining: 85s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:54:25,411366069-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   234 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [===============.............] (remaining: 40s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:54:33,496274868-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:54:33,516953812-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:54:41,858030615-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:54:41,878444795-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:54:49,818248992-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:54:49,838727922-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:54:57,813006248-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:54:57,833526991-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:55:05,979071264-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   241 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:55:05,999705772-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:55:06,016666692-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T02:55:06,026491884-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:55:06,048973761-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=185871
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4MB_write.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-17T02:55:06,067709169-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4MB_write.log.5
[1;30mlocalhost.localdomain	[2021-05-17T02:55:06,109539099-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:55:06,116025681-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4e948809-93e5-4747-926c-fe9738d110c3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.VLWVds:/tmp/ceph-asok.VLWVds', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '4194304', '-O', '4194304', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4e948809-93e5-4747-926c-fe9738d110c3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.VLWVds:/tmp/ceph-asok.VLWVds -- rados bench 20 write --pool bench_rados -b 4194304 -O 4194304 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T09:55:07.631+0000 ffff993a4010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:55:07.639+0000 ffff993a4010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:55:07.639+0000 ffff993a4010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T09:55:07.657621+0000 Maintaining 256 concurrent writes of 4194304 bytes to objects of size 4194304 for up to 20 seconds or 0 objects
2021-05-17T09:55:07.657716+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T09:55:08.605684+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T09:55:08.605684+0000     0       0         0         0         0         0           -           0
2021-05-17T09:55:09.605922+0000     1     142       142         0         0         0           -           0
2021-05-17T09:55:10.606146+0000     2     255       307        52   103.984       104     1.59147      1.6368
2021-05-17T09:55:11.606365+0000     3     255       480       225   299.948       692     1.52851     1.55596
2021-05-17T09:55:12.606599+0000     4     255       649       394   393.926       676     1.49984      1.5379
2021-05-17T09:55:13.606849+0000     5     255       825       570   455.908       704     1.47646     1.52237
2021-05-17T09:55:14.607073+0000     6     255       982       727   484.567       628     1.56581     1.52078
2021-05-17T09:55:15.607287+0000     7     255      1138       883   504.467       624     1.60225     1.53522
2021-05-17T09:55:16.607494+0000     8     255      1311      1056   527.891       692     1.54124     1.54383
2021-05-17T09:55:17.607731+0000     9     255      1471      1216   540.331       640     1.55266     1.54292
2021-05-17T09:55:18.608617+0000    10     255      1638      1383   553.047       668     1.55722     1.54639
2021-05-17T09:55:19.609203+0000    11     255      1797      1542   560.556       636     1.55361     1.54954
2021-05-17T09:55:20.609796+0000    12     255      1970      1715   571.479       692     1.50635     1.54834
2021-05-17T09:55:21.610142+0000    13     255      2140      1885   579.808       680     1.49165     1.54446
2021-05-17T09:55:22.610535+0000    14     255      2307      2052   586.089       668       1.517     1.54074
2021-05-17T09:55:23.610789+0000    15     255      2468      2213   589.939       644     1.56904     1.54084
2021-05-17T09:55:24.611011+0000    16     255      2632      2377   594.058       656     1.57694     1.54421
2021-05-17T09:55:25.611336+0000    17     255      2795      2540   597.454       652     1.57747     1.54785
2021-05-17T09:55:26.611780+0000    18     255      2954      2699    599.58       636      1.5642     1.54891
2021-05-17T09:55:27.612168+0000    19     255      3115      2860   601.905       644     1.58407      1.5516
2021-05-17T09:55:28.612592+0000 min lat: 1.44543 max lat: 1.73376 avg lat: 1.55405
2021-05-17T09:55:28.612592+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T09:55:28.612592+0000    20     255      3275      3020   603.796       640     1.57593     1.55405
2021-05-17T09:55:29.613285+0000 Total time run:         20.0434
Total writes made:      3276
Write size:             4194304
Object size:            4194304
Bandwidth (MB/sec):     653.781
Stddev Bandwidth:       189.248
Max bandwidth (MB/sec): 704
Min bandwidth (MB/sec): 0
Average IOPS:           163
Stddev IOPS:            47.3121
Max IOPS:               176
Min IOPS:               0
Average Latency(s):     1.49549
Stddev Latency(s):      0.242206
Max latency(s):         1.73376
Min latency(s):         0.0312064

[1;32mlocalhost.localdomain	[2021-05-17T02:55:30,075190584-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 185871


[1;33mlocalhost.localdomain	[2021-05-17T02:55:30,084995972-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:55:53,327702070-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.28k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:55:53,348038762-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:56:01,364956589-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.28k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:56:01,385716668-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:56:09,403496660-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.28k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:56:09,424026660-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:56:17,727319684-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.28k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:56:17,748065135-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:56:25,675720778-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.28k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:56:25,696669274-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:56:25,713778075-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T02:56:25,723291559-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:56:25,745875094-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=189292
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_4MB_seq.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-17T02:56:25,764465607-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_4MB_seq.log.5
[1;30mlocalhost.localdomain	[2021-05-17T02:56:25,805568919-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:56:25,811901710-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '4e948809-93e5-4747-926c-fe9738d110c3', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.VLWVds:/tmp/ceph-asok.VLWVds', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 4e948809-93e5-4747-926c-fe9738d110c3 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.VLWVds:/tmp/ceph-asok.VLWVds -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T09:56:27.305+0000 ffff849c9010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:56:27.313+0000 ffff849c9010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T09:56:27.313+0000 ffff849c9010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T09:56:27.334667+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T09:56:27.334667+0000     0       0         0         0         0         0           -           0
2021-05-17T09:56:28.334891+0000     1     255       300        45   179.931       180    0.919832    0.876649
2021-05-17T09:56:29.335143+0000     2     255       491       236    471.85       764     1.26911     1.05296
2021-05-17T09:56:30.336208+0000     3     255       688       433   577.006       788      1.3078     1.17292
2021-05-17T09:56:31.337933+0000     4     255       883       628   627.463       780     1.30926     1.21159
2021-05-17T09:56:32.338149+0000     5     255      1065       810   647.528       728     1.38398     1.23946
2021-05-17T09:56:33.339802+0000     6     255      1248       993   661.416       732      1.4355     1.27004
2021-05-17T09:56:34.340808+0000     7     255      1424      1169   667.399       704     1.44536     1.29392
2021-05-17T09:56:35.341046+0000     8     255      1597      1342   670.452       692     1.46384     1.31527
2021-05-17T09:56:36.341371+0000     9     255      1774      1519   674.597       708     1.44492     1.33224
2021-05-17T09:56:37.341725+0000    10     255      1956      1701   679.909       728     1.40859     1.34348
2021-05-17T09:56:38.342157+0000    11     255      2141      1886   685.342       740      1.3789     1.34813
2021-05-17T09:56:39.342437+0000    12     255      2329      2074   690.877       752     1.37973     1.35074
2021-05-17T09:56:40.342689+0000    13     255      2511      2256   693.717       728     1.38344     1.35345
2021-05-17T09:56:41.343043+0000    14     255      2691      2436   695.576       720     1.40746     1.35732
2021-05-17T09:56:42.343284+0000    15     255      2874      2619   697.992       732     1.39245     1.36088
2021-05-17T09:56:43.343995+0000    16     255      3055      2800   699.585       724     1.41221     1.36388
2021-05-17T09:56:44.344219+0000    17     255      3244      2989   702.893       756     1.35678     1.36537
2021-05-17T09:56:45.344492+0000 Total time run:       17.5857
Total reads made:     3276
Read size:            4194304
Object size:          4194304
Bandwidth (MB/sec):   745.15
Average IOPS:         186
Stddev IOPS:          34.3133
Max IOPS:             197
Min IOPS:             45
Average Latency(s):   1.32911
Max latency(s):       1.47824
Min latency(s):       0.406894

[1;32mlocalhost.localdomain	[2021-05-17T02:56:45,810802816-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 189292


[1;33mlocalhost.localdomain	[2021-05-17T02:56:45,821327518-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:57:08,729049566-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.28k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:57:08,750445364-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:57:16,687090299-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.28k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:57:16,712275279-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:57:24,718095925-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.28k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:57:24,739139688-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:57:32,631051793-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.28k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:57:32,651727858-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:57:40,765107303-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 3.28k objects, 13 GiB
    usage:   26 GiB used, 274 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:57:40,786113904-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T02:57:40,803343431-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T02:57:40,813541362-07:00][RUNNING][ROUND 1/7/40] object_size=16MB[0m


[1;33mlocalhost.localdomain	[2021-05-17T02:57:40,823341005-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T02:57:40,841767285-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40218\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.334990\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 1a797d94-a90b-4add-931b-a8c7aa4e9ebd\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 1a797d94-a90b-4add-931b-a8c7aa4e9ebd\nlast_changed 2021-05-17T02:58:11.553180-0700\ncreated 2021-05-17T02:58:11.553180-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40218/0,v1:10.10.1.2:40219/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.334990 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 b3af9b03-26fc-4d0e-aaa7-d25224ee898c\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 3dbcfd2c-f0a0-44c8-9048-6c410b83cf29\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 0fc11767-79f6-4c62-bd10-469f11839b72\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42218\n  w/ user/pass: admin / a0ea2f9f-ec7c-43f3-8a25-4ac6c89d8557\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 02:58:26 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40218
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.334990
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 1a797d94-a90b-4add-931b-a8c7aa4e9ebd
setting min_mon_release = octopus
epoch 0
fsid 1a797d94-a90b-4add-931b-a8c7aa4e9ebd
last_changed 2021-05-17T02:58:11.553180-0700
created 2021-05-17T02:58:11.553180-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40218/0,v1:10.10.1.2:40219/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.334990 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 b3af9b03-26fc-4d0e-aaa7-d25224ee898c
0
start osd.0
add osd1 3dbcfd2c-f0a0-44c8-9048-6c410b83cf29
1
start osd.1
add osd2 0fc11767-79f6-4c62-bd10-469f11839b72
2
start osd.2


restful urls: https://10.10.1.2:42218
  w/ user/pass: admin / a0ea2f9f-ec7c-43f3-8a25-4ac6c89d8557


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T02:57:41.845-0700 7f414fc871c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T02:57:41.845-0700 7f414fc871c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T02:57:41.861-0700 7fe521ffd1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T02:57:41.861-0700 7fe521ffd1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
WARNING: ceph-osd did not orderly shutdown, killing it hard!
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40218,v1:10.10.1.2:40219] --print /tmp/ceph_monmap.334990 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.334990 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.334990 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42218 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.7QJfl8AeO8 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new b3af9b03-26fc-4d0e-aaa7-d25224ee898c -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQA8PqJgl5VFFxAAM2VUeSpseZ1ekH9+Zqd2wQ== --osd-uuid b3af9b03-26fc-4d0e-aaa7-d25224ee898c 
2021-05-17T02:58:20.713-0700 7f5073bb8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T02:58:20.713-0700 7f5073bb8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T02:58:20.713-0700 7f5073bb8f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T02:58:20.785-0700 7f5073bb8f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 3dbcfd2c-f0a0-44c8-9048-6c410b83cf29 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T02:58:21.097-0700 7fec6b38bf00 -1 Falling back to public interface
2021-05-17T02:58:21.109-0700 7fec6b38bf00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQA9PqJg/aXRBRAAq2QwanOcW7glFgVtihLS7A== --osd-uuid 3dbcfd2c-f0a0-44c8-9048-6c410b83cf29 
2021-05-17T02:58:21.457-0700 7fe7fbcf4f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T02:58:21.461-0700 7fe7fbcf4f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T02:58:21.461-0700 7fe7fbcf4f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T02:58:21.513-0700 7fe7fbcf4f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 0fc11767-79f6-4c62-bd10-469f11839b72 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T02:58:21.881-0700 7fee6a343f00 -1 Falling back to public interface
2021-05-17T02:58:21.897-0700 7fee6a343f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQA9PqJgnXOhNBAAdnqjb/i4oq73k1Qs7hCviA== --osd-uuid 0fc11767-79f6-4c62-bd10-469f11839b72 
2021-05-17T02:58:22.213-0700 7fe9e15c5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T02:58:22.213-0700 7fe9e15c5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T02:58:22.213-0700 7fe9e15c5f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T02:58:22.257-0700 7fe9e15c5f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T02:58:22.645-0700 7f663ed0af00 -1 Falling back to public interface
2021-05-17T02:58:22.657-0700 7f663ed0af00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T02:58:26,532460985-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T02:58:26,542750162-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T02:58:26,627352996-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:58:26,633843119-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fb27cf37-1afe-46c1-8345-249c7be17cfd', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EKogXy:/tmp/ceph-asok.EKogXy', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fb27cf37-1afe-46c1-8345-249c7be17cfd --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EKogXy:/tmp/ceph-asok.EKogXy -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T02:58:29,549127580-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:58:29,555712594-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fb27cf37-1afe-46c1-8345-249c7be17cfd', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EKogXy:/tmp/ceph-asok.EKogXy', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fb27cf37-1afe-46c1-8345-249c7be17cfd --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EKogXy:/tmp/ceph-asok.EKogXy -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T02:58:32,614290315-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:58:32,620994944-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fb27cf37-1afe-46c1-8345-249c7be17cfd', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EKogXy:/tmp/ceph-asok.EKogXy', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fb27cf37-1afe-46c1-8345-249c7be17cfd --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EKogXy:/tmp/ceph-asok.EKogXy -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T02:58:35,372224973-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:58:35,378663383-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fb27cf37-1afe-46c1-8345-249c7be17cfd', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EKogXy:/tmp/ceph-asok.EKogXy', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fb27cf37-1afe-46c1-8345-249c7be17cfd --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EKogXy:/tmp/ceph-asok.EKogXy -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T02:58:40,830823637-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:58:40,838148922-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fb27cf37-1afe-46c1-8345-249c7be17cfd', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EKogXy:/tmp/ceph-asok.EKogXy', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fb27cf37-1afe-46c1-8345-249c7be17cfd --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EKogXy:/tmp/ceph-asok.EKogXy -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T02:58:44,583425629-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:58:44,589801432-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fb27cf37-1afe-46c1-8345-249c7be17cfd', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EKogXy:/tmp/ceph-asok.EKogXy', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fb27cf37-1afe-46c1-8345-249c7be17cfd --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EKogXy:/tmp/ceph-asok.EKogXy -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T02:58:48,097747431-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:58:48,104339046-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fb27cf37-1afe-46c1-8345-249c7be17cfd', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EKogXy:/tmp/ceph-asok.EKogXy', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fb27cf37-1afe-46c1-8345-249c7be17cfd --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EKogXy:/tmp/ceph-asok.EKogXy -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T02:58:51,283803845-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:58:51,289978074-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fb27cf37-1afe-46c1-8345-249c7be17cfd', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EKogXy:/tmp/ceph-asok.EKogXy', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fb27cf37-1afe-46c1-8345-249c7be17cfd --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EKogXy:/tmp/ceph-asok.EKogXy -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T02:58:54,318749585-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:58:54,325198031-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fb27cf37-1afe-46c1-8345-249c7be17cfd', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EKogXy:/tmp/ceph-asok.EKogXy', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fb27cf37-1afe-46c1-8345-249c7be17cfd --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EKogXy:/tmp/ceph-asok.EKogXy -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T02:58:57,669924838-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:58:57,679432606-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fb27cf37-1afe-46c1-8345-249c7be17cfd', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EKogXy:/tmp/ceph-asok.EKogXy', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fb27cf37-1afe-46c1-8345-249c7be17cfd --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EKogXy:/tmp/ceph-asok.EKogXy -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T02:59:01,110704376-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:59:01,117503972-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fb27cf37-1afe-46c1-8345-249c7be17cfd', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EKogXy:/tmp/ceph-asok.EKogXy', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fb27cf37-1afe-46c1-8345-249c7be17cfd --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EKogXy:/tmp/ceph-asok.EKogXy -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T02:59:03,843254127-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T02:59:03,849802340-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fb27cf37-1afe-46c1-8345-249c7be17cfd', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EKogXy:/tmp/ceph-asok.EKogXy', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fb27cf37-1afe-46c1-8345-249c7be17cfd --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EKogXy:/tmp/ceph-asok.EKogXy -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T02:59:06,620166693-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T02:59:29,704595008-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   192 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:59:37,569561744-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:59:45,593564042-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=======.....................] (remaining: 25s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T02:59:53,844561030-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [========....................] (remaining: 49s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:00:01,926215308-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [========....................] (remaining: 55s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:00:09,954609180-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [=========...................] (remaining: 71s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:00:17,954987696-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (40s)
      [=========...................] (remaining: 81s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:00:26,031319194-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   227 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [===============.............] (remaining: 41s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:00:34,200038169-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:00:34,220813942-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:00:42,373851302-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:00:42,396689972-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:00:50,254635699-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:00:50,275637050-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:00:58,147458713-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:00:58,168124847-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:01:06,348881863-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:01:06,370043876-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:01:06,386826371-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T03:01:06,396574601-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T03:01:06,419017578-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=203128
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16MB_write.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-17T03:01:06,437643663-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16MB_write.log.1
[1;30mlocalhost.localdomain	[2021-05-17T03:01:06,479874614-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:01:06,486320585-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fb27cf37-1afe-46c1-8345-249c7be17cfd', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EKogXy:/tmp/ceph-asok.EKogXy', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fb27cf37-1afe-46c1-8345-249c7be17cfd --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EKogXy:/tmp/ceph-asok.EKogXy -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T10:01:07.961+0000 ffff9d501010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:01:07.969+0000 ffff9d501010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:01:07.969+0000 ffff9d501010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T10:01:07.995306+0000 Maintaining 256 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 20 seconds or 0 objects
2021-05-17T10:01:07.995423+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T10:01:11.783457+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T10:01:11.783457+0000     0       0         0         0         0         0           -           0
2021-05-17T10:01:12.783698+0000     1      22        22         0         0         0           -           0
2021-05-17T10:01:13.783920+0000     2      44        44         0         0         0           -           0
2021-05-17T10:01:14.784175+0000     3      66        66         0         0         0           -           0
2021-05-17T10:01:15.784435+0000     4      88        88         0         0         0           -           0
2021-05-17T10:01:16.784686+0000     5     110       110         0         0         0           -           0
2021-05-17T10:01:17.784917+0000     6     132       132         0         0         0           -           0
2021-05-17T10:01:18.785144+0000     7     152       152         0         0         0           -           0
2021-05-17T10:01:19.785337+0000     8     173       173         0         0         0           -           0
2021-05-17T10:01:20.785527+0000     9     196       196         0         0         0           -           0
2021-05-17T10:01:21.785773+0000    10     219       219         0         0         0           -           0
2021-05-17T10:01:22.786058+0000    11     242       242         0         0         0           -           0
2021-05-17T10:01:23.786310+0000    12     255       263         8   10.6643   10.6667     11.6884     11.6796
2021-05-17T10:01:24.786538+0000    13     255       286        31   38.1453       368     11.6945     11.6846
2021-05-17T10:01:25.786786+0000    14     255       309        54   61.7004       368     11.5943     11.6536
2021-05-17T10:01:26.787064+0000    15     255       331        76   81.0481       352     11.5153     11.6244
2021-05-17T10:01:27.787365+0000    16     255       354        99   98.9769       368      11.548     11.6045
2021-05-17T10:01:28.787636+0000    17     255       378       123   115.737       384     11.4494     11.5814
2021-05-17T10:01:29.787898+0000    18     255       401       146   129.747       368     11.3355     11.5549
2021-05-17T10:01:30.788113+0000    19     255       423       168    141.44       352     11.3053     11.5226
2021-05-17T10:01:31.788369+0000 min lat: 0.113004 max lat: 11.7475 avg lat: 8.20123
2021-05-17T10:01:31.788369+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T10:01:31.788369+0000    20       2       445       443   354.316      4400    0.113004     8.20123
2021-05-17T10:01:32.788707+0000 Total time run:         20.0849
Total writes made:      445
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     354.495
Stddev Bandwidth:       969.741
Max bandwidth (MB/sec): 4400
Min bandwidth (MB/sec): 0
Average IOPS:           22
Stddev IOPS:            60.6212
Max IOPS:               275
Min IOPS:               0
Average Latency(s):     8.16492
Stddev Latency(s):      3.78732
Max latency(s):         11.7475
Min latency(s):         0.113004

[1;32mlocalhost.localdomain	[2021-05-17T03:01:33,811308317-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 203128


[1;33mlocalhost.localdomain	[2021-05-17T03:01:33,821629059-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T03:01:56,767738249-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 446 objects, 7.0 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:01:56,789891931-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:02:04,784564752-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 446 objects, 7.0 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:02:04,805582411-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:02:12,784115227-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 446 objects, 7.0 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:02:12,805373827-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:02:20,924948253-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 446 objects, 7.0 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:02:20,945892144-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:02:29,066278080-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 446 objects, 7.0 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:02:29,086954265-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:02:29,104274063-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T03:02:29,114135427-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T03:02:29,137421623-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=206580
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16MB_seq.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-17T03:02:29,156031521-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16MB_seq.log.1
[1;30mlocalhost.localdomain	[2021-05-17T03:02:29,197734982-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:02:29,204177907-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'fb27cf37-1afe-46c1-8345-249c7be17cfd', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EKogXy:/tmp/ceph-asok.EKogXy', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid fb27cf37-1afe-46c1-8345-249c7be17cfd --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.EKogXy:/tmp/ceph-asok.EKogXy -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T10:02:30.679+0000 ffffb09dd010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:02:30.687+0000 ffffb09dd010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:02:30.691+0000 ffffb09dd010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T10:02:30.724677+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T10:02:30.724677+0000     0       0         0         0         0         0           -           0
2021-05-17T10:02:31.724907+0000     1      67        67         0         0         0           -           0
2021-05-17T10:02:32.725137+0000     2     129       129         0         0         0           -           0
2021-05-17T10:02:33.725389+0000     3     194       194         0         0         0           -           0
2021-05-17T10:02:34.725646+0000     4     255       257         2   7.99769         8     3.99214      3.9878
2021-05-17T10:02:35.728734+0000     5     255       305        50   159.864       768     4.34635     4.17922
2021-05-17T10:02:36.728985+0000     6     255       351        96   255.808       736      4.4994     4.29867
2021-05-17T10:02:37.729242+0000     7     255       397       142   324.351       736     4.82581     4.42003
2021-05-17T10:02:39.449359+0000     8     136       445       309   566.656      2672     3.63183     4.51922
2021-05-17T10:02:40.449701+0000 Total time run:       9.49956
Total reads made:     445
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   749.509
Average IOPS:         46
Stddev IOPS:          56.8756
Max IOPS:             167
Min IOPS:             0
Average Latency(s):   3.91865
Max latency(s):       5.11797
Min latency(s):       1.49919

[1;32mlocalhost.localdomain	[2021-05-17T03:02:41,696711991-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 206580


[1;33mlocalhost.localdomain	[2021-05-17T03:02:41,706902691-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T03:03:04,676096515-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 446 objects, 7.0 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:03:04,700208798-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:03:12,711448007-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 446 objects, 7.0 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:03:12,732275059-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:03:20,738197261-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 446 objects, 7.0 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:03:20,758502523-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:03:28,857673334-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 446 objects, 7.0 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:03:28,878581639-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:03:36,951680973-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 446 objects, 7.0 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:03:36,972137687-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:03:36,988965850-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T03:03:36,995484763-07:00][RUNNING][ROUND 2/7/40] object_size=16MB[0m


[1;33mlocalhost.localdomain	[2021-05-17T03:03:37,005057823-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T03:03:37,023508214-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40595\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.336099\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid f876661f-2ba1-48ca-aa0e-371e9a92ff85\nsetting min_mon_release = octopus\nepoch 0\nfsid f876661f-2ba1-48ca-aa0e-371e9a92ff85\nlast_changed 2021-05-17T03:04:04.675887-0700\ncreated 2021-05-17T03:04:04.675887-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40595/0,v1:10.10.1.2:40596/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.336099 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 3f6adc1b-bcc4-4009-af36-791c07d624e8\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 93122565-ef24-4f89-a0c0-503416cf303c\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 d1f72e93-f696-4fcc-9129-c4b6b085e750\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\n'
10.10.1.2: b'restful urls: https://10.10.1.2:42595\n  w/ user/pass: admin / 28cfcda9-2e24-4c34-a1d8-6d76270cd2ae\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 03:04:19 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40595
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.336099
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid f876661f-2ba1-48ca-aa0e-371e9a92ff85
setting min_mon_release = octopus
epoch 0
fsid f876661f-2ba1-48ca-aa0e-371e9a92ff85
last_changed 2021-05-17T03:04:04.675887-0700
created 2021-05-17T03:04:04.675887-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40595/0,v1:10.10.1.2:40596/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.336099 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 3f6adc1b-bcc4-4009-af36-791c07d624e8
0
start osd.0
add osd1 93122565-ef24-4f89-a0c0-503416cf303c
1
start osd.1
add osd2 d1f72e93-f696-4fcc-9129-c4b6b085e750
2
start osd.2


restful urls: https://10.10.1.2:42595
  w/ user/pass: admin / 28cfcda9-2e24-4c34-a1d8-6d76270cd2ae


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T03:03:38.016-0700 7f589fb2a1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T03:03:38.016-0700 7f589fb2a1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T03:03:38.032-0700 7f0b5c6e61c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T03:03:38.032-0700 7f0b5c6e61c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40595,v1:10.10.1.2:40596] --print /tmp/ceph_monmap.336099 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.336099 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.336099 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42595 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.pSWruSJxr7 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 3f6adc1b-bcc4-4009-af36-791c07d624e8 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCdP6JglAeDHxAA1Swzx0UV9LBr6YQGQEGkBg== --osd-uuid 3f6adc1b-bcc4-4009-af36-791c07d624e8 
2021-05-17T03:04:13.884-0700 7efed2463f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T03:04:13.884-0700 7efed2463f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T03:04:13.884-0700 7efed2463f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T03:04:13.972-0700 7efed2463f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 93122565-ef24-4f89-a0c0-503416cf303c -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T03:04:14.276-0700 7fbb700e3f00 -1 Falling back to public interface
2021-05-17T03:04:14.288-0700 7fbb700e3f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCeP6JgyDx7EBAAwQz9FzTGTRG1wf+S1WXomg== --osd-uuid 93122565-ef24-4f89-a0c0-503416cf303c 
2021-05-17T03:04:14.636-0700 7f50637fcf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T03:04:14.636-0700 7f50637fcf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T03:04:14.636-0700 7f50637fcf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T03:04:14.692-0700 7f50637fcf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new d1f72e93-f696-4fcc-9129-c4b6b085e750 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T03:04:15.064-0700 7feffa3aef00 -1 Falling back to public interface
2021-05-17T03:04:15.080-0700 7feffa3aef00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQCfP6Jgb+KkAhAA/zzRMGiz9MIAD/16pwjGcQ== --osd-uuid d1f72e93-f696-4fcc-9129-c4b6b085e750 
2021-05-17T03:04:15.376-0700 7fcdcb778f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T03:04:15.376-0700 7fcdcb778f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T03:04:15.376-0700 7fcdcb778f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T03:04:15.464-0700 7fcdcb778f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T03:04:15.764-0700 7fc98a94df00 -1 Falling back to public interface
2021-05-17T03:04:15.780-0700 7fc98a94df00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T03:04:19,749063418-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T03:04:19,759236737-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T03:04:19,843618787-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:04:19,850245323-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b44681a-b36a-498c-b4db-b7d5e3619389', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XBa2y3:/tmp/ceph-asok.XBa2y3', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b44681a-b36a-498c-b4db-b7d5e3619389 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XBa2y3:/tmp/ceph-asok.XBa2y3 -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T03:04:22,649625082-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:04:22,656126347-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b44681a-b36a-498c-b4db-b7d5e3619389', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XBa2y3:/tmp/ceph-asok.XBa2y3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b44681a-b36a-498c-b4db-b7d5e3619389 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XBa2y3:/tmp/ceph-asok.XBa2y3 -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T03:04:25,584349484-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:04:25,590868827-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b44681a-b36a-498c-b4db-b7d5e3619389', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XBa2y3:/tmp/ceph-asok.XBa2y3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b44681a-b36a-498c-b4db-b7d5e3619389 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XBa2y3:/tmp/ceph-asok.XBa2y3 -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T03:04:28,266211203-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:04:28,272950844-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b44681a-b36a-498c-b4db-b7d5e3619389', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XBa2y3:/tmp/ceph-asok.XBa2y3', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b44681a-b36a-498c-b4db-b7d5e3619389 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XBa2y3:/tmp/ceph-asok.XBa2y3 -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T03:04:33,908202659-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:04:33,914883490-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b44681a-b36a-498c-b4db-b7d5e3619389', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XBa2y3:/tmp/ceph-asok.XBa2y3', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b44681a-b36a-498c-b4db-b7d5e3619389 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XBa2y3:/tmp/ceph-asok.XBa2y3 -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T03:04:37,838391128-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:04:37,844914582-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b44681a-b36a-498c-b4db-b7d5e3619389', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XBa2y3:/tmp/ceph-asok.XBa2y3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b44681a-b36a-498c-b4db-b7d5e3619389 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XBa2y3:/tmp/ceph-asok.XBa2y3 -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T03:04:41,153638160-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:04:41,160187988-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b44681a-b36a-498c-b4db-b7d5e3619389', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XBa2y3:/tmp/ceph-asok.XBa2y3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b44681a-b36a-498c-b4db-b7d5e3619389 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XBa2y3:/tmp/ceph-asok.XBa2y3 -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T03:04:44,373649184-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:04:44,380086973-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b44681a-b36a-498c-b4db-b7d5e3619389', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XBa2y3:/tmp/ceph-asok.XBa2y3', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b44681a-b36a-498c-b4db-b7d5e3619389 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XBa2y3:/tmp/ceph-asok.XBa2y3 -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T03:04:47,316574417-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:04:47,323108703-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b44681a-b36a-498c-b4db-b7d5e3619389', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XBa2y3:/tmp/ceph-asok.XBa2y3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b44681a-b36a-498c-b4db-b7d5e3619389 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XBa2y3:/tmp/ceph-asok.XBa2y3 -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T03:04:50,776656143-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:04:50,783245518-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b44681a-b36a-498c-b4db-b7d5e3619389', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XBa2y3:/tmp/ceph-asok.XBa2y3', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b44681a-b36a-498c-b4db-b7d5e3619389 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XBa2y3:/tmp/ceph-asok.XBa2y3 -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T03:04:54,228628752-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:04:54,234901394-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b44681a-b36a-498c-b4db-b7d5e3619389', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XBa2y3:/tmp/ceph-asok.XBa2y3', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b44681a-b36a-498c-b4db-b7d5e3619389 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XBa2y3:/tmp/ceph-asok.XBa2y3 -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T03:04:56,960716119-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:04:56,967256976-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b44681a-b36a-498c-b4db-b7d5e3619389', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XBa2y3:/tmp/ceph-asok.XBa2y3', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b44681a-b36a-498c-b4db-b7d5e3619389 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XBa2y3:/tmp/ceph-asok.XBa2y3 -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T03:04:59,759130148-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T03:05:22,622120475-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   189 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:05:30,676656940-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:05:38,842458604-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=======.....................] (remaining: 27s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:05:46,819481255-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [========....................] (remaining: 47s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:05:54,925989327-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [========....................] (remaining: 53s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:06:02,736789730-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [=========...................] (remaining: 73s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:06:10,776084758-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   211 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (40s)
      [=========...................] (remaining: 83s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:06:18,919280802-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   221 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [===============.............] (remaining: 41s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:06:27,065756647-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:06:27,086236616-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:06:35,069348516-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:06:35,090110833-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:06:43,039998620-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:06:43,060809350-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:06:51,057351290-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:06:51,077969193-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:06:59,238264306-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:06:59,258666923-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:06:59,275346100-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T03:06:59,285183094-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T03:06:59,308454019-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=220228
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16MB_write.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-17T03:06:59,327060740-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16MB_write.log.2
[1;30mlocalhost.localdomain	[2021-05-17T03:06:59,370210361-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:06:59,376835429-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b44681a-b36a-498c-b4db-b7d5e3619389', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XBa2y3:/tmp/ceph-asok.XBa2y3', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b44681a-b36a-498c-b4db-b7d5e3619389 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XBa2y3:/tmp/ceph-asok.XBa2y3 -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T10:07:01.101+0000 ffffa3a15010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:07:01.105+0000 ffffa3a15010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:07:01.109+0000 ffffa3a15010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T10:07:01.133314+0000 Maintaining 256 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 20 seconds or 0 objects
2021-05-17T10:07:01.133433+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T10:07:05.329776+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T10:07:05.329776+0000     0       0         0         0         0         0           -           0
2021-05-17T10:07:06.329998+0000     1      21        21         0         0         0           -           0
2021-05-17T10:07:07.330212+0000     2      44        44         0         0         0           -           0
2021-05-17T10:07:08.330447+0000     3      66        66         0         0         0           -           0
2021-05-17T10:07:09.330748+0000     4      88        88         0         0         0           -           0
2021-05-17T10:07:10.330959+0000     5     110       110         0         0         0           -           0
2021-05-17T10:07:11.331176+0000     6     132       132         0         0         0           -           0
2021-05-17T10:07:12.331393+0000     7     155       155         0         0         0           -           0
2021-05-17T10:07:13.331604+0000     8     178       178         0         0         0           -           0
2021-05-17T10:07:14.331830+0000     9     200       200         0         0         0           -           0
2021-05-17T10:07:15.332052+0000    10     223       223         0         0         0           -           0
2021-05-17T10:07:16.332258+0000    11     247       247         0         0         0           -           0
2021-05-17T10:07:17.332463+0000    12     255       270        15   19.9957        20     11.3651     11.3918
2021-05-17T10:07:18.332709+0000    13     255       292        37   45.5286       352     11.3307     11.3729
2021-05-17T10:07:19.332956+0000    14     255       314        59   67.4139       352     11.3433     11.3666
2021-05-17T10:07:20.333285+0000    15     255       339        84   89.5798       400      11.247     11.3488
2021-05-17T10:07:21.333605+0000    16     255       361       106   105.975       352     11.2298     11.3267
2021-05-17T10:07:22.333819+0000    17     255       384       129   121.384       368     11.1837     11.3081
2021-05-17T10:07:23.334045+0000    18     255       405       150   133.303       336     11.2287     11.2944
2021-05-17T10:07:24.334267+0000    19     255       426       171   143.967       336      11.323     11.2922
2021-05-17T10:07:25.334552+0000 min lat: 0.131702 max lat: 11.4708 avg lat: 8.17474
2021-05-17T10:07:25.334552+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T10:07:25.334552+0000    20       2       449       447   357.517      4416    0.131702     8.17474
2021-05-17T10:07:26.334838+0000 Total time run:         20.079
Total writes made:      449
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     357.787
Stddev Bandwidth:       973.106
Max bandwidth (MB/sec): 4416
Min bandwidth (MB/sec): 0
Average IOPS:           22
Stddev IOPS:            60.8235
Max IOPS:               276
Min IOPS:               0
Average Latency(s):     8.13887
Stddev Latency(s):      3.69133
Max latency(s):         11.4708
Min latency(s):         0.10925

[1;32mlocalhost.localdomain	[2021-05-17T03:07:27,183633080-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 220228


[1;33mlocalhost.localdomain	[2021-05-17T03:07:27,194357205-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T03:07:50,148576222-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 450 objects, 7.0 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:07:50,169367721-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:07:58,166875021-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 450 objects, 7.0 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:07:58,191646127-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:08:06,190381118-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 450 objects, 7.0 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:08:06,210943769-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:08:14,227829630-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 450 objects, 7.0 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:08:14,248748141-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:08:22,266449708-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 450 objects, 7.0 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:08:22,287224329-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:08:22,304119200-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T03:08:22,313952429-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T03:08:22,336871906-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=223712
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16MB_seq.dat.2 2

[1;32mlocalhost.localdomain	[2021-05-17T03:08:22,355481299-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16MB_seq.log.2
[1;30mlocalhost.localdomain	[2021-05-17T03:08:22,396702121-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:08:22,403006329-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '9b44681a-b36a-498c-b4db-b7d5e3619389', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XBa2y3:/tmp/ceph-asok.XBa2y3', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 9b44681a-b36a-498c-b4db-b7d5e3619389 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.XBa2y3:/tmp/ceph-asok.XBa2y3 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T10:08:23.891+0000 ffffa6429010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:08:23.899+0000 ffffa6429010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:08:23.915+0000 ffffa6429010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T10:08:23.947407+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T10:08:23.947407+0000     0       0         0         0         0         0           -           0
2021-05-17T10:08:24.947738+0000     1      70        70         0         0         0           -           0
2021-05-17T10:08:25.948070+0000     2     136       136         0         0         0           -           0
2021-05-17T10:08:26.948314+0000     3     206       206         0         0         0           -           0
2021-05-17T10:08:27.952072+0000     4     255       272        17   67.9175        68     3.81344     3.78563
2021-05-17T10:08:28.958624+0000     5     255       317        62   197.948       720     4.15052     3.92987
2021-05-17T10:08:29.958862+0000     6     255       364       109   290.104       752     4.41743     4.08043
2021-05-17T10:08:30.959115+0000     7     255       410       155   353.684       736     4.68919     4.22846
2021-05-17T10:08:32.102174+0000     8     203       449       246   482.651      1456     4.59141     4.44132
2021-05-17T10:08:33.267068+0000 Total time run:       9.31976
Total reads made:     449
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   770.836
Average IOPS:         48
Stddev IOPS:          33.5109
Max IOPS:             91
Min IOPS:             0
Average Latency(s):   3.83287
Max latency(s):       4.96689
Min latency(s):       1.55114

[1;32mlocalhost.localdomain	[2021-05-17T03:08:34,172468851-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 223712


[1;33mlocalhost.localdomain	[2021-05-17T03:08:34,183030409-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T03:08:57,188219522-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 450 objects, 7.0 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:08:57,209640724-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:09:05,211600509-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 450 objects, 7.0 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:09:05,232757801-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:09:13,232248849-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 450 objects, 7.0 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:09:13,253441353-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:09:21,275310551-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 450 objects, 7.0 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:09:21,296374363-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:09:29,269397756-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 450 objects, 7.0 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:09:29,292475661-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:09:29,311973626-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T03:09:29,319614622-07:00][RUNNING][ROUND 3/7/40] object_size=16MB[0m


[1;33mlocalhost.localdomain	[2021-05-17T03:09:29,331152367-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T03:09:29,353152249-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40080\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.337205\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid e62f1206-8930-47b3-84c2-20766f0fd715\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid e62f1206-8930-47b3-84c2-20766f0fd715\nlast_changed 2021-05-17T03:09:56.860884-0700\ncreated 2021-05-17T03:09:56.860884-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40080/0,v1:10.10.1.2:40081/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.337205 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 39e11e07-e687-410f-bb2f-1cf55db91f80\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 b6e6a0e2-1964-495c-9920-5dda976651e0\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 2d5aa33b-665c-468d-be18-f9b5fbf3f82a\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\nrestful urls: https://10.10.1.2:42080\n  w/ user/pass: admin / a907f077-45ea-43ad-8ade-0250ba5f6c86\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 03:10:12 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40080
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.337205
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid e62f1206-8930-47b3-84c2-20766f0fd715
setting min_mon_release = octopus
epoch 0
fsid e62f1206-8930-47b3-84c2-20766f0fd715
last_changed 2021-05-17T03:09:56.860884-0700
created 2021-05-17T03:09:56.860884-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40080/0,v1:10.10.1.2:40081/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.337205 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 39e11e07-e687-410f-bb2f-1cf55db91f80
0
start osd.0
add osd1 b6e6a0e2-1964-495c-9920-5dda976651e0
1
start osd.1
add osd2 2d5aa33b-665c-468d-be18-f9b5fbf3f82a
2
start osd.2


restful urls: https://10.10.1.2:42080
  w/ user/pass: admin / a907f077-45ea-43ad-8ade-0250ba5f6c86


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T03:09:30.351-0700 7f0b8cc5b1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T03:09:30.351-0700 7f0b8cc5b1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T03:09:30.367-0700 7fdea73b41c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T03:09:30.367-0700 7fdea73b41c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40080,v1:10.10.1.2:40081] --print /tmp/ceph_monmap.337205 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.337205 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.337205 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42080 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.MNjUTNmaq8 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 39e11e07-e687-410f-bb2f-1cf55db91f80 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQD+QKJgXxdhIxAAamKX5cZyO40QYO/PmaBGdw== --osd-uuid 39e11e07-e687-410f-bb2f-1cf55db91f80 
2021-05-17T03:10:06.916-0700 7fce209d4f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T03:10:06.916-0700 7fce209d4f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T03:10:06.916-0700 7fce209d4f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T03:10:06.996-0700 7fce209d4f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new b6e6a0e2-1964-495c-9920-5dda976651e0 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T03:10:07.324-0700 7f76f31a6f00 -1 Falling back to public interface
2021-05-17T03:10:07.336-0700 7f76f31a6f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQD/QKJg9BJnExAAOZXSkyEueKMktrTvz2naUw== --osd-uuid b6e6a0e2-1964-495c-9920-5dda976651e0 
2021-05-17T03:10:07.696-0700 7fbc8022cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T03:10:07.696-0700 7fbc8022cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T03:10:07.696-0700 7fbc8022cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T03:10:07.772-0700 7fbc8022cf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 2d5aa33b-665c-468d-be18-f9b5fbf3f82a -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T03:10:08.176-0700 7f34cd576f00 -1 Falling back to public interface
2021-05-17T03:10:08.188-0700 7f34cd576f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQAAQaJg1feKChAAGDiqaFqOSeC+IaxxRUoZhA== --osd-uuid 2d5aa33b-665c-468d-be18-f9b5fbf3f82a 
2021-05-17T03:10:08.532-0700 7fcb708e2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T03:10:08.532-0700 7fcb708e2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T03:10:08.532-0700 7fcb708e2f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T03:10:08.584-0700 7fcb708e2f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T03:10:08.920-0700 7fbdbffc0f00 -1 Falling back to public interface
2021-05-17T03:10:08.936-0700 7fbdbffc0f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T03:10:12,873364282-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T03:10:12,884171778-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T03:10:12,979323220-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:10:12,986529067-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5385991e-2920-4814-aa83-f1b5030dde54', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Sf0umW:/tmp/ceph-asok.Sf0umW', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5385991e-2920-4814-aa83-f1b5030dde54 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Sf0umW:/tmp/ceph-asok.Sf0umW -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T03:10:15,854484179-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:10:15,860941309-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5385991e-2920-4814-aa83-f1b5030dde54', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Sf0umW:/tmp/ceph-asok.Sf0umW', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5385991e-2920-4814-aa83-f1b5030dde54 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Sf0umW:/tmp/ceph-asok.Sf0umW -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T03:10:18,960308398-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:10:18,967079561-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5385991e-2920-4814-aa83-f1b5030dde54', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Sf0umW:/tmp/ceph-asok.Sf0umW', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5385991e-2920-4814-aa83-f1b5030dde54 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Sf0umW:/tmp/ceph-asok.Sf0umW -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T03:10:21,867519959-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:10:21,874050542-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5385991e-2920-4814-aa83-f1b5030dde54', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Sf0umW:/tmp/ceph-asok.Sf0umW', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5385991e-2920-4814-aa83-f1b5030dde54 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Sf0umW:/tmp/ceph-asok.Sf0umW -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T03:10:27,405232427-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:10:27,411773557-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5385991e-2920-4814-aa83-f1b5030dde54', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Sf0umW:/tmp/ceph-asok.Sf0umW', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5385991e-2920-4814-aa83-f1b5030dde54 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Sf0umW:/tmp/ceph-asok.Sf0umW -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T03:10:30,620100962-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:10:30,626483544-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5385991e-2920-4814-aa83-f1b5030dde54', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Sf0umW:/tmp/ceph-asok.Sf0umW', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5385991e-2920-4814-aa83-f1b5030dde54 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Sf0umW:/tmp/ceph-asok.Sf0umW -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T03:10:34,474467460-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:10:34,481214829-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5385991e-2920-4814-aa83-f1b5030dde54', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Sf0umW:/tmp/ceph-asok.Sf0umW', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5385991e-2920-4814-aa83-f1b5030dde54 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Sf0umW:/tmp/ceph-asok.Sf0umW -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T03:10:37,631404512-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:10:37,637912081-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5385991e-2920-4814-aa83-f1b5030dde54', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Sf0umW:/tmp/ceph-asok.Sf0umW', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5385991e-2920-4814-aa83-f1b5030dde54 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Sf0umW:/tmp/ceph-asok.Sf0umW -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T03:10:41,350614731-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:10:41,356944294-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5385991e-2920-4814-aa83-f1b5030dde54', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Sf0umW:/tmp/ceph-asok.Sf0umW', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5385991e-2920-4814-aa83-f1b5030dde54 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Sf0umW:/tmp/ceph-asok.Sf0umW -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T03:10:44,721727893-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:10:44,728210902-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5385991e-2920-4814-aa83-f1b5030dde54', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Sf0umW:/tmp/ceph-asok.Sf0umW', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5385991e-2920-4814-aa83-f1b5030dde54 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Sf0umW:/tmp/ceph-asok.Sf0umW -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T03:10:47,968149722-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:10:47,974690626-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5385991e-2920-4814-aa83-f1b5030dde54', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Sf0umW:/tmp/ceph-asok.Sf0umW', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5385991e-2920-4814-aa83-f1b5030dde54 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Sf0umW:/tmp/ceph-asok.Sf0umW -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T03:10:50,879459007-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:10:50,885936750-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5385991e-2920-4814-aa83-f1b5030dde54', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Sf0umW:/tmp/ceph-asok.Sf0umW', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5385991e-2920-4814-aa83-f1b5030dde54 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Sf0umW:/tmp/ceph-asok.Sf0umW -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   47 KiB   0 B   0 B   0 B  100 GiB     0  0.95   70      up          osd.2  
                       TOTAL  300 GiB  151 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.95/1.03  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T03:10:53,555959427-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T03:11:16,582444599-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   188 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
    Global Recovery Event (4s)
      [=======================.....] (remaining: 1s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:11:24,504200041-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   209 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (9s)
      [=========================...] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:11:32,662479627-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   209 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:11:40,739799641-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   209 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:11:48,727687043-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   209 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:11:57,026786298-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   209 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:12:04,992295366-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   209 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:12:13,230900134-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:12:13,251836457-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:12:21,237265547-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:12:21,258367648-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:12:29,369934790-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:12:29,390797030-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:12:37,613206339-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:12:37,634043165-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:12:45,780061084-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:12:45,801020556-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:12:45,818576518-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T03:12:45,828448965-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T03:12:45,851817023-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=236943
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16MB_write.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-17T03:12:45,870634595-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16MB_write.log.3
[1;30mlocalhost.localdomain	[2021-05-17T03:12:45,912721237-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:12:45,919116617-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5385991e-2920-4814-aa83-f1b5030dde54', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Sf0umW:/tmp/ceph-asok.Sf0umW', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5385991e-2920-4814-aa83-f1b5030dde54 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Sf0umW:/tmp/ceph-asok.Sf0umW -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T10:12:47.412+0000 ffff8fed9010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:12:47.420+0000 ffff8fed9010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:12:47.420+0000 ffff8fed9010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T10:12:47.446498+0000 Maintaining 256 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 20 seconds or 0 objects
2021-05-17T10:12:47.446613+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T10:12:51.474194+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T10:12:51.474194+0000     0       0         0         0         0         0           -           0
2021-05-17T10:12:52.474455+0000     1      21        21         0         0         0           -           0
2021-05-17T10:12:53.474718+0000     2      40        40         0         0         0           -           0
2021-05-17T10:12:54.474916+0000     3      58        58         0         0         0           -           0
2021-05-17T10:12:55.475121+0000     4      79        79         0         0         0           -           0
2021-05-17T10:12:56.475355+0000     5      97        97         0         0         0           -           0
2021-05-17T10:12:57.475582+0000     6     117       117         0         0         0           -           0
2021-05-17T10:12:58.475821+0000     7     138       138         0         0         0           -           0
2021-05-17T10:12:59.476066+0000     8     156       156         0         0         0           -           0
2021-05-17T10:13:00.476304+0000     9     178       178         0         0         0           -           0
2021-05-17T10:13:01.476542+0000    10     200       200         0         0         0           -           0
2021-05-17T10:13:02.476767+0000    11     221       221         0         0         0           -           0
2021-05-17T10:13:03.477008+0000    12     242       242         0         0         0           -           0
2021-05-17T10:13:04.477238+0000    13     255       261         6   7.38299   7.38462     12.7208     12.7658
2021-05-17T10:13:05.477475+0000    14     255       281        26   29.7077       320     12.8026      12.804
2021-05-17T10:13:06.477700+0000    15     255       302        47   50.1222       336     12.7037      12.767
2021-05-17T10:13:07.477915+0000    16     255       326        71   70.9843       384     12.4545      12.698
2021-05-17T10:13:08.478110+0000    17     255       348        93   87.5102       352     12.1406     12.6088
2021-05-17T10:13:09.478308+0000    18     255       369       114   101.311       336     12.1975     12.5369
2021-05-17T10:13:10.478566+0000    19     255       390       135   113.659       336     12.1741      12.491
2021-05-17T10:13:11.478939+0000 min lat: 12.0317 max lat: 12.8633 avg lat: 12.4428
2021-05-17T10:13:11.478939+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T10:13:11.478939+0000    20     255       412       157   125.571       352     12.0353     12.4428
2021-05-17T10:13:12.479225+0000 Total time run:         20.1373
Total writes made:      413
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     328.147
Stddev Bandwidth:       169.017
Max bandwidth (MB/sec): 384
Min bandwidth (MB/sec): 0
Average IOPS:           20
Stddev IOPS:            10.5804
Max IOPS:               24
Min IOPS:               0
Average Latency(s):     8.46834
Stddev Latency(s):      4.15672
Max latency(s):         12.8633
Min latency(s):         0.129928

[1;32mlocalhost.localdomain	[2021-05-17T03:13:13,325320346-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 236943


[1;33mlocalhost.localdomain	[2021-05-17T03:13:13,335650237-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T03:13:36,313908600-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 414 objects, 6.5 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:13:36,335639309-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:13:44,421365593-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 414 objects, 6.5 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:13:44,442408374-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:13:52,613442622-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 414 objects, 6.5 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:13:52,634557161-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:14:00,687275602-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 414 objects, 6.5 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:14:00,708177090-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:14:08,873226521-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 414 objects, 6.5 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:14:08,894130365-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:14:08,911328681-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T03:14:08,921410310-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T03:14:08,944462879-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=240331
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16MB_seq.dat.3 2

[1;32mlocalhost.localdomain	[2021-05-17T03:14:08,963336052-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16MB_seq.log.3
[1;30mlocalhost.localdomain	[2021-05-17T03:14:09,004878043-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:14:09,011567578-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '5385991e-2920-4814-aa83-f1b5030dde54', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Sf0umW:/tmp/ceph-asok.Sf0umW', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 5385991e-2920-4814-aa83-f1b5030dde54 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.Sf0umW:/tmp/ceph-asok.Sf0umW -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T10:14:10.682+0000 ffffb2e71010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:14:10.690+0000 ffffb2e71010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:14:10.690+0000 ffffb2e71010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T10:14:10.724242+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T10:14:10.724242+0000     0       0         0         0         0         0           -           0
2021-05-17T10:14:11.724489+0000     1      66        66         0         0         0           -           0
2021-05-17T10:14:12.724764+0000     2     127       127         0         0         0           -           0
2021-05-17T10:14:13.725008+0000     3     188       188         0         0         0           -           0
2021-05-17T10:14:14.725237+0000     4     246       246         0         0         0           -           0
2021-05-17T10:14:15.725490+0000     5     255       293        38   121.565     121.6       4.439     4.31351
2021-05-17T10:14:16.729320+0000     6     255       339        84   223.803       736     4.73775     4.46843
2021-05-17T10:14:17.733687+0000     7     255       379       124   283.039       640     5.08089     4.61619
2021-05-17T10:14:18.857488+0000     8     197       413       216   424.912      1472     4.62305     4.80615
2021-05-17T10:14:19.965387+0000 Total time run:       9.24126
Total reads made:     413
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   715.054
Average IOPS:         44
Stddev IOPS:          33.7022
Max IOPS:             92
Min IOPS:             0
Average Latency(s):   4.0288
Max latency(s):       5.31009
Min latency(s):       1.48116

[1;32mlocalhost.localdomain	[2021-05-17T03:14:20,915568470-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 240331


[1;33mlocalhost.localdomain	[2021-05-17T03:14:20,926139047-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T03:14:43,748459712-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 414 objects, 6.5 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:14:43,772622531-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:14:51,800687788-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 414 objects, 6.5 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:14:51,822387442-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:14:59,821752054-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 414 objects, 6.5 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:14:59,843277634-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:15:07,826392313-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 414 objects, 6.5 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:15:07,848512669-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:15:16,003068190-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 414 objects, 6.5 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:15:16,024428457-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:15:16,042226426-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T03:15:16,048789565-07:00][RUNNING][ROUND 4/7/40] object_size=16MB[0m


[1;33mlocalhost.localdomain	[2021-05-17T03:15:16,058596152-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T03:15:16,077268216-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40518\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.338337\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 16e20fe6-856b-43b9-b19f-9dab173b2c07\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 16e20fe6-856b-43b9-b19f-9dab173b2c07\nlast_changed 2021-05-17T03:15:43.137796-0700\ncreated 2021-05-17T03:15:43.137796-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40518/0,v1:10.10.1.2:40519/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.338337 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 1f476d1d-c744-4802-9926-7f7d37e76339\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 1b04f5e8-9fda-4290-b165-77a7dabb2fb2\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 0265109f-54b0-4e28-97d2-d5ca3fbc316b\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\n'
10.10.1.2: b'restful urls: https://10.10.1.2:42518\n  w/ user/pass: admin / 7b715824-6bf2-41f4-8944-a889ab2fd8c0\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 03:15:58 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40518
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.338337
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 16e20fe6-856b-43b9-b19f-9dab173b2c07
setting min_mon_release = octopus
epoch 0
fsid 16e20fe6-856b-43b9-b19f-9dab173b2c07
last_changed 2021-05-17T03:15:43.137796-0700
created 2021-05-17T03:15:43.137796-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40518/0,v1:10.10.1.2:40519/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.338337 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 1f476d1d-c744-4802-9926-7f7d37e76339
0
start osd.0
add osd1 1b04f5e8-9fda-4290-b165-77a7dabb2fb2
1
start osd.1
add osd2 0265109f-54b0-4e28-97d2-d5ca3fbc316b
2
start osd.2


restful urls: https://10.10.1.2:42518
  w/ user/pass: admin / 7b715824-6bf2-41f4-8944-a889ab2fd8c0


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T03:15:17.070-0700 7fd52222a1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T03:15:17.070-0700 7fd52222a1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T03:15:17.086-0700 7fdbaee031c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T03:15:17.086-0700 7fdbaee031c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40518,v1:10.10.1.2:40519] --print /tmp/ceph_monmap.338337 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.338337 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.338337 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42518 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.r6fW7SJjQw 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 1f476d1d-c744-4802-9926-7f7d37e76339 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBYQqJg5wQ8CRAA0ZFdqtP1X6PbwImhFEce+Q== --osd-uuid 1f476d1d-c744-4802-9926-7f7d37e76339 
2021-05-17T03:15:52.563-0700 7f51e7e7df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T03:15:52.563-0700 7f51e7e7df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T03:15:52.563-0700 7f51e7e7df00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T03:15:52.635-0700 7f51e7e7df00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 1b04f5e8-9fda-4290-b165-77a7dabb2fb2 -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T03:15:52.903-0700 7feb99a39f00 -1 Falling back to public interface
2021-05-17T03:15:52.915-0700 7feb99a39f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBYQqJg2PHaNRAAEJSFK5g671XejtbXPT/CSQ== --osd-uuid 1b04f5e8-9fda-4290-b165-77a7dabb2fb2 
2021-05-17T03:15:53.243-0700 7f0b4b41af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T03:15:53.243-0700 7f0b4b41af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T03:15:53.243-0700 7f0b4b41af00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T03:15:53.287-0700 7f0b4b41af00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 0265109f-54b0-4e28-97d2-d5ca3fbc316b -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T03:15:53.691-0700 7f4ba9941f00 -1 Falling back to public interface
2021-05-17T03:15:53.703-0700 7f4ba9941f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQBZQqJgKEEIKRAAme0qu8qx8+d9erd4svJUag== --osd-uuid 0265109f-54b0-4e28-97d2-d5ca3fbc316b 
2021-05-17T03:15:54.043-0700 7f5a86bb6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T03:15:54.043-0700 7f5a86bb6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T03:15:54.043-0700 7f5a86bb6f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T03:15:54.123-0700 7f5a86bb6f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T03:15:54.403-0700 7f874bc0ff00 -1 Falling back to public interface
2021-05-17T03:15:54.415-0700 7f874bc0ff00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T03:15:58,360581302-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T03:15:58,370753501-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T03:15:58,456566843-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:15:58,463081689-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd09695e5-984f-4639-bb07-c5b940c9568c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FVTxas:/tmp/ceph-asok.FVTxas', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d09695e5-984f-4639-bb07-c5b940c9568c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FVTxas:/tmp/ceph-asok.FVTxas -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T03:16:01,225815200-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:16:01,232591856-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd09695e5-984f-4639-bb07-c5b940c9568c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FVTxas:/tmp/ceph-asok.FVTxas', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d09695e5-984f-4639-bb07-c5b940c9568c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FVTxas:/tmp/ceph-asok.FVTxas -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T03:16:04,002408857-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:16:04,009030717-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd09695e5-984f-4639-bb07-c5b940c9568c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FVTxas:/tmp/ceph-asok.FVTxas', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d09695e5-984f-4639-bb07-c5b940c9568c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FVTxas:/tmp/ceph-asok.FVTxas -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T03:16:06,969851125-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:16:06,976297259-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd09695e5-984f-4639-bb07-c5b940c9568c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FVTxas:/tmp/ceph-asok.FVTxas', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d09695e5-984f-4639-bb07-c5b940c9568c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FVTxas:/tmp/ceph-asok.FVTxas -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T03:16:12,844955936-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:16:12,851702482-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd09695e5-984f-4639-bb07-c5b940c9568c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FVTxas:/tmp/ceph-asok.FVTxas', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d09695e5-984f-4639-bb07-c5b940c9568c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FVTxas:/tmp/ceph-asok.FVTxas -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T03:16:15,779852864-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:16:15,786488330-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd09695e5-984f-4639-bb07-c5b940c9568c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FVTxas:/tmp/ceph-asok.FVTxas', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d09695e5-984f-4639-bb07-c5b940c9568c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FVTxas:/tmp/ceph-asok.FVTxas -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T03:16:19,094563860-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:16:19,101015335-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd09695e5-984f-4639-bb07-c5b940c9568c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FVTxas:/tmp/ceph-asok.FVTxas', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d09695e5-984f-4639-bb07-c5b940c9568c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FVTxas:/tmp/ceph-asok.FVTxas -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T03:16:22,834015121-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:16:22,840694320-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd09695e5-984f-4639-bb07-c5b940c9568c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FVTxas:/tmp/ceph-asok.FVTxas', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d09695e5-984f-4639-bb07-c5b940c9568c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FVTxas:/tmp/ceph-asok.FVTxas -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T03:16:26,079695349-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:16:26,086536801-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd09695e5-984f-4639-bb07-c5b940c9568c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FVTxas:/tmp/ceph-asok.FVTxas', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d09695e5-984f-4639-bb07-c5b940c9568c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FVTxas:/tmp/ceph-asok.FVTxas -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T03:16:29,328223937-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:16:29,334745684-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd09695e5-984f-4639-bb07-c5b940c9568c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FVTxas:/tmp/ceph-asok.FVTxas', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d09695e5-984f-4639-bb07-c5b940c9568c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FVTxas:/tmp/ceph-asok.FVTxas -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T03:16:32,805465509-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:16:32,811987106-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd09695e5-984f-4639-bb07-c5b940c9568c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FVTxas:/tmp/ceph-asok.FVTxas', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d09695e5-984f-4639-bb07-c5b940c9568c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FVTxas:/tmp/ceph-asok.FVTxas -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T03:16:35,549998506-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:16:35,556563221-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd09695e5-984f-4639-bb07-c5b940c9568c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FVTxas:/tmp/ceph-asok.FVTxas', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d09695e5-984f-4639-bb07-c5b940c9568c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FVTxas:/tmp/ceph-asok.FVTxas -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   47 KiB   0 B   0 B   0 B  100 GiB     0  0.95   70      up          osd.2  
                       TOTAL  300 GiB  151 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.95/1.03  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T03:16:38,150983142-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T03:17:01,105761218-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   188 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:17:09,235919426-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   209 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (0s)
      [===.........................] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:17:17,328471599-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   209 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=======.....................] (remaining: 29s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:17:25,372123418-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   209 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [=======.....................] (remaining: 55s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:17:33,421299095-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   209 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (25s)
      [========....................] (remaining: 57s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:17:41,557010286-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   209 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [========....................] (remaining: 75s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:17:49,638106198-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   209 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (40s)
      [========....................] (remaining: 85s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:17:57,719822905-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   227 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [===============.............] (remaining: 42s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:18:05,863073287-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:18:05,884569523-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:18:13,959500702-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:18:13,981059394-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:18:21,933586140-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:18:21,955474259-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:18:29,929727714-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:18:29,951422824-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:18:37,983306962-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:18:38,005179582-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:18:38,022790786-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T03:18:38,032575230-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T03:18:38,056247753-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=254000
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16MB_write.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-17T03:18:38,075741477-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16MB_write.log.4
[1;30mlocalhost.localdomain	[2021-05-17T03:18:38,118011772-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:18:38,124557194-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd09695e5-984f-4639-bb07-c5b940c9568c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FVTxas:/tmp/ceph-asok.FVTxas', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d09695e5-984f-4639-bb07-c5b940c9568c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FVTxas:/tmp/ceph-asok.FVTxas -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T10:18:39.836+0000 ffffa8b10010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:18:39.840+0000 ffffa8b10010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:18:39.840+0000 ffffa8b10010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T10:18:39.869255+0000 Maintaining 256 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 20 seconds or 0 objects
2021-05-17T10:18:39.869375+0000 Object prefix: benchmark_data_localhost.localdomain_7
2021-05-17T10:18:43.835203+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T10:18:43.835203+0000     0       0         0         0         0         0           -           0
2021-05-17T10:18:44.835461+0000     1      20        20         0         0         0           -           0
2021-05-17T10:18:45.835699+0000     2      42        42         0         0         0           -           0
2021-05-17T10:18:46.835956+0000     3      62        62         0         0         0           -           0
2021-05-17T10:18:47.836269+0000     4      83        83         0         0         0           -           0
2021-05-17T10:18:48.836480+0000     5     102       102         0         0         0           -           0
2021-05-17T10:18:49.836704+0000     6     124       124         0         0         0           -           0
2021-05-17T10:18:50.836926+0000     7     146       146         0         0         0           -           0
2021-05-17T10:18:51.837135+0000     8     166       166         0         0         0           -           0
2021-05-17T10:18:52.837327+0000     9     187       187         0         0         0           -           0
2021-05-17T10:18:53.837517+0000    10     209       209         0         0         0           -           0
2021-05-17T10:18:54.837725+0000    11     231       231         0         0         0           -           0
2021-05-17T10:18:55.837946+0000    12     253       253         0         0         0           -           0
2021-05-17T10:18:56.838185+0000    13     255       275        20   24.6101   24.6154     12.0574     12.1373
2021-05-17T10:18:57.838409+0000    14     255       295        40   45.7044       320     12.0948     12.1145
2021-05-17T10:18:58.838629+0000    15     255       316        61   65.0526       336     12.1138     12.1121
2021-05-17T10:18:59.838843+0000    16     255       337        82   81.9822       336     12.1472     12.1194
2021-05-17T10:19:00.839066+0000    17     255       355       100   94.0972       288     12.1229     12.1208
2021-05-17T10:19:01.839294+0000    18     255       376       121   107.532       336     12.1691     12.1302
2021-05-17T10:19:02.839504+0000    19     255       396       141   118.711       320     12.2909     12.1435
2021-05-17T10:19:03.839728+0000 min lat: 12.0436 max lat: 12.3141 avg lat: 12.1594
2021-05-17T10:19:03.839728+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T10:19:03.839728+0000    20     255       417       162   129.572       336     12.2592     12.1594
2021-05-17T10:19:04.840040+0000 Total time run:         20.137
Total writes made:      418
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     332.125
Stddev Bandwidth:       158.315
Max bandwidth (MB/sec): 336
Min bandwidth (MB/sec): 0
Average IOPS:           20
Stddev IOPS:            9.91158
Max IOPS:               21
Min IOPS:               0
Average Latency(s):     8.54978
Stddev Latency(s):      3.99505
Max latency(s):         12.3141
Min latency(s):         0.132617

[1;32mlocalhost.localdomain	[2021-05-17T03:19:05,690762424-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 254000


[1;33mlocalhost.localdomain	[2021-05-17T03:19:05,701679201-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T03:19:28,822458783-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 419 objects, 6.5 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:19:28,843896404-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:19:36,914321281-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 419 objects, 6.5 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:19:36,937931094-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:19:44,942644163-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 419 objects, 6.5 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:19:44,964599373-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:19:53,060704417-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 419 objects, 6.5 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:19:53,082433494-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:20:01,139714586-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 419 objects, 6.5 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:20:01,161193405-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:20:01,179160530-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T03:20:01,189402027-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T03:20:01,213317397-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=257486
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16MB_seq.dat.4 2

[1;32mlocalhost.localdomain	[2021-05-17T03:20:01,232320298-07:00] INFO: > Run rados bench[0m
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16MB_seq.log.4
[1;30mlocalhost.localdomain	[2021-05-17T03:20:01,273790688-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:20:01,280202361-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', 'd09695e5-984f-4639-bb07-c5b940c9568c', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FVTxas:/tmp/ceph-asok.FVTxas', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid d09695e5-984f-4639-bb07-c5b940c9568c --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.FVTxas:/tmp/ceph-asok.FVTxas -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T10:20:02.750+0000 ffffb0ebd010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:20:02.758+0000 ffffb0ebd010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:20:02.758+0000 ffffb0ebd010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T10:20:02.791910+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T10:20:02.791910+0000     0       0         0         0         0         0           -           0
2021-05-17T10:20:03.792142+0000     1      75        75         0         0         0           -           0
2021-05-17T10:20:04.792437+0000     2     135       135         0         0         0           -           0
2021-05-17T10:20:05.792710+0000     3     204       204         0         0         0           -           0
2021-05-17T10:20:06.793100+0000     4     255       268        13   51.9819        52     3.89269     3.83198
2021-05-17T10:20:07.793350+0000     5     255       308        53   169.544       640     4.32962     4.04365
2021-05-17T10:20:08.796407+0000     6     255       353        98   261.129       720     4.64249     4.26631
2021-05-17T10:20:09.796658+0000     7     255       393       138   315.206       640     4.96169     4.42436
2021-05-17T10:20:10.873375+0000     8     172       418       246   487.028      1728     4.50545     4.62625
2021-05-17T10:20:11.873696+0000 Total time run:       9.05955
Total reads made:     418
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   738.226
Average IOPS:         46
Stddev IOPS:          37.6753
Max IOPS:             108
Min IOPS:             0
Average Latency(s):   3.97247
Max latency(s):       5.13404
Min latency(s):       1.54801

[1;32mlocalhost.localdomain	[2021-05-17T03:20:12,799469836-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 257486


[1;33mlocalhost.localdomain	[2021-05-17T03:20:12,810476103-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T03:20:35,775521751-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 419 objects, 6.5 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:20:35,797850676-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:20:43,800562377-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 419 objects, 6.5 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:20:43,822562190-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:20:51,900310302-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 419 objects, 6.5 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:20:51,922109516-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:21:00,332301575-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 419 objects, 6.5 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:21:00,354062998-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:21:08,376592568-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 419 objects, 6.5 GiB
    usage:   13 GiB used, 287 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:21:08,398532131-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:21:08,416404125-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T03:21:08,423417041-07:00][RUNNING][ROUND 5/7/40] object_size=16MB[0m


[1;33mlocalhost.localdomain	[2021-05-17T03:21:08,433375500-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T03:21:08,452724901-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40497\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.339470\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 1a0bab8f-aa16-4535-b713-6e48baacefa1\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 1a0bab8f-aa16-4535-b713-6e48baacefa1\nlast_changed 2021-05-17T03:21:35.578687-0700\ncreated 2021-05-17T03:21:35.578687-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40497/0,v1:10.10.1.2:40498/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.339470 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 e6a14b53-9cf1-4e2a-b2bd-5b74993c52fe\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 e7e1f43c-d2fa-4b69-b2c7-38abb673d19b\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 17947890-e6ce-4655-a91d-5692f2dcbbf7\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n'
10.10.1.2: b'\nrestful urls: https://10.10.1.2:42497\n  w/ user/pass: admin / e1e27f59-6d4d-47c0-a65e-75e4e9ab052a\n'
10.10.1.2: b'\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 03:21:50 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40497
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.339470
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 1a0bab8f-aa16-4535-b713-6e48baacefa1
setting min_mon_release = octopus
epoch 0
fsid 1a0bab8f-aa16-4535-b713-6e48baacefa1
last_changed 2021-05-17T03:21:35.578687-0700
created 2021-05-17T03:21:35.578687-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40497/0,v1:10.10.1.2:40498/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.339470 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 e6a14b53-9cf1-4e2a-b2bd-5b74993c52fe
0
start osd.0
add osd1 e7e1f43c-d2fa-4b69-b2c7-38abb673d19b
1
start osd.1
add osd2 17947890-e6ce-4655-a91d-5692f2dcbbf7
2
start osd.2


restful urls: https://10.10.1.2:42497
  w/ user/pass: admin / e1e27f59-6d4d-47c0-a65e-75e4e9ab052a


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T03:21:09.457-0700 7fbd2c7631c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T03:21:09.457-0700 7fbd2c7631c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T03:21:09.473-0700 7efc093cd1c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T03:21:09.473-0700 7efc093cd1c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40497,v1:10.10.1.2:40498] --print /tmp/ceph_monmap.339470 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.339470 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.339470 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42497 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.81F89jlhMB 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new e6a14b53-9cf1-4e2a-b2bd-5b74993c52fe -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQC4Q6JgWQKYHxAAaTY2TrUFhhj9JS0KGFpFOw== --osd-uuid e6a14b53-9cf1-4e2a-b2bd-5b74993c52fe 
2021-05-17T03:21:44.898-0700 7fc6ea1faf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T03:21:44.898-0700 7fc6ea1faf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T03:21:44.898-0700 7fc6ea1faf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T03:21:44.950-0700 7fc6ea1faf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new e7e1f43c-d2fa-4b69-b2c7-38abb673d19b -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T03:21:45.230-0700 7fb82aa33f00 -1 Falling back to public interface
2021-05-17T03:21:45.242-0700 7fb82aa33f00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQC5Q6Jg6WmmDRAA1supscy9dE1rYwEcbC+C0Q== --osd-uuid e7e1f43c-d2fa-4b69-b2c7-38abb673d19b 
2021-05-17T03:21:45.578-0700 7fb957536f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T03:21:45.578-0700 7fb957536f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T03:21:45.578-0700 7fb957536f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T03:21:45.622-0700 7fb957536f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 17947890-e6ce-4655-a91d-5692f2dcbbf7 -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T03:21:45.938-0700 7fe2a8176f00 -1 Falling back to public interface
2021-05-17T03:21:45.954-0700 7fe2a8176f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQC5Q6JgDywNOBAAyXYayUJa0QsUOq6RE74jkw== --osd-uuid 17947890-e6ce-4655-a91d-5692f2dcbbf7 
2021-05-17T03:21:46.298-0700 7f5aa479bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T03:21:46.298-0700 7f5aa479bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T03:21:46.298-0700 7f5aa479bf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T03:21:46.398-0700 7f5aa479bf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T03:21:46.762-0700 7fa87168af00 -1 Falling back to public interface
2021-05-17T03:21:46.774-0700 7fa87168af00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T03:21:50,588824605-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T03:21:50,599323516-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T03:21:50,683187554-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:21:50,689582410-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '300f6908-d783-4b5b-87d2-a62ada0307b6', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.jlwQw1:/tmp/ceph-asok.jlwQw1', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 300f6908-d783-4b5b-87d2-a62ada0307b6 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.jlwQw1:/tmp/ceph-asok.jlwQw1 -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T03:21:53,537901754-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:21:53,544491958-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '300f6908-d783-4b5b-87d2-a62ada0307b6', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.jlwQw1:/tmp/ceph-asok.jlwQw1', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 300f6908-d783-4b5b-87d2-a62ada0307b6 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.jlwQw1:/tmp/ceph-asok.jlwQw1 -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T03:21:56,346621333-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:21:56,353073274-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '300f6908-d783-4b5b-87d2-a62ada0307b6', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.jlwQw1:/tmp/ceph-asok.jlwQw1', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 300f6908-d783-4b5b-87d2-a62ada0307b6 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.jlwQw1:/tmp/ceph-asok.jlwQw1 -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T03:21:59,143097115-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:21:59,149695226-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '300f6908-d783-4b5b-87d2-a62ada0307b6', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.jlwQw1:/tmp/ceph-asok.jlwQw1', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 300f6908-d783-4b5b-87d2-a62ada0307b6 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.jlwQw1:/tmp/ceph-asok.jlwQw1 -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T03:22:04,783809766-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:22:04,790429166-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '300f6908-d783-4b5b-87d2-a62ada0307b6', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.jlwQw1:/tmp/ceph-asok.jlwQw1', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 300f6908-d783-4b5b-87d2-a62ada0307b6 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.jlwQw1:/tmp/ceph-asok.jlwQw1 -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T03:22:08,475506316-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:22:08,483428493-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '300f6908-d783-4b5b-87d2-a62ada0307b6', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.jlwQw1:/tmp/ceph-asok.jlwQw1', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 300f6908-d783-4b5b-87d2-a62ada0307b6 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.jlwQw1:/tmp/ceph-asok.jlwQw1 -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T03:22:12,206602741-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:22:12,213157014-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '300f6908-d783-4b5b-87d2-a62ada0307b6', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.jlwQw1:/tmp/ceph-asok.jlwQw1', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 300f6908-d783-4b5b-87d2-a62ada0307b6 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.jlwQw1:/tmp/ceph-asok.jlwQw1 -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T03:22:15,146528955-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:22:15,154190597-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '300f6908-d783-4b5b-87d2-a62ada0307b6', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.jlwQw1:/tmp/ceph-asok.jlwQw1', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 300f6908-d783-4b5b-87d2-a62ada0307b6 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.jlwQw1:/tmp/ceph-asok.jlwQw1 -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T03:22:18,545096846-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:22:18,552354634-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '300f6908-d783-4b5b-87d2-a62ada0307b6', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.jlwQw1:/tmp/ceph-asok.jlwQw1', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 300f6908-d783-4b5b-87d2-a62ada0307b6 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.jlwQw1:/tmp/ceph-asok.jlwQw1 -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T03:22:21,738192457-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:22:21,744655710-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '300f6908-d783-4b5b-87d2-a62ada0307b6', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.jlwQw1:/tmp/ceph-asok.jlwQw1', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 300f6908-d783-4b5b-87d2-a62ada0307b6 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.jlwQw1:/tmp/ceph-asok.jlwQw1 -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T03:22:25,153066079-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:22:25,159485089-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '300f6908-d783-4b5b-87d2-a62ada0307b6', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.jlwQw1:/tmp/ceph-asok.jlwQw1', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 300f6908-d783-4b5b-87d2-a62ada0307b6 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.jlwQw1:/tmp/ceph-asok.jlwQw1 -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T03:22:28,128782166-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:22:28,135321827-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '300f6908-d783-4b5b-87d2-a62ada0307b6', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.jlwQw1:/tmp/ceph-asok.jlwQw1', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 300f6908-d783-4b5b-87d2-a62ada0307b6 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.jlwQw1:/tmp/ceph-asok.jlwQw1 -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  149 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.03   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   47 KiB   0 B   0 B   0 B  100 GiB     0  0.95   70      up          osd.2  
                       TOTAL  300 GiB  151 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.95/1.03  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T03:22:30,966898814-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T03:22:53,885362201-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   198 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
    Global Recovery Event (4s)
      [=====================.......] (remaining: 1s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:23:02,002600821-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=========================...] 
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:23:10,026508212-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:23:18,125324595-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:23:26,165187814-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:23:34,310597697-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:23:42,246089569-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   219 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:23:50,271778265-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   230 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:23:50,293634714-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:23:58,542892617-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:23:58,564796609-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:24:06,530734285-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:24:06,554496164-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:24:14,584207796-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:24:14,606237240-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:24:22,505252549-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   240 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:24:22,526939260-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:24:22,544797692-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T03:24:22,555039040-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T03:24:22,578859954-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=270632
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16MB_write.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-17T03:24:22,598168444-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16MB_write.log.5
[1;30mlocalhost.localdomain	[2021-05-17T03:24:22,640270548-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:24:22,646789010-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '300f6908-d783-4b5b-87d2-a62ada0307b6', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.jlwQw1:/tmp/ceph-asok.jlwQw1', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '16777216', '-O', '16777216', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 300f6908-d783-4b5b-87d2-a62ada0307b6 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.jlwQw1:/tmp/ceph-asok.jlwQw1 -- rados bench 20 write --pool bench_rados -b 16777216 -O 16777216 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T10:24:24.499+0000 ffff977cb010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:24:24.507+0000 ffff977cb010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:24:24.507+0000 ffff977cb010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T10:24:24.535162+0000 Maintaining 256 concurrent writes of 16777216 bytes to objects of size 16777216 for up to 20 seconds or 0 objects
2021-05-17T10:24:24.535280+0000 Object prefix: benchmark_data_localhost.localdomain_6
2021-05-17T10:24:28.407369+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T10:24:28.407369+0000     0       0         0         0         0         0           -           0
2021-05-17T10:24:29.407641+0000     1      21        21         0         0         0           -           0
2021-05-17T10:24:30.407861+0000     2      44        44         0         0         0           -           0
2021-05-17T10:24:31.408139+0000     3      66        66         0         0         0           -           0
2021-05-17T10:24:32.408370+0000     4      90        90         0         0         0           -           0
2021-05-17T10:24:33.408629+0000     5     113       113         0         0         0           -           0
2021-05-17T10:24:34.408928+0000     6     135       135         0         0         0           -           0
2021-05-17T10:24:35.409152+0000     7     157       157         0         0         0           -           0
2021-05-17T10:24:36.409634+0000     8     180       180         0         0         0           -           0
2021-05-17T10:24:37.409865+0000     9     203       203         0         0         0           -           0
2021-05-17T10:24:38.410150+0000    10     225       225         0         0         0           -           0
2021-05-17T10:24:39.410387+0000    11     246       246         0         0         0           -           0
2021-05-17T10:24:40.410578+0000    12     255       269        14   18.6619   18.6667     11.4173     11.4358
2021-05-17T10:24:41.410808+0000    13     255       291        36   44.2966       352     11.4119     11.4327
2021-05-17T10:24:42.411013+0000    14     255       314        59   67.4119       368     11.3392     11.4182
2021-05-17T10:24:43.411255+0000    15     255       338        83   88.5114       384     11.3283     11.3957
2021-05-17T10:24:44.411515+0000    16     255       360       105   104.974       352     11.3874     11.3922
2021-05-17T10:24:45.411832+0000    17     255       382       127   119.499       352     11.4293     11.3966
2021-05-17T10:24:46.412164+0000    18     255       404       149    132.41       352     11.4194     11.4012
2021-05-17T10:24:47.412387+0000    19     255       425       170   143.121       336     11.4487     11.4047
2021-05-17T10:24:48.412729+0000 min lat: 11.3104 max lat: 11.5634 avg lat: 11.4185
2021-05-17T10:24:48.412729+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T10:24:48.412729+0000    20     255       446       191    152.76       336     11.5634     11.4185
2021-05-17T10:24:49.413015+0000 Total time run:         20.129
Total writes made:      447
Write size:             16777216
Object size:            16777216
Bandwidth (MB/sec):     355.308
Stddev Bandwidth:       177.456
Max bandwidth (MB/sec): 384
Min bandwidth (MB/sec): 0
Average IOPS:           22
Stddev IOPS:            11.0972
Max IOPS:               24
Min IOPS:               0
Average Latency(s):     8.23818
Stddev Latency(s):      3.72138
Max latency(s):         11.5634
Min latency(s):         0.13254

[1;32mlocalhost.localdomain	[2021-05-17T03:24:50,259031539-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 270632


[1;33mlocalhost.localdomain	[2021-05-17T03:24:50,269951842-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T03:25:13,459176693-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 448 objects, 7.0 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:25:13,480626589-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:25:21,540684661-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 448 objects, 7.0 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:25:21,562194147-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:25:29,732320129-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 448 objects, 7.0 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:25:29,753709339-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:25:37,672517140-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 448 objects, 7.0 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:25:37,693513388-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:25:45,542251170-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 448 objects, 7.0 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:25:45,563619046-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:25:45,581390794-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T03:25:45,591262538-07:00] STAGE: Start benchmarking of mode seq ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T03:25:45,615805916-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=274119
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_16MB_seq.dat.5 2

[1;32mlocalhost.localdomain	[2021-05-17T03:25:45,635403026-07:00] INFO: > Run rados bench[0m
# ./bench-rados:200 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_16MB_seq.log.5
# ./bench-rados:197 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
[1;30mlocalhost.localdomain	[2021-05-17T03:25:45,677427732-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:25:45,684127515-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '300f6908-d783-4b5b-87d2-a62ada0307b6', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.jlwQw1:/tmp/ceph-asok.jlwQw1', '--', 'rados', 'bench', '99999999', 'seq', '--pool', 'bench_rados', '--concurrent-ios', '256', '--show-time'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 300f6908-d783-4b5b-87d2-a62ada0307b6 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.jlwQw1:/tmp/ceph-asok.jlwQw1 -- rados bench 99999999 seq --pool bench_rados --concurrent-ios 256 --show-time
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T10:25:47.325+0000 ffff80758010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:25:47.333+0000 ffff80758010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:25:47.333+0000 ffff80758010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T10:25:47.363796+0000   sec Cur ops   started  finished  avg MB/s  cur MB/s last lat(s)  avg lat(s)
2021-05-17T10:25:47.363796+0000     0       0         0         0         0         0           -           0
2021-05-17T10:25:48.364025+0000     1      71        71         0         0         0           -           0
2021-05-17T10:25:49.364306+0000     2     135       135         0         0         0           -           0
2021-05-17T10:25:50.364566+0000     3     204       204         0         0         0           -           0
2021-05-17T10:25:51.364855+0000     4     255       269        14   55.9825        56     3.86377      3.8262
2021-05-17T10:25:52.371958+0000     5     255       314        59   188.485       720     4.21648     3.99003
2021-05-17T10:25:53.372230+0000     6     255       360       105   279.598       736     4.46842     4.14435
2021-05-17T10:25:54.375723+0000     7     255       406       151   344.546       736     4.78696     4.30025
2021-05-17T10:25:55.550838+0000     8     218       447       229   447.526      1248     4.81665     4.52688
2021-05-17T10:25:56.589835+0000     9      35       447       412   714.485      2928     2.07915      4.0934
2021-05-17T10:25:57.590103+0000 Total time run:       9.42421
Total reads made:     447
Read size:            16777216
Object size:          16777216
Bandwidth (MB/sec):   758.896
Average IOPS:         47
Stddev IOPS:          59.1314
Max IOPS:             183
Min IOPS:             0
Average Latency(s):   3.91421
Max latency(s):       5.10897
Min latency(s):       1.52174

[1;32mlocalhost.localdomain	[2021-05-17T03:25:58,538639475-07:00] INFO: > Stop system activity collection service at localhost[0m
# ./bench-rados:208 - rados_bench() > kill -INT 274119


[1;33mlocalhost.localdomain	[2021-05-17T03:25:58,550958632-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T03:26:21,504169192-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 448 objects, 7.0 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:26:21,526237253-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:26:29,554169554-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 448 objects, 7.0 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:26:29,575582978-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:26:37,724236762-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 448 objects, 7.0 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:26:37,745980183-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:26:45,739460754-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 448 objects, 7.0 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:26:45,762673120-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:26:53,848143696-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 448 objects, 7.0 GiB
    usage:   14 GiB used, 286 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:26:53,869751748-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:26:53,887770858-07:00] INFO: > The cluster is idle now.[0m


[1;7;39;49m[2021-05-17T03:26:53,898266935-07:00][RUNNING][ROUND 1/8/40] object_size=64MB[0m


[1;33mlocalhost.localdomain	[2021-05-17T03:26:53,908491884-07:00] STAGE: Launch a Ceph cluster on host 10.10.1.2 ...[0m
# ./bench-rados:56 - launch_ceph_cluster() > /home/ubuntu/ceph-research/scripts/parallel-ssh --host ljishen@10.10.1.2 sudo bash
[1;30mlocalhost.localdomain	[2021-05-17T03:26:53,927626615-07:00] DEBUG: command from stdin[0m
# /home/ubuntu/ceph-research/scripts/parallel-ssh:30 -  > parallel-ssh --par 5 --timeout 0 --extra-args '-o GlobalKnownHostsFile=/dev/null -o LogLevel=ERROR -o PasswordAuthentication=no -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null' --inline --send-input --print --host ljishen@10.10.1.2 sudo bash
10.10.1.2: b'ip 10.10.1.2\nport 40479\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/keyring\n'
10.10.1.2: b'/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.340587\n/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 2ed46f1d-fbc4-45fc-8573-4a314a3b81f8\nsetting min_mon_release = octopus\n'
10.10.1.2: b'epoch 0\nfsid 2ed46f1d-fbc4-45fc-8573-4a314a3b81f8\nlast_changed 2021-05-17T03:27:21.100767-0700\ncreated 2021-05-17T03:27:21.100767-0700\nmin_mon_release 15 (octopus)\nelection_strategy: 1\n0: [v2:10.10.1.2:40479/0,v1:10.10.1.2:40480/0] mon.a\n/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.340587 (1 monitors)\n'
10.10.1.2: b'\n[mgr]\n\tmgr/telemetry/enable = false\n\tmgr/telemetry/nag = false\n'
10.10.1.2: b'creating /mnt/sda8/ceph/build/dev/mgr.x/keyring\n'
10.10.1.2: b'Restarting RESTful API server...\n'
10.10.1.2: b'add osd0 d20a1a9c-e30c-42e0-bfc9-6c1f000301c4\n'
10.10.1.2: b'0\n'
10.10.1.2: b'start osd.0\n'
10.10.1.2: b'add osd1 69917726-8a34-4a69-ba1a-7c5fefb2c77b\n'
10.10.1.2: b'1\n'
10.10.1.2: b'start osd.1\n'
10.10.1.2: b'add osd2 196c67da-0606-478b-a578-ca1522b27c2e\n'
10.10.1.2: b'2\n'
10.10.1.2: b'start osd.2\n'
10.10.1.2: b'\n\n'
10.10.1.2: b'restful urls: https://10.10.1.2:42479\n  w/ user/pass: admin / 1d82e102-45f2-4af3-90dd-5bd5f5d64254\n\n\n'
10.10.1.2: b'export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH\nexport LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH\nexport PATH=/mnt/sda8/ceph/build/bin:$PATH\nalias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell\n'
10.10.1.2: b'CEPH_DEV=1\n'
[1] 03:27:36 [SUCCESS] ljishen@10.10.1.2
ip 10.10.1.2
port 40479
creating /mnt/sda8/ceph/build/keyring
/mnt/sda8/ceph/build/bin/monmaptool: monmap file /tmp/ceph_monmap.340587
/mnt/sda8/ceph/build/bin/monmaptool: generated fsid 2ed46f1d-fbc4-45fc-8573-4a314a3b81f8
setting min_mon_release = octopus
epoch 0
fsid 2ed46f1d-fbc4-45fc-8573-4a314a3b81f8
last_changed 2021-05-17T03:27:21.100767-0700
created 2021-05-17T03:27:21.100767-0700
min_mon_release 15 (octopus)
election_strategy: 1
0: [v2:10.10.1.2:40479/0,v1:10.10.1.2:40480/0] mon.a
/mnt/sda8/ceph/build/bin/monmaptool: writing epoch 0 to /tmp/ceph_monmap.340587 (1 monitors)

[mgr]
	mgr/telemetry/enable = false
	mgr/telemetry/nag = false
creating /mnt/sda8/ceph/build/dev/mgr.x/keyring
Restarting RESTful API server...
add osd0 d20a1a9c-e30c-42e0-bfc9-6c1f000301c4
0
start osd.0
add osd1 69917726-8a34-4a69-ba1a-7c5fefb2c77b
1
start osd.1
add osd2 196c67da-0606-478b-a578-ca1522b27c2e
2
start osd.2


restful urls: https://10.10.1.2:42479
  w/ user/pass: admin / 1d82e102-45f2-4af3-90dd-5bd5f5d64254


export PYTHONPATH=/mnt/sda8/ceph/src/pybind:/mnt/sda8/ceph/build/lib/cython_modules/lib.3:/mnt/sda8/ceph/src/python-common:$PYTHONPATH
export LD_LIBRARY_PATH=/mnt/sda8/ceph/build/lib:$LD_LIBRARY_PATH
export PATH=/mnt/sda8/ceph/build/bin:$PATH
alias cephfs-shell=/mnt/sda8/ceph/src/tools/cephfs/cephfs-shell
CEPH_DEV=1
Stderr: 2021-05-17T03:26:54.913-0700 7fd0c97a51c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T03:26:54.913-0700 7fd0c97a51c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T03:26:54.929-0700 7f86557e51c0 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T03:26:54.929-0700 7f86557e51c0 -1 WARNING: all dangerous and experimental features are enabled.
WARNING:  ceph-osd still alive after 1 seconds
WARNING:  ceph-osd still alive after 2 seconds
WARNING:  ceph-osd still alive after 4 seconds
WARNING:  ceph-osd still alive after 7 seconds
WARNING:  ceph-osd still alive after 12 seconds
** going verbose **
rm -f core* 
/mnt/sda8/ceph/build/bin/ceph-authtool --create-keyring --gen-key --name=mon. /mnt/sda8/ceph/build/keyring --cap mon 'allow *' 
/mnt/sda8/ceph/build/bin/ceph-authtool --gen-key --name=client.admin --cap mon 'allow *' --cap osd 'allow *' --cap mds 'allow *' --cap mgr 'allow *' /mnt/sda8/ceph/build/keyring 
/mnt/sda8/ceph/build/bin/monmaptool --create --clobber --addv a [v2:10.10.1.2:40479,v1:10.10.1.2:40480] --print /tmp/ceph_monmap.340587 
rm -rf -- /mnt/sda8/ceph/build/dev/mon.a 
mkdir -p /mnt/sda8/ceph/build/dev/mon.a 
/mnt/sda8/ceph/build/bin/ceph-mon --mkfs -c /mnt/sda8/ceph/build/ceph.conf -i a --monmap=/tmp/ceph_monmap.340587 --keyring=/mnt/sda8/ceph/build/keyring 
rm -- /tmp/ceph_monmap.340587 
/mnt/sda8/ceph/build/bin/ceph-mon -i a -c /mnt/sda8/ceph/build/ceph.conf 
Populating config ...
Setting debug configs ...
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -i /mnt/sda8/ceph/build/dev/mgr.x/keyring auth add mgr.x mon 'allow profile mgr' mds 'allow *' osd 'allow *' 
added key for mgr.x
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/prometheus/x/server_port 9283 --force 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring config set mgr mgr/restful/x/server_port 42479 --force 
Starting mgr.x
/mnt/sda8/ceph/build/bin/ceph-mgr -i x -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
waiting for mgr restful module to start
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring -h 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-self-signed-cert 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring restful create-key admin -o /tmp/tmp.dK3xG97ube 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new d20a1a9c-e30c-42e0-bfc9-6c1f000301c4 -i /mnt/sda8/ceph/build/dev/osd0/new.json 
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQASRaJgn1n4ChAAGit4boOOZMbsYIUQJ3DPOA== --osd-uuid d20a1a9c-e30c-42e0-bfc9-6c1f000301c4 
2021-05-17T03:27:30.541-0700 7fc4c49e4f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T03:27:30.541-0700 7fc4c49e4f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T03:27:30.541-0700 7fc4c49e4f00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd0/keyring: (2) No such file or directory
2021-05-17T03:27:30.585-0700 7fc4c49e4f00 -1 memstore(/mnt/sda8/ceph/build/dev/osd0) /mnt/sda8/ceph/build/dev/osd0
/mnt/sda8/ceph/build/bin/ceph-osd -i 0 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 69917726-8a34-4a69-ba1a-7c5fefb2c77b -i /mnt/sda8/ceph/build/dev/osd1/new.json 
2021-05-17T03:27:30.897-0700 7f8a27e6ff00 -1 Falling back to public interface
2021-05-17T03:27:30.909-0700 7f8a27e6ff00 -1 osd.0 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQASRaJgG0WVNRAAazmj1XlWI3HBCJ/SDwhjzg== --osd-uuid 69917726-8a34-4a69-ba1a-7c5fefb2c77b 
2021-05-17T03:27:31.229-0700 7fb4f2a6ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T03:27:31.229-0700 7fb4f2a6ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T03:27:31.229-0700 7fb4f2a6ff00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd1/keyring: (2) No such file or directory
2021-05-17T03:27:31.273-0700 7fb4f2a6ff00 -1 memstore(/mnt/sda8/ceph/build/dev/osd1) /mnt/sda8/ceph/build/dev/osd1
/mnt/sda8/ceph/build/bin/ceph-osd -i 1 -c /mnt/sda8/ceph/build/ceph.conf 
/mnt/sda8/ceph/build/bin/ceph -c /mnt/sda8/ceph/build/ceph.conf -k /mnt/sda8/ceph/build/keyring osd new 196c67da-0606-478b-a578-ca1522b27c2e -i /mnt/sda8/ceph/build/dev/osd2/new.json 
2021-05-17T03:27:31.585-0700 7f80a1317f00 -1 Falling back to public interface
2021-05-17T03:27:31.597-0700 7f80a1317f00 -1 osd.1 0 log_to_monitors {default=true}
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf --mkfs --key AQATRaJgJo7HIhAAG6G/M4i0OTrGFv4+yguzAA== --osd-uuid 196c67da-0606-478b-a578-ca1522b27c2e 
2021-05-17T03:27:31.945-0700 7f4ee951cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T03:27:31.945-0700 7f4ee951cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T03:27:31.945-0700 7f4ee951cf00 -1 auth: unable to find a keyring on /mnt/sda8/ceph/build/dev/osd2/keyring: (2) No such file or directory
2021-05-17T03:27:32.017-0700 7f4ee951cf00 -1 memstore(/mnt/sda8/ceph/build/dev/osd2) /mnt/sda8/ceph/build/dev/osd2
/mnt/sda8/ceph/build/bin/ceph-osd -i 2 -c /mnt/sda8/ceph/build/ceph.conf 
2021-05-17T03:27:32.313-0700 7fd9fb103f00 -1 Falling back to public interface
2021-05-17T03:27:32.329-0700 7fd9fb103f00 -1 osd.2 0 log_to_monitors {default=true}
OSDs started
vstart cluster complete. Use stop.sh to stop. See out/* (e.g. 'tail -f out/????') for debug output.


[1;33mlocalhost.localdomain	[2021-05-17T03:27:36,345564989-07:00] STAGE: Update local ceph.conf as a client...[0m


[1;33mlocalhost.localdomain	[2021-05-17T03:27:36,356209380-07:00] STAGE: Configure Ceph pool...[0m
# ./bench-rados:95 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_max_backfills 32
[1;30mlocalhost.localdomain	[2021-05-17T03:27:36,442274793-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:27:36,448659552-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '344e95da-22bf-4c33-b0a7-14fc97ab67d1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.KG2eX7:/tmp/ceph-asok.KG2eX7', '--', 'ceph', 'config', 'set', 'osd', 'osd_max_backfills', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 344e95da-22bf-4c33-b0a7-14fc97ab67d1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.KG2eX7:/tmp/ceph-asok.KG2eX7 -- ceph config set osd osd_max_backfills 32
# ./bench-rados:96 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_active 32
[1;30mlocalhost.localdomain	[2021-05-17T03:27:39,336359718-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:27:39,343854817-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '344e95da-22bf-4c33-b0a7-14fc97ab67d1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.KG2eX7:/tmp/ceph-asok.KG2eX7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_active', '32'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 344e95da-22bf-4c33-b0a7-14fc97ab67d1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.KG2eX7:/tmp/ceph-asok.KG2eX7 -- ceph config set osd osd_recovery_max_active 32
# ./bench-rados:97 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_max_single_start 8
[1;30mlocalhost.localdomain	[2021-05-17T03:27:42,158495320-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:27:42,164784633-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '344e95da-22bf-4c33-b0a7-14fc97ab67d1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.KG2eX7:/tmp/ceph-asok.KG2eX7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_max_single_start', '8'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 344e95da-22bf-4c33-b0a7-14fc97ab67d1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.KG2eX7:/tmp/ceph-asok.KG2eX7 -- ceph config set osd osd_recovery_max_single_start 8
# ./bench-rados:98 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config set osd osd_recovery_op_priority 63
[1;30mlocalhost.localdomain	[2021-05-17T03:27:45,112585881-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:27:45,119178906-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '344e95da-22bf-4c33-b0a7-14fc97ab67d1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.KG2eX7:/tmp/ceph-asok.KG2eX7', '--', 'ceph', 'config', 'set', 'osd', 'osd_recovery_op_priority', '63'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 344e95da-22bf-4c33-b0a7-14fc97ab67d1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.KG2eX7:/tmp/ceph-asok.KG2eX7 -- ceph config set osd osd_recovery_op_priority 63
# ./bench-rados:101 - configure_ceph_pool() > grep 'osd_max_backfills\|osd_recovery'
# ./bench-rados:101 - configure_ceph_pool() > column -t -s ' '
# ./bench-rados:100 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph config show-with-defaults osd.0
osd_max_backfills                        1000       override  mon[32]
osd_recovery_cost                        20971520   default
osd_recovery_delay_start                 0.000000   default
osd_recovery_max_active                  1000       override  mon[32]
osd_recovery_max_active_hdd              1000       override
osd_recovery_max_active_ssd              1000       override
osd_recovery_max_chunk                   8388608    default
osd_recovery_max_omap_entries_per_chunk  8096       default
osd_recovery_max_single_start            8          mon
osd_recovery_op_priority                 63         mon
osd_recovery_op_warn_multiple            16         default
osd_recovery_priority                    5          default
osd_recovery_retry_interval              30.000000  default
osd_recovery_sleep                       0.000000   override
osd_recovery_sleep_hdd                   0.000000   override
osd_recovery_sleep_hybrid                0.000000   override
osd_recovery_sleep_ssd                   0.000000   override
# ./bench-rados:103 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
[1;30mlocalhost.localdomain	[2021-05-17T03:27:50,911999075-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:27:50,918907574-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '344e95da-22bf-4c33-b0a7-14fc97ab67d1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.KG2eX7:/tmp/ceph-asok.KG2eX7', '--', 'ceph', 'osd', 'pool', 'create', 'bench_rados', '128', '128', 'replicated', '--size', '2', '--pg-num-min', '128'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 344e95da-22bf-4c33-b0a7-14fc97ab67d1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.KG2eX7:/tmp/ceph-asok.KG2eX7 -- ceph osd pool create bench_rados 128 128 replicated --size 2 --pg-num-min 128
# ./bench-rados:105 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados min_size 1
[1;30mlocalhost.localdomain	[2021-05-17T03:27:54,078942214-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:27:54,085426951-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '344e95da-22bf-4c33-b0a7-14fc97ab67d1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.KG2eX7:/tmp/ceph-asok.KG2eX7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'min_size', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 344e95da-22bf-4c33-b0a7-14fc97ab67d1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.KG2eX7:/tmp/ceph-asok.KG2eX7 -- ceph osd pool set bench_rados min_size 1
# ./bench-rados:106 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados pg_autoscale_mode off
[1;30mlocalhost.localdomain	[2021-05-17T03:27:57,513183465-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:27:57,519714830-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '344e95da-22bf-4c33-b0a7-14fc97ab67d1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.KG2eX7:/tmp/ceph-asok.KG2eX7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'pg_autoscale_mode', 'off'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 344e95da-22bf-4c33-b0a7-14fc97ab67d1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.KG2eX7:/tmp/ceph-asok.KG2eX7 -- ceph osd pool set bench_rados pg_autoscale_mode off
# ./bench-rados:107 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool application enable bench_rados benchmark
[1;30mlocalhost.localdomain	[2021-05-17T03:28:00,807730613-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:28:00,814278045-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '344e95da-22bf-4c33-b0a7-14fc97ab67d1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.KG2eX7:/tmp/ceph-asok.KG2eX7', '--', 'ceph', 'osd', 'pool', 'application', 'enable', 'bench_rados', 'benchmark'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 344e95da-22bf-4c33-b0a7-14fc97ab67d1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.KG2eX7:/tmp/ceph-asok.KG2eX7 -- ceph osd pool application enable bench_rados benchmark
# ./bench-rados:108 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados noscrub 1
[1;30mlocalhost.localdomain	[2021-05-17T03:28:04,382312671-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:28:04,388711623-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '344e95da-22bf-4c33-b0a7-14fc97ab67d1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.KG2eX7:/tmp/ceph-asok.KG2eX7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'noscrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 344e95da-22bf-4c33-b0a7-14fc97ab67d1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.KG2eX7:/tmp/ceph-asok.KG2eX7 -- ceph osd pool set bench_rados noscrub 1
# ./bench-rados:109 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool set bench_rados nodeep-scrub 1
[1;30mlocalhost.localdomain	[2021-05-17T03:28:07,667269914-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:28:07,673693995-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '344e95da-22bf-4c33-b0a7-14fc97ab67d1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.KG2eX7:/tmp/ceph-asok.KG2eX7', '--', 'ceph', 'osd', 'pool', 'set', 'bench_rados', 'nodeep-scrub', '1'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 344e95da-22bf-4c33-b0a7-14fc97ab67d1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.KG2eX7:/tmp/ceph-asok.KG2eX7 -- ceph osd pool set bench_rados nodeep-scrub 1
# ./bench-rados:110 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd pool ls detail
[1;30mlocalhost.localdomain	[2021-05-17T03:28:10,897275044-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:28:10,903882277-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '344e95da-22bf-4c33-b0a7-14fc97ab67d1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.KG2eX7:/tmp/ceph-asok.KG2eX7', '--', 'ceph', 'osd', 'pool', 'ls', 'detail'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 344e95da-22bf-4c33-b0a7-14fc97ab67d1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.KG2eX7:/tmp/ceph-asok.KG2eX7 -- ceph osd pool ls detail
pool 1 'device_health_metrics' replicated size 3 min_size 1 crush_rule 0 object_hash rjenkins pg_num 1 pgp_num 1 autoscale_mode on last_change 10 flags hashpspool stripe_width 0 pg_num_min 1 application mgr_devicehealth
pool 2 'bench_rados' replicated size 2 min_size 1 crush_rule 0 object_hash rjenkins pg_num 128 pgp_num 128 autoscale_mode off last_change 18 flags hashpspool,noscrub,nodeep-scrub stripe_width 0 pg_num_min 128 application benchmark

# ./bench-rados:111 - configure_ceph_pool() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- ceph osd df tree
[1;30mlocalhost.localdomain	[2021-05-17T03:28:13,749982047-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:28:13,756605062-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '344e95da-22bf-4c33-b0a7-14fc97ab67d1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.KG2eX7:/tmp/ceph-asok.KG2eX7', '--', 'ceph', 'osd', 'df', 'tree'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 344e95da-22bf-4c33-b0a7-14fc97ab67d1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.KG2eX7:/tmp/ceph-asok.KG2eX7 -- ceph osd df tree
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA  OMAP  META  AVAIL    %USE  VAR   PGS  STATUS  TYPE NAME      
-1         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -          root default   
-3         0.29306         -  300 GiB  150 KiB   0 B   0 B   0 B  300 GiB     0  1.00    -              host node-0
 0    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   92      up          osd.0  
 1    hdd  0.09769   1.00000  100 GiB   51 KiB   0 B   0 B   0 B  100 GiB     0  1.02   97      up          osd.1  
 2    hdd  0.09769   1.00000  100 GiB   48 KiB   0 B   0 B   0 B  100 GiB     0  0.96   70      up          osd.2  
                       TOTAL  300 GiB  152 KiB   0 B   0 B   0 B  300 GiB     0                                    
MIN/MAX VAR: 0.96/1.02  STDDEV: 0


[1;33mlocalhost.localdomain	[2021-05-17T03:28:16,446537902-07:00] STAGE: Wait for the cluster to become idle...[0m

[1;32mlocalhost.localdomain	[2021-05-17T03:28:39,655350612-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   188 KiB used, 300 GiB / 300 GiB avail
    pgs:     8.333% pgs not active
             176 active+clean
             16  peering
 
  progress:
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:28:47,688783125-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (5s)
      [=======.....................] (remaining: 13s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:28:56,223304790-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (10s)
      [=======.....................] (remaining: 27s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:29:04,563885422-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (20s)
      [========....................] (remaining: 48s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:29:12,702814531-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (30s)
      [========....................] (remaining: 69s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:29:20,586881784-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (35s)
      [========....................] (remaining: 75s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:29:28,586157949-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   210 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (45s)
      [========....................] (remaining: 96s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:29:36,496191059-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 
  progress:
    Global Recovery Event (50s)
      [==============..............] (remaining: 44s)
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:29:44,509902383-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:29:44,531762463-07:00] INFO: >> Counting down idle state [1/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:29:52,599367595-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:29:52,620835073-07:00] INFO: >> Counting down idle state [2/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:30:00,622915642-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:30:00,644360782-07:00] INFO: >> Counting down idle state [3/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:30:08,782379165-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:30:08,803739376-07:00] INFO: >> Counting down idle state [4/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:30:16,919055769-07:00] INFO: >>   data:
    pools:   2 pools, 192 pgs
    objects: 0 objects, 0 B
    usage:   231 KiB used, 300 GiB / 300 GiB avail
    pgs:     192 active+clean
 [0m

[1;32mlocalhost.localdomain	[2021-05-17T03:30:16,940396791-07:00] INFO: >> Counting down idle state [5/5][0m

[1;32mlocalhost.localdomain	[2021-05-17T03:30:16,958212227-07:00] INFO: > The cluster is idle now.[0m


[1;33mlocalhost.localdomain	[2021-05-17T03:30:16,968443103-07:00] STAGE: Start benchmarking of mode write ...[0m

[1;32mlocalhost.localdomain	[2021-05-17T03:30:16,992556793-07:00] INFO: > Start system activity collection service at localhost[0m
# ./bench-rados:176 - rados_bench() > sar_pid=287735
# ./bench-rados:176 - rados_bench() > S_TIME_FORMAT=ISO
# ./bench-rados:176 - rados_bench() > sar -A -o /home/ubuntu/ceph-research/scripts/benchmarks/output/sys_activity_64MB_write.dat.1 2

[1;32mlocalhost.localdomain	[2021-05-17T03:30:17,012102594-07:00] INFO: > Run rados bench[0m
# ./bench-rados:183 - rados_bench() > taskset --cpu-list 1-7 /home/ubuntu/ceph-research/scripts/ceph-shell -- rados bench 20 write --pool bench_rados -b 67108864 -O 67108864 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
# ./bench-rados:192 - rados_bench() > tee /home/ubuntu/ceph-research/scripts/benchmarks/output/rados_bench_64MB_write.log.1
[1;30mlocalhost.localdomain	[2021-05-17T03:30:17,054371003-07:00] DEBUG: HOST_PARAMS: [][0m
[1;30mlocalhost.localdomain	[2021-05-17T03:30:17,060870637-07:00] DEBUG: CEPHADM_PARAMS: ['shell', '--config', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf', '--keyring', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring', '--fsid', '344e95da-22bf-4c33-b0a7-14fc97ab67d1', '--mount', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out', '/home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.KG2eX7:/tmp/ceph-asok.KG2eX7', '--', 'rados', 'bench', '20', 'write', '--pool', 'bench_rados', '-b', '67108864', '-O', '67108864', '--concurrent-ios', '256', '--show-time', '--write-object', '--write-omap', '--write-xattr', '--no-cleanup'][0m
# ./parallel-cephadm:68 -  > sudo /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/usr/sbin/cephadm shell --config /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.conf --keyring /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/etc/ceph/ceph.client.admin.keyring --fsid 344e95da-22bf-4c33-b0a7-14fc97ab67d1 --mount /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/lib:/mnt/sda8/ceph/build/lib /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/mnt/sda8/ceph/build/out:/mnt/sda8/ceph/build/out /home/ubuntu/ceph-research/scripts/benchmarks/deployment_data_root/tmp/ceph-asok.KG2eX7:/tmp/ceph-asok.KG2eX7 -- rados bench 20 write --pool bench_rados -b 67108864 -O 67108864 --concurrent-ios 256 --show-time --write-object --write-omap --write-xattr --no-cleanup
Using recent ceph image ceph/ceph@sha256:16d37584df43bd6545d16e5aeba527de7d6ac3da3ca7b882384839d2d86acc7d
2021-05-17T10:30:18.623+0000 ffffb77b4010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:30:18.631+0000 ffffb77b4010 -1 WARNING: all dangerous and experimental features are enabled.
2021-05-17T10:30:18.631+0000 ffffb77b4010 -1 WARNING: all dangerous and experimental features are enabled.
hints = 1
2021-05-17T10:30:18.691476+0000 Maintaining 256 concurrent writes of 67108864 bytes to objects of size 67108864 for up to 20 seconds or 0 objects
2021-05-17T10:30:18.691595+0000 Object prefix: benchmark_data_localhost.localdomain_6
